0:00:00.000,0:00:02.360
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.360,0:00:05.040
各位同學大家好，我們來上課吧

0:00:09.020,0:00:11.140
今天的規劃是這樣子

0:00:11.140,0:00:14.660
就是我們等一下，會先公告 final

0:00:14.660,0:00:19.420
那 final 有三個選擇，所以是需要花時間跟大家講一下的

0:00:19.420,0:00:21.420
所以，今天的規劃是

0:00:21.420,0:00:26.760
我們等一下上課，大概上到 1: 20 以後下課

0:00:26.760,0:00:32.340
不對，腦殘了，到 11:20 吧

0:00:32.340,0:00:36.120
不是 11:20，大概是 10:20 的時候下課

0:00:36.120,0:00:38.480
然後，等一下

0:00:38.480,0:00:41.160
剩下的時間就讓助教來把

0:00:41.160,0:00:44.100
三個 final 都跟大家講完

0:00:45.780,0:00:50.240
那今天我們要講的是 Word Embedding

0:00:50.240,0:00:54.820
我們之前已經講了 Dimension Reduction

0:00:54.820,0:00:59.580
那 Word Embedding 其實是 Dimension Reduction 一個

0:00:59.580,0:01:02.440
非常好、非常廣為人知的應用

0:01:05.920,0:01:08.620
如果我們今天要你用一個 vector

0:01:08.620,0:01:10.960
來表示一個 word，你會怎麼做呢？

0:01:10.960,0:01:12.620
最 typical 的作法

0:01:12.620,0:01:14.920
叫做 1-of-N encoding

0:01:14.920,0:01:18.200
每一個 word，我們用一個 vector來表示

0:01:18.200,0:01:22.160
這個 vector 的 dimension，就是這個世界上

0:01:22.160,0:01:24.280
可能有的 word 數目

0:01:24.280,0:01:26.440
假設這個世界上可能有十萬個 word

0:01:26.440,0:01:28.640
那 1-of-N encoding 的 dimension

0:01:28.640,0:01:30.500
就是十萬維

0:01:30.500,0:01:34.680
那每一個 word，對應到其中一維

0:01:34.680,0:01:38.420
所以，apple 它就是第一維是 1，其他都是 0

0:01:38.420,0:01:42.480
bag 就是第二維是 1，cat 就是第三維是 1，以此類推，等等

0:01:42.480,0:01:45.960
如果你用這種方式來描述一個 word

0:01:45.960,0:01:48.980
你的這個 vector 一點都不 informative

0:01:48.980,0:01:53.020
你每一個 word，它的 vector 都是不一樣的

0:01:53.020,0:01:56.020
所以從這個 vector 裡面，你沒有辦法得到任何的資訊

0:01:56.020,0:01:57.560
你沒有辦法知道說

0:01:57.560,0:01:59.180
比如說，bag 跟 cat

0:01:59.180,0:02:01.600
bag 跟 cat，沒什麼關係

0:02:01.600,0:02:03.100
比如說，cat 跟 dog

0:02:03.100,0:02:05.160
它們都是動物這件事

0:02:05.160,0:02:07.100
你沒有辦法知道

0:02:07.100,0:02:08.840
那怎麼辦呢？

0:02:09.900,0:02:13.280
有一個方法就叫做建 Word Class

0:02:13.280,0:02:16.480
也就是你把不同的 word

0:02:16.480,0:02:17.800
有同樣性質的 word

0:02:17.860,0:02:20.380
把它們 cluster 成一群一群的

0:02:20.540,0:02:25.120
然後就用那一個 word 所屬的 class 來表示這個 word

0:02:25.120,0:02:29.560
這個就是我們之前，在做 Dimension Reduction 的時候

0:02:29.560,0:02:31.600
講的 clustering 的概念

0:02:31.600,0:02:35.380
比如說，dog, cat 跟 bird，它們都是class 1

0:02:35.380,0:02:38.500
ran, jumped, walk 是 class 2

0:02:38.500,0:02:40.640
flower, tree, apple 是class 3，等等

0:02:40.640,0:02:43.940
但是，光用 class 是不夠的

0:02:43.940,0:02:47.020
我們之前有講過說，光做 clustering 是不夠的

0:02:47.020,0:02:50.440
因為，如果光做 clustering 的話呢

0:02:50.440,0:02:51.794
我們找了一些 information

0:02:51.800,0:02:54.140
比如說

0:02:54.140,0:02:55.660
這個是屬於動物的 class

0:02:55.660,0:02:57.260
這是屬於植物的 class

0:02:57.260,0:02:58.740
它們都是屬於生物

0:02:58.740,0:03:00.980
但是在 class 裡面，沒有辦法呈現這一件事情

0:03:00.980,0:03:04.320
或者是說，class 1 是動物

0:03:04.320,0:03:07.680
而 class 2 代表是，動物可以做的行為

0:03:07.680,0:03:10.140
但是， class 3 是植物

0:03:10.140,0:03:12.780
class 2 裡面的行為是 class 3 裡面沒有辦法做的

0:03:12.780,0:03:15.360
所以，class 2 跟 class 1 是有一些關聯的

0:03:15.360,0:03:17.700
沒有辦法用 Word Class 呈現出來

0:03:17.700,0:03:19.400
所以怎麼辦呢？

0:03:19.400,0:03:22.680
我們需要的，是 Word Embedding

0:03:22.680,0:03:24.260
Word Embedding 是這樣

0:03:24.260,0:03:25.840
把每一個 word

0:03:25.840,0:03:30.040
都 project 到一個 high dimensional 的 space 上面

0:03:30.040,0:03:32.180
把每一個 word 都 project 到一個

0:03:32.180,0:03:33.800
high dimensional space 上面

0:03:33.800,0:03:35.880
雖然說，這邊這個 space 是 high dimensional

0:03:35.880,0:03:39.720
但是它其實遠比 1-of-N encoding 的 dimension 還要低

0:03:39.720,0:03:42.940
1-of-N encoding，你這個 vector 通常是

0:03:42.960,0:03:45.420
比如說，英文有 10 萬詞彙，這個就是 10 萬維

0:03:45.420,0:03:47.320
但是，如果是用 Word Embedding 的話呢

0:03:47.320,0:03:50.840
通常比如說 50 維、100維這個樣子的 dimension

0:03:50.840,0:03:54.320
這個是一個從 1-of-N encoding 到 Word Embedding

0:03:54.320,0:03:58.260
這是 Dimension Reduction 的 process

0:03:58.260,0:04:00.400
那我們希望在這個 Word Embedding 的

0:04:00.400,0:04:02.160
這個圖上

0:04:02.160,0:04:04.000
我們可以看到的結果是

0:04:04.000,0:04:07.400
同樣，類似 semantic，類似語意的詞彙

0:04:07.400,0:04:09.500
它們能夠在這個圖上

0:04:09.500,0:04:10.660
是比較接近的

0:04:10.660,0:04:14.280
而且在這個 high dimensional space 裡面呢

0:04:14.280,0:04:15.920
在這個 Word Embedding 的 space 裡面

0:04:15.920,0:04:19.360
每一個 dimension，可能都有它特別的含意

0:04:19.360,0:04:23.600
比如說，假設我們現在做完 Word Embedding 以後

0:04:23.600,0:04:25.260
每一個 word 的這個

0:04:25.260,0:04:27.980
Word Embedding 的 feature vector長的是這個樣子

0:04:27.980,0:04:29.880
所以，可能就可以知道說

0:04:29.880,0:04:32.620
這個 dimension 代表了

0:04:32.620,0:04:37.620
生物和其他的東西之間的差別

0:04:37.620,0:04:41.660
這個 dimension 可能就代表了，比如說

0:04:41.660,0:04:44.460
這是會動的，跟動作有關的東西

0:04:44.460,0:04:46.460
動物是會動的，還有這個是動作

0:04:46.460,0:04:47.880
跟動作有關的東西

0:04:47.880,0:04:51.560
和不會動的，是靜止的東西的差別，等等

0:04:51.560,0:04:55.160
那怎麼做 Word Embedding

0:04:55.160,0:04:58.740
Word Embedding 是一個 unsupervised approach

0:04:58.740,0:05:00.840
也就是，我們怎麼讓 machine

0:05:00.840,0:05:02.760
知道每一個詞彙的含義，是什麼呢？

0:05:02.760,0:05:06.660
你只要透過讓 machine 閱讀大量的文章

0:05:06.660,0:05:08.880
你只要讓 machine 透過閱讀大量的文章

0:05:08.880,0:05:11.080
它就可以知道，每一個詞彙

0:05:11.080,0:05:15.540
它的 embedding 的 feature vector 應該長什麼樣子

0:05:15.540,0:05:18.480
這是一個 unsupervised 的 problem

0:05:18.480,0:05:20.140
因為我們要做的事情就是

0:05:20.140,0:05:22.200
learn 一個 Neural Network

0:05:22.200,0:05:23.720
找一個  function

0:05:23.720,0:05:26.060
那你的 input 是一個詞彙

0:05:26.060,0:05:30.120
output 就是那一個詞彙所對應的 Word Embedding

0:05:30.120,0:05:31.500
它所對應的 Word Embedding

0:05:31.500,0:05:33.700
的那一個 vector

0:05:33.700,0:05:35.860
而我們手上有的 train data

0:05:35.860,0:05:37.980
是一大堆的文字

0:05:37.980,0:05:41.360
所以我們只有Input，我們只有 input

0:05:41.360,0:05:44.800
但是我們沒有 output ，我們沒有 output

0:05:44.800,0:05:45.776
我們不知道

0:05:45.776,0:05:49.680
每一個 Word Embedding 應該長什麼樣子

0:05:49.680,0:05:51.960
所以，對於我們要找的  function

0:05:51.960,0:05:53.940
我們只有單項

0:05:53.940,0:05:56.280
我們只知道輸入，不知道輸出

0:05:56.280,0:06:01.280
所以，這是一個 unsupervised learning 的問題

0:06:01.280,0:06:04.480
那這個問題要怎麼解呢？

0:06:04.720,0:06:06.420
我們之前有講過

0:06:06.420,0:06:09.160
一個 deep learning base 的 Dimension Reduction 的方法

0:06:09.160,0:06:11.140
叫做 Auto-encoder

0:06:11.280,0:06:14.620
也就是 learn 一個 network，讓它輸入等於輸出

0:06:14.620,0:06:16.320
這邊某一個 hidden layer 拿出來

0:06:16.320,0:06:18.200
就是 Dimension Reduction 的結果

0:06:18.200,0:06:19.400
在這個地方

0:06:19.400,0:06:21.880
你覺得你可以用 Auto-encoder 嗎 ?

0:06:21.880,0:06:24.540
給大家一秒鐘時間的想一想

0:06:24.540,0:06:28.800
你覺得這邊可以用 Auto-encoder 的同學，舉手一下

0:06:28.800,0:06:34.060
你覺得不能用 auto-encoder 的同學請舉手一下，謝謝

0:06:34.060,0:06:36.040
把手放下，大多數的同學都覺得

0:06:36.040,0:06:38.100
不能用 Auto-encoder 來處理這一問題

0:06:38.100,0:06:42.000
沒錯，這個問題你沒辦法用 Auto-encoder 來解

0:06:42.000,0:06:43.900
你沒辦法用 Auto-encoder 來解

0:06:43.900,0:06:46.300
這件事情有點難解釋

0:06:46.300,0:06:48.160
或許讓大家自己回去想一想

0:06:48.160,0:06:49.240
你可以想想看

0:06:49.240,0:06:51.960
如果你是用 1-of-N encoding 當作它的 input

0:06:51.960,0:06:54.320
如果你用 1-of-N encoding 當作它的 input

0:06:54.320,0:06:58.180
對 1-of-N encoding 來說，每一個詞彙都是 independent

0:06:58.180,0:07:00.520
你把這樣子的 vector 做 Auto-encoder

0:07:00.520,0:07:03.200
你其實，沒有辦法 learn 出

0:07:03.200,0:07:05.860
任何 informative 的 information

0:07:05.860,0:07:09.380
所以，在 Word Embedding 這個 task 裡面

0:07:09.380,0:07:13.340
用 Auto-encoder 是沒有辦法的

0:07:13.340,0:07:15.040
如果你這一邊 input 是 1-of-N encoding

0:07:15.180,0:07:17.040
用 Auto-encoder 是沒有辦法的

0:07:17.040,0:07:19.420
除非你說，你用這個

0:07:19.420,0:07:22.220
你用 character，比如說你用

0:07:22.220,0:07:24.200
character 的 n-gram 來描述一個 word

0:07:24.200,0:07:27.220
或許，它可以抓到一些字首、字根的含義

0:07:27.220,0:07:30.300
不過基本上，現在大家並不是這麼做的

0:07:30.300,0:07:33.580
那怎麼找這個 Word Embedding 呢

0:07:33.580,0:07:36.080
這邊的作法是這樣子

0:07:36.080,0:07:37.680
它基本的精神就是

0:07:37.680,0:07:40.820
你要如何了解一個詞彙的含義呢

0:07:40.820,0:07:43.500
你要看這個詞彙的 contest

0:07:43.500,0:07:45.080
每一個詞彙的含義

0:07:45.080,0:07:50.560
可以根據它的上下文來得到

0:07:50.560,0:07:52.260
舉例來說

0:07:52.260,0:07:54.600
假設機器讀了一段文字是說

0:07:54.600,0:07:56.740
馬英九520宣誓就職

0:07:56.740,0:07:59.140
它又讀了另外一段新聞說

0:07:59.140,0:08:02.040
蔡英文520宣誓就職

0:08:02.040,0:08:05.360
對機器來說，雖然它不知道馬英九指的是什麼

0:08:05.360,0:08:07.440
他不知道蔡英文指的是什麼

0:08:07.440,0:08:10.460
但是馬英九後面有接520宣誓就職

0:08:10.460,0:08:12.700
蔡英文的後面也有接520宣誓就職

0:08:12.700,0:08:16.600
對機器來說，只要它讀了大量的文章

0:08:16.600,0:08:21.640
發現說，馬英九跟蔡英文後面都有類似的 contest

0:08:21.640,0:08:23.420
它前後都有類似的文字

0:08:23.420,0:08:25.280
機器就可以推論說

0:08:25.280,0:08:29.240
蔡英文跟馬英九代表了某種有關係的 object

0:08:29.240,0:08:32.620
他們是某些有關係的物件

0:08:32.620,0:08:34.340
所以它可能也不知道他們是人

0:08:34.340,0:08:36.840
但它知道說，蔡英文跟馬英九這兩個詞彙

0:08:36.840,0:08:40.220
代表了，可能有同樣地位的東西

0:08:40.220,0:08:44.180
那怎麼來體現這一件事呢

0:08:44.180,0:08:49.420
怎麼用這個精神來找出 Word Embedding 的 vector 呢

0:08:49.420,0:08:52.700
有兩個不同體系的作法

0:08:52.700,0:08:56.760
一個做法叫做 Count based 的方法

0:08:56.760,0:08:58.920
Count based 的方法是這樣

0:08:58.920,0:09:05.600
如果我們現在有兩個詞彙，wi, wj

0:09:05.600,0:09:09.980
它們常常在同一個文章中出現

0:09:09.980,0:09:12.060
它們常常一起 co-occur

0:09:12.060,0:09:14.580
那它們的 word vector

0:09:14.580,0:09:17.200
我們用 V(wi) 來代表

0:09:17.200,0:09:18.980
wi 的 word vector

0:09:18.980,0:09:22.220
我們用 V(wj) 來代表，wj 的 word vector

0:09:28.500,0:09:33.920
如果 wi 跟 wj，它們常常一起出現的話

0:09:33.920,0:09:39.820
V(wi) 跟 V(wj) 它們就會比較接近

0:09:39.820,0:09:42.080
那這種方法

0:09:42.080,0:09:43.940
有一個很代表性的例子

0:09:43.940,0:09:46.140
叫做 Glove vector

0:09:46.140,0:09:49.160
我把它的 reference 附在這邊

0:09:49.160,0:09:50.580
給大家參考

0:09:51.820,0:09:54.960
那這個方法的原則是這樣子

0:09:54.960,0:09:56.260
假設我們知道

0:09:56.260,0:09:59.480
wi 的 word vector 是 V(wi)

0:09:59.480,0:10:02.920
wj 的 word vector 是 V(wj)

0:10:02.920,0:10:06.000
那我們可以計算它的 inner product

0:10:06.000,0:10:12.300
假設 Nij 是 wi 跟 wj

0:10:12.300,0:10:16.940
它們 co-occur 在同樣的 document 裡面的次數

0:10:16.940,0:10:20.860
那我們就希望為 wi 找一組 vector

0:10:20.860,0:10:23.520
為 wj 找一個組 vector

0:10:23.520,0:10:26.560
然後，希望這兩個

0:10:26.560,0:10:28.840
這兩件事情

0:10:28.840,0:10:31.900
越接近越好

0:10:31.900,0:10:33.620
你會發現說，這個概念

0:10:33.620,0:10:36.960
跟我們之前講的 LSA 是

0:10:36.960,0:10:40.340
跟我們講的 matrix factorization 的概念

0:10:40.340,0:10:41.900
其實是一樣的

0:10:41.900,0:10:43.480
其實是一樣的

0:10:43.480,0:10:45.100
另外一個方法

0:10:45.100,0:10:48.100
叫做 Prediction based 的方法

0:10:48.100,0:10:52.800
我發現我這一邊拼錯了

0:10:52.800,0:10:54.920
這應該是 Prediction based 的方法

0:10:54.920,0:10:58.840
那我不知道說

0:10:58.840,0:11:00.700
就我所知，好像沒有人

0:11:00.700,0:11:02.720
很認真的比較過說

0:11:02.720,0:11:04.420
Prediction based 方法

0:11:04.420,0:11:06.100
跟 Count based 的方法

0:11:06.100,0:11:09.460
它們有什麼樣非常不同的差異

0:11:09.460,0:11:11.280
或者是誰優誰劣

0:11:11.280,0:11:13.540
如果你有知道這方面的 information，或許

0:11:13.540,0:11:15.740
你可以貼在我們的社團上面

0:11:15.740,0:11:19.800
我講一下， Prediction based 的方法是怎麼做的呢

0:11:19.800,0:11:22.820
Prediction based 的方法，它的想法是這樣

0:11:22.820,0:11:25.800
我們來 learn 一個  neural network

0:11:25.800,0:11:30.040
它做的事情是 prediction，predict 什麼呢？

0:11:30.040,0:11:34.340
這個 neural network 做的事情是 given 前一個 word

0:11:34.340,0:11:36.280
假設給我一個 sentence

0:11:36.280,0:11:39.300
這邊每一個 w 代表一個 word

0:11:39.300,0:11:44.560
given w(下標 i-1)，這個 prediction 的 model

0:11:44.560,0:11:47.020
這個 neural network，它的工作是要

0:11:47.020,0:11:50.820
predict 下一個可能出現的 word 是誰

0:11:50.820,0:11:54.580
那我們知道說，每一個 word

0:11:54.580,0:11:59.780
我們都用 1-of-N encoding，可以把它表示成一個 feature vector

0:11:59.780,0:12:03.360
所以，如果我們要做 prediction 這一件事情的話

0:12:03.360,0:12:06.920
我們就是要 learn 一個 neural network

0:12:06.920,0:12:08.860
它的 input

0:12:08.860,0:12:13.300
就是 w(下標 i-1) 的 1-of-N encoding 的 feature vector

0:12:13.440,0:12:15.440
1-of-N encoding 的 vector

0:12:15.540,0:12:18.160
那它的 output 就是

0:12:18.160,0:12:23.460
下一個 word, wi 是某一個 word 的機率

0:12:23.460,0:12:26.880
也就是說，這一個 model 它的 output

0:12:26.880,0:12:30.100
它 output 的 dimension 就是 vector 的 size

0:12:30.100,0:12:32.380
假設現在世界上有 10 萬個 word

0:12:32.380,0:12:34.580
這個 model 的 output 就是 10 萬維

0:12:34.580,0:12:37.100
每一維代表了某一個 word

0:12:37.100,0:12:39.160
是下一個 word 的機率

0:12:39.160,0:12:40.640
每一維代表某一個 word

0:12:40.640,0:12:43.760
是會被當作 wi

0:12:43.760,0:12:46.480
它會是下一個 word, wi 的機率

0:12:46.480,0:12:51.040
所以 input 跟 output 都是 lexicon size

0:12:51.040,0:12:53.040
只是它們代表的意思是不一樣的

0:12:53.040,0:12:56.760
input 是 1-of-N encoding，output 是下一個 word 的機率

0:12:56.760,0:13:00.080
那假設這就是一個

0:13:00.080,0:13:02.520
一般我們所熟知的

0:13:02.520,0:13:06.960
multi-layer 的 Perceptron，一個 deep neural network

0:13:06.960,0:13:10.580
那你把它丟進去的時候

0:13:10.580,0:13:13.120
你把這個 input feature vector 丟進去的時候

0:13:13.120,0:13:14.420
它會通過很多 hidden layer

0:13:14.420,0:13:17.760
通過一些 hidden layer，你就會得到一些 output

0:13:17.760,0:13:24.460
接下來，我們把第一個 hidden layer 的 input 拿出來

0:13:24.460,0:13:27.240
我們把第一個 hidden layer 的 input 拿出來

0:13:27.240,0:13:30.060
假設第一個 hidden layer 的 input

0:13:30.060,0:13:32.840
我們這一邊寫作，它的第一個 dimension 是 Z1

0:13:32.840,0:13:34.840
第二個 dimension 是 Z2，以此類推

0:13:34.840,0:13:36.380
這邊把它寫作 Z

0:13:36.380,0:13:39.620
那我們用這個 Z

0:13:39.620,0:13:42.340
就可以代表一個 word

0:13:42.340,0:13:45.400
input 不同的 1-of-N encoding

0:13:45.400,0:13:47.860
這邊的 Z 就會不一樣

0:13:47.860,0:13:51.280
所以，我們就把這邊的 Z

0:13:51.280,0:13:54.500
拿來代表一個詞彙

0:13:54.500,0:13:56.020
你 input 同一個詞彙

0:13:56.020,0:13:57.940
它有同樣的 1-of-N encoding

0:13:57.940,0:14:00.080
在這邊它的 Z 就會一樣

0:14:00.080,0:14:02.900
input 不同的詞彙，這邊的 Z 就會不一樣

0:14:02.900,0:14:04.900
所以，我們就用這個 Z

0:14:04.900,0:14:08.780
這一個 input 1-of-N encoding 得到 Z 的這個 vector

0:14:08.780,0:14:10.460
來代表一個 word

0:14:10.460,0:14:12.700
來當作那一個 word 的 embedding

0:14:12.700,0:14:14.660
你就可以得到這一個現象

0:14:14.660,0:14:18.320
你就可以得到這樣的 vector

0:14:18.320,0:14:20.520
為什麼用這個 Prediction based 的方法

0:14:20.520,0:14:22.580
就可以得到這樣的 vector 呢

0:14:22.580,0:14:24.220
Prediction based 的方法

0:14:24.220,0:14:26.180
是怎麼體現我們說的

0:14:26.180,0:14:28.420
可以根據一個詞彙的上下文

0:14:28.420,0:14:32.260
來了解一個詞彙的涵義，這一件事情呢？

0:14:32.260,0:14:34.020
這邊是這樣子的

0:14:34.020,0:14:37.360
假設我們的 training data 裡面呢

0:14:37.360,0:14:39.080
有一個文章是

0:14:39.080,0:14:42.300
告訴我們說，馬英九跟蔡英文宣誓就職

0:14:42.300,0:14:44.840
另外一個是馬英九宣誓就職

0:14:44.840,0:14:46.100
在第一個句子裡面

0:14:46.100,0:14:49.320
蔡英文是 w(下標 i-1)，宣誓就職是 w(下標 i)

0:14:49.320,0:14:50.660
在另外一篇文章裡面

0:14:50.660,0:14:54.480
馬英九是 w(下標 i-1)，宣誓就職是  w(下標 i)

0:14:54.480,0:14:58.440
那你在訓練這個 Prediction  model 的時候

0:14:58.440,0:15:02.020
不管是 input 蔡英文，還是馬英九

0:15:02.020,0:15:04.500
不管是 input 蔡英文還是馬英九的 1-of-N encoding

0:15:04.500,0:15:07.120
你都會希望 learn 出來的結果

0:15:07.120,0:15:11.260
是宣誓就職的機率比較大

0:15:11.260,0:15:12.900
因為馬英九和蔡英文後面

0:15:12.900,0:15:15.620
接宣誓就職的機率都是高的

0:15:15.620,0:15:18.720
所以，你會希望說 input 馬英九跟蔡英文的時候

0:15:18.720,0:15:23.120
它 output，是 output 對應到宣誓就職那一個詞彙

0:15:23.120,0:15:26.780
它的那個 dimension 的機率是高的

0:15:26.780,0:15:28.720
為了要讓

0:15:28.720,0:15:31.140
蔡英文和馬英九雖然是不同的 input

0:15:31.140,0:15:33.780
但是，為了要讓最後在 output 的地方

0:15:33.780,0:15:36.160
得到一樣的 output

0:15:36.160,0:15:40.120
你就必須再讓中間的 hidden layer 做一些事情

0:15:40.120,0:15:42.940
中間的 hidden layer 必須要學到說

0:15:42.940,0:15:44.640
這兩個不同的詞彙

0:15:44.640,0:15:47.000
必需要把他們 project 到

0:15:47.000,0:15:48.800
必須要通過這個

0:15:48.800,0:15:50.860
必須要通過 weight 的轉換

0:15:50.860,0:15:52.540
通過這個參數的轉換以後

0:15:52.540,0:15:56.000
把它們對應到同樣的空間

0:15:56.000,0:15:58.060
在 input 進入 hidden layer 之前

0:15:58.060,0:16:00.980
必須把它們對應到接近的空間

0:16:00.980,0:16:02.880
這樣我們最後在 output 的時候

0:16:02.880,0:16:05.720
它們才能夠有同樣的機率

0:16:05.720,0:16:08.740
所以，當我們 learn 一個 prediction model 的時候

0:16:08.740,0:16:12.100
考慮一個 word 的 context這一件事情

0:16:12.100,0:16:15.940
就自動的，被考慮在這個 prediction model裡面

0:16:15.940,0:16:17.900
所以我們把這個 prediction model 的

0:16:17.900,0:16:19.580
第一個 hidden layer 拿出來

0:16:19.580,0:16:20.980
我們就可以得到

0:16:21.100,0:16:24.120
我們想要找的這種 word embedding 的特性

0:16:24.120,0:16:27.160
那你可能會想說

0:16:27.160,0:16:31.260
如果只用 w(下標 i-1)去 predict w(下標 i)

0:16:31.260,0:16:32.980
好像覺得太弱了

0:16:32.980,0:16:34.980
就算是人，你給一個詞彙

0:16:34.980,0:16:36.380
要 predict 下一個詞彙

0:16:36.380,0:16:37.920
感覺也很難

0:16:37.920,0:16:39.080
因為，如果只看一個詞彙，

0:16:39.080,0:16:42.180
下一個詞彙的可能性，是千千萬萬的

0:16:42.180,0:16:43.400
是千千萬萬的

0:16:43.400,0:16:47.160
那怎麼辦呢？怎麼辦呢？

0:16:47.160,0:16:50.400
你可以拓展這個問題

0:16:50.400,0:16:52.320
比如說，你可以拓展說

0:16:52.320,0:16:55.320
我希望 machine learn 的是 input 前面兩個詞彙

0:16:55.320,0:16:59.540
w(下標 i-2) 跟 w(下標 i-1)

0:16:59.540,0:17:02.520
predict 下一個 word, w(下標 i)

0:17:02.520,0:17:05.860
那你可以輕易地把這個 model 拓展到 N 個詞彙

0:17:05.860,0:17:10.080
一般我們，如果你真的要 learn 這樣的 word vector 的話

0:17:10.080,0:17:11.800
你可能會需要你的 input

0:17:11.800,0:17:14.140
可能通常是至少 10 個詞彙

0:17:14.140,0:17:15.680
你這樣才能夠 learn 出

0:17:15.680,0:17:16.920
比較 reasonable 的結果

0:17:16.920,0:17:19.660
只 input 一個或兩個，這都太少了

0:17:19.660,0:17:22.500
那我們這邊用 input 兩個 word 當作例子

0:17:22.500,0:17:23.940
那你可以輕易地把

0:17:23.940,0:17:27.020
這個 model 拓展到 10 個 word

0:17:28.200,0:17:31.460
那這邊要注意的事情是這個樣子

0:17:31.460,0:17:33.740
本來，如果是一般的 neural network

0:17:33.740,0:17:37.440
你就把 input w(下標 i-2) 和 w(下標 i-1) 的

0:17:37.440,0:17:41.060
1-of-N encoding 的 vector，把它接在一起

0:17:41.060,0:17:43.180
變成一個很長的 vector

0:17:43.180,0:17:45.160
直接丟都到 neural network 裡面

0:17:45.160,0:17:47.400
當作 input 就可以了

0:17:47.400,0:17:50.140
但是實際上，你在做的時候

0:17:50.140,0:17:54.860
你會希望 w(下標 i-2) 的

0:17:54.860,0:17:57.520
這個跟它相連的 weight

0:17:57.520,0:17:59.780
跟和 w(下標 i-1) 相連的 weight

0:17:59.780,0:18:02.760
它們是被 tight 在一起的

0:18:02.760,0:18:05.060
所謂 tight 在一起的意思是說

0:18:05.060,0:18:08.420
w(下標 i-2) 的第一個 dimension

0:18:08.420,0:18:11.900
跟第一個 hidden layer 的第一個 neuron

0:18:11.900,0:18:13.340
它們中間連的 weight

0:18:13.340,0:18:16.480
和 w(下標 i-1) 的第一個 dimension

0:18:16.480,0:18:19.880
和第一個 hidden layer 的 neuron，它們之間連的位置

0:18:19.880,0:18:23.720
這兩個 weight 必須是一樣的

0:18:23.720,0:18:27.760
所以，我這邊故意用同樣的顏色來表示它

0:18:27.760,0:18:30.740
這一個 dimension，它連到這個的 weight

0:18:30.740,0:18:35.420
跟第一個 dimension，它連到這邊的 weight

0:18:35.420,0:18:37.240
它必須是一樣的

0:18:37.240,0:18:39.980
所以，我這邊故意用同樣的顏色來表示他

0:18:39.980,0:18:42.600
這一個 dimension，它連到它的 weight

0:18:42.600,0:18:44.840
跟它連到它的 weight，必須是一樣的

0:18:44.840,0:18:46.600
以此類推

0:18:46.600,0:18:50.360
希望大家知道知道我的意思

0:18:50.360,0:18:53.500
那為什麼要這樣做呢？

0:18:53.500,0:18:55.680
為什麼要這樣做呢？

0:18:55.680,0:18:58.220
一個顯而易見的理由是這樣

0:18:58.220,0:19:00.400
一個顯而易見的理由是說

0:19:00.400,0:19:02.900
如果，我們不這麼做

0:19:02.900,0:19:07.260
如果我們不這麼做，你把不同的 word

0:19:07.260,0:19:11.320
你把同一個 word 放在

0:19:11.320,0:19:15.700
w(下標 i-2) 的位置跟放在 w(下標 i-1) 的位置

0:19:15.700,0:19:19.460
通過這個 transform 以後

0:19:19.460,0:19:22.740
它得到的 embedding 就會不一樣

0:19:22.740,0:19:26.760
如果，你必須要讓這一組 weight

0:19:26.760,0:19:28.820
和這一組weight 是一樣的

0:19:28.820,0:19:31.760
那你把一個 word 放在這邊，通過這個 transform

0:19:31.760,0:19:33.640
跟把一個 weight 放在這邊，通過一個 transform

0:19:33.640,0:19:36.320
它們得到的 weight 才會是一樣的

0:19:36.320,0:19:38.360
當然另外一個理由你可以說

0:19:38.360,0:19:40.620
我們做這一件事情的好處是

0:19:40.620,0:19:43.160
我們可以減少參數量

0:19:43.160,0:19:46.200
因為 input 這個 dimension 很大，它是十萬維

0:19:46.200,0:19:48.580
所以這個 feature vector，就算你這一邊是50 維

0:19:48.580,0:19:52.120
它也是一個非常非常、碩大無朋的 matrix

0:19:52.120,0:19:54.680
有一個已經覺得夠卡了

0:19:54.680,0:19:57.060
所以，有兩個更是吃不消

0:19:57.060,0:20:00.540
更何況說，我們現在 input 往往是 10 個 word

0:20:00.540,0:20:02.280
所以，如果我們強迫讓

0:20:02.280,0:20:04.460
所有的 1-of-N encoding

0:20:04.460,0:20:06.240
它後面接的 weight 是一樣的

0:20:06.240,0:20:08.580
那你就不會隨著你的 contest 的增長

0:20:08.580,0:20:12.660
而需要這個更多的參數

0:20:12.660,0:20:16.120
或許我們用 formulation 來表示

0:20:16.120,0:20:17.920
會更清楚一點

0:20:17.920,0:20:23.280
現在，假設 w(下標 i-2) 的 1-of-N encoding 就是 X2

0:20:23.280,0:20:26.280
w(下標 i-1) 的 1-of-N encoding 就是 X1

0:20:26.280,0:20:28.760
那它們的這個長度

0:20:28.760,0:20:31.400
都是 V 的絕對值

0:20:31.400,0:20:34.220
它們的長度我這邊都寫成 V 的絕對值

0:20:34.220,0:20:41.200
那這個 hidden layer 的 input

0:20:41.200,0:20:43.200
我們把它寫一個 vector, Z

0:20:43.200,0:20:47.180
Z 的長度，是 Z 的絕對值

0:20:47.180,0:20:51.980
那我們把這個 Z 跟

0:20:51.980,0:20:55.740
X(i-2) 跟 X(i-1) 有什麼樣的關係

0:20:55.740,0:21:03.520
Z 等於 X(i-2) * W1 + X(i-1) * W2

0:21:03.520,0:21:09.800
你把 X(i-2) * W1 + X(i-1) * W2，就會得到這個 Z

0:21:09.800,0:21:11.440
那現在這個 W1 跟 W2

0:21:11.440,0:21:17.660
它們都是一個 Z 乘上一個 V dimension 的 weight matrix

0:21:17.660,0:21:19.780
那在這邊，我們做的事情是

0:21:19.780,0:21:23.500
我們強制讓 W1 要等於 W2

0:21:23.500,0:21:26.500
要等於一個一模一樣的 matrix, W

0:21:26.500,0:21:31.280
所以，我們今天實際上在處理這個問題的時候

0:21:31.280,0:21:36.580
你可以把 X(i-2) 跟 X(i-1) 直接先加起來

0:21:36.580,0:21:38.700
因為 W1 跟 W2 是一樣的

0:21:38.700,0:21:40.180
你可以把 W 提出來

0:21:40.180,0:21:42.920
你可以把 X(i-1) 跟X(i-2) 先加起來

0:21:42.920,0:21:45.360
再乘上 W 的這個 transform

0:21:45.360,0:21:47.980
就會得到 z

0:21:47.980,0:21:51.900
那你今天如果要得到一個 word 的 vector 的時候

0:21:51.900,0:21:55.060
你就把一個 word 的 1-of-N encoding

0:21:55.060,0:21:57.080
乘上這個 W

0:21:57.080,0:22:00.700
你就可以得到那一個 word 的 Word Embedding

0:22:02.960,0:22:07.360
那這一邊會有一個問題，就是我們在實做上

0:22:07.360,0:22:10.260
如果你真的要自己實做的話

0:22:10.260,0:22:13.400
你怎麼讓這個 W1 跟 W2

0:22:13.400,0:22:16.360
它們的位 weight 一定都要一樣呢

0:22:16.360,0:22:21.640
事實上我們在 train CNN 的時候

0:22:21.640,0:22:23.540
也有一樣類似的問題

0:22:23.540,0:22:25.000
我們在 train CNN 的時候

0:22:25.000,0:22:27.040
我們也要讓 W1 跟 W2

0:22:27.040,0:22:30.560
我們也要讓某一些參數，它們的 weight

0:22:30.560,0:22:33.380
必須是一樣的

0:22:33.860,0:22:36.740
那怎麼做呢？這個做法是這樣子

0:22:36.740,0:22:40.980
假設我們現在有兩個 weight, wi 跟 wj

0:22:40.980,0:22:44.000
那我們希望 wi 跟 wj，它的 weight 是一樣的

0:22:44.000,0:22:45.260
那怎麼做呢？

0:22:45.260,0:22:50.660
首先，你要給 wi 跟 wj 一樣的 initialization

0:22:50.660,0:22:53.240
訓練的時候要給它們一樣的初始值

0:22:53.240,0:22:56.480
接下來，你計算 wi 的

0:22:56.480,0:23:00.540
wi 對你最後 cost function 的偏微分

0:23:00.540,0:23:02.440
然後 update wi

0:23:02.440,0:23:05.380
然後，你計算 wj 對 cost function 的偏微分

0:23:05.380,0:23:06.720
然後 update wj

0:23:06.720,0:23:09.200
你可能會說 wi 跟 wj 

0:23:09.200,0:23:11.860
如果它們對 C 的偏微分是不一樣的

0:23:11.860,0:23:13.400
那做 update 以後

0:23:13.400,0:23:16.000
它們的值，不就不一樣了嗎？

0:23:16.000,0:23:18.300
所以，如果你只有列這樣的式子

0:23:18.300,0:23:21.720
wi 跟 wj 經過一次 update 以後，它們的值就不一樣了

0:23:21.720,0:23:23.960
initialize 值一樣也沒有用

0:23:23.960,0:23:25.140
那怎麼辦呢？

0:23:25.140,0:23:29.860
我們就把 wi 再減掉

0:23:29.860,0:23:32.740
再減掉 wj 對 C 的偏微分

0:23:32.740,0:23:36.800
把 wj 再減掉 wi 對 C 的偏微分

0:23:36.800,0:23:39.940
也就是說 wi 有這樣的 update 

0:23:39.940,0:23:42.540
wj 也要有一個一模一樣的 update

0:23:42.540,0:23:44.020
wj 有這樣的 update

0:23:44.020,0:23:46.800
wi 也要有一個一模一樣的 update

0:23:46.800,0:23:49.000
如果你用這樣的方法的話呢

0:23:49.000,0:23:52.480
你就可以確保 wi 跟 wj，它們是

0:23:52.480,0:23:54.820
在這個 update 的過程中

0:23:54.820,0:23:56.060
在訓練的過程中

0:23:56.060,0:23:59.700
它們的 weight 永遠都是被 tight 在一起的

0:23:59.700,0:24:02.860
永遠都是一樣

0:24:04.500,0:24:07.240
那要怎麼訓練這個 network 呢？

0:24:07.240,0:24:08.560
這個 network 的訓練

0:24:08.560,0:24:11.340
完全是 unsupervised 的

0:24:11.340,0:24:15.660
也就是說，你只要 collect 一大堆文字的data

0:24:15.660,0:24:17.500
collect 文字的 data 很簡單

0:24:17.500,0:24:19.340
就寫一個程式上網去爬就好

0:24:19.340,0:24:20.740
寫一個程式爬一下

0:24:20.740,0:24:22.940
八卦版的 data

0:24:22.940,0:24:25.320
就可以爬到一大堆文字

0:24:25.320,0:24:27.500
然後，接下來就可以 train 你的 model

0:24:27.500,0:24:29.340
怎麼 train，比如說這邊有一個句子就是

0:24:29.340,0:24:31.900
潮水退了，就知道誰沒穿褲子

0:24:31.900,0:24:34.440
那你就讓你的 model

0:24:34.440,0:24:38.140
讓你的 neural network input "潮水" 跟 "退了"

0:24:38.140,0:24:40.760
希望它的 output 是 "就" 這個樣子

0:24:40.760,0:24:44.760
你會希望你的 output 跟"就" 的 cross entropy

0:24:44.760,0:24:47.440
"就" 也是一個 1-of-N encoding 來表示

0:24:47.440,0:24:48.880
所以，你希望你的 network 的 output 

0:24:48.880,0:24:51.460
跟 "就" 的 1-of-N encoding

0:24:51.460,0:24:53.340
是 minimize cross entropy

0:24:53.340,0:24:55.780
然後，再來就 input "退了 " 跟 "就"

0:24:55.780,0:24:59.680
然後，希望它的 output 跟 "知道" 越接近越好

0:24:59.680,0:25:01.560
然後 output "就" 跟 "知道" 

0:25:01.560,0:25:04.460
然後就，希望它跟 "誰" 越接近越好

0:25:05.260,0:25:07.320
那剛才講的

0:25:07.320,0:25:09.220
只是最基本的型態

0:25:09.220,0:25:12.120
其實這個 Prediction based 的 model 

0:25:12.120,0:25:15.020
可以有種種的變形

0:25:15.020,0:25:16.560
目前我還不確定說

0:25:16.560,0:25:21.140
在各種變形之中哪一種是比較好的

0:25:21.140,0:25:23.080
感覺上，它的 performance

0:25:23.080,0:25:25.420
在不同的 task上互有勝負

0:25:25.420,0:25:28.500
所以，很難說哪一種方法一定是比較好的

0:25:28.500,0:25:31.160
那有一招叫做

0:25:31.160,0:25:33.420
Continuous bag of word, (CBOW)

0:25:33.420,0:25:35.220
那 CBOW 是這個樣子的

0:25:35.220,0:25:38.560
CBOW 是說，我們剛才是拿前面的詞彙

0:25:38.560,0:25:41.100
去 predict 接下來的詞彙

0:25:41.100,0:25:43.040
那 CBOW 的意思是說

0:25:43.040,0:25:44.920
我們拿某一個詞彙的 context

0:25:44.920,0:25:46.500
去 predict 中間這個詞彙

0:25:46.500,0:25:50.320
我們拿 W(i-1) 跟 W(i+1) 去 predict Wi

0:25:50.320,0:25:54.220
用 W(i-1) 跟 W(i+1)去 predict Wi

0:25:54.220,0:25:56.460
那 Skip-gram 是說

0:25:56.460,0:26:02.040
我們拿中間的詞彙去 predict 接下來的 context

0:26:02.040,0:26:08.880
我們拿 Wi 去 predict W(i-1) 跟 W(i+1)

0:26:08.880,0:26:12.700
也就是 given 中間的 word，我們要去 predict 它的周圍

0:26:12.700,0:26:14.440
會是長什麼樣子

0:26:14.860,0:26:17.380
講到這邊大家有問題嗎？

0:26:18.700,0:26:21.140
講到這邊常常會有人問我一個問題

0:26:21.140,0:26:25.100
假設你有足夠 word vector 相關的文獻的話

0:26:25.100,0:26:26.540
你可能會說

0:26:26.540,0:26:29.940
其實這個 network 它不是 deep 的阿

0:26:29.940,0:26:31.940
雖然，常常在講 deep learning 的時候

0:26:31.940,0:26:33.800
大家都會提到 word vector

0:26:33.800,0:26:36.200
把它當作 deep learning 的一個 application

0:26:36.200,0:26:39.320
但是，如果你真的有讀過 word vector 的文獻的話

0:26:39.320,0:26:40.580
你會發現說

0:26:40.580,0:26:43.820
這個 neural network，它不是 deep 的

0:26:43.820,0:26:45.440
它其實就是一個 hidden layer

0:26:45.440,0:26:47.540
它其實是一個 linear 的 hidden layer

0:26:47.540,0:26:48.800
了解嗎？就是

0:26:48.800,0:26:50.800
這個 neural network，它只有一個 hidden layer

0:26:50.800,0:26:53.280
所以，你把 word input 以後，你就得到 word embedding

0:26:53.280,0:26:56.540
你就直接再從那個 hidden layer，就可以得到 output 

0:26:56.540,0:26:59.180
它不是 deep 的，為什麼呢？

0:26:59.180,0:27:01.740
為什麼？常常有人 問我這個問題

0:27:01.740,0:27:04.380
那為了回答這個問題 

0:27:04.380,0:27:09.100
我邀請了  Tomas Mikolov 來台灣玩這樣

0:27:09.100,0:27:13.740
Tomas Mikolov 就是 propose word vector 的作者

0:27:13.740,0:27:16.520
所以，如果你有用過 word vector 的 toolkit 的話

0:27:16.520,0:27:18.600
你可能有聽過他的名字

0:27:18.600,0:27:21.660
那就問他說，為什麼這個 model不是 deep 的呢？

0:27:21.660,0:27:22.920
他給我兩個答案

0:27:22.920,0:27:24.340
他說，首先第一個就是

0:27:24.340,0:27:27.860
他並不是第一個 propose word vector 的人

0:27:27.860,0:27:30.240
在過去就有很多這樣的概念

0:27:30.240,0:27:31.940
那他最 famous 的地方是

0:27:31.940,0:27:35.220
他把他寫的一個非常好的 toolkit 放在網路上

0:27:35.220,0:27:38.060
他在他的 toolkit 裡面，如果你看他的 code 的話

0:27:38.060,0:27:41.320
他有種種的 tip

0:27:41.320,0:27:44.020
所以，你自己做的時候做不出他的 performance 的

0:27:44.020,0:27:46.540
他是一個非常非常強 的 engineer

0:27:46.540,0:27:48.700
他有各種他自己直覺的 sense

0:27:48.700,0:27:51.940
所以你自己做，你做不出他的 performance

0:27:51.940,0:27:54.520
用他的 toolkit，跑出來的 performance 就是特別好

0:27:54.520,0:27:57.740
所以，這是一個

0:27:57.740,0:27:59.360
他非常厲害的地方

0:27:59.360,0:28:02.540
他說，在他之前其實就有很多人做過

0:28:02.540,0:28:05.440
word vector，也有提出類似的概念

0:28:05.440,0:28:10.460
他說他寫的，他有一篇 word vector 的文章跟 toolkit

0:28:10.460,0:28:12.320
他想要 verify 最重要的一件事情是說

0:28:12.320,0:28:14.700
過去其實其他人就是用 deep

0:28:14.700,0:28:18.400
他想要講的是說，其實這個 task

0:28:18.400,0:28:20.060
不用 deep 就做起來了

0:28:20.060,0:28:22.840
不用 deep 的好處就是減少運算量

0:28:22.840,0:28:26.180
所以它可以跑很大量、很大量、很大量的 data

0:28:26.180,0:28:28.120
那我聽他這樣講

0:28:28.120,0:28:29.960
我就想起來，其實過去確實是

0:28:29.960,0:28:32.440
有人已經做過 word vector

0:28:32.440,0:28:34.380
過去確實已經有做過 word vector 這件事情

0:28:34.380,0:28:37.820
只是那些結果沒有紅起來

0:28:37.820,0:28:39.460
我記得說，我大學的時候

0:28:39.460,0:28:41.320
就看過類似的 paper

0:28:41.320,0:28:42.920
我大學的時候就有看過

0:28:42.920,0:28:46.520
其實就是一樣，就是 learn 一個 Prediction model

0:28:46.520,0:28:48.120
predict 下一個 word 的做法

0:28:48.180,0:28:50.000
只是那個時候是 deep

0:28:50.000,0:28:51.740
在我大學的時候

0:28:51.740,0:28:52.940
那時候 deep learning 還不紅

0:28:52.940,0:28:54.020
我看到那一篇 paper  的時候

0:28:54.020,0:28:56.560
他最後講說我 train 了這個 model

0:28:56.560,0:29:00.560
我花了 3 週，然後我沒有辦法把實驗跑完

0:29:00.560,0:29:02.540
所以結果是很好的

0:29:02.880,0:29:06.080
就其他方法，他可以跑很多的 iteration

0:29:06.080,0:29:07.860
然後說這個 neural network 的方法

0:29:07.860,0:29:12.560
我跑了 5 個 epoch，花了 3 週，我實在做不下去

0:29:12.560,0:29:14.080
所以，performance 沒有特別好

0:29:14.080,0:29:16.260
而且想說，這是什麼荒謬的做法

0:29:16.260,0:29:19.080
但是，現在運算量不同

0:29:19.080,0:29:20.340
所以，現在要做這一件事情呢

0:29:20.340,0:29:22.700
都沒有問題

0:29:22.700,0:29:25.600
其實像 word embedding 這個概念

0:29:25.600,0:29:30.820
在語音界，大概是在 2010 年的時候開始紅起來的

0:29:30.820,0:29:34.760
那個時候我們把它叫做 continuous 的 language model

0:29:34.760,0:29:35.600
一開始的時候

0:29:35.600,0:29:37.800
也不是用 neural network 來得到這個 word embedding的

0:29:37.800,0:29:39.680
因為 neural network  的運算量比較大

0:29:39.680,0:29:41.580
所以，一開始並不是選擇 neural network

0:29:41.580,0:29:44.200
而是用一些其他方法來

0:29:44.200,0:29:46.980
一些比較簡單的方法來得到這個 word 的 embedding

0:29:46.980,0:29:49.040
只是，後來大家逐漸發現說

0:29:49.040,0:29:51.840
用 neural network 得到的結果才是最好的

0:29:51.840,0:29:53.840
過去其他不是 neural network 的方法

0:29:53.840,0:29:55.100
就逐漸式微

0:29:55.100,0:29:57.320
通通都變成 neural network based 的方法

0:29:57.900,0:29:59.760
還有一個勵志的故事

0:29:59.760,0:30:02.060
就是Tomas Mikolov 那個

0:30:02.060,0:30:03.880
word vector paper不是非常 famous 嗎？

0:30:03.880,0:30:06.020
它的 citation，我不知道，搞不好都有 1 萬了

0:30:06.020,0:30:08.740
他說他第一次投那 一篇 paper 的時候

0:30:08.740,0:30:10.820
他先投到一個，我已經忘記名字的

0:30:10.820,0:30:12.780
很小很小的會，accept rate 有 70%

0:30:12.780,0:30:14.740
然後就被 reject 了

0:30:16.380,0:30:20.120
他還得到一個 comment，就是這是什麼東西

0:30:20.120,0:30:22.120
我覺得這東西一點用都沒有

0:30:22.120,0:30:25.380
所以，這是一個非常勵志的故事

0:30:27.240,0:30:29.840
那我們知道說

0:30:29.840,0:30:32.300
word vector 可以得到一些有趣的特性

0:30:32.300,0:30:34.880
我們可以看到說

0:30:34.880,0:30:39.400
如果你把同樣類型的東西的 word vector 擺在一起

0:30:39.400,0:30:42.180
比如說，我們把這個 Italy

0:30:42.180,0:30:44.660
跟它的首都 Rome 擺在一起

0:30:44.660,0:30:47.120
我們把Germany 跟它的首都 Berlin 擺在一起

0:30:47.120,0:30:49.800
我們把 Japan

0:30:49.800,0:30:51.660
跟它的首都 Tokyo 擺在一起

0:30:51.660,0:30:53.487
你會發現說

0:30:53.487,0:30:56.800
它們之間是有某種固定的關係的

0:30:56.800,0:30:59.860
或者是，你把一個動詞的三態擺在一起

0:30:59.860,0:31:03.500
你會發現說，動詞的三態

0:31:03.500,0:31:04.940
同一個動詞的三態

0:31:04.940,0:31:07.280
它們中間有某種固定的關係

0:31:07.280,0:31:08.660
成為這個三角形

0:31:09.080,0:31:11.560
所以從這個 word vector 裡面呢

0:31:11.560,0:31:15.080
你可以 discover 你不知道的 word 跟 word 之間的關係

0:31:15.080,0:31:17.540
比如說，還有人發現說

0:31:17.540,0:31:19.420
如果你今天把

0:31:19.420,0:31:23.420
兩個 word vector 和 word vector 之間，兩兩相減

0:31:23.420,0:31:27.140
這個結果是把 word vector 跟 word vector 之間兩兩相減

0:31:27.140,0:31:30.940
然後 project 到一個 2 dimensional 的 space 上面

0:31:30.940,0:31:33.560
那你會發現說，在這一區

0:31:33.560,0:31:35.880
如果今天 word vector 兩兩相減

0:31:35.880,0:31:38.160
它得到的結果是落在這個位置的話

0:31:38.160,0:31:42.780
那這兩個 word vector 之間，它們就有，比如說

0:31:42.780,0:31:46.920
某一個 word 是包含於某一個 word 之間的關係

0:31:46.920,0:31:49.960
比如說，你把 (這一邊這個字比較小)

0:31:49.960,0:31:53.820
比如說，你把海豚跟會轉彎的白海豚相減

0:31:53.820,0:31:56.400
它的 vector 落在這邊

0:31:56.400,0:31:59.980
你把演員跟主角相減，落在這一邊

0:31:59.980,0:32:02.120
你把工人跟木匠相減，落在這邊

0:32:02.120,0:32:06.280
你把職員跟售貨員相減，落在這一邊

0:32:06.280,0:32:08.720
你把羊跟公羊相減，落在這邊

0:32:08.720,0:32:10.360
如果，某一個東西是

0:32:10.360,0:32:12.500
屬於另外一個東西的話

0:32:12.500,0:32:13.980
你把它們兩個 word vector 相減

0:32:13.980,0:32:16.940
它們的結果呢，會是很類似的

0:32:16.940,0:32:20.200
所以用 word vector 的這一個的概念

0:32:20.200,0:32:24.120
我們可以做一些簡單的推論

0:32:24.120,0:32:27.020
舉例來說， 因為我們知道說

0:32:27.020,0:32:28.880
比如說，hotter 的 word vector

0:32:28.880,0:32:31.360
減掉 hot 的 word vector 會很接近

0:32:31.360,0:32:33.780
bigger 的 word vector 減掉 big 的 word vector 

0:32:33.780,0:32:36.020
或是 Rome 的 vector 減掉 Italy 的 vector

0:32:36.020,0:32:38.560
會很接近 Berlin 的 vector 減掉 Germany 的 vector 

0:32:38.560,0:32:40.960
或是 King 的 vector 減掉 queen 的 vector 會很接近

0:32:40.960,0:32:43.120
uncle 的 vector 減掉 aunt 的 vector

0:32:43.120,0:32:46.060
如果有人問你說，羅馬之於義大利

0:32:46.060,0:32:48.160
就好像 Berlin 之於什麼？

0:32:48.160,0:32:49.900
智力測驗都會考這樣的問題

0:32:49.900,0:32:51.560
機器可以回答這種問題了

0:32:51.560,0:32:54.440
怎麼做呢？因為我們知道說

0:32:54.440,0:32:57.700
今天這個問題的答案

0:32:57.700,0:33:00.180
Germany 的 vector 會很接近 Berlin 的 vector

0:33:00.180,0:33:03.000
減掉 Rome 的 vector 加 Italy 的 vector

0:33:03.000,0:33:05.860
因為這 4 個 word vector 中間有這樣的關係

0:33:05.860,0:33:07.880
所以你可以把 Germany  放在一邊

0:33:07.880,0:33:09.940
把另外三個 vector 放在右邊

0:33:09.940,0:33:12.640
所以 Germany 的 vector 會接近 Berlin 的 vector

0:33:12.640,0:33:15.160
減掉 Rome 的 vector 再加上 Italy 的 vector

0:33:15.160,0:33:17.520
所以，如果你要回答這個問題

0:33:17.520,0:33:20.420
假設你不知道答案是 Germany 的話

0:33:20.420,0:33:22.540
那你要做的事情就是

0:33:22.540,0:33:24.020
計算 Berlin 的 vector

0:33:24.020,0:33:27.080
減掉 Rome的 vector，再加 Italy 的 vector

0:33:27.080,0:33:29.420
然後看看它跟哪一個 vector 最接近

0:33:29.420,0:33:32.620
你可能得到的答案就是 Germany

0:33:32.620,0:33:36.680
這邊有一個 word vector 的 demo

0:33:36.680,0:33:40.520
就讓機器讀了大量 PTT 的文章以後

0:33:40.520,0:33:42.520
它就像這樣

0:33:42.840,0:33:46.000
那 word vector 還可以做很多其他的事情

0:33:46.000,0:33:51.280
比如說，你可以把不同的語言的 word vector

0:33:51.280,0:33:53.120
把它拉在一起

0:33:53.120,0:33:55.080
如果，你今天有一個中文的 corpus

0:33:55.080,0:33:56.680
有一個英文的 corpus

0:33:56.680,0:34:00.400
你各自去、分別去 train 一組 word vector

0:34:00.400,0:34:01.920
你會發現說

0:34:01.920,0:34:03.720
中文跟英文的 word vector

0:34:03.720,0:34:06.820
它是完全沒有任何的關係的

0:34:06.820,0:34:09.260
它們的每一個 dimension

0:34:09.260,0:34:11.540
對應的含義並沒有任何關係，為什麼？

0:34:11.540,0:34:14.080
因為你要 train word vector 的時候

0:34:14.080,0:34:17.240
它憑藉的就是上下文之間的關係

0:34:17.240,0:34:19.380
所以，如果你今天的 corpus 裡面

0:34:19.380,0:34:22.160
沒有中文跟英文的句子混雜在一起

0:34:22.160,0:34:25.120
沒有中文跟英文的詞彙混雜在一起

0:34:25.120,0:34:26.760
那 machine 就沒有辦法判斷

0:34:26.760,0:34:30.240
中文的詞彙跟英文的詞彙他們之間的關係

0:34:30.240,0:34:34.440
但是，今天假如你已經事先知道說

0:34:34.440,0:34:35.720
某幾個詞彙

0:34:35.720,0:34:38.920
某幾個中文的詞彙和某幾個英文的詞彙

0:34:38.920,0:34:40.340
它們是對應在一起的

0:34:40.340,0:34:42.500
那你先得到一組中文的 vector 

0:34:42.500,0:34:43.880
再得到一組英文的 vector

0:34:43.880,0:34:46.280
接下來，你可以再 learn 一個  model

0:34:46.280,0:34:49.280
它把中文和英文對應的詞彙

0:34:49.280,0:34:52.220
比如說，我們知道 "加大" 對應到 "enlarge"

0:34:52.220,0:34:53.820
"下跌" 對應到 "fall" 

0:34:53.820,0:34:56.580
你把對應的詞彙，通過這個 projection 以後，

0:34:56.580,0:34:59.220
把它們 project 在 space上的同一個點

0:34:59.220,0:35:01.680
把它們 project 在 space 上面的同一個點

0:35:01.680,0:35:06.660
那在這個圖上，綠色的然後下面又有

0:35:06.660,0:35:08.620
這個綠色的英文的代表是

0:35:08.620,0:35:13.000
已經知道對應關係的中文和英文的詞彙

0:35:13.000,0:35:16.820
然後，如果你做這個 transform 以後

0:35:16.820,0:35:19.940
接下來有新的中文的詞彙和新的英文的詞彙

0:35:19.940,0:35:22.260
你都可以用同樣的 projection

0:35:22.260,0:35:24.300
把它們 project 到同一個 space 上面

0:35:24.300,0:35:26.300
比如說，你就可以自動知道說

0:35:26.300,0:35:32.060
中文的降低跟的英文的 reduce

0:35:32.060,0:35:34.580
它們都應該落在這個位置

0:35:34.580,0:35:38.440
都應該落在差不多的位置等等這樣

0:35:38.440,0:35:39.940
你就可以自動做到

0:35:39.940,0:35:42.980
比如說，類似翻譯這個樣子的效果

0:35:42.980,0:35:49.280
那這個 embedding不只限於文字

0:35:49.280,0:35:52.420
你也可以對影像做 embedding

0:35:52.420,0:35:54.180
這邊有一個很好的例子

0:35:54.180,0:35:55.360
這個例子是這樣做的

0:35:55.360,0:35:58.120
它說，我們先已經找到一組 word vector

0:35:58.120,0:36:00.260
比如說，dog 的 vector、horse 的 vector

0:36:00.260,0:36:02.240
auto 的 vector 和 cat 的 vector

0:36:02.240,0:36:04.420
它們分佈在空間上是這樣子的位置

0:36:04.420,0:36:07.040
接下來，你 learn 一個 model 

0:36:07.040,0:36:08.500
它是 input 一張 image

0:36:08.500,0:36:10.660
output 是跟一個跟 word vector

0:36:10.660,0:36:12.420
一樣 dimension 的 vector

0:36:12.420,0:36:14.100
那你會希望說

0:36:14.100,0:36:16.500
狗的 vector 就散佈在狗的周圍

0:36:16.500,0:36:18.660
馬的 vector 就散佈在馬的周圍

0:36:18.660,0:36:22.300
車輛的 vector 就散佈在 auto 的周圍

0:36:22.300,0:36:24.380
那假設有一些 image 

0:36:24.380,0:36:27.300
你已經知道他們是屬於哪一類

0:36:27.300,0:36:30.240
你已經知道說這個是狗、這個是馬、這個是車

0:36:30.240,0:36:32.620
你可以把它們 project 到

0:36:32.620,0:36:35.120
它們所對應到的 word vector 附近

0:36:35.120,0:36:36.980
那這個東西有什麼用呢？

0:36:36.980,0:36:40.780
假如你今天有一個新的 image 進來

0:36:40.780,0:36:42.940
比如說，這個東西，它是個貓

0:36:42.940,0:36:44.060
但是你不知道它是貓

0:36:44.060,0:36:45.080
機器不知道它是貓

0:36:45.080,0:36:46.960
但是你通過它們的 projection 

0:36:46.960,0:36:49.820
把它 project 到這個 space 上以後

0:36:49.820,0:36:53.500
神奇的是你就會發現它可能就在貓的附近

0:36:53.500,0:36:55.340
那你的 machine 就會自動知道說

0:36:55.340,0:36:57.520
這個東西叫做貓

0:36:57.520,0:37:00.180
當我們一般在做影像分類的時候

0:37:00.180,0:37:03.380
大家都已經有做過作業三

0:37:03.380,0:37:05.840
作業三就是影像分類的問題

0:37:05.840,0:37:07.860
在做影像分類的問題的時候

0:37:07.860,0:37:10.240
你的 machine 其實很難去處理

0:37:10.240,0:37:13.580
新增加的，它沒有辦法看過的 object

0:37:13.580,0:37:15.140
舉例來說，作業 3 裡面

0:37:15.140,0:37:17.480
我們就先已經訂好 10 個 class

0:37:17.480,0:37:19.160
你 learn 出來的 model

0:37:19.160,0:37:22.160
就是只能分這 10 個 class

0:37:22.160,0:37:23.900
如果今天有一個新的東西

0:37:23.900,0:37:25.220
不在這10個 class 裡面

0:37:25.220,0:37:27.420
你的 model 是完全是無能為力 的

0:37:27.420,0:37:28.660
它根本不知道它叫做什麼

0:37:28.660,0:37:30.960
但是，如果你用這個方法的話

0:37:30.960,0:37:32.500
就算有一張 image

0:37:32.500,0:37:34.660
是你在 training 的時候，你沒有看過的 class

0:37:34.660,0:37:36.080
比如說，貓這個 image

0:37:36.080,0:37:38.380
它從來都沒有看過

0:37:38.380,0:37:40.780
但是如果貓的這個 image

0:37:40.780,0:37:42.920
可以 project 到 cat 的 vector 附近的話

0:37:42.920,0:37:46.680
你就會知道說，這一張 image 叫做 cat

0:37:46.680,0:37:49.100
如果你可以做到這一件事，就好像是

0:37:49.100,0:37:51.920
 machine 先閱讀了大量的文章以後

0:37:51.920,0:37:53.880
它知道說，每一個詞彙

0:37:53.880,0:37:55.680
指的是什麼意思

0:37:55.680,0:37:59.620
它知道說，狗啊，貓啊，馬啊

0:37:59.620,0:38:01.760
它們之間有什麼樣的關係

0:38:01.760,0:38:06.280
它透過閱讀大量的文章，先了解詞彙間的關係

0:38:06.280,0:38:07.920
接下來，在看 image 的時候

0:38:07.920,0:38:10.280
它就可以根據它已經閱讀得到的知識

0:38:10.280,0:38:12.120
去 mapping 每一個 image

0:38:12.120,0:38:13.800
所應該對應的東西

0:38:13.800,0:38:16.780
這樣就算是它看到它沒有看過的東西

0:38:16.780,0:38:19.360
它也可能可以把它的名字叫出來

0:38:20.360,0:38:22.440
那剛才講的呢

0:38:22.440,0:38:24.400
都是 word embedding 

0:38:24.400,0:38:26.920
也可以做 document 的 embedding

0:38:26.920,0:38:29.240
不只是把一個 word 變成一個 vector

0:38:29.240,0:38:33.980
也可以把一個 document  變成一個 vector

0:38:33.980,0:38:38.420
那怎麼把一個 document 變成一個 vector 呢

0:38:38.420,0:38:41.320
最簡單的方法，我們之前已經講過了

0:38:41.320,0:38:44.960
就是把一個 document 變成一個 word

0:38:44.960,0:38:46.620
然後，用 Auto-encoder

0:38:46.620,0:38:48.000
你就可以 learn 出

0:38:48.000,0:38:50.420
這個 document 的 Semantic Embedding

0:38:50.420,0:38:53.820
但光這麼做是不夠的

0:38:53.820,0:38:57.420
我們光用這個 word 來描述一篇 document

0:38:57.420,0:38:58.880
是不夠的，為什麼呢？

0:38:58.900,0:39:01.240
因為我們知道說，詞彙的順序

0:39:01.240,0:39:03.280
代表了很重要的含

0:39:03.280,0:39:04.780
舉例來說

0:39:04.780,0:39:07.260
這一邊有兩個詞彙，有兩個句子

0:39:07.260,0:39:11.900
一個是： white blood cells destroying an infection

0:39:11.900,0:39:14.840
另外一個是：an infection destroying white blood cells

0:39:14.880,0:39:18.040
這兩句話，如果你看它的 bag-of-word 的話

0:39:18.040,0:39:21.200
它們的 bag-of-word 是一模一樣的

0:39:21.200,0:39:23.860
因為它們都有出現有這 6 個詞彙

0:39:23.860,0:39:25.480
它們都有出現這 6 個詞彙

0:39:25.480,0:39:26.980
只是順序是不一樣的

0:39:26.980,0:39:29.080
但是因為它們的順序是不一樣的

0:39:29.080,0:39:30.300
所以上面這一句話 

0:39:30.300,0:39:33.300
白血球消滅了傳染病，這個是 positive

0:39:33.300,0:39:36.480
下面這句話，它是 negative

0:39:36.480,0:39:38.480
雖然說，它們有同樣的 bag-of-word

0:39:38.480,0:39:41.160
它們在語意上，完全是不一樣的

0:39:41.160,0:39:44.540
所以，光只是用 bag-of-word

0:39:44.540,0:39:48.800
來描述一張 image 是非常不夠的

0:39:48.800,0:39:53.280
用 bag-of-word 來描述 一篇 document 是非常不足的

0:39:53.280,0:39:57.580
你用 bag-of-word 會失去很多重要的 information

0:39:57.580,0:39:59.640
那怎麼做呢？

0:39:59.640,0:40:00.980
我們這一邊就不細講

0:40:00.980,0:40:04.520
這邊就列了一大堆的 reference  給大家參考

0:40:04.520,0:40:07.100
上面這 3 個方法，它是 unsupervised

0:40:07.100,0:40:09.120
也就是說你只要 collect

0:40:09.120,0:40:11.840
一大堆的 document

0:40:11.840,0:40:13.460
你就可以讓它自己去學

0:40:13.460,0:40:17.320
那下面這幾個方法算是 supervised

0:40:17.320,0:40:19.580
因為，在這一些方法裡面

0:40:19.580,0:40:21.720
你需要對每一個 document

0:40:21.720,0:40:22.960
進行額外的 label

0:40:22.960,0:40:26.320
你不用 label  說，每一個 document 對應的 vector是什麼

0:40:26.320,0:40:28.440
但是你要給它其他的 label

0:40:28.440,0:40:29.900
才能夠 learn 這一些 vector

0:40:29.900,0:40:32.580
所以下面，不算是完全 unsupervised

0:40:32.580,0:40:34.960
我把 reference 列在這邊，給大家參考

0:40:34.960,0:40:38.540
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
