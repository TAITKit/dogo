0:00:00.230,0:00:02.200
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.200,0:00:04.200
各位同學大家好，我們來上課吧！

0:00:11.110,0:00:15.290
好，那接下來呢，我們來講一下
Recurrent Neural Network

0:00:15.880,0:00:20.140
那 Recurrent Neural Network呢
它其實也可以做到我們在前一堂課裡面

0:00:20.180,0:00:25.000
講的 sequence labeling 的 task
那最後呢，我們再來說他們有

0:00:25.240,0:00:27.240
甚麼樣的不同

0:00:28.570,0:00:33.520
那我們這邊要舉的例子呢
是 Slot Filling，那我們知道說

0:00:33.750,0:00:38.300
現在很流行呢，做一些智慧客服之類的東西

0:00:38.390,0:00:43.230
譬如說，做一些智慧的訂票系統
那這種智慧客服或智慧訂票系統裡面呢

0:00:43.700,0:00:48.140
你往往需要 Slot Filling 這個技術
Slot Filling 指的是甚麼呢

0:00:48.760,0:00:53.600
Slot Filling 指的是說
比如說假設有一個人對你的訂票系統說

0:00:53.860,0:00:58.800
I would like to arrive Taipei on November 2nd
那你的系統要自動知道說

0:00:59.220,0:01:03.480
你的系統裡面有一些 slot
比如說，在這個

0:01:03.540,0:01:07.980
訂票系統裡面呢
它會有一個 slot叫做 Destination

0:01:08.300,0:01:13.280
它會有一個 slot 叫做 Time of Arrival
那你的系統要自動知道說呢

0:01:13.600,0:01:18.060
這邊的每一個詞彙，它屬於哪一個 slot

0:01:18.320,0:01:23.270
那你的系統要知道說
Taipei 屬於 Destination 這個 slot

0:01:23.360,0:01:28.180
那你的系統要知道說
November 2nd 屬於 Time of Arrival 這個 slot

0:01:28.240,0:01:32.850
那其他的詞彙呢，就不屬於任何 slot 裡面

0:01:33.200,0:01:37.890
那這個問題要怎麼解呢，其實這個問題

0:01:37.950,0:01:42.740
你當然也可以用一個 Feedforward 的 
Neural Network 來解。也就是說，我疊一個

0:01:43.630,0:01:45.630
Feedforward  的 Neural Network

0:01:45.630,0:01:48.020
然後它的 input 就是一個詞彙

0:01:48.390,0:01:52.600
比如說，你把 Taipei 變成一個 vector 
丟到這個 Neural Network裡面去

0:01:53.720,0:01:57.540
但是你要把一個詞彙丟到Neural Network 裡面去
你必須要先把它

0:01:58.000,0:02:02.000
用一個向量 vector 來表示

0:02:02.000,0:02:04.960
那怎麼把一個詞彙用一個向量來表示呢

0:02:04.960,0:02:07.310
方法實在太多了，最 naive的方法呢

0:02:07.370,0:02:11.940
就是 1-of-N encoding
我想這個呢，應該就不需要再細講

0:02:12.690,0:02:17.330
當然你用 word vector來表示一個詞彙呢

0:02:17.780,0:02:22.110
也是可以的，那或者是呢，有一些

0:02:22.180,0:02:27.120
Beyond 1-of-N encoding 的方法，比如說

0:02:27.380,0:02:32.130
有時候，如果你只是用 1-of-N encoding
來描述一個詞彙的話，你會遇到一些問題

0:02:32.340,0:02:37.180
因為有很多詞彙，你可能從來都沒有見過

0:02:37.240,0:02:39.240
所以你會需要在1-of-N encoding 裡面

0:02:39.240,0:02:42.240
多加一個 dimension，這個 dimension 代表

0:02:42.340,0:02:47.300
代表 other，然後所有的詞彙如果它不是在我們的

0:02:47.390,0:02:52.320
辭典裡有的詞彙，就歸類到 other 裡面去

0:02:52.440,0:02:57.340
比如說， Gandalf 不在我們的 vocabulary 裡面

0:02:57.740,0:02:59.740
它就歸類到 other

0:02:59.750,0:03:00.320
或者是

0:03:02.270,0:03:07.170
Sauron 不在這個 vocabulary 裡面，它就歸類到這個 vector

0:03:07.710,0:03:12.650
那你也可以用某一個詞彙的字母，來表示

0:03:13.130,0:03:18.040
它的 vector ，那如果你用某一個詞彙的字母的 n-gram 來表示那個 vector 的話呢

0:03:18.180,0:03:23.060
你就不會有某一個詞彙不在辭典中的問題
比如說，你有一個詞彙叫做 "apple"

0:03:23.960,0:03:27.680
那 apple 呢，它裡面有出現 "app"

0:03:27.800,0:03:31.660
有出現 "ppl", 有出現 "ple"

0:03:32.060,0:03:37.400
那在這個 vector 裡面，對應到 "app" 的
那個 dimension 就是 1

0:03:37.400,0:03:42.400
對應到 "ppl" 的 dimension 就是 1
那其他的都是 0，無論如何

0:03:42.620,0:03:46.420
假設我們可以把一個詞彙表示成一個 vector

0:03:46.980,0:03:51.400
那就可以把這個 vector 丟到一個 Feedforward 的 Network 裡面去，那在 slot filling 這個 task 裡面

0:03:51.440,0:03:55.560
你就會希望你的 output 是一個 probability distribution

0:03:55.880,0:04:00.840
這個 probability distribution 
代表說我們現在 input 的這個詞彙

0:04:00.900,0:04:05.100
屬於 哪一個 slot 的機率，舉例來說

0:04:05.200,0:04:10.110
Taipei 屬於 destination 的機率，
還有 Taipei 屬於 time of departure 的機率，等等

0:04:13.260,0:04:18.140
但是呢，光只有這樣是不夠的
Feedforward Network 呢

0:04:18.180,0:04:23.040
沒有辦法 solve 這個 problem
為甚麼呢？假設現在，有一個使用者說

0:04:23.160,0:04:26.700
要 arrive Taipei on November 2nd

0:04:26.700,0:04:29.300
那 arrive 是other , Taipei 是 destination ，on 是 other

0:04:29.300,0:04:33.160
November 2nd 都是時間，但是如果另外一個

0:04:33.340,0:04:37.620
使用者說 leave Taipei on November 2nd
那 Taipei 這個時候

0:04:37.680,0:04:43.400
它應該是 place of departure
它應該是出發地而不是目的地

0:04:43.480,0:04:47.600
但是對 neural network 來說，input 一樣的東西

0:04:48.000,0:04:51.700
output 就是一樣的東西，而你 input 台北這個詞彙

0:04:52.200,0:04:55.180
output 要馬都是 destination 的機率最高

0:04:55.180,0:04:57.990
要馬就都是 place of departure

0:04:58.160,0:05:03.120
要馬就都是目的地的機率最高
要馬就都是出發地的機率最高

0:05:03.290,0:05:07.900
你沒有辦法讓它有時候出發地的機率最高
有時候目的地的機率高

0:05:09.000,0:05:12.060
那怎麼辦呢？這個時候呢，我們就希望

0:05:12.060,0:05:16.100
我們的neural network 它是有記憶力的

0:05:16.500,0:05:18.690
如果今天 neural network 是有記憶力的

0:05:18.760,0:05:21.980
它記得它在看過這個紅色的台北之前

0:05:21.980,0:05:25.000
它就已經看過 arrive 這個詞彙

0:05:25.080,0:05:28.420
它記得它在看過這個綠色的 Taipei 之前

0:05:28.500,0:05:33.450
它就已經看過 leave 這個詞彙
它就可以根據一段話的上下文

0:05:33.940,0:05:38.160
產生不同的 output 
所以如果我們讓我們的 neural network

0:05:39.270,0:05:44.230
它是有記憶力的話，它就可以解決input 同樣的詞彙

0:05:44.350,0:05:49.010
但是 output 必須是不同的這個問題

0:05:49.330,0:05:51.580
好那這種有記憶力的 neural network 呢

0:05:51.580,0:05:53.990
就叫做 Recurrent Neural Network

0:05:54.100,0:05:58.860
它的縮寫，是 RNN
那在 Recurrent Neural Network 裡面呢

0:06:00.000,0:06:04.790
每一次我們的 hidden layer

0:06:04.990,0:06:09.690
每一次我們的 hidden layer 
裡面的 neuron 產生 output 的時候

0:06:09.760,0:06:14.730
這個 output 都會被存到 memory 裡面去

0:06:14.850,0:06:19.800
這邊呢，用藍色的方塊來表示 memory

0:06:19.860,0:06:24.090
當這些 hidden layer 裡面的 neuron 有 output 的時候

0:06:25.710,0:06:28.940
它就會被存到這個藍色的方塊裡面去，那下一次

0:06:29.520,0:06:33.610
如果當有 input 的時候，這個 hidden layer

0:06:35.210,0:06:39.550
這些 neuron 它不是只會考慮

0:06:39.670,0:06:43.330
input 的這個 x1 跟 x2，它還會考慮

0:06:43.400,0:06:45.560
存在這些 memory 裡面的值。

0:06:45.560,0:06:48.300
對它來說，除了 x1 跟 x2 以外

0:06:49.100,0:06:51.080
這些存在 memory 裡面的值呢

0:06:51.080,0:06:56.300
a1、a2 也會影響它的 output

0:06:58.580,0:07:03.120
那我想直接舉個例子，大家可能會比較清楚

0:07:03.450,0:07:08.220
假設我們現在圖上的這個 network
它所有的 weight 都是 1

0:07:08.400,0:07:13.000
然後所有的 neuron 都沒有任何的 bias 值

0:07:13.170,0:07:17.850
然後假設所有的 activation function 都是 linear

0:07:17.930,0:07:20.620
這樣可以不要讓計算太複雜

0:07:20.900,0:07:24.400
那現在假設我們的 input 是一個 sequence

0:07:24.440,0:07:27.930
我們的 input 是 [11] [11] [22]

0:07:28.000,0:07:32.160
那我們把[11] [11] [22] 這個 sequence

0:07:32.810,0:07:35.860
input 到這個  Recurrent Neural Network 裡面去

0:07:35.860,0:07:37.200
會發生甚麼事呢？

0:07:37.780,0:07:39.780
那首先呢，在你開始要

0:07:42.100,0:07:46.120
使用這個 Recurrent Neural Network的時候呢

0:07:46.120,0:07:49.600
你必須要給 memory 起始值

0:07:49.680,0:07:52.960
你必須要給 memory

0:07:53.560,0:07:58.410
在還沒有放進任何東西的時候，
你必須要給它一個初始值，比如說這邊呢

0:07:58.580,0:08:02.600
我們在假設還沒有放進任何東西之前
memory 裡面的值是 0

0:08:03.510,0:08:08.490
那現在輸入第一個輸入 [1 1]

0:08:08.700,0:08:12.280
那接下來會發生甚麼事呢

0:08:12.280,0:08:13.690
對這個 neuron 來說

0:08:13.780,0:08:18.680
它除了接到 input 的 [1 1] 以外

0:08:18.780,0:08:23.420
它還接到 memory 的 [0 0]，
那因為我們說所有的 weight 都是 1

0:08:23.750,0:08:26.760
所以它的 output  就是 2，
那這個 neuron 的 output 也一樣是 2

0:08:28.510,0:08:30.510
那接下來呢

0:08:33.730,0:08:37.940
接下來呢，因為所有的 weight 都是 1

0:08:38.100,0:08:41.500
所以紅色這兩個 neuron 呢，它們的 output 就是 4

0:08:41.500,0:08:46.500
所以 input [1 1] 的時候，它的 output 就是 [4 4]

0:08:48.330,0:08:52.580
接下來呢，Recurrent Neural Network 會把這些

0:08:52.930,0:08:56.280
綠色的 neuron 他的 output 存到 memory 裡面去

0:08:56.280,0:08:57.880
所以呢

0:08:57.950,0:09:02.720
memory 裡面的值就被 update 成 2
這個2呢，會被寫進來；這個2，會被寫進來

0:09:03.140,0:09:07.940
所以 memory 裡面的值就 update 變成 2，接下來呢

0:09:07.960,0:09:11.860
再輸入 1 跟 1，這時候綠色的 neuron會有什麼樣的輸出呢？

0:09:11.860,0:09:13.800
它的輸入有 4 個

0:09:14.040,0:09:18.900
[1 1] 跟 [2 2]，然後 weight 都是 1，
所以你把 2 + 2 + 1 + 1

0:09:19.350,0:09:21.500
得到的結果呢，是 6

0:09:21.500,0:09:22.800
那最後呢

0:09:23.500,0:09:28.920
紅色 neuron 的輸出就是 6 + 6 是 12
所以當輸入 [1 1] 的時候

0:09:29.000,0:09:31.340
第二次再輸 [1 1] 的時候，輸出就是 [12 12]

0:09:31.500,0:09:33.980
所以對 Recurrent Neural Network 來說

0:09:33.980,0:09:38.440
你就算是輸入一樣的東西，你就算給它一模一樣的 input

0:09:38.500,0:09:42.780
在這個 case 裡面都是 1 跟 1

0:09:43.560,0:09:48.320
你就算給它一模一樣的 input
在這個 case 裡面都是 1 跟 1，
它的 output 是有可能會不一樣的

0:09:48.490,0:09:50.490
因為存在 memory 裡面的值呢，是不一樣的

0:09:52.530,0:09:57.220
那原來的值呢，在這個綠色的 neuron 的 output 是 6 跟 6

0:09:57.320,0:10:01.860
那接下來 6 跟 6 呢，就會被存到 memory 裡面去，
就會被存到 memory 裡面去

0:10:01.990,0:10:04.100
所以 2 就被洗掉，變成 6

0:10:04.100,0:10:06.700
那接下來呢，我們的 input 是

0:10:07.040,0:10:11.000
2 跟 2，假設 input 是 [2 2]，這邊每一個綠色的 neuron

0:10:11.320,0:10:15.360
它考慮的 4 個 input ：2 跟 2 跟 6 跟 6

0:10:15.360,0:10:19.300
所以 6 + 6 + 2 + 2 是多少呢，得到的值是 16

0:10:19.300,0:10:22.580
那紅色 neuron 的 output 就是 32

0:10:22.580,0:10:25.140
所以 input 2 跟 2 的時候呢，output 是 32

0:10:26.580,0:10:30.460
那今天在做 Recurrent neural network 的時候呢
有一件很重要的事情就是

0:10:31.390,0:10:36.180
這個 input 的 sequence

0:10:36.380,0:10:40.540
Recurrent neural network 在
考慮它的時候，並不是 independent

0:10:41.230,0:10:45.150
今天如果你任意調換 input sequence 的順序
比如說把 2 跟 2 挪到最前面來

0:10:46.220,0:10:49.140
那 output 呢，是會完全不一樣的

0:10:49.140,0:10:51.080
所以在 Recurrent neural network 裡面，

0:10:51.640,0:10:53.980
它會考慮 input 這個 sequence 的 order

0:10:59.450,0:11:04.270
所以今天如果我們要用 Recurrent neural network 來處理 slot filling 這個問題的話

0:11:04.530,0:11:09.500
它看起來就像是這樣，有一個使用者說
arrive Taipei on November 2nd

0:11:09.950,0:11:14.740
那 arrive 就變成一個 vector，丟到 neural network裡面去

0:11:14.840,0:11:19.520
neural network 的 hidden layer
它的 output 這邊寫成 a1

0:11:19.870,0:11:24.820
這個 a1 是一排 neuron 的 output，所以它其實是一個 vector

0:11:24.900,0:11:26.440
然後根據這個 a1，我們產生 y1

0:11:26.440,0:11:31.400
這個 y1 就是 arrive 屬於哪一個 slot 的機率

0:11:31.400,0:11:34.500
接下來呢，a1 會被存到 memory 裡面去

0:11:34.930,0:11:37.700
接下來呢，Taipei 會變成 input

0:11:39.810,0:11:44.540
那這個 hidden layer 會同時考慮 Taipei 這個 input

0:11:44.630,0:11:49.300
跟存在 memory 裡面的 a1，得到 a2

0:11:49.480,0:11:54.350
然後再根據 a2 產生 y2
y2 是 Taipei 屬於哪一個 slot 的機率

0:11:54.930,0:11:57.180
這個 process 呢，就以此類推

0:11:57.180,0:11:59.140
我們再把 a2 存到 memory 裡面

0:11:59.140,0:12:00.680
再把 on 丟進去

0:12:00.900,0:12:04.920
那 hidden layer 同時考慮 input "on" 這個詞彙的 vector

0:12:04.980,0:12:09.500
跟存在 memory 裡面的 a2，得到 a3，然後a3 再得到 y3

0:12:09.680,0:12:14.470
它代表 on 屬於哪一個 slot 的機率，
那這邊要注意的事情是

0:12:14.540,0:12:19.160
有人看到這個圖就說，這邊有3個 network

0:12:19.420,0:12:23.300
這個不是3個 network，這個是同一個 network

0:12:24.460,0:12:28.830
在三個不同的時間點，被使用了3次

0:12:29.390,0:12:34.350
我這邊呢，特別把同樣的 weight，用同樣的顏色來表示

0:12:34.730,0:12:38.170
同樣的 weight，就用同樣的顏色來表示，
希望大家看得出來

0:12:40.920,0:12:45.690
那，所以如果我們有了 memory 以後

0:12:45.750,0:12:50.610
剛才我們講的輸入同一個詞彙，
我們希望它 output 不同的這個問題

0:12:50.740,0:12:55.640
就有可能被解決，比如說，
如果同樣是輸入 Taipei 這個詞彙

0:12:56.060,0:13:00.960
但是因為紅色 Taipei 前面接的是 leave，
綠色 Taipei 前面接的是 arrive

0:13:01.080,0:13:03.420
因為 leave 跟 arrive 它們的 vector 不一樣

0:13:03.420,0:13:05.680
所以 hidden layer 的 output 也會不同

0:13:06.240,0:13:11.100
所以存在 memory 裡面的值呢，也會不同

0:13:11.200,0:13:16.080
雖然現在 x2 是一模一樣的
但是因為存在 memory 裡面的值不同

0:13:16.290,0:13:20.800
所以 hidden layer 的 output 也會不一樣
所以最後的 output 也就會不一樣

0:13:22.530,0:13:27.380
好那這個是 Recurrent Neural Network 的基本觀念

0:13:29.160,0:13:34.040
當然 Recurrent Neural Network 的架構
你是可以任意設計的

0:13:34.180,0:13:39.100
比如說，它當然可以是 deep，我們剛才看到
 Recurrent Neural Network 它只有一個 hidden layer

0:13:39.860,0:13:44.600
當然它可以是 deep 的 Recurrent Neural Network

0:13:44.890,0:13:46.800
比如說，我們把 x1 丟進去以後

0:13:46.800,0:13:49.790
它可以通過一個 hidden layer，在通過第二個 hidden layer

0:13:50.110,0:13:54.780
以此類推，通過很多個 hidden layer 以後，
才得到最後的 output

0:13:55.130,0:13:59.420
那每一個 hidden layer 的 output 
都會被存在 memory 裡面

0:13:59.510,0:14:04.190
在下一個時間點的時候呢，每一個 hidden layer

0:14:04.360,0:14:08.010
會再把前一個時間點存的值，再讀出來

0:14:08.240,0:14:13.200
再把前一個時間點存的值呢，再讀出來，最後得到

0:14:13.280,0:14:15.280
最後的 output，這個 process 就一直持續下去

0:14:15.280,0:14:17.670
這個 deep 你要疊幾層呢

0:14:18.320,0:14:20.320
都是可以的

0:14:21.400,0:14:26.060
那 Recurrent Neural Network 有不同的變形
我們剛才講的呢

0:14:26.600,0:14:31.400
叫做 Elman Network，
如果我們今天是把 hidden layer的值

0:14:31.530,0:14:34.800
存起來，在下一個時間點再讀出來

0:14:35.200,0:14:39.920
那麼這個叫做 Elman Network，那有另外一種呢

0:14:40.210,0:14:41.440
叫做 Jordan Network

0:14:41.440,0:14:46.700
Jordan Network 它存的是整個 network 的 output 的值

0:14:46.760,0:14:50.050
那它再把 output 的值，在下一個時間點

0:14:50.520,0:14:55.510
再讀進來，它是把 output 的值存在 memory 裡面

0:14:55.730,0:14:59.500
那傳說呢，Jordan Network
可以得到比較好的 performance

0:14:59.560,0:15:00.420
因為

0:15:01.600,0:15:05.200
這邊的 hidden layer，它是沒有 target 的

0:15:05.200,0:15:08.920
所以有點難控制說
它學到什麼樣的 hidden 的 information

0:15:08.920,0:15:11.300
它學到把甚麼東西放到 memory 裡面

0:15:11.300,0:15:16.280
但是這個 y ，它是有 target 的
所以我們今天可以比較清楚我們

0:15:16.330,0:15:18.460
放在 memory 裡面的，是甚麼樣的東西

0:15:21.800,0:15:25.840
這個 Recurrent Neural Network ，
它還可以是雙向的

0:15:26.470,0:15:29.060
甚麼意思呢

0:15:29.060,0:15:31.230
我們剛才看到 Recurrent Neural Network

0:15:31.390,0:15:36.260
你 input 一個句子的話，它就是從句首一直

0:15:36.440,0:15:40.700
讀到句尾，假設句子裡面的每一個詞彙
我們都用 x^t 來表示它的話

0:15:41.540,0:15:45.950
它就是先讀 x^t，再讀 x^(t+1)，再讀 x^(t+2)

0:15:47.440,0:15:52.100
但是，其實它的讀取方向也可以是反過來

0:15:52.910,0:15:57.860
它可以先讀 x^(t+2)，再讀 x^(t+1)

0:15:57.930,0:16:00.320
再讀 x^t

0:16:00.320,0:16:05.600
你可以同時 train 一個正向的 Recurrent Neural Network

0:16:05.660,0:16:09.500
又同時 train 一個逆向的 Recurrent Neural Network

0:16:09.660,0:16:12.330
然後把這兩個 Recurrent Neural Network

0:16:13.900,0:16:17.680
的 hidden layer 拿出來

0:16:17.780,0:16:21.700
把這兩個 Recurrent Neural Network 的 
hidden layer 拿出來，都接給一個 output layer

0:16:21.700,0:16:22.900
得到最後的 y

0:16:23.140,0:16:29.000
所以你把正向的 network，在 input x^t 的時候的 output

0:16:29.180,0:16:33.180
跟逆向的 network，在 input x^t 的時候的 output

0:16:33.740,0:16:38.070
都丟給 output layer

0:16:38.290,0:16:42.500
讓 output layer 產生 y^t
然後產生 y^(t+1) 完，產生 y^(t+2)

0:16:43.560,0:16:48.380
那用 Bidirectional RNN 的好處呢

0:16:49.250,0:16:54.080
你的 network 呢，它在產生 output 的時候，它看的範圍

0:16:54.230,0:16:58.620
是比較廣的，如果今天你只有正向的 network

0:17:00.700,0:17:04.370
在產生 y^t , 跟 y^(t+1) 的時候

0:17:04.480,0:17:10.600
你的 network 只看過 x1 一直到 x^(t+1) 的部分

0:17:10.660,0:17:14.380
但是如果我們今天是 Bidirectional 的 RNN

0:17:15.210,0:17:19.660
在產生 y^(t+1) 的時候，你的 network 不只是看了

0:17:19.890,0:17:24.830
x1 到 x^(t+1) 所有的 input，它也看了從句尾

0:17:25.090,0:17:27.090
一直到 x^(t+1) 的 input

0:17:27.090,0:17:31.600
那你的 network 等於是看了整個 input 的 sequence

0:17:31.620,0:17:34.600
假設你今天考慮的是 slot filling 的話

0:17:34.800,0:17:40.010
你的 network 等於是看了整個 sentence 以後，
才決定每一個詞彙的 slot 應該是甚麼

0:17:40.880,0:17:45.550
那這樣當然會比只看句子的一半，
有更好的 performance

0:17:48.060,0:17:52.680
那我們剛才講的 Recurrent Neural Network，其實只是

0:17:52.770,0:17:57.430
Recurrent Neural Network 一個最 simple 的版本

0:17:57.780,0:18:02.520
那其實只是一個最 simple 的版本

0:18:02.740,0:18:06.340
那我們剛才講的 memory 呢，是最單純的

0:18:06.340,0:18:10.100
就是我們隨時都可以把值存到 memory 裡面去

0:18:10.180,0:18:14.300
也可以隨時把值從 memory 裡面讀出來

0:18:14.380,0:18:17.100
但現在比較常用的 memory 呢

0:18:17.160,0:18:21.220
稱之為 Long Short-term 的 memory

0:18:22.150,0:18:27.150
這種 Long Short-term 的 memory 呢
它的簡寫是 LSTM

0:18:27.230,0:18:29.480
這種 Long Short-term 的 memory 呢
它是比較複雜的

0:18:29.480,0:18:32.060
這個 Long Short-term 的 memory 呢
它有 3 個 gate

0:18:32.800,0:18:37.590
當外界，當 neural network 的其他部分

0:18:37.900,0:18:42.130
某個 neuron 的 output 想要被寫到 memory cell 裡面的時候

0:18:42.770,0:18:47.590
它必須先通過一個閘門，通過一個 input gate

0:18:47.860,0:18:51.560
那這個 input gate 呢，它要被打開的時候

0:18:51.560,0:18:55.700
你才能夠把值寫到 memory cell 裡面去

0:18:55.700,0:18:57.920
如果它被關起來的時候，

0:18:58.040,0:19:01.920
其他 neuron 就沒有辦法把值寫進去

0:19:01.920,0:19:05.800
那至於這個 input gate 它是打開還是關起來

0:19:06.480,0:19:09.100
這個是 neural network 自己學

0:19:09.160,0:19:12.330
所以它可以自己學說甚麼時候要把 input gate 打開

0:19:12.450,0:19:15.000
甚麼時候要把 input gate 關起來

0:19:16.800,0:19:18.400
那輸出的地方呢

0:19:18.400,0:19:20.900
輸出的地方也有一個 output gate

0:19:20.900,0:19:22.550
那這個 output gate 會決定說

0:19:22.870,0:19:27.610
外界、其他的 neuron 
可不可以從 memory 裡面把值讀出來

0:19:28.900,0:19:33.710
那當 output gate 關閉的時候，就沒有辦法把值讀出來

0:19:33.940,0:19:36.260
只有 output gate 被打開的時候，才可以把值讀出來

0:19:36.260,0:19:38.440
那跟 input gate 一樣，output gate 甚麼時候是打開

0:19:38.760,0:19:42.500
甚麼時候是關起來，network 也是自己學到

0:19:45.720,0:19:50.120
那還有第三個 gate 呢，叫做 forget gate

0:19:50.280,0:19:54.470
那 forget gate 是決定說，甚麼時候 memory

0:19:54.970,0:19:58.460
要把過去記得的東西忘掉

0:19:58.460,0:20:02.800
或是它甚麼時候要把過去記得的東西做一下 format

0:20:02.900,0:20:04.650
把它 format 掉

0:20:05.020,0:20:09.820
那這個 forget gate 甚麼時候
會把存在 memory 裡面的值 format 掉

0:20:10.070,0:20:12.460
甚麼時候會把存在 memory 裡面的值繼續保留下來

0:20:12.460,0:20:14.830
這個也是 network 自己學到的

0:20:15.950,0:20:20.790
那整個 LSTM 呢，可以看成它有4個 input

0:20:21.350,0:20:26.120
1 個 output，這 4 個input 是甚麼呢，一個是

0:20:26.350,0:20:28.320
想要被存到 memory cell 裡面的值

0:20:28.320,0:20:33.000
然後它不一定存得進去，這 depend on 
input gate 要不要讓這個 information 過去

0:20:33.100,0:20:35.140
跟操控 input gate 的這個訊號

0:20:36.250,0:20:39.340
操控 output gate 的這個訊號
和操控 forget gate 的訊號

0:20:39.340,0:20:41.200
所以一個 LSTM 的 cell 呢

0:20:41.260,0:20:47.000
它有 4 個 input，那有這 4 個 input
但它只會得到一個 output

0:20:57.190,0:21:01.770
那這邊有一個小小的冷知識

0:21:02.200,0:21:07.040
就是這個 dash，你覺得它應該被放在哪裡

0:21:08.570,0:21:13.430
我投影片把它放在這邊，但並不代表我投影片就是對的

0:21:13.510,0:21:16.220
我有可能只是突然發現我寫錯了想要改一下這樣子

0:21:16.220,0:21:18.470
好你覺得這個 dash

0:21:18.800,0:21:22.360
應該放在 long 和 short 之間的舉手一下

0:21:22.360,0:21:23.690
沒有

0:21:23.800,0:21:26.500
那你覺得它應該放在 short 跟 term 之間的同學舉手一下

0:21:27.670,0:21:31.000
好，手放下，沒錯它應該放在 short 跟 term 之間

0:21:31.000,0:21:33.540
有時候我會看到有人放在 long 和 short 之間

0:21:33.540,0:21:37.040
那其實這是比較不 make sense 的
應該放在 short 跟 term 之間

0:21:37.140,0:21:41.530
因為它其實呢，還是一個 short-term 的 memory

0:21:42.630,0:21:47.200
它只是比較長的 short-term memory

0:21:47.480,0:21:51.390
所以按照這個字面意思，
它是比較長的 short-term memory

0:21:54.820,0:21:57.420
因為之前我們看那個 Recurrent neural network 阿

0:21:57.420,0:21:59.770
它的 memory 在每一個時間點

0:21:59.900,0:22:04.650
都會被洗掉

0:22:04.720,0:22:08.980
只要每次有新的 input 進來，在每一個時間點呢

0:22:09.090,0:22:13.660
Recurrent neural network 都會把 memory 洗掉
所以這個 short-term 是非常 short

0:22:14.380,0:22:17.160
它只記得前一個時間點的事情

0:22:17.160,0:22:19.240
但是如果是 long short-term 的話，它可以記得

0:22:20.160,0:22:25.150
比較長一點，只要 forget gate不要決定要 format 的話

0:22:25.430,0:22:27.430
它的值就會被存起來

0:22:28.520,0:22:32.660
好那這個 memory 的 cell 呢，如果

0:22:33.550,0:22:38.120
更仔細地來看它的 formulation的話，它長得像這樣

0:22:38.670,0:22:43.230
這個是外界的 input，
外界要存到 cell 裡面的 input，這個是 input gate

0:22:43.750,0:22:45.750
這個是 forget gate, 這個是 output gate

0:22:47.810,0:22:52.700
那我們假設呢，現在讓要被存到 cell 裡面的 input 叫做 z

0:22:53.440,0:22:56.860
操控 input gate 的 signal 叫做 zi

0:22:56.860,0:23:01.000
好這個所謂操控 input gate 的 signal，
它也就是一個 scalar

0:23:01.200,0:23:03.110
也是一個數值

0:23:03.490,0:23:05.460
那我們等一下會講說這個數值它是從哪裡來的

0:23:07.000,0:23:08.380
反正這邊就是有一個數值

0:23:08.550,0:23:13.320
被當作這個 cell 的 input，好那這個 forget gate

0:23:13.530,0:23:17.780
有一個操控它的數值是 zf，
output gate 有一個操控它的數值是 zo

0:23:18.460,0:23:23.100
綜合這些東西以後，最後會得到一個 output

0:23:23.550,0:23:25.000
這邊寫作 a

0:23:25.400,0:23:28.490
好，假設我們現在 cell 裡面呢

0:23:29.080,0:23:33.870
在輸入這些，在有這 4 個輸入之前

0:23:34.370,0:23:36.740
它裡面已經存了值 c

0:23:36.740,0:23:39.270
那現在呢，假設要輸入

0:23:39.460,0:23:41.100
輸入的部分呢，輸入z

0:23:41.100,0:23:44.440
那三個 gate 呢，分別是由zi, zo, zf 所操控的

0:23:44.660,0:23:46.860
那 output a 會長甚麼樣子呢？

0:23:46.860,0:23:49.570
我們把 z 呢，通過一個 activation function

0:23:49.910,0:23:53.860
得到 g(z)，然後把 zi 通過

0:23:54.220,0:23:57.200
另外一個 activation function，得到 f(zi)

0:23:57.300,0:23:58.260
那這邊呢

0:23:59.940,0:24:04.440
這3個 zi, zf, zo 他們通過的這3個 activation function f 阿

0:24:04.700,0:24:09.560
通常我們會選擇 sigmoid function
那選擇 sigmoid function 它的意義就是

0:24:09.660,0:24:14.600
sigmoid 的值是介在 0~1 之間的，而這個 0~1 之間的值

0:24:14.970,0:24:17.140
代表了這個 gate 被打開的程度

0:24:17.140,0:24:19.760
如果這個 f 的 output

0:24:20.160,0:24:24.920
這個 activation function 的 output 是 1
代表這個 gate 是處於被打開的狀態

0:24:24.980,0:24:29.460
反之呢，代表這個 gate 是被關起來的

0:24:33.800,0:24:38.720
接下來呢，我們就把 g(z) 乘上這個 input gate 的值

0:24:38.860,0:24:43.440
f(zi) 得到 g(z)*f(zi)

0:24:44.770,0:24:47.040
那這個 forget gate 的 zf 呢

0:24:47.040,0:24:50.900
zf 這個 signal 也通過這個 sigmoid activation function

0:24:51.000,0:24:54.740
得到 f(zf)，接下來呢

0:24:55.690,0:25:00.550
我們把存在 memory 裡面的值

0:25:01.000,0:25:05.820
c 乘上 f(zf)

0:25:05.890,0:25:09.900
得到 c*f(zf)，然後接下來

0:25:11.190,0:25:16.160
把 c*f(zf) 加上 g(z)*f(zi)

0:25:16.250,0:25:21.110
把這兩項加起來，得到 c'

0:25:21.260,0:25:25.610
c' 就是新的存在 memory 裡面的值

0:25:25.830,0:25:30.530
新的存在 memory 裡面的值就是 c'，所以

0:25:30.630,0:25:35.270
根據到目前為止的運算可以發現說呢
這個 f(zi) 就是 control 這個 g(z)

0:25:35.640,0:25:40.580
可不可以輸入的一個關卡，因為假設 f(zi) = 0

0:25:41.360,0:25:46.050
那 g(z) * f(zi) 就等於 0，就好像是沒有輸入一樣

0:25:46.150,0:25:50.680
若 f(zi) 是等於 1，那就等於是直接把 g(z) 當作輸入

0:25:51.780,0:25:56.490
那這個 f(zf)，就是決定說我們要不要

0:25:57.520,0:26:01.640
把存在 memory 裡面的值洗掉，假設 f(zf) 是  1

0:26:06.000,0:26:07.650
假設 f(zf) 是 1

0:26:07.850,0:26:09.260
也就是 forget gate 是被開啟的時候

0:26:09.260,0:26:12.780
forget gate 被開啟的時候呢，這個時候

0:26:12.970,0:26:17.860
c 會直接通過，就等於是

0:26:18.080,0:26:23.050
把之前存的值，還是記得，
那如果是 f(zf) = 0

0:26:23.150,0:26:28.150
也就是 forget gate 被關閉的時候， 0 乘上 c 
過去存在 memory  裡面的值呢，就會變成 0

0:26:29.640,0:26:34.630
然後把這兩個值呢，加起來當作

0:26:34.850,0:26:39.830
我們把這兩個值加起來，寫到 memory 裡面得到 c'

0:26:39.900,0:26:44.480
那 forget gate 它的開關呢，跟我們直覺的想法

0:26:44.850,0:26:49.720
是相反的，這個 forget gate 它打開的時候代表

0:26:49.970,0:26:54.920
是記得，它被關閉的時候，代表的是遺忘

0:26:54.990,0:26:59.690
所以我覺得它的名字取的有點怪，
或許不該叫它 forget gate

0:26:59.880,0:27:03.940
不過反正，習慣上呢，就是這麼做

0:27:05.010,0:27:09.760
好那把這個 c' 通過 h，得到 h(c')

0:27:10.090,0:27:13.440
然後接下來呢，這邊有一個 output gate

0:27:13.440,0:27:15.800
這個 output gate 受 zo 所操控

0:27:15.820,0:27:19.560
zo 通過 f 得到 f(zo)

0:27:19.760,0:27:24.580
f(zo) 如果是 1 的話，那這邊我們會把 f(zo)

0:27:24.640,0:27:29.300
跟這個 h(c') 乘起來，如果 f(zo)是 1，就等於是

0:27:29.360,0:27:33.370
h(c') 可以通過這個 output gate，如果 f(zo) 是 0

0:27:34.320,0:27:39.180
就等於這個 output 會變成0
就代表存在 memory 的值呢

0:27:39.290,0:27:44.020
沒有辦法通過 output gate，被讀取出來

0:27:44.220,0:27:48.850
也許這樣你還是沒有很清楚，
所以後面呢，我打算做一個韌體 LSTM

0:27:49.500,0:27:54.100
我從來沒有在其他地方看過韌體 LSTM

0:27:54.100,0:27:56.920
所以你可以想到我這個投影片是做很久

0:27:59.940,0:28:02.900
那我們先講一下我們要舉的例子

0:28:02.900,0:28:04.600
等一下我們要舉的例子是這樣子

0:28:04.940,0:28:09.630
在 network 裡面，只有一個 LSTM 的 cell

0:28:10.160,0:28:12.640
那我們的 input 都是三維的 vector

0:28:12.640,0:28:16.200
output 都是一維的 vector

0:28:19.390,0:28:24.110
那這個三維的 vector，它跟 output 還有 
memory 裡面的值的關係是甚麼呢

0:28:24.580,0:28:26.060
這個關係是這樣子

0:28:28.000,0:28:29.190
假設

0:28:29.280,0:28:33.010
第二個 dimension x2 的值是 1 的時候

0:28:33.680,0:28:37.160
x1 的值就會被寫到 memory 裡面去

0:28:37.160,0:28:41.200
那x2 是1的時候，x1的值就會被存到 memory 裡面去

0:28:41.300,0:28:44.070
假設 x2 的值是 -1 的時候

0:28:44.130,0:28:46.760
memory 就會被 reset

0:28:48.290,0:28:51.080
memory 存的值就會被遺忘

0:28:51.080,0:28:53.230
假設 x3 等於 1 的時候

0:28:53.290,0:28:57.200
你才會把 output gate 打開，才能夠看到輸出

0:28:58.000,0:28:59.000
所以呢

0:29:01.500,0:29:04.420
假設我們原來存在 memory 裡面的值是 0

0:29:05.340,0:29:09.670
那當這邊是1的時候

0:29:10.020,0:29:14.780
當 x2 = 1 的時候，3會被存到 memory 裡面去

0:29:15.020,0:29:16.520
這裡得到的值呢，就變成 3

0:29:16.520,0:29:19.860
那這邊又出現一次 1

0:29:19.940,0:29:24.160
所以 4 會被存到 memory 裡面去，所以就得到 7

0:29:25.390,0:29:30.100
x3 = 1，所以 7 會被輸出

0:29:30.220,0:29:35.030
所以得到 7，那這邊是 -1，如果是 -1 的話呢

0:29:35.250,0:29:40.200
就會把 memory 裡面的值洗掉
所以看到 -1，下一個時間點的值就變成 0

0:29:40.310,0:29:44.890
然後看到 1 就會把 6 存進去，所以得到的值是 6

0:29:44.950,0:29:48.290
這邊 1呢是輸出，所以得到的值是 6

0:29:49.670,0:29:54.490
那我們就來實際做一下運算

0:29:54.950,0:29:59.050
那這個是一個 memory cell，一個 LSTM 的 memory cell

0:29:59.980,0:30:04.320
那我們知道 LSTM 的 memory cell 呢，總共有 4 個 input

0:30:04.710,0:30:06.720
這 4 個 input 都是 scalar

0:30:06.720,0:30:09.480
這 4 個 input 的 scalar 是怎麼來的呢

0:30:09.780,0:30:14.600
這 4 個 scalar是我們 input 的三維的 vector 乘上一個

0:30:16.690,0:30:20.600
linear 的 transform 以後，所得到的結果

0:30:20.600,0:30:23.600
你就把這 3 個 vector 乘上這 3 個值

0:30:23.740,0:30:28.460
再加上 bias，就得到這邊的 input，這三個值

0:30:28.960,0:30:33.650
再乘上三個 weight，再加上 bias，就得到它的 input

0:30:34.040,0:30:36.040
以此類推

0:30:38.920,0:30:43.840
那這些值，就是 input 的 x1, x2, x3

0:30:43.930,0:30:48.760
要乘上哪些值，還有 bias 的值應該要是多少這件事情呢

0:30:48.980,0:30:53.930
是透過 training data，透過 gradient descent 去學到的

0:30:54.380,0:30:59.300
那我們今天只是假設說，我已經知道這些值是多少

0:30:59.480,0:31:04.270
然後我現在用這樣的輸入，它會得到怎麼樣的輸出

0:31:04.380,0:31:08.830
那我們就來實際地運算一下，
不過在實際運算之前

0:31:09.200,0:31:13.830
我們先根據它的 input，根據這些參數呢

0:31:13.920,0:31:18.000
來分析一下我們可能會得到的結果，
那你看，在這個地方

0:31:18.900,0:31:23.620
x1 乘 1，其他都是乘 0，所以呢

0:31:23.680,0:31:28.210
這邊呢，就是直接把 x1 當作輸入
好，那我們看 input gate 的地方

0:31:28.320,0:31:32.330
它是 x2 * 100

0:31:33.760,0:31:38.720
bias 是 -10，也就是說呢，假設 x2 沒有值的時候

0:31:38.810,0:31:43.130
因為 bias 是 -10，所以通常input gate 呢，是被關閉的

0:31:44.300,0:31:48.880
如果 bias 是 -10的話，那通過 activation function 以後呢

0:31:49.300,0:31:54.040
通過 sigmoid 的 activation function 之後
它的值會接近 0

0:31:54.220,0:31:56.220
所以呢，代表它是被關閉的

0:31:57.840,0:32:02.640
那只有在 x2 有值的時候，如果 x2 有值
它就會比 bias 的這個 -10 還要大

0:32:03.140,0:32:08.060
如果 x2 代 1的話呢，它就會比 bias大
這個時候呢，input 就會是很大的正值

0:32:08.210,0:32:10.540
代表 input gate 被打開

0:32:10.540,0:32:13.110
那 forget gate 呢

0:32:13.190,0:32:18.080
forget gate 平常都是被打開的，你會發現說，因為它 bias 是 10

0:32:18.370,0:32:22.990
所以平常呢，它都是被打開的
所以平常都會一直記得東西，只有在 x2

0:32:23.270,0:32:27.560
給它一個很大的負值的時候，它會壓過這個 bias
才會把 forget gate 關起來

0:32:27.560,0:32:29.000
那 output gate 呢

0:32:30.940,0:32:34.980
output gate 平常也都是被關閉的，
因為它的 bias 是很大的負值

0:32:34.980,0:32:37.200
但是如果今天 x3 有一個很大的正值的話

0:32:37.840,0:32:40.000
它就可以壓過 bias，把 output gate 打開

0:32:40.440,0:32:44.700
所以我們就實際地來 input 一下看看

0:32:44.850,0:32:49.600
我們假設 g 跟 h 都是 linear 的，這樣計算比較方便

0:32:50.670,0:32:54.890
假設存在 memory 裡面的初始值是 0

0:32:55.130,0:32:59.680
好那我們現在 input 第一個 vector [3 1 0]

0:32:59.940,0:33:05.180
那 input [3 1 0] 會發生甚麼事呢，
3 乘上 1，所以這邊進來的值是 3

0:33:05.180,0:33:11.440
然後 1 乘 100 減 100，所以這邊的 input gate 約等於 1

0:33:11.440,0:33:13.820
所以它是被打開的，那 forget gate 呢

0:33:14.440,0:33:19.440
1*3，通過 input gate 以後得到的值是 3

0:33:19.520,0:33:24.180
那 forget gate 呢，input 是 [3 1 0]，forget gate 呢

0:33:25.070,0:33:29.040
是被打開的，所以 forget gate 是被打開的

0:33:29.130,0:33:33.890
把 0 乘上 1 加上 3，所以 forget gate 是被打開的
不過裡面本來就沒有存值

0:33:34.020,0:33:38.810
也沒有甚麼影響，0 * 1 + 3
所以存在 memory 裡面的值變成 3

0:33:39.240,0:33:42.990
然後接下來呢，看 output gate，[3 1 0]

0:33:44.240,0:33:49.020
output gate 還是被關起來的，3無法通過，所以輸出就是 0

0:33:49.300,0:33:51.300
好，接下來呢 input

0:33:53.300,0:33:58.240
input [4 1 0]，這個 input 的地方還是 4

0:33:58.480,0:34:03.320
然後這個 [4 1 0] 會把 input gate 打開

0:34:03.380,0:34:08.360
forget gate 也會被打開

0:34:08.450,0:34:13.350
forget gate 被打開的關係，所以 3 * 1 + 4
所以 memory 裡面存的值會變成 7

0:34:13.790,0:34:18.720
那 output 仍然是被關閉的

0:34:18.960,0:34:23.600
所以 7 呢，仍然無法被輸出，
所以整個 memory 的輸出仍然是 0

0:34:26.950,0:34:30.660
那接下來呢，input [2 0 0]，會發生甚麼事呢？

0:34:30.940,0:34:35.300
input [2 0 0 ]，所以現在 input 變成 2

0:34:36.190,0:34:40.690
這個 input gate 會怎樣呢，input gate 現在是 [2 0 0]

0:34:41.030,0:34:45.910
所以它 activation function 的 input 是 -10

0:34:45.980,0:34:50.770
所以 output 是趨近於0，0 * 2 = 0

0:34:51.760,0:34:56.690
等於 input 的 2 被 input gate 擋住了，那 forget gate 呢

0:34:57.140,0:35:01.570
[2 0 0] 得到的 forget gate，得到
activation function 的 input 是 10

0:35:01.900,0:35:06.530
所以 forget gate 還是打開的，所以 7 * 1 + 0

0:35:06.690,0:35:11.540
原來存在 memory 裡面的值是不動的，還是 7

0:35:11.660,0:35:16.000
那這個 7 它沒有辦法被輸出，因為 output gate
仍然是關閉的，所以 output 仍然是 0

0:35:16.560,0:35:20.910
好接下來呢，input 是 [1 0 1]

0:35:21.620,0:35:25.980
那 input [1 0 1]會發生甚麼事呢？這邊 input 仍然是 1

0:35:26.850,0:35:31.580
那這個 input gate 是被關閉的，那 forget gate 呢

0:35:31.710,0:35:36.050
forget gate 這個時候仍然跟原來一樣，它是被打開的

0:35:36.370,0:35:39.140
所以 memory 裡面存的值是不變的

0:35:39.140,0:35:42.120
那 output gate 呢，當你 input [1 0 1] 的時候

0:35:42.120,0:35:46.360
你會打開 output gate，這時候 activation function 
的 input 變成 90

0:35:46.670,0:35:49.740
通過 activation function 以後呢，得到 1

0:35:51.560,0:35:56.120
那 1 * 7 = 7 這樣子

0:35:56.290,0:36:01.220
所以 output 的地方會變成是有值的，
memory 裡面的值呢

0:36:01.480,0:36:06.450
存在 memory 裡面的值 7 呢，會被讀取出來

0:36:06.840,0:36:10.700
最後，讓我們試一下 [3 -1 0]

0:36:12.360,0:36:15.510
[3 -1 0] 這個 3 呢，就被讀進來

0:36:16.900,0:36:21.490
input gate 會被關起來，那 forget gate 呢？

0:36:23.120,0:36:29.320
因為這個值是 -1，所以 forget gate 的
activation function input 是 -90

0:36:29.320,0:36:32.440
activation output 就是 0

0:36:32.550,0:36:36.150
所以呢， memory 裡面存的值會被洗掉

0:36:36.320,0:36:40.320
memory 裡面存的值會乘上 forget gate 的 output，
會被洗掉變成 0

0:36:40.460,0:36:46.360
那 output gate 呢，這時候仍然是關起來的，不過它有開沒開也沒差，因為反正現在存在 memory 裡面的值變成 0

0:36:46.420,0:36:49.820
那它讀出來的值也是 0

0:36:51.900,0:36:56.560
好那你看到這邊你可能會有一個問題，這個東西

0:36:56.690,0:37:01.410
跟我們原來看到的 neural network 感覺很不像阿

0:37:01.490,0:37:05.840
它跟原來的 neural network 到底有甚麼樣的關係呢，
你可以這樣想

0:37:07.910,0:37:12.210
在我們原來的 neural network 裡面會有很多的 neuron

0:37:12.800,0:37:19.320
我們會把 input 乘上很多不同的 weight

0:37:19.500,0:37:22.600
然後當作是不同 neuron 的輸入

0:37:22.720,0:37:25.460
然後每一個 neuron 它都是一個 function

0:37:25.460,0:37:28.920
它輸入一個 scalar，output 另外一個 scalar

0:37:28.920,0:37:32.020
但是如果是 LSTM 的話呢

0:37:32.250,0:37:37.130
你其實只要把那個 LSTM 的那個 memory cell 
想成是一個 neuron 就好

0:37:37.570,0:37:41.690
所以如果我們今天要用一個 LSTM 的 network

0:37:42.720,0:37:47.610
你做的事情只是把原來一個簡單的 neuron

0:37:47.740,0:37:51.660
換成一個 LSTM 的 cell

0:37:52.980,0:37:57.910
而現在的 input x1, x2 它會乘上不同的 weight

0:37:57.970,0:38:02.890
當作 LSTM 的不同的輸入

0:38:02.960,0:38:06.790
也就是說 x1, x2 乘上某一組 weight，變成

0:38:07.850,0:38:11.330
假設我們現在這個 hidden layer 只有兩個 neuron

0:38:12.550,0:38:16.700
也就是只有兩個 LSTM，但實際上你不會只有兩個 neuron

0:38:17.150,0:38:21.780
你可能會有比如說 1000 個 neuron，
1000 個 LSTM 的 memory cell

0:38:22.010,0:38:26.630
現在假設只有兩個neuron，那 x1, x2乘上某一組 weight

0:38:26.840,0:38:31.800
會去操控第一個 LSTM 的 output gate，乘上另外一組 weight，操控第一個 LSTM 的input gate

0:38:31.980,0:38:38.960
乘上一組 weight，當作第一個 LSTM的input，乘上另外一組 weight，當作另外一個 LSTM的forget gate的 input

0:38:38.960,0:38:42.000
第二個 LSTM 也是一樣

0:38:42.000,0:38:46.600
x1, x2 乘上某一組 weight 操控它的 output，它會操控它的 input

0:38:47.270,0:38:51.350
操控它的 output gate，操控它的 input gate，
操控它的 input，操控它的 forget gate 等等

0:38:52.160,0:38:57.660
所以我們剛才講過說 LSTM 它就是有 4 個 input ，1 個 output

0:38:57.720,0:39:01.560
而對一個 LSTM 來說

0:39:02.200,0:39:07.160
它的這 4 個 input 是不一樣的，這 4 個 input 都是不一樣的

0:39:07.190,0:39:12.070
這 4 個 input 都是不一樣的，在原來的 neural network 裡面

0:39:12.130,0:39:17.030
一個 neuron 就是一個 input，一個 output，
在 LSTM 裡面它需要 4 個 input

0:39:17.920,0:39:21.960
它才能夠產生一個 output

0:39:21.960,0:39:27.440
就好像說有的機器呢，他只要插一個電源線它就可以跑，那像 LSTM 呢，它就要插 4 個電源線才能跑

0:39:28.540,0:39:31.020
那所以 LSTM 因為它需要 4 個 input

0:39:31.020,0:39:33.050
而這 4 個 input 都是不一樣的

0:39:33.160,0:39:37.490
所以 LSTM 它需要的參數量，假設你現在用的 neuron 的數目

0:39:37.720,0:39:42.630
假設 LSTM 的 network 跟原來的

0:39:42.850,0:39:47.150
neuron 的 network，他們的 neuron數目是一樣的時候

0:39:47.440,0:39:52.500
LSTM 需要的參數量會是一般的 neural network 的 4 倍

0:39:53.360,0:39:57.720
那從這個圖上，你可以很明顯地

0:39:58.000,0:40:02.650
看出來，一般的 neural network 只需要這個部分的參數，只需要這個部分的參數

0:40:03.040,0:40:07.990
但 LSTM 還要操控另外三個 gate，所以他需要 4 倍的參數

0:40:08.120,0:40:12.810
不過這樣講你可能還是沒有辦法很了解，你沒有辦法體會的可能是跟

0:40:13.400,0:40:17.730
Recurrent Neural Network 的關係是甚麼

0:40:18.550,0:40:23.050
這個好像看起來不太像 Recurrent Neural Network

0:40:23.220,0:40:28.200
所以呢，我們要畫另外一個圖呢來表示它，
你可以想像這個圖呢

0:40:28.260,0:40:32.970
也是要畫非常久，假設我們現在有一整排的

0:40:33.340,0:40:37.760
neuron，假設有一整排的 LSTM

0:40:38.190,0:40:43.110
那這一整排的 LSTM 裡面，
它們每一個人的 memory 裡面

0:40:43.290,0:40:48.150
都存了一個值，每一個 LSTM 的 cell

0:40:48.520,0:40:52.690
它裡面都存了一個 scalar

0:40:53.220,0:40:58.000
把所有的 scalar 接起來，把這些 scalar 接起來

0:40:58.060,0:41:01.720
它就變成一個 vector，這邊寫成 c^(t-1)

0:41:03.870,0:41:08.200
那你可以想乘這邊每一個 memory 它裡面存的 scalar

0:41:08.470,0:41:12.670
就是代表這個 vector 裡面的一個 dimension

0:41:13.270,0:41:17.850
現在，在時間點 t

0:41:18.420,0:41:23.170
input 一個 vector, x^t

0:41:23.360,0:41:27.700
這個 vector，它會先乘上一個 linear 的 transform

0:41:27.810,0:41:32.250
乘上一個 matrix，變成另外一個 vector z

0:41:32.380,0:41:36.510
你把 x^t 乘上一個 matrix 變成 z

0:41:37.490,0:41:41.650
那這個 z，也是一個 vector，
那 z 這個 vector 代表甚麼呢

0:41:42.200,0:41:46.700
z 這個 vector 的每一個 dimension 呢

0:41:47.230,0:41:51.740
z 這個 vector 的每一個 dimension 呢，就代表了操控

0:41:52.270,0:41:56.300
每一個 LSTM 的 input

0:41:57.100,0:42:02.030
所以 z 它的 dimension 就正好是 LSTM 的

0:42:02.100,0:42:06.510
memory cell 的數目，那正好就是它的數目

0:42:06.620,0:42:12.920
那這個 z 的第一維就丟給第一個 cell，
第二維就丟給第二個 cell，以此類推

0:42:12.920,0:42:14.760
希望大家知道我的意思

0:42:15.880,0:42:18.840
好那這個 x^t 會再乘上另外一個 transform

0:42:18.840,0:42:21.620
得到 z^i

0:42:21.620,0:42:25.500
然後這個 z^i 呢，它的 dimension 也跟 cell 的數目一樣

0:42:26.080,0:42:30.950
z^i 的每一個 dimension

0:42:31.210,0:42:36.020
都會去操控一個 memory，所以 z^i 的第一維就是

0:42:36.120,0:42:41.040
去操控第一個 cell 的 input gate，第二維
就是操控第二個 cell 的 input gate，最後一維

0:42:41.120,0:42:45.350
就是操控最後一個 cell 的 input gate，
那 forget gate 呢？

0:42:47.060,0:42:51.580
跟 output gate 也是一樣，這邊就不再贅述

0:42:51.580,0:42:56.680
把 x^t 乘上一個 transform，得到 z^f
z^f 會去操控每一個 forget gate

0:42:56.800,0:43:04.220
然後 x^t 乘上另外一個 transform，得到 z^o
z^o 會去操控每一個 cell 的 output gate

0:43:04.220,0:43:08.740
好，所以我們把 x^t 乘上 4 個不同的 transform

0:43:08.740,0:43:12.040
得到 4 個不同的 vector，這 4 個 vector 的 dimension

0:43:12.340,0:43:17.270
都跟 cell 的數目是一樣的，那這 4 個 vector 合起來

0:43:17.470,0:43:21.270
就會去操控

0:43:23.130,0:43:25.570
這些 memory cell 的運作

0:43:28.100,0:43:32.080
好那我們知道一個 memory cell 就是長這樣

0:43:32.100,0:43:37.180
那現在 input 分別是 z, z^i, z^f, z^o,  那注意一下就是這 4 個

0:43:37.710,0:43:42.680
這 4 個 z 其實都是 vector

0:43:42.820,0:43:47.630
丟到 cell 裡面的值呢，
其實只是每一個 vector 的一個 dimension

0:43:47.880,0:43:52.540
那因為每一個 cell 他們 input 的 dimension 都是不一樣的

0:43:52.700,0:43:57.160
所以他們 input 的值都會是不一樣的，但是

0:43:57.160,0:44:02.300
所有的 cell 是可以共同一起被運算的
怎麼一起共同被運算呢

0:44:02.590,0:44:06.300
我們說 z 要乘上 z^i

0:44:06.430,0:44:10.920
要把 z^i 先通過 activation function，
然後把它跟 z 相乘

0:44:11.850,0:44:16.560
所以我們就把 z^i 先通過 activation function，跟 z 相乘

0:44:16.740,0:44:21.450
這個乘呢，是這個 element-wise 的 product 的意思

0:44:21.640,0:44:26.300
element-wise 的相乘，好那這個 z^f 也要通過

0:44:26.360,0:44:30.790
forget gate 的 activation function，z^f 通過這個 activation function

0:44:31.010,0:44:34.570
它跟之前已經存在 cell 裡面的值呢，相乘這件事情

0:44:36.420,0:44:41.290
它跟原來存在 memory cell 裡面的值相乘，它把它跟它相乘

0:44:41.810,0:44:46.750
然後接下來呢，也要把這兩個值加起來

0:44:47.900,0:44:51.700
你就是把 z^i 跟 z 相乘的值加上 z^f

0:44:52.080,0:44:55.660
跟 c^(t-1) 相乘的值，把他們加起來

0:44:56.480,0:45:01.320
好那 output gate 呢，
z^o 通過 activation function，然後呢

0:45:01.430,0:45:06.360
把這個 output 跟相加以後的結果呢，

0:45:06.700,0:45:11.080
再相乘，最後就得到最後的 output 的 y

0:45:13.100,0:45:18.000
這個時候相加以後的結果，就是 memory 裡面存的值

0:45:18.940,0:45:23.720
相加以後的結果，也就是 memory 裡面存的值
也就是 c^t

0:45:23.990,0:45:26.820
那這 process 呢，就反覆地繼續下去

0:45:26.820,0:45:30.800
在下一個時間點，input x^(t+1)

0:45:30.800,0:45:34.100
然後呢，你把 z 跟 input gate 相乘

0:45:34.810,0:45:39.600
你把 forget gate 跟存在 memory 裡面的值相乘

0:45:39.760,0:45:44.140
然後再把這個值跟這個值加起來，
再乘上 output gate 的值

0:45:44.820,0:45:47.680
然後得到下一個時間點的輸出這樣子

0:45:47.680,0:45:51.860
那你可能覺得說這已經很複雜了，如果你自己
做投影片的話，顯然是要做非常久

0:45:56.380,0:45:59.700
這個不是 LSTM 的最終型態

0:45:59.700,0:46:02.880
這個只是一個 simplified 的 version

0:46:02.900,0:46:05.840
真正的 LSTM 會怎麼做呢

0:46:05.970,0:46:08.060
它會把這個地方

0:46:10.520,0:46:14.400
這個地方的輸出呢，把它接進來

0:46:18.360,0:46:23.960
它會把這個 hidden layer 的輸出把它接進來
當作下一個時間點的 input

0:46:24.080,0:46:28.320
也就是說，下一個時間點操控這些 gate 的值，不是只看

0:46:28.520,0:46:33.160
那個時間點的 input x，也看前一個時間點的 output h

0:46:33.440,0:46:37.480
然後其實還不只這樣，
還會加一個東西呢，叫做 "peephole"

0:46:37.780,0:46:42.920
這個 peephole 是甚麼呢？
這個 peephole 就是把存在 memory cell 裡面的值呢

0:46:43.730,0:46:48.690
也拉過來，所以在操縱 LSTM 的 4個 gate 的時候

0:46:48.850,0:46:53.320
你是同時考慮了 x, 同時考慮了 h, 同時考慮了 c

0:46:53.560,0:46:58.560
你把這 3 個 vector 並在一起，乘上4個
不同的 transform，得到這4個不同的 vector

0:46:58.680,0:47:00.940
再去操控 LSTM

0:47:00.940,0:47:03.300
那 LSTM 通常不會只有一層

0:47:03.300,0:47:06.320
現在胡亂都要疊個五、六層才爽這樣

0:47:06.320,0:47:08.440
所以他就長的大概是這個樣子

0:47:08.800,0:47:13.500
然後每一個第一次看到這個東西的人啊

0:47:13.750,0:47:15.750
他的反應都是這樣子

0:47:16.470,0:47:21.370
就我記得，大家知道 sequence 有 sequence model 嗎

0:47:21.530,0:47:26.340
Google Brain 的 proposed，然後我有聽過他的這個 talk

0:47:26.860,0:47:31.330
他說他第一次看到 LSTM 的時候，
他的想法呢，就跟這個圖上是一樣的

0:47:31.820,0:47:36.650
這個太複雜了，這應該不 work 吧，我認識的每一個人第一次看到 LSTM 都覺得說

0:47:36.930,0:47:41.840
這個應該不 work 這樣，
但是他其實現在還 quite standard 這樣

0:47:42.200,0:47:46.880
當有一個人告訴你說，當有一個人說

0:47:47.060,0:47:51.820
我用 RNN 做了甚麼事情的時候，
就是你不要去問他說為甚麼你不用 LSTM

0:47:51.880,0:47:56.770
因為他其實就是用 LSTM ，
因為現在當你說你在做 RNN 的時候

0:47:57.540,0:47:59.340
其實你指的就是用 LSTM

0:47:59.340,0:48:02.480
所以呢，他其實是比較 standard 的

0:48:02.550,0:48:07.380
其實 Keras 裡面有支援 LSTM 啦，所以就算是

0:48:07.620,0:48:12.420
剛才講得這麼複雜的東西你沒聽懂就算了
在 Keras 裡面就是打 LSTM 這4個字母

0:48:12.480,0:48:14.480
然後就結束了

0:48:17.030,0:48:20.460
那 Keras 它其實支援三種 Recurrent neural networks，一個是 LSTM

0:48:20.460,0:48:21.990
一個是 GRU

0:48:22.050,0:48:25.120
GRU 是 LSTM 的一個稍微簡化的版本

0:48:25.120,0:48:27.040
它只有兩個 gate

0:48:27.140,0:48:32.340
據說少了一個 gate，但是 performance 跟 LSTM
差不多，而且少了 1/3 的參數

0:48:32.480,0:48:34.400
所以比較不容易 over-fitting

0:48:34.400,0:48:40.120
如果你要用一般的、我們這堂課最一開始講的那種
最簡單的 RNN 的話

0:48:40.120,0:48:43.960
也要說是 SimpleRNN 才行這樣子

0:48:45.390,0:48:49.720
好那我想我們今天就上到這邊好了，那我們就下課

0:48:50.500,0:48:51.300
謝謝

0:48:51.300,0:48:56.980
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
