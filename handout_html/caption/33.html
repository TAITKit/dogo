<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.060<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.060,0:00:03.500<br>
那我猜剛才 Policy Gradient 的這個部分<br>
<br>
0:00:03.500,0:00:05.500<br>
你其實沒有聽得太懂<br>
<br>
0:00:05.500,0:00:08.400<br>
就算你聽得懂的式子<br>
<br>
0:00:08.400,0:00:11.060<br>
你也不知道要如何實作<br>
<br>
0:00:11.620,0:00:16.260<br>
好，所以我們來講一下，如果是實際實作的時候<br>
<br>
0:00:16.260,0:00:19.520<br>
你到底是怎麼做的<br>
<br>
0:00:19.520,0:00:21.140<br>
怎麼做呢？<br>
<br>
0:00:21.140,0:00:24.720<br>
首先，這整個大的 picture 是這樣<br>
<br>
0:00:24.720,0:00:29.280<br>
我們先有一個 actor，它的參數是 θ<br>
<br>
0:00:29.280,0:00:33.360<br>
一開始，你就先 random initialize 就好了<br>
<br>
0:00:33.360,0:00:39.060<br>
接下來，你有了這個初始的 actor, θ 以後<br>
<br>
0:00:39.060,0:00:43.300<br>
你就拿這個初始的 actor, θ 去玩 N 次遊戲<br>
<br>
0:00:43.300,0:00:47.620<br>
那玩 N 次遊戲，你就收集到 N 個 trajectory<br>
<br>
0:00:47.620,0:00:50.280<br>
那假設你收集到一個 τ^1 (trajectory 1)<br>
<br>
0:00:50.280,0:00:53.520<br>
那這個 τ^1 裡面有 state 1<br>
<br>
0:00:53.520,0:00:56.420<br>
這個 state 1 採取了 action  a1<br>
<br>
0:00:56.420,0:00:57.860<br>
我把我的雷射筆叫出來<br>
<br>
0:00:57.860,0:01:01.500<br>
state 2 採取了 action a2<br>
<br>
0:01:01.500,0:01:02.760<br>
以此類推<br>
<br>
0:01:02.760,0:01:05.000<br>
然後，玩完這個遊戲以後呢<br>
<br>
0:01:05.000,0:01:08.220<br>
你可以算出一個  total reward, R(τ)<br>
<br>
0:01:08.220,0:01:11.380<br>
那有一個 τ^1，在 τ^2 裡面<br>
<br>
0:01:11.380,0:01:13.640<br>
你也有 τ^2 的 state 1<br>
<br>
0:01:13.640,0:01:16.480<br>
還有在 state 1  採取 action a1<br>
<br>
0:01:16.480,0:01:20.340<br>
你有 τ^2 跟 state 2，你有在 state 2裡面採取 action a2<br>
<br>
0:01:20.340,0:01:24.720<br>
你要可以算出 τ^2 的 reward<br>
<br>
0:01:24.720,0:01:27.000<br>
好，那這個東西都沒有什麼問題<br>
<br>
0:01:27.000,0:01:32.160<br>
你有一個 policy，random 初始的，去<br>
<br>
0:01:32.160,0:01:33.680<br>
這個實作上沒有什麼問題<br>
<br>
0:01:33.680,0:01:35.400<br>
你把 state action 都記錄下來<br>
<br>
0:01:35.400,0:01:36.520<br>
沒有什麼問題<br>
<br>
0:01:36.520,0:01:39.540<br>
好，那有了這一些 data 以後<br>
<br>
0:01:39.540,0:01:44.320<br>
你就拿這一些 data 去 update 你的參數，θ<br>
<br>
0:01:44.320,0:01:46.920<br>
怎麼拿這些 data 去 update 你的 參數 θ 呢<br>
<br>
0:01:46.920,0:01:49.240<br>
就用剛才我們在前一頁投影片裡面<br>
<br>
0:01:49.240,0:01:51.320<br>
推出來的這個式子<br>
<br>
0:01:51.320,0:01:55.300<br>
那等一下，我們會再詳細解釋一下這個式子<br>
<br>
0:01:55.300,0:01:59.500<br>
到底實作上，你要怎麼 implement<br>
<br>
0:01:59.500,0:02:01.500<br>
現在看起來有點複雜<br>
<br>
0:02:01.500,0:02:04.260<br>
或是，式子你懂，實際上要怎麼做<br>
<br>
0:02:04.260,0:02:06.480<br>
你也許不太清楚<br>
<br>
0:02:06.480,0:02:10.220<br>
那沒關係，反正就是，我們收集，我們等一下再解釋<br>
<br>
0:02:10.220,0:02:12.340<br>
反正我們就是收集到一堆 data 以後<br>
<br>
0:02:12.340,0:02:16.980<br>
我們可以拿這一些 data 去 update 我們的參數<br>
<br>
0:02:16.980,0:02:18.980<br>
update 完參數以後<br>
<br>
0:02:18.980,0:02:21.920<br>
你有了一個新的 actor<br>
<br>
0:02:21.920,0:02:23.700<br>
你有了一個新的 actor 以後<br>
<br>
0:02:23.700,0:02:25.360<br>
你再去玩 N 次遊戲<br>
<br>
0:02:25.360,0:02:27.340<br>
因為 actor 是新的<br>
<br>
0:02:27.340,0:02:28.940<br>
它跟之前的 actor 不一樣了<br>
<br>
0:02:28.940,0:02:31.080<br>
所以在玩 N 次遊戲的時候<br>
<br>
0:02:31.080,0:02:34.120<br>
你可能會得到不太一樣的分布<br>
<br>
0:02:34.120,0:02:36.040<br>
你會得到一個不太一樣的結果<br>
<br>
0:02:36.040,0:02:38.700<br>
你再把這個結果收集起來<br>
<br>
0:02:38.700,0:02:41.000<br>
再去調你的 model<br>
<br>
0:02:41.000,0:02:45.820<br>
然後，有了新的 model 以後再去跟環境互動 N 次<br>
<br>
0:02:45.820,0:02:48.480<br>
再收集資料，再調你的 model<br>
<br>
0:02:48.480,0:02:50.360<br>
再收集資料，再調你的 model<br>
<br>
0:02:50.360,0:02:53.300<br>
就這樣，陷入一個循環這樣<br>
<br>
0:02:53.300,0:02:58.160<br>
所以我想，這個應該是很清楚的，對吧<br>
<br>
0:02:58.160,0:03:01.280<br>
這個大家有問題嗎 ？<br>
<br>
0:03:01.700,0:03:04.000<br>
如果大家沒有問題的話<br>
<br>
0:03:04.000,0:03:05.560<br>
接下來的問題就是<br>
<br>
0:03:05.560,0:03:11.080<br>
這一項， 到底是什麼意思<br>
<br>
0:03:11.080,0:03:15.480<br>
這個 summation over 所有的 trajectory 你知道<br>
<br>
0:03:15.480,0:03:19.480<br>
summation over 某一個 trajectory 所有的 time step<br>
<br>
0:03:19.480,0:03:21.600<br>
這個意思，你也知道<br>
<br>
0:03:21.600,0:03:23.820<br>
R(τ) 你也知道<br>
<br>
0:03:23.820,0:03:27.240<br>
就這個常數項你知道<br>
<br>
0:03:27.240,0:03:33.840<br>
但是，∇ log p( a(上標 n, 下標 t) | s(上標n, 下標t) )<br>
<br>
0:03:33.840,0:03:37.380<br>
到底是什麼，也許你有點困惑<br>
<br>
0:03:37.380,0:03:40.260<br>
那我們來考慮另外一個 case<br>
<br>
0:03:40.260,0:03:45.140<br>
我們假設，我們現在要做的是一個分類的問題<br>
<br>
0:03:45.140,0:03:48.840<br>
我們現在有一個 network，我們現在有一個 actor<br>
<br>
0:03:48.840,0:03:52.400<br>
我們把這個 actor 當作是一個 classify<br>
<br>
0:03:52.400,0:03:54.700<br>
這個 classify 做的事情是<br>
<br>
0:03:54.700,0:03:57.740<br>
given 一個畫面 S<br>
<br>
0:03:57.740,0:04:02.020<br>
它分類說，我們現在應該要採取哪一個 action<br>
<br>
0:04:02.020,0:04:04.260<br>
現在有 3 個可以採取的 action<br>
<br>
0:04:04.260,0:04:07.640<br>
就是說，現在是一個有 3 個類別的<br>
<br>
0:04:10.180,0:04:12.960<br>
3 個類別的，分類的問題<br>
<br>
0:04:12.960,0:04:16.260<br>
那我們說，我們在做分類的時候<br>
<br>
0:04:16.260,0:04:17.640<br>
你要 train 一個 classifier<br>
<br>
0:04:17.640,0:04:22.020<br>
你要有 labeled data， 你要給你的 network 一個 target<br>
<br>
0:04:22.020,0:04:24.080<br>
你要給你的 network  一個目標<br>
<br>
0:04:24.080,0:04:28.040<br>
那我們就說，現在的目標是 1、0、0<br>
<br>
0:04:28.040,0:04:31.700<br>
也就是 1，left 是正確的類別<br>
<br>
0:04:31.700,0:04:35.560<br>
right 跟 fire 是錯誤的類別<br>
<br>
0:04:35.560,0:04:39.480<br>
那我們把 network 的 output 叫做 yi<br>
<br>
0:04:39.480,0:04:43.920<br>
把 target 叫做 yi\head<br>
<br>
0:04:43.920,0:04:45.420<br>
那你記不記得<br>
<br>
0:04:45.420,0:04:48.820<br>
我們說，在做 classification problem 的時候<br>
<br>
0:04:48.840,0:04:50.780<br>
我們 minimize 的是什麼？<br>
<br>
0:04:50.780,0:04:54.600<br>
我們 minimize 的是 cross entropy<br>
<br>
0:04:54.600,0:04:56.100<br>
對不對<br>
<br>
0:04:56.100,0:04:58.280<br>
你記不記得，我們說 <br>
<br>
0:04:58.280,0:04:59.500<br>
我們今天要 train 一個<br>
<br>
0:04:59.500,0:05:01.920<br>
可以拿來做 classification network 的時候<br>
<br>
0:05:01.920,0:05:04.020<br>
如果，我們在做 Regression 的時候<br>
<br>
0:05:04.020,0:05:07.780<br>
我們都做 minimize 的就是 cross entropy<br>
<br>
0:05:07.780,0:05:09.560<br>
那 cross entropy 就是<br>
<br>
0:05:09.560,0:05:13.640<br>
summation over 每一個 dimension、<br>
summation over 所有的 class<br>
<br>
0:05:13.640,0:05:20.020<br>
然後，把 yi\head * log(yi)，前面取負號<br>
<br>
0:05:20.360,0:05:23.400<br>
但是因為多數的 yi\head 都是零<br>
<br>
0:05:23.400,0:05:26.840<br>
這邊 y2\head 跟 y3\head 都是 0<br>
<br>
0:05:26.840,0:05:29.220<br>
只有 y1\head 是 1<br>
<br>
0:05:29.220,0:05:32.800<br>
所以，實際上我們在做的事情<br>
<br>
0:05:32.800,0:05:35.280<br>
這邊本來是一個負號加 minimize<br>
<br>
0:05:35.280,0:05:36.680<br>
負號加 minimize<br>
<br>
0:05:36.680,0:05:39.380<br>
可以看作是 maximize<br>
<br>
0:05:39.380,0:05:42.380<br>
所以，我們實際上在做的事情就是<br>
<br>
0:05:42.380,0:05:44.700<br>
maximize log(yi)<br>
<br>
0:05:44.700,0:05:49.660<br>
實際上做的事情就是 maximize log(yi)<br>
<br>
0:05:49.660,0:05:51.000<br>
那我們知道說<br>
<br>
0:05:51.000,0:05:57.480<br>
所謂的 yi ，其實就是 P("left"|s)<br>
<br>
0:05:57.480,0:05:59.520<br>
所謂的 y1<br>
<br>
0:05:59.520,0:06:03.100<br>
這邊我該把它寫 y1 比較對<br>
<br>
0:06:03.100,0:06:07.340<br>
因為今天在這個 specific 的例子裡面，y1\head 是 1<br>
<br>
0:06:07.340,0:06:09.800<br>
所以，只有 log(y1) 會被留下<br>
<br>
0:06:09.800,0:06:12.000<br>
所以，這項應該是 log(y1)<br>
<br>
0:06:12.000,0:06:16.640<br>
log(y1) 其實就是 log[P("left"|s)]<br>
<br>
0:06:16.640,0:06:19.120<br>
就是這個 network output<br>
<br>
0:06:19.120,0:06:24.260<br>
這個 network 覺得要採取 action left 的機率<br>
<br>
0:06:25.260,0:06:30.540<br>
如果你今天要做的事情是<br>
<br>
0:06:30.540,0:06:34.260<br>
minimize 這個 cross entropy<br>
<br>
0:06:34.260,0:06:35.740<br>
或者是<br>
<br>
0:06:35.740,0:06:38.620<br>
同等的 maximize下面這一項<br>
<br>
0:06:38.620,0:06:40.460<br>
你會怎麼做呢？<br>
<br>
0:06:40.460,0:06:42.060<br>
你是不是就說<br>
<br>
0:06:42.060,0:06:44.360<br>
這是我們的 objective function<br>
<br>
0:06:44.360,0:06:46.280<br>
我要去 maximize 它<br>
<br>
0:06:46.280,0:06:49.560<br>
我當然就是對它算一個 gradient<br>
<br>
0:06:49.560,0:06:53.980<br>
所以，我們就對 log[P("left"|s)]<br>
<br>
0:06:53.980,0:06:56.520<br>
求它的 gradient<br>
<br>
0:06:56.520,0:06:58.040<br>
再乘上 learning rate<br>
<br>
0:06:58.040,0:06:59.640<br>
再加給你的參數<br>
<br>
0:06:59.640,0:07:04.200<br>
再用這項去 update 你的參數<br>
<br>
0:07:04.200,0:07:07.880<br>
所以當你看到你 update 的式子裡面<br>
<br>
0:07:07.880,0:07:09.740<br>
有這個項的時候<br>
<br>
0:07:09.740,0:07:11.880<br>
當你看到你 update 的式子裡面<br>
<br>
0:07:11.880,0:07:16.160<br>
有這個  log[P("left"|s)] 的時候<br>
<br>
0:07:16.160,0:07:19.480<br>
它的意思，其實是說<br>
<br>
0:07:19.480,0:07:24.100<br>
我們希望這一個 objective function 越大越好<br>
<br>
0:07:24.100,0:07:26.040<br>
或這個 objective function 越小越好<br>
<br>
0:07:26.040,0:07:30.320<br>
或者是說，我們其實是在解一個分類的問題<br>
<br>
0:07:30.320,0:07:32.400<br>
然後，我們希望我們 network 的 output<br>
<br>
0:07:32.400,0:07:35.760<br>
跟我們訂下來的 target 越接近越好<br>
<br>
0:07:35.760,0:07:37.640<br>
而在我們的 target 裡面<br>
<br>
0:07:37.640,0:07:41.020<br>
就是left，就是正確的類別<br>
<br>
0:07:41.020,0:07:44.160<br>
而其他的是錯誤的類別<br>
<br>
0:07:44.600,0:07:48.180<br>
這樣大家了解我的意思嗎？<br>
<br>
0:07:48.180,0:07:50.560<br>
假如你可以了解這個意思的話<br>
<br>
0:07:50.560,0:07:56.020<br>
那接下來，我們怎麼解釋這個式子呢？<br>
<br>
0:07:56.020,0:07:57.800<br>
怎麼解釋這個式子呢？<br>
<br>
0:07:57.800,0:08:01.180<br>
我們先把 R 拿掉<br>
<br>
0:08:01.180,0:08:04.580<br>
我們先把 R(τ^n) 拿掉，當作那一項等於 1<br>
<br>
0:08:04.580,0:08:08.180<br>
你就當作所有的 trajectory reward 都是 1<br>
<br>
0:08:08.180,0:08:09.740<br>
就不要管它了<br>
<br>
0:08:09.740,0:08:16.480<br>
那這個 ∇ log[P("left"|s)] 是什麼意思<br>
<br>
0:08:16.480,0:08:18.440<br>
它的意思就是說<br>
<br>
0:08:18.440,0:08:22.860<br>
我們現在 training data裡面有一個 s1、a1<br>
<br>
0:08:22.860,0:08:24.780<br>
有一個 s1，a1<br>
<br>
0:08:24.780,0:08:29.640<br>
那我們假設說 a1就是 left<br>
<br>
0:08:29.640,0:08:32.760<br>
那我們要做的事情就是<br>
<br>
0:08:32.760,0:08:34.480<br>
我們把 s1 丟到 network 裡面<br>
<br>
0:08:34.480,0:08:37.620<br>
它給我們 left、right 跟 fire 的機率<br>
<br>
0:08:37.620,0:08:41.680<br>
那我們希望這個機率跟 1、0、0 越接近越好<br>
<br>
0:08:41.680,0:08:44.820<br>
因為現在的 a1 是 left<br>
<br>
0:08:44.820,0:08:46.640<br>
a1 是 left<br>
<br>
0:08:46.640,0:08:47.780<br>
所以我們希望<br>
<br>
0:08:47.780,0:08:50.380<br>
所以就告訴我們說 left 是正確的<br>
<br>
0:08:50.380,0:08:53.120<br>
我們希望 left 的分數越大越好<br>
<br>
0:08:53.120,0:08:56.760<br>
然後，right 跟 fire 的分數，越接近 0 越好<br>
<br>
0:08:58.080,0:09:01.640<br>
那今天如果我們有一個 state 2<br>
<br>
0:09:01.640,0:09:03.500<br>
那如果今天<br>
<br>
0:09:03.500,0:09:06.420<br>
我們再另外一個 trajectory 裡面<br>
<br>
0:09:06.420,0:09:09.160<br>
我們有一個 s (上標 2, 下標 1)<br>
<br>
0:09:09.160,0:09:11.040<br>
那把 s (上標 2, 下標 1)<br>
<br>
0:09:11.040,0:09:12.920<br>
也丟到那個 network 裡面<br>
<br>
0:09:13.700,0:09:15.100<br>
這樣我們假設在<br>
<br>
0:09:15.100,0:09:19.660<br>
這個 s1 在 trajectory 的第一個 state 的時候<br>
<br>
0:09:19.660,0:09:22.240<br>
我們採取的 action 是 fire 的話<br>
<br>
0:09:22.240,0:09:23.420<br>
那意思就是說<br>
<br>
0:09:23.420,0:09:28.440<br>
我們希望 fire 的分數越大<br>
<br>
0:09:28.440,0:09:31.080<br>
我們希望這個 fire 的分數是 1<br>
<br>
0:09:31.080,0:09:32.800<br>
其他的分數是 0<br>
<br>
0:09:32.800,0:09:36.540<br>
所以，這就變成了一個分類的問題<br>
<br>
0:09:36.540,0:09:40.640<br>
你有發現嗎？這其實就是一個分類的問題<br>
<br>
0:09:40.640,0:09:44.540<br>
你就實際上，我們在 update 這個 式子的時候<br>
<br>
0:09:45.760,0:09:48.120<br>
我們真正在做的事情是<br>
<br>
0:09:48.120,0:09:49.420<br>
我們希望說<br>
<br>
0:09:49.420,0:09:51.420<br>
等於是 machine 告訴我們說<br>
<br>
0:09:51.420,0:09:53.420<br>
現在有一筆 training data<br>
<br>
0:09:53.420,0:09:55.760<br>
它的 input 就是這個樣子<br>
<br>
0:09:55.760,0:09:57.460<br>
它的 target 就是這個樣子<br>
<br>
0:09:57.460,0:09:58.900<br>
它的 input 就是這個樣子<br>
<br>
0:09:58.900,0:10:01.200<br>
它的 target 是這個樣子，input 就是這樣子<br>
<br>
0:10:01.200,0:10:02.200<br>
它的 target 就是這個樣子<br>
<br>
0:10:02.200,0:10:05.660<br>
然後，你把這個分類問題做對<br>
<br>
0:10:06.160,0:10:09.340<br>
之前，machine 看到 s1 的時候<br>
<br>
0:10:09.340,0:10:13.020<br>
它就採取，它採取 a1 這個 action<br>
<br>
0:10:13.020,0:10:14.820<br>
今天 machine learning 的目標<br>
<br>
0:10:14.820,0:10:17.260<br>
就是一樣， 今天 machine learning 的目標就是<br>
<br>
0:10:17.260,0:10:20.020<br>
希望在 input s1 的時候<br>
<br>
0:10:20.020,0:10:24.220<br>
它的目標就是要採取 action  a1<br>
<br>
0:10:24.220,0:10:26.360<br>
那你可能會想說<br>
<br>
0:10:26.360,0:10:30.080<br>
你就跟 machine 原來做的事情是一樣的嗎 ?<br>
<br>
0:10:30.080,0:10:34.300<br>
machine 本來就 input s1，就會採取 a1<br>
<br>
0:10:34.300,0:10:36.940<br>
那你說你是要 learn 一個 network，它的目標就是<br>
<br>
0:10:36.940,0:10:39.960<br>
input s1，output 就是 target a1<br>
<br>
0:10:39.960,0:10:41.680<br>
那不就跟原來的 network<br>
<br>
0:10:41.680,0:10:44.280<br>
原來的 actor 做的事情是一樣的嗎<br>
<br>
0:10:44.280,0:10:46.180<br>
但是有一個不一樣的地方就是<br>
<br>
0:10:46.180,0:10:48.760<br>
我們前面有了 reward<br>
<br>
0:10:48.760,0:10:51.760<br>
我們會把每一個 example<br>
<br>
0:10:51.760,0:10:55.200<br>
在我們假設把它當成一個分類問題的話<br>
<br>
0:10:55.200,0:10:57.940<br>
我會把這個分類問題的每一個 example<br>
<br>
0:10:57.940,0:11:01.980<br>
前面都乘上 R(τ^n)<br>
<br>
0:11:01.980,0:11:03.660<br>
每一個分類問題前面<br>
<br>
0:11:03.660,0:11:06.140<br>
我們都乘上 R(τ^n)<br>
<br>
0:11:10.060,0:11:13.760<br>
那這件事情到底是什麼意思呢？<br>
<br>
0:11:13.760,0:11:17.300<br>
把每一筆的 training data 都乘上 R(τ^n)<br>
<br>
0:11:17.300,0:11:19.540<br>
到底是什麼意思呢？<br>
<br>
0:11:19.540,0:11:22.220<br>
如果簡單的想<br>
<br>
0:11:22.220,0:11:26.520<br>
假設現在 R(τ^1) 的值是 2<br>
<br>
0:11:26.520,0:11:29.200<br>
假設 R(τ^2) 的值是 1<br>
<br>
0:11:29.200,0:11:32.160<br>
我們說，把每一筆 training data 前面<br>
<br>
0:11:32.160,0:11:34.060<br>
都 weighted by R(τ^n)<br>
<br>
0:11:34.060,0:11:35.120<br>
意思就是說<br>
<br>
0:11:35.120,0:11:38.860<br>
我們現在把 input s (上標1, 下標1)<br>
<br>
0:11:38.860,0:11:40.598<br>
output 要是 left 這件事情<br>
<br>
0:11:40.598,0:11:43.240<br>
這個 example 複製兩次<br>
<br>
0:11:43.240,0:11:47.560<br>
因為它的 reward 是 2，所以就複製兩次<br>
<br>
0:11:47.560,0:11:50.140<br>
那 input s (上標 2, 下標 1)<br>
<br>
0:11:50.140,0:11:51.840<br>
output fire 這件事情<br>
<br>
0:11:51.840,0:11:53.120<br>
它的 reward 只有  1<br>
<br>
0:11:53.120,0:11:54.800<br>
所以我們就複製一次<br>
<br>
0:11:54.800,0:11:58.240<br>
然後 ，拿這個東西去 train 我們的 network <br>
<br>
0:11:58.240,0:12:01.600<br>
然後你就可以 update 一次你的 network 參數<br>
<br>
0:12:01.600,0:12:03.540<br>
然後再重新去 sample 得到<br>
<br>
0:12:03.540,0:12:05.340<br>
再重新 sample 得到新的 training data<br>
<br>
0:12:05.340,0:12:07.120<br>
再用這個方法去 update 參數<br>
<br>
0:12:07.120,0:12:09.440<br>
再去 sample data，再去 update 參數<br>
<br>
0:12:09.440,0:12:11.960<br>
就結束了這樣<br>
<br>
0:12:11.960,0:12:14.940<br>
這樣有沒有覺得很簡單呢<br>
<br>
0:12:14.940,0:12:18.100<br>
這樣有沒有覺得實做很簡單呢<br>
<br>
0:12:18.100,0:12:20.400<br>
你唯一需要的<br>
<br>
0:12:20.400,0:12:24.720<br>
你唯一需要的， 你只有你需要在 learn classify 的時候<br>
<br>
0:12:24.720,0:12:26.680<br>
給你的 training 的 example<br>
<br>
0:12:26.680,0:12:28.060<br>
給它 weight<br>
<br>
0:12:28.060,0:12:30.220<br>
你唯一需要改程式的<br>
<br>
0:12:30.220,0:12:31.860<br>
只有這個部分<br>
<br>
0:12:31.860,0:12:34.940<br>
那甚至 Keras，如果我沒有記錯的話<br>
<br>
0:12:34.940,0:12:36.820<br>
它其實有支援這個功能<br>
<br>
0:12:36.820,0:12:38.080<br>
那其他部分<br>
<br>
0:12:38.980,0:12:42.180<br>
你根本就不用就改了你的 code<br>
<br>
0:12:42.180,0:12:46.260<br>
這樣大家了解我的意思嗎？<br>
<br>
0:12:47.100,0:12:49.420<br>
所以，其實有人會覺得說<br>
<br>
0:12:49.420,0:12:50.740<br>
Reinforcement learning 很難<br>
<br>
0:12:50.740,0:12:51.720<br>
沒有很難<br>
<br>
0:12:51.720,0:12:55.120<br>
你不用改 code 就可以做 Reinforcement learning<br>
<br>
0:12:55.120,0:12:58.100<br>
只是很花時間，可能就是真的<br>
<br>
0:12:58.100,0:13:00.440<br>
因為，你每一次收集完 data 以後<br>
<br>
0:13:00.440,0:13:02.940<br>
你都要解一次分類的問題<br>
<br>
0:13:02.940,0:13:04.080<br>
train 一次 neural network<br>
<br>
0:13:04.080,0:13:06.580<br>
然後再去收集 data ，再 train 一次 neural network<br>
<br>
0:13:06.580,0:13:08.060<br>
跟我們之前做的分類的問題都不一樣<br>
<br>
0:13:08.060,0:13:08.920<br>
之前做分類問題<br>
<br>
0:13:08.920,0:13:11.040<br>
data 收集好，就在那一邊 train 完一次<br>
<br>
0:13:11.040,0:13:12.400<br>
就結束了<br>
<br>
0:13:12.400,0:13:14.140<br>
但是，在  Reinforcement learning 問題裡面<br>
<br>
0:13:14.140,0:13:16.700<br>
等於你要 train 你的 network 很多次<br>
<br>
0:13:16.700,0:13:19.160<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
