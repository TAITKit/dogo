<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.240<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.240,0:00:04.840<br>
我把 Dimension Reduction 分成兩種<br>
<br>
0:00:04.940,0:00:10.240<br>
一種做的事情，叫做化繁為簡<br>
<br>
0:00:10.240,0:00:14.300<br>
它可以分成兩大類，一種是做 Clustering<br>
<br>
0:00:14.300,0:00:16.780<br>
一種是做 Dimension Reduction<br>
<br>
0:00:16.780,0:00:20.220<br>
所謂的化繁為簡的意思是說<br>
<br>
0:00:20.220,0:00:23.260<br>
你現在有很多種不同的 input<br>
<br>
0:00:23.260,0:00:26.160<br>
比如說，你現在找一個 function<br>
<br>
0:00:26.160,0:00:29.120<br>
然後它可以 input 看起來像樹的東西<br>
<br>
0:00:29.120,0:00:31.960<br>
output 都是抽象的樹<br>
<br>
0:00:31.960,0:00:35.300<br>
也就是把本來比較複雜的 input<br>
<br>
0:00:35.300,0:00:37.940<br>
變成比較簡單的 output<br>
<br>
0:00:37.940,0:00:40.440<br>
那在做 Unsupervised Learning 的時候<br>
<br>
0:00:40.440,0:00:44.480<br>
你通常只會有 function 的其中一邊<br>
<br>
0:00:44.480,0:00:46.840<br>
比如說，我們要找一個 function，它可以把<br>
<br>
0:00:46.840,0:00:48.980<br>
所有的樹都變成抽象的樹<br>
<br>
0:00:48.980,0:00:52.040<br>
但是，你所能擁有的 training data<br>
<br>
0:00:52.040,0:00:55.400<br>
就只有一大堆的 image<br>
<br>
0:00:55.400,0:00:58.100<br>
一大堆的、各種不同的 image<br>
<br>
0:00:58.100,0:01:01.740<br>
你不知道說它的 output，應該要是長什麼樣子<br>
<br>
0:01:02.280,0:01:07.640<br>
那另外一個 Unsupervised Learning 會做的事情呢<br>
<br>
0:01:07.640,0:01:10.820<br>
是 Generation，也就是無中生有<br>
<br>
0:01:10.820,0:01:13.140<br>
它做的事情是這樣，我們要找一個 function<br>
<br>
0:01:13.140,0:01:17.120<br>
這個 function，你隨機給它一個 input<br>
<br>
0:01:17.120,0:01:19.860<br>
我給它一個 random number，比如說輸入一個數字 1<br>
<br>
0:01:19.860,0:01:21.260<br>
然後，它就會 output 這棵樹<br>
<br>
0:01:21.260,0:01:23.240<br>
輸入數字 2，就 output 另外一棵樹<br>
<br>
0:01:23.240,0:01:24.840<br>
輸入數字 3，就 output 另外一棵<br>
<br>
0:01:24.840,0:01:28.180<br>
你輸入一個，給它一個 random number<br>
<br>
0:01:28.180,0:01:30.540<br>
它就自動畫一張圖出來<br>
<br>
0:01:30.540,0:01:33.900<br>
不同的 number 給它，它畫出來的圖就不一樣<br>
<br>
0:01:33.900,0:01:37.700<br>
在這個 task 裡面，我們要找的這個可以畫圖的 function<br>
<br>
0:01:37.700,0:01:40.480<br>
你只有這一個 function 的 output<br>
<br>
0:01:40.480,0:01:43.180<br>
但是你沒有這一個 function 的 input<br>
<br>
0:01:43.180,0:01:44.700<br>
你就只有一大堆的 image<br>
<br>
0:01:44.700,0:01:46.360<br>
但是，你不知道輸入怎麼樣的 code<br>
<br>
0:01:46.360,0:01:48.540<br>
才可以得到這些 image<br>
<br>
0:01:48.540,0:01:51.640<br>
那在這一份投影片，我們先 focus 在<br>
<br>
0:01:51.640,0:01:53.880<br>
Dimension Reduction 這一件事情上<br>
<br>
0:01:53.880,0:01:59.600<br>
而且我們只 focus 在 linear 的 Dimension Reduction<br>
<br>
0:01:59.600,0:02:03.660<br>
那我們現在說一下什麼是 clustering，什麼是 clustering 呢<br>
<br>
0:02:03.660,0:02:05.880<br>
clustering 就是說，這個太直覺了<br>
<br>
0:02:05.880,0:02:09.520<br>
這種地方比較沒有什麼特別好講的<br>
<br>
0:02:09.520,0:02:13.320<br>
有一大堆的 image，假設我們現在要做 image 的 clustering<br>
<br>
0:02:13.320,0:02:14.680<br>
就有一大堆 image<br>
<br>
0:02:14.680,0:02:17.760<br>
然後，你就把它們分成一類、一類、一類的<br>
<br>
0:02:17.760,0:02:21.440<br>
之後，你就可以說<br>
<br>
0:02:21.440,0:02:24.220<br>
這一邊所有的 image 都是屬於 cluster 1<br>
<br>
0:02:24.220,0:02:26.640<br>
這邊都屬於 cluster 2，這邊都屬於 cluster 3<br>
<br>
0:02:26.640,0:02:29.020<br>
就給它們貼標籤的意思<br>
<br>
0:02:29.020,0:02:32.300<br>
那這樣你就把本來有些不同的 image<br>
<br>
0:02:32.300,0:02:34.640<br>
都用同一個 class 來表示<br>
<br>
0:02:34.640,0:02:37.040<br>
就可以做到化繁為簡這一件事情<br>
<br>
0:02:37.040,0:02:39.240<br>
那這邊最 critical 的問題就是<br>
<br>
0:02:39.240,0:02:42.940<br>
到底應該要有幾個 cluster，那這個東西沒有什麼好的方法<br>
<br>
0:02:42.940,0:02:46.480<br>
就跟 neural network 要幾層一樣，是 empirical 地去決定<br>
<br>
0:02:46.480,0:02:49.140<br>
你要需要幾個 cluster，這個當然不能太多<br>
<br>
0:02:49.140,0:02:50.100<br>
比如說，你多到說<br>
<br>
0:02:50.100,0:02:52.800<br>
這邊有 9 張 image，你就有 9 個 cluster<br>
<br>
0:02:52.800,0:02:54.380<br>
那你乾脆就不要做 clustering<br>
<br>
0:02:54.380,0:02:55.780<br>
這樣 image 就自己一個 cluster<br>
<br>
0:02:55.780,0:02:57.900<br>
那這樣有做跟沒有做是一樣的<br>
<br>
0:02:57.900,0:03:00.280<br>
或者是你說，我們全部的 image<br>
<br>
0:03:00.280,0:03:02.040<br>
我們只有一個 cluster<br>
<br>
0:03:02.040,0:03:03.820<br>
全部的 image 都放在同一個 cluster 裡面<br>
<br>
0:03:03.820,0:03:05.440<br>
那也是有做跟沒有做一樣<br>
<br>
0:03:05.440,0:03:08.060<br>
但是，要怎麼樣選擇適當的 cluster<br>
<br>
0:03:08.060,0:03:10.240<br>
這個你就要 empirical 的來決定它<br>
<br>
0:03:10.240,0:03:12.640<br>
那在 clustering 的方法裡面<br>
<br>
0:03:12.640,0:03:15.600<br>
最常用的叫做 K-means<br>
<br>
0:03:15.600,0:03:17.340<br>
很快地講一下，K-means 是怎麼做的<br>
<br>
0:03:17.340,0:03:21.580<br>
K-means 就是這樣，我們有一大堆的 data<br>
<br>
0:03:21.580,0:03:23.320<br>
它們都是 unlabeled<br>
<br>
0:03:23.320,0:03:26.980<br>
一大堆的 data，x^1 一直到 x^N<br>
<br>
0:03:26.980,0:03:29.980<br>
那這邊每一個 x，它可能就代表一張 image<br>
<br>
0:03:29.980,0:03:33.700<br>
那我們要把它做成 K 個 cluster<br>
<br>
0:03:33.700,0:03:37.560<br>
怎麼做呢，我們要先<br>
<br>
0:03:37.560,0:03:41.500<br>
找這些 cluster 的 center<br>
<br>
0:03:41.500,0:03:45.680<br>
假如這一邊每一個<br>
<br>
0:03:45.680,0:03:47.840<br>
object 都是用一個 vector 來表示的話<br>
<br>
0:03:47.840,0:03:52.320<br>
這一邊這些 center，也是一樣長度的 vector<br>
<br>
0:03:52.320,0:03:56.000<br>
那我們每一個 cluster 都要先找一個 center<br>
<br>
0:03:56.000,0:04:00.360<br>
有 K 個 cluster，我們就需要 c^1 到 c^K 個 center<br>
<br>
0:04:00.360,0:04:01.820<br>
那這些 center<br>
<br>
0:04:01.820,0:04:03.360<br>
這個初始的 center 怎麼來的呢<br>
<br>
0:04:03.360,0:04:05.500<br>
你可以從你的 training data 裡面<br>
<br>
0:04:05.500,0:04:09.240<br>
隨機的找 K 個 object 出來<br>
<br>
0:04:09.240,0:04:11.580<br>
就是你的 K個 center<br>
<br>
0:04:11.580,0:04:18.780<br>
接下來，你要對所有在 training data 裡面的 x<br>
<br>
0:04:18.780,0:04:21.540<br>
都做以下的事情，都做什麼事情呢<br>
<br>
0:04:21.540,0:04:25.640<br>
你決定說現在的每一個 object<br>
<br>
0:04:25.640,0:04:30.340<br>
屬於 1 到 K 的哪一個 cluster<br>
<br>
0:04:30.340,0:04:35.340<br>
那假設現在你的 object, x^n<br>
<br>
0:04:35.340,0:04:40.320<br>
跟第 i 個 cluster 的 center 最接近的話<br>
<br>
0:04:40.320,0:04:43.900<br>
那 x^n 就屬於 c^i<br>
<br>
0:04:43.900,0:04:48.560<br>
那我們用一個 binary 的 value，b (上標n, 下標 i)<br>
<br>
0:04:48.560,0:04:53.640<br>
來代表說，第 n 個 object 有沒有屬於第 i 個 class<br>
<br>
0:04:53.640,0:04:55.880<br>
如果第 n 個 object 屬於第 i 個 class 的話<br>
<br>
0:04:55.880,0:04:58.400<br>
那這一個 binary 的 value 就是 1<br>
<br>
0:04:58.400,0:05:00.280<br>
反之就是 0<br>
<br>
0:05:00.280,0:05:03.060<br>
接下來，你要 update 你的 cluster<br>
<br>
0:05:03.060,0:05:05.120<br>
怎麼 update 你的 cluster 呢<br>
<br>
0:05:05.120,0:05:07.960<br>
這個方法也是很直覺<br>
<br>
0:05:07.960,0:05:11.360<br>
你就把所有屬於<br>
<br>
0:05:11.360,0:05:14.480<br>
假設你要 update 第 i 個 cluster 的 center<br>
<br>
0:05:14.480,0:05:18.500<br>
你就把所有屬於第 i 個 cluster 的 object<br>
<br>
0:05:18.500,0:05:20.500<br>
通通拿出來，做平均<br>
<br>
0:05:20.500,0:05:24.540<br>
你就得到第 i 個 cluster 的 center<br>
<br>
0:05:24.540,0:05:28.140<br>
然後，你要反覆的做<br>
<br>
0:05:28.140,0:05:31.660<br>
那這邊之所以你在做 initialization 的時候<br>
<br>
0:05:31.660,0:05:33.520<br>
你會想要直接從你的<br>
<br>
0:05:33.520,0:05:36.900<br>
這個 database 裡面去挑 K 個 example<br>
<br>
0:05:36.900,0:05:38.800<br>
挑 K 個 object 出來做 center 呢<br>
<br>
0:05:38.800,0:05:41.840<br>
有一個很重要的原因是，假設你是純粹隨機的<br>
<br>
0:05:41.840,0:05:43.460<br>
你不是從 data points 裡面去挑<br>
<br>
0:05:43.460,0:05:48.000<br>
你很有可能在第一次 assign 這個 object 的時候<br>
<br>
0:05:48.000,0:05:52.920<br>
就沒有任何 example 跟某一個 cluster 很像<br>
<br>
0:05:52.920,0:05:56.940<br>
又有可能是，有某一個 cluster，它沒有任何 example<br>
<br>
0:05:56.940,0:05:58.240<br>
你現在 update 的時候<br>
<br>
0:05:58.240,0:05:59.820<br>
程式就會 segmentation fault<br>
<br>
0:05:59.820,0:06:03.720<br>
所以，你最好就是從你的 training data 裡面<br>
<br>
0:06:03.720,0:06:07.220<br>
選 K 筆 example 出來當 initialization<br>
<br>
0:06:09.200,0:06:11.480<br>
clustering 有另外一個方法叫做<br>
<br>
0:06:11.480,0:06:13.840<br>
Hierarchical Agglomerative Clustering (HAC)<br>
<br>
0:06:13.840,0:06:15.560<br>
那這個方法<br>
<br>
0:06:15.560,0:06:20.540<br>
是先建一個 tree，假設你現在有 5 個 example<br>
<br>
0:06:20.540,0:06:22.600<br>
你想要把它做 cluster<br>
<br>
0:06:22.600,0:06:24.380<br>
那你先做成一個 tree structure<br>
<br>
0:06:24.380,0:06:25.780<br>
怎麼建這個 tree structure 呢<br>
<br>
0:06:25.780,0:06:27.820<br>
你把這 5 個 example<br>
<br>
0:06:27.820,0:06:30.360<br>
兩兩、兩兩去算他的相似度<br>
<br>
0:06:30.360,0:06:33.620<br>
然後挑最相似的那一個 pair 出來<br>
<br>
0:06:33.620,0:06:37.560<br>
假設現在最相似的 pair，是第一個和第二個 example<br>
<br>
0:06:37.560,0:06:41.760<br>
你就把第一個 example 和第二個 example merge 起來<br>
<br>
0:06:41.760,0:06:43.120<br>
比如說，把它們平均起來<br>
<br>
0:06:43.120,0:06:45.260<br>
得到一個新的 vector<br>
<br>
0:06:45.260,0:06:48.540<br>
這個 vector 同時代表第一個和第二個 example<br>
<br>
0:06:48.540,0:06:50.260<br>
接下來，你再算說<br>
<br>
0:06:50.260,0:06:52.760<br>
現在變成有四個 example 了<br>
<br>
0:06:52.760,0:06:55.260<br>
現在只剩下 4 筆 data 了<br>
<br>
0:06:55.260,0:06:57.840<br>
把這 4 筆 data，再兩兩去算它的相似度<br>
<br>
0:06:57.840,0:07:00.880<br>
然後發現說，第三筆和第四筆<br>
<br>
0:07:00.880,0:07:02.740<br>
最後這兩筆是最像的<br>
<br>
0:07:02.740,0:07:05.180<br>
那你再把它們 merge 起來，把它們平均起來<br>
<br>
0:07:05.180,0:07:07.400<br>
得到另外一筆 data<br>
<br>
0:07:07.400,0:07:10.660<br>
現在只剩三筆 data，再去兩兩算它的 singularity<br>
<br>
0:07:10.660,0:07:11.740<br>
然後，你發現說<br>
<br>
0:07:11.740,0:07:14.280<br>
黃色這一個和中間這一個最像<br>
<br>
0:07:14.280,0:07:16.560<br>
你就再把它們平均起來<br>
<br>
0:07:16.560,0:07:22.400<br>
你會發現，最後只剩紅色跟綠色，你只好把它平均起來<br>
<br>
0:07:22.400,0:07:25.040<br>
那你就得到這個 tree 的 root<br>
<br>
0:07:25.040,0:07:28.060<br>
你就根據這 5 筆 data，它們之間的相似度<br>
<br>
0:07:28.060,0:07:31.380<br>
建立出一個 tree structure<br>
<br>
0:07:31.380,0:07:33.200<br>
接下來，你要決定一個<br>
<br>
0:07:33.200,0:07:36.360<br>
你沒有做 clustering，你只是建了一個 tree structure<br>
<br>
0:07:36.360,0:07:38.100<br>
這個 tree structure 可以告訴我們說<br>
<br>
0:07:38.100,0:07:41.620<br>
哪些 example 是比較像的<br>
<br>
0:07:41.620,0:07:44.380<br>
比較早分支代表比較不像<br>
<br>
0:07:44.380,0:07:47.480<br>
比如說 ，這三個跟這兩個<br>
<br>
0:07:47.480,0:07:49.580<br>
一開始在 root 的地方就分成兩類<br>
<br>
0:07:49.580,0:07:52.240<br>
所以，它們這三個跟這兩個就比較不像<br>
<br>
0:07:53.580,0:07:55.800<br>
那接下來你要做 clustering，怎麼做呢<br>
<br>
0:07:55.800,0:07:58.280<br>
你要決定在這個樹上<br>
<br>
0:07:58.280,0:08:01.060<br>
在這個 tree structure 上面切一刀<br>
<br>
0:08:01.060,0:08:03.020<br>
比如說，你決定切在這一邊<br>
<br>
0:08:03.020,0:08:05.760<br>
如果你切在這個地方的時候<br>
<br>
0:08:05.760,0:08:08.440<br>
你就把你的五筆 data<br>
<br>
0:08:08.440,0:08:10.340<br>
變成三個 cluster<br>
<br>
0:08:10.340,0:08:12.380<br>
根據這一刀<br>
<br>
0:08:12.380,0:08:14.680<br>
這兩個 example 是一個 cluster<br>
<br>
0:08:14.680,0:08:17.560<br>
它自己一個 cluster，這兩個 example 是一個 cluster<br>
<br>
0:08:17.560,0:08:20.760<br>
如果你這一刀切在這個地方<br>
<br>
0:08:21.400,0:08:23.920<br>
那就變成這 3 筆是一個 cluster<br>
<br>
0:08:23.920,0:08:26.980<br>
這三筆是一個 cluster，這兩筆是一個 cluster<br>
<br>
0:08:26.980,0:08:29.340<br>
如果，你這一刀切在這邊，那你就變成<br>
<br>
0:08:29.340,0:08:31.700<br>
分成總共四個 cluster<br>
<br>
0:08:31.700,0:08:35.580<br>
這個是 HAC 的做法<br>
<br>
0:08:35.580,0:08:39.480<br>
那 HAC 跟 K-means，我覺得最大的差別就是<br>
<br>
0:08:39.480,0:08:41.560<br>
你如何決定你 cluster 的數目<br>
<br>
0:08:41.560,0:08:43.120<br>
在 K-means 裡面，你要<br>
<br>
0:08:43.120,0:08:45.220<br>
決定你那個 K 的 value 是多少<br>
<br>
0:08:45.220,0:08:49.260<br>
有時候你覺得說，到底有多少個 cluster 我不容易想<br>
<br>
0:08:49.260,0:08:51.160<br>
那你可以換成 HAC<br>
<br>
0:08:51.160,0:08:53.040<br>
好處就是，你現在把<br>
<br>
0:08:53.040,0:08:55.420<br>
你現在不直接決定幾個 cluster，而是<br>
<br>
0:08:55.420,0:08:58.280<br>
決定你要在樹上切<br>
<br>
0:08:58.280,0:09:00.660<br>
切在這個樹的 structure 的哪裡<br>
<br>
0:09:00.660,0:09:02.160<br>
那有人會覺得說<br>
<br>
0:09:02.160,0:09:06.620<br>
有時候你會覺得這樣子是比較容易的話<br>
<br>
0:09:06.620,0:09:09.140<br>
那學 HAC 對你來說，就有一些 benefit<br>
<br>
0:09:09.840,0:09:12.160<br>
但是，光只做 cluster<br>
<br>
0:09:12.160,0:09:13.900<br>
是非常卡的<br>
<br>
0:09:13.900,0:09:15.800<br>
在做 cluster 的時候<br>
<br>
0:09:15.800,0:09:18.400<br>
我們就是以偏概全，因為每一個 object<br>
<br>
0:09:18.400,0:09:20.880<br>
都必須要屬於某一個 cluster<br>
<br>
0:09:20.880,0:09:23.020<br>
這就好像是說，我們知道說<br>
<br>
0:09:23.020,0:09:24.940<br>
念能力有分成六大類<br>
<br>
0:09:24.940,0:09:27.580<br>
然後，每一個人<br>
<br>
0:09:27.580,0:09:30.860<br>
都會被 assign 成這六個念能力的其中一類<br>
<br>
0:09:30.860,0:09:32.460<br>
他怎麼決定他是哪一類呢？<br>
<br>
0:09:32.460,0:09:33.900<br>
你就做水鑑識這樣子<br>
<br>
0:09:33.900,0:09:36.880<br>
你拿一杯水，然後看看它會有什麼反應<br>
<br>
0:09:36.880,0:09:38.580<br>
然後，就把他塞成某一類<br>
<br>
0:09:38.580,0:09:40.040<br>
比如說，水滿出來了<br>
<br>
0:09:40.040,0:09:43.760<br>
就是強化，所以小傑就是強化系<br>
<br>
0:09:43.760,0:09:46.240<br>
那我們知道說，這樣子把每一個人<br>
<br>
0:09:46.240,0:09:49.600<br>
都 assign 成念能力裡面的某一個系<br>
<br>
0:09:49.600,0:09:51.880<br>
是不夠的<br>
<br>
0:09:51.880,0:09:54.420<br>
是太過粗糙武斷<br>
<br>
0:09:54.420,0:09:56.720<br>
比如說，像比司吉就有說<br>
<br>
0:09:56.720,0:10:02.740<br>
小傑其實是接近放出系的強化系念能力者<br>
<br>
0:10:02.740,0:10:05.420<br>
所以小傑，如果你只是說他是強化系<br>
<br>
0:10:05.420,0:10:07.760<br>
其實是 loss 掉很多 information<br>
<br>
0:10:07.760,0:10:09.420<br>
你應該這樣來表示小傑<br>
<br>
0:10:09.420,0:10:12.360<br>
你應該說，他強化系是 0.7<br>
<br>
0:10:12.360,0:10:16.120<br>
他放出系是0.2，那其實強化系跟變化系是比較接近的<br>
<br>
0:10:16.120,0:10:19.480<br>
所以有強化系，你也會有部分的變化系的能力<br>
<br>
0:10:19.480,0:10:22.260<br>
所以，變化系是 0.05，像小傑可以用剪刀<br>
<br>
0:10:22.260,0:10:25.240<br>
那個是變化系的能力，然後其他系是 0<br>
<br>
0:10:25.240,0:10:27.040<br>
所以，你應該要用<br>
<br>
0:10:27.040,0:10:29.460<br>
如果你只是把你手上所有的 object<br>
<br>
0:10:29.460,0:10:32.160<br>
分別 assign 到它屬於哪一個 cluster<br>
<br>
0:10:32.160,0:10:34.060<br>
這樣是以偏概全，這樣太粗了<br>
<br>
0:10:34.060,0:10:36.840<br>
你應該要用一個 vector<br>
<br>
0:10:36.840,0:10:39.540<br>
來表示你的 object<br>
<br>
0:10:39.540,0:10:42.080<br>
那這個 vector裡面的每一個 dimension<br>
<br>
0:10:42.080,0:10:44.060<br>
就代表了某一種特質<br>
<br>
0:10:44.060,0:10:46.060<br>
某一種 attribute<br>
<br>
0:10:46.060,0:10:47.260<br>
那這件事情<br>
<br>
0:10:47.260,0:10:51.120<br>
就叫做 Distributed 的 representation<br>
<br>
0:10:51.120,0:10:53.500<br>
如果你原來的 object 是一個<br>
<br>
0:10:53.500,0:10:55.840<br>
非常 high dimension 的東西，比如說 image<br>
<br>
0:10:55.840,0:10:58.060<br>
那你現在把它用它的<br>
<br>
0:10:58.060,0:11:00.880<br>
attribute，把它用它的特質來描述<br>
<br>
0:11:00.880,0:11:05.340<br>
它就會從比較高維的空間變成比較低維的空間<br>
<br>
0:11:05.340,0:11:08.400<br>
這一件事情就叫做 Dimension Reduction<br>
<br>
0:11:08.400,0:11:10.560<br>
那它們其實是一樣的事情<br>
<br>
0:11:10.560,0:11:12.480<br>
只是有不同的稱呼而已<br>
<br>
0:11:12.780,0:11:17.080<br>
那我們從另外一個角度來看一下<br>
為什麼 Dimension Reduction<br>
<br>
0:11:17.080,0:11:19.500<br>
是可能是有用的<br>
<br>
0:11:19.500,0:11:23.580<br>
舉例來說，假設你的 data 的分布<br>
<br>
0:11:23.580,0:11:25.120<br>
是長這個樣子<br>
<br>
0:11:25.120,0:11:26.880<br>
在 3D 的空間裡面<br>
<br>
0:11:26.880,0:11:31.380<br>
你現在的 data 的分布是長這個螺旋的樣子<br>
<br>
0:11:31.380,0:11:33.560<br>
但是，用3D的空間<br>
<br>
0:11:33.560,0:11:35.660<br>
來描述這些 data<br>
<br>
0:11:35.660,0:11:36.860<br>
其實是很浪費的<br>
<br>
0:11:36.860,0:11:38.800<br>
你從直覺上就會知道說<br>
<br>
0:11:38.800,0:11:41.540<br>
其實你可以把這個<br>
<br>
0:11:41.540,0:11:43.980<br>
這個類似地毯捲起來的東西<br>
<br>
0:11:43.980,0:11:46.420<br>
把它攤開、把它攤平<br>
<br>
0:11:46.420,0:11:47.520<br>
就變成這樣<br>
<br>
0:11:47.520,0:11:52.300<br>
所以，你其實只需要在 2D 的空間<br>
<br>
0:11:52.300,0:11:55.240<br>
就可以描述這個 3D 的 information<br>
<br>
0:11:55.240,0:11:58.120<br>
你根本不需要把這個問題放到 3D 來解<br>
<br>
0:11:58.120,0:11:59.680<br>
這樣是把問題複雜化<br>
<br>
0:11:59.680,0:12:02.900<br>
你其實在 2D 就可以做這個 task<br>
<br>
0:12:02.900,0:12:05.640<br>
或者是，我們舉一個比較具體的例子<br>
<br>
0:12:05.640,0:12:07.620<br>
比如說，我們考慮 MNIST<br>
<br>
0:12:07.620,0:12:10.820<br>
在 MNIST 裡面，每一個 input 的 digit<br>
<br>
0:12:10.820,0:12:14.480<br>
它是一個 image，它都用 28*28 的 dimension<br>
<br>
0:12:14.480,0:12:15.720<br>
來描述它<br>
<br>
0:12:15.720,0:12:17.380<br>
但是，實際上<br>
<br>
0:12:17.380,0:12:21.720<br>
多數 28*28 的 dimension 的 vector<br>
<br>
0:12:21.720,0:12:23.740<br>
你把它轉成一個 image<br>
<br>
0:12:23.740,0:12:25.600<br>
看起來都不像是一個數字<br>
<br>
0:12:25.600,0:12:28.120<br>
你 random sample 一個 28*28 的 vector<br>
<br>
0:12:28.120,0:12:29.540<br>
轉成 image 看起來可能是這樣<br>
<br>
0:12:29.540,0:12:31.480<br>
它其實根本就不像數字<br>
<br>
0:12:31.480,0:12:35.160<br>
所以在這個 28維 * 28維的空間裡面<br>
<br>
0:12:35.160,0:12:42.140<br>
是 digit 的 vector，其實是很少的<br>
<br>
0:12:42.140,0:12:46.460<br>
所以，其實你要描述一個 digit<br>
<br>
0:12:46.460,0:12:49.240<br>
或許根本不需要用到 28*28 維<br>
<br>
0:12:49.240,0:12:50.680<br>
你要描述一個 digit<br>
<br>
0:12:50.680,0:12:52.440<br>
你要的 dimension 可能是遠比<br>
<br>
0:12:52.440,0:12:55.240<br>
28維 * 28維少<br>
<br>
0:12:55.240,0:12:57.300<br>
所以，如果我們舉一個很極端的例子<br>
<br>
0:12:57.300,0:13:01.120<br>
比如這邊有一堆 3，然後這一堆 3<br>
<br>
0:13:01.120,0:13:03.660<br>
如果你是從 pixel 來看待它的話<br>
<br>
0:13:03.660,0:13:05.800<br>
你要用 28維 * 28維<br>
<br>
0:13:05.800,0:13:07.680<br>
來描述每一張 image<br>
<br>
0:13:07.680,0:13:11.600<br>
然而，實際上這一些 3，只需要用一個維度<br>
<br>
0:13:11.600,0:13:13.020<br>
就可以來表示<br>
<br>
0:13:13.020,0:13:16.300<br>
為甚麼呢？因為這些 3 就只是說<br>
<br>
0:13:16.300,0:13:18.360<br>
把原來的 3 放正<br>
<br>
0:13:18.360,0:13:20.560<br>
是中間這張 image<br>
<br>
0:13:20.560,0:13:22.360<br>
右轉 10 度轉就變成它<br>
<br>
0:13:22.360,0:13:26.200<br>
右轉20度就變它，左轉10度變它，左轉20度就變它<br>
<br>
0:13:26.200,0:13:28.480<br>
所以，你唯一需要記錄的事情只有<br>
<br>
0:13:28.480,0:13:32.680<br>
今天這張 image，它是左轉多少度、右轉多少度<br>
<br>
0:13:32.680,0:13:35.960<br>
你就可以知道說，它在 28 維的空間裡面<br>
<br>
0:13:35.960,0:13:37.860<br>
應該長什麼樣子<br>
<br>
0:13:37.860,0:13:40.940<br>
你只需要抓住這一個重點<br>
<br>
0:13:40.940,0:13:42.460<br>
你只要抓住這個角度的變化<br>
<br>
0:13:42.460,0:13:44.700<br>
你就可以知道 28 維空間中的變化<br>
<br>
0:13:44.700,0:13:47.480<br>
所以，只需要用一維就可以描述這些 image<br>
<br>
0:13:47.480,0:13:49.140<br>
所以，這就像我剛才舉的例子<br>
<br>
0:13:49.140,0:13:53.380<br>
這個是樊一翁，這個就是他的頭這樣子<br>
<br>
0:13:55.300,0:13:59.540<br>
那怎麼做 Dimension Reduction 呢<br>
<br>
0:13:59.540,0:14:01.240<br>
在做 Dimension Reduction 的時候<br>
<br>
0:14:01.240,0:14:02.660<br>
我們要做的事情就是<br>
<br>
0:14:02.660,0:14:04.200<br>
找一個 function<br>
<br>
0:14:04.200,0:14:08.300<br>
這個 function 的 input 是一個 vector, x<br>
<br>
0:14:08.300,0:14:12.100<br>
它的 output 是另外一個 vector, z<br>
<br>
0:14:12.100,0:14:14.440<br>
但是，因為是 Dimension Reduction，所以<br>
<br>
0:14:14.440,0:14:16.080<br>
你 output 的這個 vector, z<br>
<br>
0:14:16.080,0:14:20.660<br>
它的 dimension 要比 input 的 x 還要小<br>
<br>
0:14:20.660,0:14:23.980<br>
這樣你才是做 Dimension Reduction<br>
<br>
0:14:23.980,0:14:25.680<br>
在做 Dimension Reduction 裡面<br>
<br>
0:14:25.680,0:14:28.420<br>
最簡單的方法是 Feature Selection<br>
<br>
0:14:28.420,0:14:30.800<br>
這個方法就沒有什麼好講的，這個方法是這樣<br>
<br>
0:14:30.800,0:14:35.480<br>
現在如果你把你的 data 的分布拿出來看一下<br>
<br>
0:14:35.480,0:14:37.160<br>
本來在二維的平面上<br>
<br>
0:14:37.160,0:14:41.200<br>
但是，其實你發現都集中在 x2 的 dimension 而已<br>
<br>
0:14:41.200,0:14:44.020<br>
所以，x1 這個 dimension 沒什麼用，把它拿掉<br>
<br>
0:14:44.020,0:14:45.400<br>
就只有 x2 這個 dimension<br>
<br>
0:14:45.400,0:14:48.500<br>
你就等於是做到 Dimension Reduction 這件事<br>
<br>
0:14:48.500,0:14:52.320<br>
但是這個方法不見得總是有用，因為有很多時候是<br>
<br>
0:14:52.320,0:14:55.740<br>
你的 case 是，你任何一個 dimension 都<br>
<br>
0:14:55.740,0:14:58.360<br>
其實都不能拿掉<br>
<br>
0:14:59.660,0:15:02.440<br>
那另外一個常見的方法叫做<br>
<br>
0:15:02.440,0:15:04.620<br>
Principle Component Analysis<br>
<br>
0:15:04.620,0:15:08.040<br>
叫做 PCA<br>
<br>
0:15:08.040,0:15:11.580<br>
這個 PCA 怎麼做呢？這個 PCA 做的事情是這樣<br>
<br>
0:15:11.580,0:15:17.180<br>
它說這個 function 是一個很簡單的 linear function<br>
<br>
0:15:17.180,0:15:21.940<br>
這個 input, x 跟這個 output, z 之間的關係<br>
<br>
0:15:21.940,0:15:24.960<br>
就是一個 linear 的 transform<br>
<br>
0:15:24.960,0:15:28.880<br>
也就是你把這個 x 乘上一個 matrix, W<br>
<br>
0:15:28.880,0:15:31.620<br>
你就得到它的 output, z<br>
<br>
0:15:31.620,0:15:33.580<br>
那現在要做的事情就是<br>
<br>
0:15:33.580,0:15:36.880<br>
根據一大堆的 x，我們現在不知道 z 長什麼樣子<br>
<br>
0:15:36.880,0:15:38.260<br>
只有一大堆的 x<br>
<br>
0:15:38.260,0:15:43.840<br>
根據一大堆的 x，我們要把這個 W 找出來<br>
<br>
0:15:43.840,0:15:46.560<br>
那如果你要知道比較細節的東西的話<br>
<br>
0:15:46.560,0:15:50.080<br>
你可以看一下 Bishop 的第十二章<br>
<br>
0:15:50.080,0:15:53.600<br>
接下來，我們介紹一下 PCA<br>
<br>
0:15:53.600,0:15:55.880<br>
我剛才講過 PCA 要做的事情<br>
<br>
0:15:55.880,0:15:59.740<br>
就是找這一個 W<br>
<br>
0:15:59.740,0:16:01.640<br>
那這個 W 怎麼找呢？<br>
<br>
0:16:01.640,0:16:03.840<br>
假設我們現在考慮一個<br>
<br>
0:16:03.840,0:16:05.460<br>
比較簡單的 case<br>
<br>
0:16:05.460,0:16:09.000<br>
我們考慮一個 one dimensional 的 case<br>
<br>
0:16:09.000,0:16:10.440<br>
什麼意思呢<br>
<br>
0:16:10.440,0:16:14.220<br>
我們現在假設，我們只要把我們的 data project 到<br>
<br>
0:16:14.220,0:16:15.600<br>
一維的空間上面<br>
<br>
0:16:15.600,0:16:16.860<br>
也就是我們的 z 呢<br>
<br>
0:16:16.860,0:16:20.260<br>
它只是一個一維的 vector，所謂一維的 vector<br>
<br>
0:16:20.260,0:16:23.200<br>
就是一個 scalar，我們的 z 是一個 scalar<br>
<br>
0:16:23.200,0:16:25.300<br>
或者是說，我們可以這樣寫<br>
<br>
0:16:25.300,0:16:30.780<br>
就是，如果我們這邊 z 只是一個 scalar 的話<br>
<br>
0:16:30.780,0:16:33.200<br>
那我們的 W 其實就是一個<br>
<br>
0:16:33.200,0:16:35.920<br>
一個 row 對不對，就是一個 row 而已<br>
<br>
0:16:35.920,0:16:39.800<br>
那我們就用 w^1 來表示<br>
<br>
0:16:39.800,0:16:42.080<br>
w 的第一個 row<br>
<br>
0:16:42.080,0:16:46.760<br>
我們把 x 跟 w 的第一個 row, w^1<br>
<br>
0:16:46.760,0:16:52.620<br>
做 inner product，我們就得到一個 scalar, z1<br>
<br>
0:16:52.620,0:16:54.580<br>
接下來，我們要問的問題是<br>
<br>
0:16:54.580,0:16:57.500<br>
我們要找的這個 w^1<br>
<br>
0:16:57.500,0:17:01.880<br>
應該要長什麼樣子<br>
<br>
0:17:01.880,0:17:09.380<br>
首先，我們先假設 w^1 的長度是 1<br>
<br>
0:17:09.380,0:17:11.160<br>
w^1 的 2-norm 是 1<br>
<br>
0:17:11.160,0:17:14.460<br>
這個假設是有必要的，等一下你會看得更清楚為什麼<br>
<br>
0:17:14.460,0:17:16.120<br>
一定要有這一個假設<br>
<br>
0:17:16.120,0:17:19.660<br>
如果 w^1 的 2-norm 是 1 的話<br>
<br>
0:17:19.660,0:17:22.500<br>
那 w^1 跟 x 做 inner product<br>
<br>
0:17:22.500,0:17:24.520<br>
得到的 z1 意味著什麼<br>
<br>
0:17:24.520,0:17:27.140<br>
它意謂著說，現在呢<br>
<br>
0:17:27.140,0:17:32.420<br>
w 跟 x 是高維空間中的一個點<br>
<br>
0:17:32.420,0:17:37.420<br>
w^1 是高維空間中的一個 vector<br>
<br>
0:17:37.420,0:17:40.980<br>
那所謂的 z1 就是 x<br>
<br>
0:17:40.980,0:17:43.400<br>
在 w^1 上面的投影<br>
<br>
0:17:43.400,0:17:45.660<br>
它這個投影的值<br>
<br>
0:17:45.660,0:17:48.140<br>
就是 w^1 跟 x 的 inner product<br>
<br>
0:17:48.140,0:17:51.460<br>
這個就沒有什麼問題了<br>
<br>
0:17:51.460,0:17:54.320<br>
這個就是大一線代裡面教過的東西<br>
<br>
0:17:54.320,0:17:57.000<br>
所以，我們現在要做的事情就是<br>
<br>
0:17:57.000,0:17:59.540<br>
把一堆 x<br>
<br>
0:17:59.540,0:18:04.140<br>
透過 w^1 把它投影變成 z1<br>
<br>
0:18:04.140,0:18:07.180<br>
我們就得到一堆 z1，每一個 x 都變成一個 z1<br>
<br>
0:18:07.180,0:18:11.940<br>
現在的問題就是，w^1 應該長什麼樣子呢<br>
<br>
0:18:11.940,0:18:14.560<br>
要選哪一個 w^1，舉例來說<br>
<br>
0:18:14.560,0:18:17.180<br>
假設這個是 x 的分布<br>
<br>
0:18:17.180,0:18:18.780<br>
這個 x 的分布是什麼呢<br>
<br>
0:18:18.780,0:18:22.880<br>
它是橫座標，在每一個點代表一隻寶可夢<br>
<br>
0:18:22.880,0:18:26.120<br>
它的橫座標是攻擊力<br>
<br>
0:18:26.120,0:18:29.360<br>
它的縱座標是防禦力<br>
<br>
0:18:29.900,0:18:33.220<br>
那今天如果我要選<br>
<br>
0:18:33.220,0:18:37.400<br>
我們要把這個二維的，這邊這個 x 都是二維的<br>
<br>
0:18:37.400,0:18:39.980<br>
我把這個二維投影到一維<br>
<br>
0:18:39.980,0:18:43.240<br>
我應該要選什麼樣的 w^1？<br>
<br>
0:18:43.240,0:18:46.480<br>
我可以選這樣的 w^1<br>
<br>
0:18:46.480,0:18:49.520<br>
我可以選 w^1 指向這個方向<br>
<br>
0:18:49.520,0:18:52.880<br>
我也可以選 w^1 指向這個方向<br>
<br>
0:18:52.880,0:18:54.500<br>
我選不同的方向<br>
<br>
0:18:54.500,0:18:58.080<br>
我最後得到的結果，得到的 projection 的結果<br>
<br>
0:18:58.080,0:19:00.160<br>
它會是不一樣的<br>
<br>
0:19:00.860,0:19:03.800<br>
那我們會想要<br>
<br>
0:19:03.800,0:19:07.480<br>
你總是要給我一個目標，我才知道要選什麼樣的 w^1<br>
<br>
0:19:07.480,0:19:09.460<br>
現在的目標是這樣<br>
<br>
0:19:09.460,0:19:12.040<br>
我們希望選一個 w^1<br>
<br>
0:19:12.040,0:19:16.980<br>
它經過 projection 以後，得到的這些 z1 的分布<br>
<br>
0:19:16.980,0:19:19.240<br>
是越大越好<br>
<br>
0:19:19.240,0:19:22.540<br>
也就是說，我們不希望通過這個 projection 以後<br>
<br>
0:19:22.540,0:19:24.860<br>
所有的點通通擠在一起<br>
<br>
0:19:24.860,0:19:30.080<br>
就變成把本來 data point 和 data point 之間的奇異度<br>
<br>
0:19:30.080,0:19:32.160<br>
拿掉了，我們是希望說<br>
<br>
0:19:32.160,0:19:35.680<br>
經過這個 projection 以後，不同的 data point<br>
<br>
0:19:35.680,0:19:37.120<br>
它們之間的區別<br>
<br>
0:19:37.120,0:19:38.900<br>
我們仍然是可以看得出來<br>
<br>
0:19:38.900,0:19:42.460<br>
所以，我們希望找一個 projection 的方向，它可以讓<br>
<br>
0:19:42.460,0:19:45.900<br>
projection 後的 variance 越大越好<br>
<br>
0:19:45.900,0:19:49.860<br>
如果我們看這一個例子的話<br>
<br>
0:19:49.860,0:19:53.120<br>
你就會覺得說，如果是選這個方向的話<br>
<br>
0:19:53.120,0:19:55.640<br>
經過 projection 以後，你的點可能是分布在<br>
<br>
0:19:55.640,0:19:58.240<br>
這個地方，大概分布在這個 range<br>
<br>
0:19:58.240,0:20:01.820<br>
那如果是 projection 在這個方向的話<br>
<br>
0:20:01.820,0:20:04.200<br>
那你點的分布可能是這個 range<br>
<br>
0:20:04.200,0:20:07.040<br>
所以，如果 projection 在這個方向上的話<br>
<br>
0:20:07.040,0:20:08.980<br>
你會有比較大的 variance<br>
<br>
0:20:08.980,0:20:11.880<br>
但是在這個方向上的話，你會有一個比較小的 variance<br>
<br>
0:20:11.880,0:20:13.600<br>
所以你要選 w^1 的時候，你可能<br>
<br>
0:20:13.600,0:20:18.180<br>
選 w^1 的方向是指向這個方向<br>
<br>
0:20:18.180,0:20:20.600<br>
其實這個 w^1 代表什麼意思呢<br>
<br>
0:20:20.600,0:20:22.980<br>
如果從這個圖，你可以看到說<br>
<br>
0:20:22.980,0:20:26.940<br>
這個 w^1 其實是代表了寶可夢的強度<br>
<br>
0:20:26.940,0:20:32.020<br>
寶可夢可能會有一個隱藏的 vector 代表它的強度<br>
<br>
0:20:32.020,0:20:35.460<br>
這個隱藏的 vector，同時影響了它的<br>
<br>
0:20:35.460,0:20:38.240<br>
防禦力跟攻擊力<br>
<br>
0:20:38.240,0:20:43.400<br>
所以，防禦力跟攻擊力是會同時上升的<br>
<br>
0:20:44.040,0:20:47.460<br>
如果我們要用 equation 來表示它的話<br>
<br>
0:20:47.460,0:20:51.440<br>
你就會說我們現在要去 maximize 的對象是<br>
<br>
0:20:51.440,0:20:53.760<br>
z1 的 variance<br>
<br>
0:20:53.760,0:20:57.340<br>
z1 的 variance 就是 summation over 所有的 z1<br>
<br>
0:20:57.340,0:21:03.480<br>
(z1 - z1\bar) 的平方，z1\bar 就是做 z1 的平均<br>
<br>
0:21:05.220,0:21:07.960<br>
那這樣子<br>
<br>
0:21:07.960,0:21:11.460<br>
我們等一下再講怎麼做<br>
<br>
0:21:11.460,0:21:13.840<br>
假設你知道怎麼做，你解一解<br>
<br>
0:21:13.840,0:21:17.060<br>
你找到一個 w^1，你就可以讓 z1 最大<br>
<br>
0:21:17.060,0:21:18.880<br>
那你就找到這個 w^1，就結束了<br>
<br>
0:21:19.120,0:21:22.040<br>
再來，你可能不只要投影到一維<br>
<br>
0:21:22.040,0:21:26.180<br>
你想要投影到更多維，比如說，你想要投影到一個<br>
<br>
0:21:26.180,0:21:27.820<br>
二維的平面<br>
<br>
0:21:27.820,0:21:31.240<br>
那如果你想要投影到二維的平面的時候<br>
<br>
0:21:31.240,0:21:35.380<br>
這個時候你就把 x 跟另外一個 w^2<br>
<br>
0:21:35.380,0:21:37.340<br>
做 inner product，得到 z2<br>
<br>
0:21:37.340,0:21:39.780<br>
這個 z1 跟 z2 串起來就得到這邊的 z<br>
<br>
0:21:39.780,0:21:43.220<br>
這個 w^1 跟 w^2 的 transpose 排起來就是<br>
<br>
0:21:43.220,0:21:45.580<br>
W 的第一個 row 跟第二個 row<br>
<br>
0:21:46.400,0:21:49.180<br>
好，那這一個 z2<br>
<br>
0:21:49.180,0:21:51.200<br>
我們要怎麼找這一個 w^2 呢<br>
<br>
0:21:51.200,0:21:54.440<br>
跟剛才找 z1 一樣<br>
<br>
0:21:54.440,0:21:57.920<br>
我們希望，首先 w^2 它的 2-norm 是 1<br>
<br>
0:21:57.920,0:22:03.500<br>
然後，接下來這個 z2 它的分佈也是越大越好<br>
<br>
0:22:03.560,0:22:04.920<br>
也是越大越好<br>
<br>
0:22:04.920,0:22:06.940<br>
但是，如果你只是要讓<br>
<br>
0:22:06.940,0:22:10.420<br>
讓 z2 的 variance 越大越好，讓這個式子越大越好<br>
<br>
0:22:10.420,0:22:13.800<br>
你找出來就不是 w^1，w^1 剛才已經找過了<br>
<br>
0:22:13.800,0:22:16.220<br>
對不對，所以你就等於什麼事都沒有做<br>
<br>
0:22:16.220,0:22:19.100<br>
所以，你要再加一個 constraint<br>
<br>
0:22:19.100,0:22:22.540<br>
這個 constraint 是我們剛才已經先找過 w^1 了<br>
<br>
0:22:22.540,0:22:26.680<br>
這個 w^2 要跟 w^1是垂直的<br>
<br>
0:22:26.680,0:22:31.520<br>
或者是 w^1 跟 w^2 是 orthogonal<br>
<br>
0:22:31.520,0:22:35.600<br>
w^1 跟 w^2，它們做 inner product 等於 0<br>
<br>
0:22:35.600,0:22:37.800<br>
藉由這個方法，你就可以<br>
<br>
0:22:37.800,0:22:40.420<br>
先找 w^1，再找 w^2，再找 w^3<br>
<br>
0:22:40.420,0:22:42.880<br>
就看你要 project 到幾維<br>
<br>
0:22:42.880,0:22:45.320<br>
你要 project 到幾維是你要自己決定的<br>
<br>
0:22:45.320,0:22:46.780<br>
這個就跟我們要幾個 cluster<br>
<br>
0:22:46.780,0:22:48.760<br>
要幾個 hidden layer 也是自己決定的<br>
<br>
0:22:48.760,0:22:51.200<br>
你要 project 到 K 維<br>
<br>
0:22:51.200,0:22:53.960<br>
那你就找 w^1, w^2 到 w^K<br>
<br>
0:22:53.960,0:22:57.600<br>
你就把你所有找出來的 w^1, w^2 到 w^K<br>
<br>
0:22:57.600,0:23:00.560<br>
排起來當作 W row<br>
<br>
0:23:00.560,0:23:04.700<br>
放在這一邊，就結束了<br>
<br>
0:23:04.700,0:23:08.720<br>
那這邊有一件事情就是，這個找出來的 W<br>
<br>
0:23:08.720,0:23:12.000<br>
它會是一個 orthogonal的 matrix<br>
<br>
0:23:12.000,0:23:13.040<br>
為什麼呢？<br>
<br>
0:23:13.040,0:23:15.040<br>
如果你看它的 row 的話<br>
<br>
0:23:15.040,0:23:18.720<br>
它的 w^1 跟 w^2 是 orthogonal的<br>
<br>
0:23:18.720,0:23:23.080<br>
然後 w^1 的 2-norm 跟 w^2 的 2-norm 都是 1<br>
<br>
0:23:23.080,0:23:25.640<br>
所以，它的 row 是<br>
<br>
0:23:25.640,0:23:28.320<br>
norm 是 1，而且互相之間都是 orthogonal<br>
<br>
0:23:28.320,0:23:32.780<br>
所以，它是一個 orthogonal的 matrix<br>
<br>
0:23:32.780,0:23:34.700<br>
接下來的問題就是<br>
<br>
0:23:34.700,0:23:38.440<br>
怎麼找 w^1 跟 w^2 呢？<br>
<br>
0:23:38.440,0:23:40.800<br>
怎麼解這個問題呢？<br>
<br>
0:23:40.800,0:23:42.460<br>
怎麼解這個問題呢？<br>
<br>
0:23:42.460,0:23:48.120<br>
這邊其實有一個，這邊的這個解法其實是蠻容易的<br>
<br>
0:23:48.120,0:23:51.720<br>
這個怎麼解呢？<br>
<br>
0:23:51.720,0:23:55.780<br>
你其實要用 Lagrange multiplier<br>
<br>
0:23:55.780,0:23:59.520<br>
這邊有一個 warning od map，如果你沒有聽懂的話就算了<br>
<br>
0:23:59.520,0:24:02.140<br>
這個可以都直接 call 套件<br>
<br>
0:24:02.140,0:24:03.300<br>
這個其實 call 套件就有了<br>
<br>
0:24:03.300,0:24:05.520<br>
而且就算是你不會下面這套東西的話<br>
<br>
0:24:05.520,0:24:07.920<br>
你其實可以用 Gradient Descent 的方法<br>
<br>
0:24:07.920,0:24:10.100<br>
這我們之後會講，你可以把一個<br>
<br>
0:24:10.100,0:24:13.520<br>
你可以把 PCA 這件事情，描述成一個 neural network<br>
<br>
0:24:13.520,0:24:17.480<br>
然後，就用 Gradient Descent 的方法來解它<br>
<br>
0:24:17.480,0:24:23.340<br>
所以不一定要用 Lagrange multiplier 來做這個 PCA<br>
<br>
0:24:23.340,0:24:28.320<br>
這個 PCA 可以把它看成是一個 neural network 一樣<br>
<br>
0:24:28.320,0:24:32.180<br>
但我們現在很快講一下 Lagrange multiplier 這個方法<br>
<br>
0:24:32.180,0:24:35.140<br>
這個很經典的方法，它是怎麼做的呢？它是這樣子<br>
<br>
0:24:35.140,0:24:38.620<br>
我們說 z1 等於 w^1 跟 x 的 inner product<br>
<br>
0:24:38.620,0:24:42.480<br>
那 z1 的平均值是 summation over 所有的 z1<br>
<br>
0:24:42.480,0:24:46.160<br>
也就是 summation over 所有 w^1 跟 x 的 inner product<br>
<br>
0:24:47.440,0:24:49.300<br>
這邊是 summation over 所有 data point<br>
<br>
0:24:49.300,0:24:51.700<br>
跟 w^1 無關，所以可以把 w^1 提出來<br>
<br>
0:24:51.700,0:24:53.900<br>
變成先 summation over 所有的 x<br>
<br>
0:24:53.900,0:24:55.880<br>
再跟 w^1 做 inner product<br>
<br>
0:24:55.880,0:24:59.840<br>
得到 w^1 跟 x 的平均的 inner product<br>
<br>
0:24:59.840,0:25:03.420<br>
接下來，我們說我們要 maximize 的對象<br>
<br>
0:25:03.420,0:25:06.540<br>
是 z1 的 variance<br>
<br>
0:25:06.540,0:25:08.600<br>
那 z1 的 variance 我們可以寫成<br>
<br>
0:25:08.600,0:25:12.520<br>
(z1 - 它的平均值)的平方，再 summation over 所有的 z1<br>
<br>
0:25:12.520,0:25:13.980<br>
我們先把這一項整理一下<br>
<br>
0:25:13.980,0:25:16.460<br>
這項整理一下變成什麼樣子呢<br>
<br>
0:25:16.460,0:25:19.400<br>
z1 是 w^1 跟 x 的 inner product<br>
<br>
0:25:19.400,0:25:22.740<br>
z1\bar 是 w^1 跟 x\bar 的 inner product<br>
<br>
0:25:22.740,0:25:24.320<br>
然後，再取平方<br>
<br>
0:25:24.320,0:25:26.720<br>
那都有 w^1，所以把 w^1 提出來<br>
<br>
0:25:26.720,0:25:32.180<br>
變成 summation over [w^1 * (x - x\bar)]^2<br>
<br>
0:25:32.180,0:25:34.840<br>
那這個平方<br>
<br>
0:25:34.840,0:25:36.780<br>
你可以把它做一下轉化<br>
<br>
0:25:36.780,0:25:38.460<br>
怎麼轉化呢？<br>
<br>
0:25:38.460,0:25:42.420<br>
w^1 是一個 vector，(x - x\bar) 是另外一個 vector<br>
<br>
0:25:42.420,0:25:47.220<br>
兩個 vector，比如說 w^1 就是 a，(x - x\bar) 就是 b<br>
<br>
0:25:47.220,0:25:49.400<br>
a 跟 b 的 inner product 的平方<br>
<br>
0:25:49.400,0:25:52.620<br>
可以寫成 a 的 transpose 乘 b 的平方<br>
<br>
0:25:52.620,0:25:57.400<br>
可以寫成 a 的 transpose 乘 b，<br>
再乘上 a 的 transpose 乘 b<br>
<br>
0:25:57.400,0:26:00.540<br>
然後這一項，其實可以寫成<br>
<br>
0:26:00.540,0:26:05.080<br>
a 的 transpose 乘 b，然後乘上<br>
<br>
0:26:05.080,0:26:08.000<br>
(a 的 transpose 乘 b) 再 transpose<br>
<br>
0:26:08.000,0:26:10.400<br>
為甚麼這邊可以直接加 transpose 呢<br>
<br>
0:26:10.400,0:26:13.320<br>
因為 (a^T)*b 是一個 scalar，所以再 transpose<br>
<br>
0:26:13.320,0:26:15.320<br>
還是它自己<br>
<br>
0:26:15.320,0:26:17.880<br>
那 transpose 以後<br>
<br>
0:26:17.880,0:26:23.200<br>
[(a^T)*b]^T 就是把它們順序對調，再加上 transpose<br>
<br>
0:26:23.200,0:26:26.400<br>
所以變成 (a^T)*b*(b^T)*a<br>
<br>
0:26:26.400,0:26:29.360<br>
然後把 a 代回 w^1<br>
<br>
0:26:29.360,0:26:33.900<br>
b 代回 (x - x\bar)，你就得到這樣的式子<br>
<br>
0:26:33.900,0:26:39.640<br>
(w^1)^T * (x - x\bar) * (x - x\bar)^T * w^1<br>
<br>
0:26:39.640,0:26:40.860<br>
接下來呢<br>
<br>
0:26:40.860,0:26:43.560<br>
你這一邊是 summation over 所有的 data<br>
<br>
0:26:43.560,0:26:46.260<br>
summation over 所有的 data，所以跟 w 無關<br>
<br>
0:26:46.260,0:26:49.940<br>
跟 w 無關，所以把 w^1 的 transpose 拿出去<br>
<br>
0:26:49.940,0:26:51.360<br>
把 w^1 拿出去<br>
<br>
0:26:51.360,0:26:53.560<br>
注意一下，這邊 summation 是 summation over<br>
<br>
0:26:53.560,0:26:57.400<br>
(x - x\bar)*(x - x\bar)^T 這一項<br>
<br>
0:26:57.400,0:27:00.320<br>
w^1 被拿出去了<br>
<br>
0:27:00.320,0:27:02.240<br>
那 (x - x\bar) 的 transpose<br>
<br>
0:27:02.240,0:27:06.560<br>
(x - x\bar) * (x - x\bar)^T <br>
summation over 所有 data point 是甚麼呢<br>
<br>
0:27:06.560,0:27:09.460<br>
它是 x 的 covariance<br>
<br>
0:27:09.460,0:27:13.740<br>
對不對，它是 x 的 covariance matrix，所以這一項<br>
<br>
0:27:13.740,0:27:18.280<br>
其實就是 (w^1)^T * Cov(x) * w^1<br>
<br>
0:27:18.280,0:27:22.360<br>
我們用 S 來描述 x 的 covariance matrix<br>
<br>
0:27:22.360,0:27:25.800<br>
所以，現在我們要解的問題是這樣<br>
<br>
0:27:25.800,0:27:27.120<br>
找出一個 w^1<br>
<br>
0:27:27.120,0:27:29.840<br>
它可以 maximize w^1 的 transpose<br>
<br>
0:27:29.840,0:27:31.400<br>
乘上一個 matrix, x<br>
<br>
0:27:31.400,0:27:33.240<br>
再乘上 w^1<br>
<br>
0:27:33.240,0:27:36.300<br>
但這個 optimization 的對象是有 constraint 的<br>
<br>
0:27:36.300,0:27:37.660<br>
如果沒有 constraint 的話<br>
<br>
0:27:37.660,0:27:41.100<br>
這個問題它會有無聊的 solution<br>
<br>
0:27:41.100,0:27:47.040<br>
你把 w^1 的每一個值都變無窮大，就結束了<br>
<br>
0:27:47.040,0:27:49.600<br>
所以，你要有 constraint<br>
<br>
0:27:49.600,0:27:51.820<br>
它的 constraint 是說<br>
<br>
0:27:51.820,0:27:55.540<br>
w^1 的 2-norm 要等於 1<br>
<br>
0:27:55.540,0:27:57.420<br>
那有了這些以後<br>
<br>
0:27:57.420,0:28:01.000<br>
我們就要解這一個 optimization 的 problem<br>
<br>
0:28:01.000,0:28:05.640<br>
這邊這個 S，S 是 covariance matrix，<br>
x 的 covariance matrix<br>
<br>
0:28:05.640,0:28:06.760<br>
它是 symmetric 的<br>
<br>
0:28:06.760,0:28:09.280<br>
而且因為它是 covariance matrix 的關係<br>
<br>
0:28:09.280,0:28:11.000<br>
它又是半正定<br>
<br>
0:28:11.000,0:28:12.600<br>
它是 positive-semidefinite<br>
<br>
0:28:12.600,0:28:16.160<br>
也就是說它所有的 eigenvalue<br>
<br>
0:28:16.160,0:28:18.240<br>
都是 non-negative 的<br>
<br>
0:28:18.240,0:28:19.820<br>
如果你對這件事情有困惑的話<br>
<br>
0:28:19.820,0:28:23.020<br>
你就回去翻一下線代課本，你其實可以在<br>
<br>
0:28:23.020,0:28:27.960<br>
我的個人的網頁上找到線代的教學<br>
<br>
0:28:27.960,0:28:31.320<br>
就在 machine learning 的網頁下面<br>
<br>
0:28:33.820,0:28:38.520<br>
我們就先講結論，假設你不想聽中間的過程的話<br>
<br>
0:28:38.520,0:28:42.560<br>
結論就是，這一個 programming 的 problem<br>
<br>
0:28:42.560,0:28:44.860<br>
這個 problem 它的 solution 就是<br>
<br>
0:28:44.860,0:28:50.220<br>
w^1 是 covariance matrix 的 eigenvector<br>
<br>
0:28:50.220,0:28:53.260<br>
它不只是一個  eigenvector<br>
<br>
0:28:53.260,0:28:56.740<br>
它是對應到最大的  eigenvalue<br>
<br>
0:28:56.740,0:29:00.100<br>
λ1 的那一個  eigenvector<br>
<br>
0:29:00.100,0:29:01.600<br>
這個就是結論<br>
<br>
0:29:01.600,0:29:03.580<br>
那中間的過程是怎麼樣呢？<br>
<br>
0:29:03.580,0:29:06.420<br>
中間的過程是，首先我們要用 Lagrange multiplier<br>
<br>
0:29:06.420,0:29:08.120<br>
不過，你對這個東西有碰過的話<br>
<br>
0:29:08.120,0:29:09.860<br>
這不是我們這一堂課應該講的<br>
<br>
0:29:09.860,0:29:12.980<br>
你就看一下 Bishop 的 Appendix<br>
<br>
0:29:12.980,0:29:16.280<br>
在做 Lagrange multiplier 的時候，這邊有個式子<br>
<br>
0:29:16.280,0:29:18.540<br>
這個式子長這樣子<br>
<br>
0:29:18.540,0:29:20.980<br>
它是先把這一項拿到這邊<br>
<br>
0:29:20.980,0:29:25.020<br>
再減掉 α 乘上這個 constraint<br>
<br>
0:29:25.020,0:29:26.360<br>
這個 constraint<br>
<br>
0:29:26.360,0:29:28.540<br>
然後接下來，你把這個 g<br>
<br>
0:29:28.540,0:29:33.900<br>
對所有的 w 做偏微分<br>
<br>
0:29:33.900,0:29:38.020<br>
那 w 是一個 vector<br>
<br>
0:29:38.020,0:29:39.600<br>
它裡面有很多的 element<br>
<br>
0:29:39.600,0:29:40.780<br>
所以你把這個 function<br>
<br>
0:29:40.780,0:29:42.840<br>
對 w 的第一個 element 做偏微分<br>
<br>
0:29:42.840,0:29:44.580<br>
對第二個 element 做偏微分<br>
<br>
0:29:44.580,0:29:48.100<br>
然後，令這些式子通通等於 0<br>
<br>
0:29:48.100,0:29:50.980<br>
整理一下以後，你會得到一個式子<br>
<br>
0:29:50.980,0:29:52.640<br>
這個式子，是這樣告訴我們的<br>
<br>
0:29:52.640,0:29:54.500<br>
這個式子告訴我們說<br>
<br>
0:29:54.500,0:30:00.860<br>
這邊的這個 solution<br>
<br>
0:30:00.860,0:30:03.400<br>
它會滿足下面這個式子<br>
<br>
0:30:03.400,0:30:09.420<br>
S*(w^1) - α*(w^1) 等於 0，再整理一下<br>
<br>
0:30:09.420,0:30:12.080<br>
變成這樣<br>
<br>
0:30:12.080,0:30:15.360<br>
那這個 w^1 呢<br>
<br>
0:30:15.360,0:30:22.260<br>
如果你寫成這樣，就是 S*(w^1) = α*(w^1) 的話<br>
<br>
0:30:22.260,0:30:26.240<br>
那 w^1 就是一個 eigenvector<br>
<br>
0:30:26.240,0:30:28.040<br>
w^1 就是 S 的 eigenvector<br>
<br>
0:30:28.040,0:30:31.660<br>
因為 (w^1)*S 等於自己乘上某一個 scalar<br>
<br>
0:30:31.660,0:30:33.740<br>
所以，w^1 是 S 的 eigenvector<br>
<br>
0:30:35.780,0:30:39.960<br>
但是現在，S 的 eigenvector 有一大把<br>
<br>
0:30:39.960,0:30:45.060<br>
有很多，而且你可以找到一大把 eigenvector<br>
<br>
0:30:45.060,0:30:47.860<br>
它的 norm 都是 1<br>
<br>
0:30:47.860,0:30:49.800<br>
所以，接下來你要做的事情是<br>
<br>
0:30:49.800,0:30:51.820<br>
看哪一個 eigenvector<br>
<br>
0:30:51.820,0:30:54.960<br>
代到這個式子裡面，可以 maximize<br>
<br>
0:30:54.960,0:30:57.820<br>
(w^1)^T * S * (w^1)<br>
<br>
0:30:57.820,0:30:59.640<br>
誰可以 maximize 它呢？<br>
<br>
0:30:59.640,0:31:03.120<br>
我們把這個整理一下<br>
<br>
0:31:03.120,0:31:07.440<br>
把 (w^1)^T * S * (w^1) 整理一下，它變成<br>
<br>
0:31:07.440,0:31:12.000<br>
這個 S*(w^1) 就是 α*(w^1)<br>
<br>
0:31:12.000,0:31:16.960<br>
這一項就變成 α * (w^1)^T * (w^1)<br>
<br>
0:31:16.960,0:31:20.820<br>
然後，(w^1)^T * (w^1)，這個是 1<br>
<br>
0:31:20.820,0:31:25.040<br>
所以，這一邊就得到個 α<br>
<br>
0:31:25.460,0:31:28.440<br>
然後，接下來就是找<br>
<br>
0:31:28.440,0:31:32.820<br>
誰可以讓這個 α 最大呢？<br>
<br>
0:31:32.820,0:31:35.600<br>
既然這個值等於 α 的話，誰可以讓 α 最大呢？<br>
<br>
0:31:35.600,0:31:42.880<br>
w^1 是對應到最大的<br>
那個 eigenvalue 的 eigenvector 的時候<br>
<br>
0:31:42.880,0:31:47.860<br>
它可以讓 α 最大，這個 α 就是最大的 eigenvalues, λ1<br>
<br>
0:31:47.860,0:31:51.620<br>
如果你沒有聽懂的話，你只要記得這個結論就好<br>
<br>
0:31:51.620,0:31:54.580<br>
第二個，如果要找 w^2 的話<br>
<br>
0:31:54.580,0:31:59.240<br>
我們要解什麼樣的式子呢？我們如果要找 w^2 的話<br>
<br>
0:31:59.240,0:32:01.240<br>
我們要解的是<br>
<br>
0:32:01.240,0:32:03.420<br>
這樣子的一個 equation<br>
<br>
0:32:03.420,0:32:06.320<br>
我們要解說，我們要 maximize<br>
<br>
0:32:06.320,0:32:11.220<br>
我們要 maximize 根據 w^2 投影以後的 variance<br>
<br>
0:32:11.220,0:32:14.980<br>
這個寫成這樣，(w^2)^T * S * (w^2)<br>
<br>
0:32:14.980,0:32:17.520<br>
同時，w^2 要滿足 norm 等於 1<br>
<br>
0:32:17.520,0:32:22.900<br>
同時，w^2 跟 w^1 的 inner product<br>
<br>
0:32:22.900,0:32:26.460<br>
w^1 跟 w^2，它們要是 orthogonal 的<br>
<br>
0:32:26.460,0:32:28.500<br>
那這個結論是什麼呢？<br>
<br>
0:32:28.500,0:32:30.440<br>
解完這個問題，你會得到什麼呢？<br>
<br>
0:32:30.440,0:32:36.480<br>
你會得到說，w^2 也是 covariance matrix 的一個<br>
<br>
0:32:36.480,0:32:40.600<br>
eigenvector，然後，它對應到<br>
<br>
0:32:40.600,0:32:45.140<br>
第二大的 eigenvalue, λ2<br>
<br>
0:32:46.100,0:32:49.940<br>
現在，我們就要用 Lagrange Multiplier 來解它<br>
<br>
0:32:49.940,0:32:53.400<br>
這個解法就是，你先寫一個 function, g<br>
<br>
0:32:53.400,0:32:56.740<br>
這個 function, g 裡面包含了你要 maximize 的對象<br>
<br>
0:32:56.740,0:33:00.120<br>
還有你的兩個 constraint<br>
<br>
0:33:00.120,0:33:04.000<br>
然後，分別要乘上 α 跟 β<br>
<br>
0:33:04.000,0:33:07.340<br>
接下來，你對你所有的參數做偏微分<br>
<br>
0:33:07.340,0:33:12.460<br>
你對 w^2 裡面的每一個參數都做偏微分<br>
<br>
0:33:12.460,0:33:15.340<br>
你對 w^2 的第一個 element，w^2 的第二個 element<br>
<br>
0:33:15.340,0:33:17.480<br>
都做偏微分，做完以後<br>
<br>
0:33:17.480,0:33:19.780<br>
你得到這個值<br>
<br>
0:33:19.780,0:33:24.000<br>
S*(w^2) - α*(^2) - β*(^1) = 0<br>
<br>
0:33:24.000,0:33:28.120<br>
接下來，左邊同乘 w^1 的 transpose<br>
<br>
0:33:28.120,0:33:32.680<br>
乘 w^1 的 transpose<br>
<br>
0:33:32.680,0:33:34.800<br>
乘 w^1 的 transpose 會發生什麼事呢？<br>
<br>
0:33:34.800,0:33:39.440<br>
乘 w^1 的 transpose 以後，(w^1)^T * (w^1) 等於 1<br>
<br>
0:33:39.440,0:33:43.400<br>
這邊 (w^1)^T * (w^2) 等於 0<br>
<br>
0:33:43.400,0:33:48.980<br>
然後，這邊 (w^1)^T * S * (w^2) 等於甚麼呢？<br>
<br>
0:33:48.980,0:33:52.220<br>
因為這一項，它是一個 scalar<br>
<br>
0:33:52.220,0:33:55.260<br>
這是一個 vector，這是一個 matrix<br>
<br>
0:33:55.260,0:33:58.840<br>
這是一個 vector，所以，乘完以後是一個 scalar<br>
<br>
0:33:58.840,0:34:00.740<br>
scalar 在做 transpose 以後還是它自己<br>
<br>
0:34:00.740,0:34:02.200<br>
所以，你可以直接把它 transpose<br>
<br>
0:34:02.200,0:34:03.900<br>
結果是一樣的<br>
<br>
0:34:03.900,0:34:05.900<br>
做完 transpose 以後<br>
<br>
0:34:05.900,0:34:09.640<br>
你得到 (w^2)^T * (S^T) * (w^1)<br>
<br>
0:34:09.640,0:34:11.140<br>
因為 S 是 symmetric 的<br>
<br>
0:34:11.140,0:34:12.920<br>
所以，它做 transpose 以後還是它自己<br>
<br>
0:34:12.920,0:34:17.600<br>
所以，這一項 S^T 變成它自己，所以變這樣<br>
<br>
0:34:17.600,0:34:19.600<br>
接下來，我們已經知道<br>
<br>
0:34:19.600,0:34:22.760<br>
w^1是 S 的 eigenvector<br>
<br>
0:34:22.760,0:34:26.200<br>
而且它對應到最大的 eigenvalue, λ^1<br>
<br>
0:34:26.200,0:34:28.600<br>
所以，S*(w^1) = (λ^1)*(w^1)<br>
<br>
0:34:28.600,0:34:31.800<br>
所以，S*(w^1) = (λ^1)*(w^1)<br>
<br>
0:34:31.800,0:34:34.940<br>
然後，(w^1)*(w^2)^T 又等於 0<br>
<br>
0:34:34.940,0:34:37.520<br>
所以，這一項第一項是 0<br>
<br>
0:34:37.520,0:34:39.760<br>
所以，從這邊我們得到什麼結論呢？<br>
<br>
0:34:39.760,0:34:41.500<br>
我們得到的結論是<br>
<br>
0:34:41.500,0:34:44.900<br>
β = 0，得到的結論是 β = 0<br>
<br>
0:34:44.900,0:34:49.020<br>
如果 β 等於 0 的話，這一項就會被拿掉<br>
<br>
0:34:49.020,0:34:54.460<br>
所以剩下的就是 S*(w^2) - α*(w^2) = 0<br>
<br>
0:34:54.460,0:34:56.300<br>
然後，它會告訴我們說<br>
<br>
0:34:56.300,0:35:01.540<br>
S*(w^2) 等於 α*(w^2)<br>
<br>
0:35:01.540,0:35:05.420<br>
所以，你知道 w^2 是一個 eigenvector<br>
<br>
0:35:05.420,0:35:09.040<br>
但是，它是哪一個 eigenvector 呢<br>
<br>
0:35:09.040,0:35:11.500<br>
如果你選它是<br>
<br>
0:35:11.500,0:35:13.800<br>
我們知道說如果<br>
<br>
0:35:13.800,0:35:17.680<br>
我們都知道說這一項等於 eigenvalue 的值<br>
<br>
0:35:17.680,0:35:19.040<br>
等於 eigenvalue 的值<br>
<br>
0:35:19.040,0:35:23.700<br>
但是你不能夠選 eigenvalue 最大的那一個 eigenvector<br>
<br>
0:35:23.700,0:35:26.540<br>
因為它會跟，它跟 w^1<br>
<br>
0:35:26.540,0:35:29.860<br>
不是 orthogonal，但是你可以選第二大<br>
<br>
0:35:29.860,0:35:34.240<br>
憑甚麼選第二大的就跟第一大的是 orthogonal 的呢？<br>
<br>
0:35:34.240,0:35:37.880<br>
你就去查一下你的線代課本<br>
<br>
0:35:37.880,0:35:39.780<br>
因為 S 是 symmetric 的<br>
<br>
0:35:39.780,0:35:41.820<br>
所以，你可以這麼做<br>
<br>
0:35:44.160,0:35:46.880<br>
那我們今天就把 map 的部分講完<br>
<br>
0:35:46.880,0:35:50.720<br>
最後，這個地方要說，在 End of Warning之前<br>
<br>
0:35:50.720,0:35:52.260<br>
就只剩下一頁投影片<br>
<br>
0:35:52.260,0:35:55.000<br>
這頁投影片要說甚麼呢？這頁投影片是這樣說的<br>
<br>
0:35:55.000,0:35:57.860<br>
說 z = W*x<br>
<br>
0:35:57.860,0:35:59.940<br>
這邊有一個神奇的地方，就是<br>
<br>
0:35:59.940,0:36:04.340<br>
z 的 covariance 會是一個 diagonal matrix<br>
<br>
0:36:04.340,0:36:08.180<br>
也就是說，如果我們今天做 PCA<br>
<br>
0:36:08.180,0:36:09.960<br>
你原來的 data distribution<br>
<br>
0:36:09.960,0:36:12.020<br>
可能是這個樣子<br>
<br>
0:36:12.020,0:36:15.860<br>
做完 PCA 以後，你會做 decorrelation<br>
<br>
0:36:15.860,0:36:20.520<br>
你會讓你的不同的 dimension 間的 covariance 是 0<br>
<br>
0:36:20.520,0:36:22.460<br>
也就是說，如果你算<br>
<br>
0:36:22.460,0:36:25.480<br>
z 這個 vector 的 covariance matrix 的話<br>
<br>
0:36:25.480,0:36:27.880<br>
會發現它是 diagonal<br>
<br>
0:36:27.880,0:36:30.000<br>
這樣做有什麼好處呢<br>
<br>
0:36:30.000,0:36:31.700<br>
這樣做，有時候會有幫助的<br>
<br>
0:36:31.700,0:36:37.620<br>
假設你現在的 PCA 所得到的新的 feature<br>
<br>
0:36:37.620,0:36:40.500<br>
你這個 z，是一種新的 feature<br>
<br>
0:36:40.500,0:36:43.960<br>
這個新的 feature 是要給其他的 model 用的<br>
<br>
0:36:43.960,0:36:45.980<br>
而你的 model 假設，比如說是<br>
<br>
0:36:45.980,0:36:47.780<br>
一個 generative model<br>
<br>
0:36:47.780,0:36:51.760<br>
那你用 Gaussian 來描述某一個 class 的 distribution<br>
<br>
0:36:51.760,0:36:54.700<br>
而你在做這個 Gaussian 的假設的時候<br>
<br>
0:36:54.700,0:37:02.740<br>
你假設說 input data，它的 covariance 就是 diagonal<br>
<br>
0:37:02.740,0:37:04.940<br>
你假設不同的 dimension 之間<br>
<br>
0:37:04.940,0:37:06.420<br>
沒有 correlation<br>
<br>
0:37:06.420,0:37:08.680<br>
這樣可以減少你的參數量<br>
<br>
0:37:08.680,0:37:12.280<br>
你做 PCA 的時候，接下來的 model 就可以<br>
<br>
0:37:12.280,0:37:15.460<br>
你把你原來的 input data 做 PCA 以後<br>
<br>
0:37:15.460,0:37:16.900<br>
再丟給其他的 model<br>
<br>
0:37:16.900,0:37:19.460<br>
其他的 model 就可以假設現在的 input data<br>
<br>
0:37:19.460,0:37:22.960<br>
它 dimension 間沒有 correlation<br>
<br>
0:37:22.960,0:37:25.060<br>
所以，它就可以用比較簡單的 model<br>
<br>
0:37:25.060,0:37:26.380<br>
來處理你的 input data<br>
<br>
0:37:26.380,0:37:28.840<br>
這樣可以避免 overfitting 的情形<br>
<br>
0:37:28.840,0:37:31.480<br>
這件事情，怎麼說明呢？<br>
<br>
0:37:31.480,0:37:32.900<br>
這個也是很 trivial 的<br>
<br>
0:37:32.900,0:37:36.280<br>
z 的 covariance 就是<br>
<br>
0:37:36.280,0:37:40.780<br>
(z-z的平均) * (z-z的平均)^T<br>
<br>
0:37:40.780,0:37:44.440<br>
那這一項，你仔細想想看<br>
<br>
0:37:44.440,0:37:50.320<br>
把它展開，它是 W*S*(W^T)<br>
<br>
0:37:50.320,0:37:53.240<br>
S 是 x 的 covariance<br>
<br>
0:37:53.240,0:37:56.500<br>
然後，你就把它展開<br>
<br>
0:37:56.500,0:37:59.780<br>
這個 W 的<br>
<br>
0:37:59.780,0:38:04.800<br>
這邊有沒有寫錯，沒有寫錯<br>
<br>
0:38:04.800,0:38:09.220<br>
這個 W 的 transpose，它的第一個 column 就是 w^1<br>
<br>
0:38:09.220,0:38:11.360<br>
一直到第 K 個 column 是 w^K<br>
<br>
0:38:11.360,0:38:13.740<br>
把 x 乘進去<br>
<br>
0:38:13.740,0:38:15.720<br>
變成這個樣子<br>
<br>
0:38:15.720,0:38:17.580<br>
把 x 乘進去以後呢<br>
<br>
0:38:17.580,0:38:20.400<br>
S*(w^1)是什麼呢？<br>
<br>
0:38:20.400,0:38:22.800<br>
w^1 是 S 的 eigenvector<br>
<br>
0:38:22.800,0:38:25.520<br>
所以，(w^1)*S 等於 λ1*(w^1)<br>
<br>
0:38:25.520,0:38:29.540<br>
(w^K)*S 等於 λK*(w^K)<br>
<br>
0:38:29.540,0:38:34.160<br>
這個 w 是 eigenvector，然後 λ 是 eigenvalue<br>
<br>
0:38:34.160,0:38:38.560<br>
然後，我們再把 W 乘進去<br>
<br>
0:38:38.560,0:38:46.660<br>
那 W*(w^1) 會是什麼呢？<br>
<br>
0:38:46.660,0:38:52.660<br>
想想看 (w^1) 其實是 W 的第一個 row<br>
<br>
0:38:52.660,0:38:56.320<br>
而 W 是一個 orthogonal 的 matrix<br>
<br>
0:38:56.320,0:39:01.040<br>
所以，W*(w^1) 會等於 e1，e1 就是<br>
<br>
0:39:01.040,0:39:05.920<br>
一個 vector，它的第一維是 1，其他都是 0<br>
<br>
0:39:05.920,0:39:11.320<br>
W*(w^K) 會等於 eK，eK 就是第 K 維是 1，其他都是 0<br>
<br>
0:39:11.320,0:39:17.940<br>
這一個東西就是一個 diagonal 的 matrix，然後<br>
<br>
0:39:17.940,0:39:21.120<br>
warning 的部分就講完了<br>
<br>
0:39:21.120,0:39:24.700<br>
這個部分，或許你覺得沒有太容易理解<br>
<br>
0:39:24.700,0:39:27.160<br>
那我們下次從另外一個角度來看 PCA<br>
<br>
0:39:27.160,0:39:30.820<br>
你可能就會更清楚說 PCA 是在做什麼<br>
<br>
0:39:30.820,0:39:33.540<br>
今天就先講到這邊，謝謝<br>
<br>
0:39:45.600,0:39:49.260<br>
各位同學大家好，我們來上課吧<br>
<br>
0:39:50.760,0:39:54.860<br>
我們上次講到 PCA<br>
<br>
0:39:55.920,0:40:01.900<br>
然後，PCA 有一個很冗長的證明<br>
<br>
0:40:01.900,0:40:05.980<br>
然後我們說，PCA <br>
<br>
0:40:05.980,0:40:10.340<br>
每次找出來的 w<br>
<br>
0:40:10.340,0:40:15.220<br>
第一次找出來的 w^1 是 covariance matrix<br>
<br>
0:40:15.220,0:40:18.480<br>
對應到最大的 eigenvalue 的 eigenvector<br>
<br>
0:40:18.480,0:40:20.660<br>
然後，第二個找出來的 w^2 呢<br>
<br>
0:40:20.660,0:40:24.620<br>
就是對應到第二大的 eigenvalue 的 eigenvector<br>
<br>
0:40:24.620,0:40:26.280<br>
以此類推，等等<br>
<br>
0:40:26.280,0:40:29.240<br>
然後，有一個很長的證明告訴你說<br>
<br>
0:40:29.240,0:40:30.360<br>
這麼做的話<br>
<br>
0:40:30.360,0:40:33.860<br>
我們每一次投影的時候都可以讓 variance 最大<br>
<br>
0:40:33.860,0:40:36.580<br>
假如現在這些東西，你聽不懂的話<br>
<br>
0:40:36.580,0:40:38.320<br>
就算了<br>
<br>
0:40:38.320,0:40:44.100<br>
我們來看看另外一個，可能是比較直觀的說明<br>
<br>
0:40:44.100,0:40:51.200<br>
另外一個比較直觀的 PCA 的想法是這樣子<br>
<br>
0:40:51.200,0:40:55.520<br>
假設我們現在考慮的是手寫數字<br>
<br>
0:40:55.520,0:40:59.500<br>
那我們知道說，這些數字其實是由一些<br>
<br>
0:40:59.500,0:41:02.760<br>
basic 的 component 所組成的<br>
<br>
0:41:02.760,0:41:06.140<br>
這些 basic 的 component 可能就代表筆劃<br>
<br>
0:41:06.140,0:41:09.100<br>
舉例來說，人所寫的數字<br>
<br>
0:41:09.100,0:41:11.860<br>
可能是有這些 basic 的 component 所組成的<br>
<br>
0:41:11.860,0:41:16.620<br>
有斜的直線、橫的直線、比較長的直線<br>
<br>
0:41:16.620,0:41:19.920<br>
還有小圈、大圈等等，所組成的<br>
<br>
0:41:19.920,0:41:23.360<br>
這些 basic 的 component 把它加起來以後<br>
<br>
0:41:23.360,0:41:25.100<br>
就可以得到一個數字<br>
<br>
0:41:25.100,0:41:27.340<br>
那這些 basic 的 component<br>
<br>
0:41:27.340,0:41:30.280<br>
我們這邊寫作 u^1, u^2, u^3 等等<br>
<br>
0:41:30.280,0:41:32.400<br>
那這些 basic 的 component，其實就是<br>
<br>
0:41:32.400,0:41:35.880<br>
一個一個的 vector，假設我們現在考慮的是 MNIST 的話<br>
<br>
0:41:35.880,0:41:39.240<br>
MNIST 的一張 image 是 28*28 pixel<br>
<br>
0:41:39.240,0:41:41.680<br>
也就是 28*28 維的一個 vector<br>
<br>
0:41:41.680,0:41:45.940<br>
那這些 component，其實也就是 28*28 維的 vector<br>
<br>
0:41:45.940,0:41:47.980<br>
把這些 vector 加起來以後<br>
<br>
0:41:47.980,0:41:50.000<br>
你所得到的那個 vector<br>
<br>
0:41:50.000,0:41:52.880<br>
把這些 vector 所代表的 component 加起來以後<br>
<br>
0:41:52.880,0:41:54.380<br>
你所得到的 vector<br>
<br>
0:41:54.380,0:41:56.440<br>
就代表了一個 digit<br>
<br>
0:41:56.440,0:41:59.500<br>
或者如果 j我們把它寫成 formulation 的話<br>
<br>
0:41:59.500,0:42:01.360<br>
寫起來像是這個樣子<br>
<br>
0:42:01.360,0:42:07.680<br>
x 代表了某一張 image 裡面的 pixel<br>
<br>
0:42:07.680,0:42:10.260<br>
某一個 image 可以用一個 vector 來表示它<br>
<br>
0:42:10.260,0:42:12.320<br>
那這個 vector，這邊寫作 x<br>
<br>
0:42:12.320,0:42:16.500<br>
那這個 x 會等於 u^1 這個 component<br>
<br>
0:42:16.500,0:42:20.680<br>
乘上 c^1 加上 u^2 這個 component 乘上 c2<br>
<br>
0:42:20.680,0:42:24.580<br>
一直加到 u^K 這個 component 乘上 cK<br>
<br>
0:42:24.580,0:42:28.180<br>
假設我們現在總共有 K 個 component 的話<br>
<br>
0:42:28.180,0:42:30.900<br>
然後再加上 x\bar<br>
<br>
0:42:30.900,0:42:36.420<br>
x\bar 是所有的 image 的平均，是 x\bar<br>
<br>
0:42:36.420,0:42:37.920<br>
所以，每一張 image<br>
<br>
0:42:37.920,0:42:40.540<br>
就是有一堆 component 的 linear combination<br>
<br>
0:42:40.540,0:42:43.540<br>
然後，再加上它的平均所組成的<br>
<br>
0:42:43.540,0:42:46.180<br>
舉例來說，我們說 7<br>
<br>
0:42:46.180,0:42:51.160<br>
是這一個 component、這一個 component <br>
和這一個 component 加起來以後的結果<br>
<br>
0:42:51.160,0:42:54.140<br>
所以，這個<br>
<br>
0:42:54.140,0:42:58.100<br>
對 7 來說，假設 7 就是 x 的話<br>
<br>
0:42:58.100,0:43:00.840<br>
c1 就是 1，c2 就是 0<br>
<br>
0:43:00.840,0:43:03.620<br>
c3就是 1，以此類推<br>
<br>
0:43:03.620,0:43:06.820<br>
那你可以用 c1, c2 到 cK<br>
<br>
0:43:06.820,0:43:09.720<br>
來表示一張 image<br>
<br>
0:43:09.720,0:43:13.620<br>
假設你這個 component 的數目是<br>
<br>
0:43:13.620,0:43:15.760<br>
遠比 pixel 的數目少的話<br>
<br>
0:43:15.760,0:43:18.500<br>
你就可以用這些<br>
<br>
0:43:18.500,0:43:21.120<br>
這個 component 的 weight 來描述一張 image<br>
<br>
0:43:21.120,0:43:25.200<br>
如果你 component 的數目比 pixel 的數目少的話<br>
<br>
0:43:25.200,0:43:27.600<br>
那這個描述，是會比較有效的<br>
<br>
0:43:27.600,0:43:30.720<br>
舉例來說，7 是一倍的 u^1，<br>
<br>
0:43:30.720,0:43:33.460<br>
一倍的 u^3，一倍的 u^5 所組合而成<br>
<br>
0:43:33.460,0:43:36.400<br>
所以，7 你就可以說它是一個 vector<br>
<br>
0:43:36.400,0:43:40.560<br>
它第一維、第三維、第五維是 1<br>
<br>
0:43:41.640,0:43:44.600<br>
那我們現在知道說<br>
<br>
0:43:44.600,0:43:47.780<br>
我們現在知道說 x<br>
<br>
0:43:47.780,0:43:51.080<br>
等於一堆 component 的 linear combination<br>
<br>
0:43:51.080,0:43:54.400<br>
再加上平均，我們現在把平均移到左邊<br>
<br>
0:43:54.400,0:43:58.180<br>
所以，x 減掉所有的 image 的平均<br>
<br>
0:43:58.180,0:44:01.440<br>
等於一堆 component 的 linear combination<br>
<br>
0:44:01.440,0:44:04.720<br>
那我們說，這一些 linear combination 的結果<br>
<br>
0:44:04.720,0:44:07.800<br>
我們寫作 x\head<br>
<br>
0:44:07.800,0:44:12.160<br>
現在假設我們不知道這些 component 是什麼<br>
<br>
0:44:12.160,0:44:15.400<br>
我們不知道 u^1 到 u^K 的這一些<br>
<br>
0:44:15.400,0:44:18.240<br>
這 K 個 vector，它們長什麼樣子<br>
<br>
0:44:18.240,0:44:21.880<br>
那我們要怎麼找這 K 個 vector 出來呢？<br>
<br>
0:44:21.880,0:44:23.600<br>
我們要做的事情<br>
<br>
0:44:23.600,0:44:27.300<br>
就是我們去找這 K 個 vector<br>
<br>
0:44:27.300,0:44:32.180<br>
使得 x\head 跟 (x-x\bar)<br>
<br>
0:44:32.180,0:44:33.460<br>
越接近越好<br>
<br>
0:44:33.460,0:44:35.580<br>
我們要找 K 個 vector<br>
<br>
0:44:35.580,0:44:39.580<br>
讓 (x-x\bar) 跟 x\head 越接近越好<br>
<br>
0:44:39.580,0:44:42.100<br>
那它們中間的差呢<br>
<br>
0:44:42.100,0:44:45.560<br>
沒有辦法用這個 component 來描述的部分<br>
<br>
0:44:45.560,0:44:47.780<br>
叫做 Reconstruction error<br>
<br>
0:44:47.780,0:44:49.660<br>
那接下來，我們要做的事情就是<br>
<br>
0:44:49.660,0:44:53.280<br>
找 K 個 component，其實就是 K 個 vector<br>
<br>
0:44:53.280,0:44:55.920<br>
它可以 minimize 這個 Reconstruction error<br>
<br>
0:44:55.920,0:44:57.240<br>
這個 Reconstruction error<br>
<br>
0:44:57.240,0:44:59.620<br>
如果你要把 formulation 寫出來的話呢<br>
<br>
0:44:59.620,0:45:03.400<br>
就是我有一個 Reconstruction error 寫成 L<br>
<br>
0:45:03.400,0:45:06.880<br>
我們要找 K 個 vector 去 minimize 它<br>
<br>
0:45:06.880,0:45:10.220<br>
要 minimize 的對象就是 (x-x\bar)<br>
<br>
0:45:10.220,0:45:15.260<br>
減掉下面這一項的 x\head<br>
<br>
0:45:15.260,0:45:17.740<br>
減掉 x\head 的 2-norm<br>
<br>
0:45:17.740,0:45:21.440<br>
x\head 是一堆 component 的 linear combination<br>
<br>
0:45:22.480,0:45:25.320<br>
那我們先來回憶一下 PCA<br>
<br>
0:45:25.320,0:45:27.700<br>
在 PCA 裡面，我們講說<br>
<br>
0:45:27.700,0:45:30.840<br>
我們要找一個 matrix, W<br>
<br>
0:45:30.840,0:45:34.680<br>
我們原來的 vector, x 乘上 W 以後<br>
<br>
0:45:34.680,0:45:39.280<br>
就得到 Dimension Reduction 以後的結果，z<br>
<br>
0:45:39.280,0:45:43.340<br>
那我們可以把 W 的每一個 row 都寫出來<br>
<br>
0:45:43.340,0:45:45.440<br>
w1, w2 一直到 wK<br>
<br>
0:45:51.740,0:45:53.520<br>
那我們說 w1, w2 一直到 wK<br>
<br>
0:45:53.520,0:45:56.700<br>
都是 covariance matrix 的 eigenvector<br>
<br>
0:45:56.700,0:46:00.380<br>
事實上，如果你要解這個式子<br>
<br>
0:46:00.380,0:46:04.040<br>
你要解這個式子，找出 u^1, u^2 到 u^K<br>
<br>
0:46:04.620,0:46:09.660<br>
這個 w^1 到 w^K，就是由 PCA 找出來的這個解<br>
<br>
0:46:09.660,0:46:14.040<br>
其實，就是可以讓上面這一個式子<br>
<br>
0:46:14.040,0:46:17.520<br>
最小化，就可以讓這個 Reconstruction error 最小<br>
<br>
0:46:17.520,0:46:20.080<br>
的 u^1 到 u^K<br>
<br>
0:46:20.080,0:46:23.260<br>
這個在 Bishop 裡面是有 prove 的<br>
<br>
0:46:23.260,0:46:25.400<br>
那我這邊講的跟 Bishop 有點不太一樣而已<br>
<br>
0:46:25.400,0:46:28.720<br>
用一個比較簡單的方式來說明給大家聽<br>
<br>
0:46:28.720,0:46:34.600<br>
我們現在在 database 裡面有一大堆的 x<br>
<br>
0:46:34.600,0:46:35.880<br>
一大堆的 x<br>
<br>
0:46:35.880,0:46:37.720<br>
現在假設有一個 x^1<br>
<br>
0:46:37.720,0:46:41.020<br>
這個 x^1 減掉平均，x\bar<br>
<br>
0:46:41.020,0:46:45.620<br>
等於 u^1 乘上 component 的 weight, c(上標 1, 下標 1)<br>
<br>
0:46:45.620,0:46:47.940<br>
這邊 c(上標 1, 下標 1)的意思是說<br>
<br>
0:46:47.940,0:46:51.400<br>
c(下標 1) 代表說它是 u^1 的 weight<br>
<br>
0:46:51.400,0:46:58.760<br>
上標 1 代表說它是 x^1 的 u1 這個 component 的 weight<br>
<br>
0:46:58.760,0:47:01.920<br>
我想這個大家應該知道我的意思<br>
<br>
0:47:02.620,0:47:09.320<br>
所以，(x^1 - x\bar) 就等於 c1*(u^1) + c2*(u^2)<br>
<br>
0:47:09.320,0:47:12.900<br>
那 (x^1-x\bar)，它是一個 vector<br>
<br>
0:47:12.900,0:47:16.220<br>
那我們把這個 vector 拿出來<br>
<br>
0:47:16.220,0:47:19.360<br>
這個 u^1, u^2 到 u^K<br>
<br>
0:47:19.360,0:47:22.840<br>
它們是一排 vector，就把它們排起來<br>
<br>
0:47:22.840,0:47:25.600<br>
其實，它排起來就是一個 matrix<br>
<br>
0:47:25.600,0:47:27.640<br>
這個 column 的數目<br>
<br>
0:47:27.640,0:47:30.180<br>
是 K 個 column<br>
<br>
0:47:30.180,0:47:34.380<br>
那前面 c1, c2 呢？<br>
<br>
0:47:34.380,0:47:36.240<br>
它們就是我們把這個<br>
<br>
0:47:36.240,0:47:39.620<br>
c1, c2，這邊有個錯誤的動畫<br>
<br>
0:47:39.620,0:47:43.420<br>
我們把這個 c1, c2 排成一排<br>
<br>
0:47:43.420,0:47:45.100<br>
排成一排這在這邊<br>
<br>
0:47:45.100,0:47:48.380<br>
那你現在把這個 c1 乘上它<br>
<br>
0:47:48.380,0:47:50.000<br>
把這個 c2 乘上它<br>
<br>
0:47:50.000,0:47:51.660<br>
就會得到這個 vector<br>
<br>
0:47:51.660,0:47:53.440<br>
也就是說，你把這一個 vector<br>
<br>
0:47:53.440,0:47:57.760<br>
你把這些 component 的 weight 排成一排<br>
<br>
0:47:57.760,0:47:59.940<br>
這個是一個 vector，這個 vector 乘以這個 matrix<br>
<br>
0:47:59.940,0:48:02.320<br>
就會得到這個 vector<br>
<br>
0:48:02.320,0:48:05.940<br>
那我們這個 data set 裡面不是只有一筆 data<br>
<br>
0:48:05.940,0:48:08.400<br>
我們還有很多，比如說，這邊有一個 x^2<br>
<br>
0:48:08.400,0:48:11.360<br>
那 (x^2-x\bar)<br>
<br>
0:48:11.360,0:48:14.240<br>
它就是這個第二個黃色的 vector<br>
<br>
0:48:14.240,0:48:17.500<br>
u^1, u^2 就在這邊<br>
<br>
0:48:17.500,0:48:20.440<br>
第二個 component 的 c1, c2<br>
<br>
0:48:20.440,0:48:22.860<br>
第二個 component 的 c1, c2 跟第一個 component 的 c1, c2<br>
<br>
0:48:22.860,0:48:25.740<br>
它們是不一樣的，它們的 notation 這邊是不一樣的<br>
<br>
0:48:25.740,0:48:29.960<br>
我們把這個值擺在這一邊<br>
<br>
0:48:29.960,0:48:33.740<br>
那你把這個 vector 乘上這個 matrix<br>
<br>
0:48:33.740,0:48:35.880<br>
就會得到這個 vector<br>
<br>
0:48:35.880,0:48:37.600<br>
以此類推<br>
<br>
0:48:37.600,0:48:39.660<br>
你把這個 vector<br>
<br>
0:48:39.660,0:48:44.820<br>
x3 的 component 的 weight 乘上<br>
<br>
0:48:44.820,0:48:46.820<br>
component，就會得到這個 vector<br>
<br>
0:48:46.820,0:48:49.880<br>
那如果我們把所有的 data<br>
<br>
0:48:49.880,0:48:54.000<br>
都用這個式子來表示，都把它畫在下面的話<br>
<br>
0:48:54.000,0:48:57.200<br>
那這一邊我們就得到一個 matrix<br>
<br>
0:48:57.200,0:48:59.960<br>
在這個 matrix 的橫軸<br>
<br>
0:48:59.960,0:49:03.200<br>
column 的數目就是你的 data 的數目<br>
<br>
0:49:03.200,0:49:06.800<br>
你有一萬筆 data，這個橫軸就是一萬<br>
<br>
0:49:06.800,0:49:10.780<br>
那我們在要做的事情就是<br>
<br>
0:49:10.780,0:49:15.000<br>
用這一個 matrix 去乘上這一個 matrix<br>
<br>
0:49:15.000,0:49:17.520<br>
這兩個都是 matrix，把這兩個 matrix<br>
<br>
0:49:17.520,0:49:20.260<br>
乘上這個 matrix，那希望它<br>
<br>
0:49:20.260,0:49:22.980<br>
越接近這個 matrix 越好<br>
<br>
0:49:22.980,0:49:27.520<br>
所以你要 minimize 這兩個相乘以後得到的 matrix<br>
<br>
0:49:27.520,0:49:29.940<br>
跟左邊這個 matrix 之間的差距<br>
<br>
0:49:29.940,0:49:32.720<br>
你要 minimize 這個 Reconstruction error<br>
<br>
0:49:32.720,0:49:35.160<br>
那怎麼解這個問題呢？<br>
<br>
0:49:35.160,0:49:37.660<br>
假如你有修過大一線代的話<br>
<br>
0:49:37.660,0:49:42.320<br>
你就知道這個問題是怎麼解的<br>
<br>
0:49:42.320,0:49:45.680<br>
以下是我教線代的時候的投影片<br>
<br>
0:49:45.680,0:49:47.880<br>
放在這邊給大家參考<br>
<br>
0:49:47.880,0:49:50.340<br>
這個在線代裡面是怎麼說的呢？<br>
<br>
0:49:50.340,0:49:55.120<br>
每一個 matrix, X<br>
<br>
0:49:55.120,0:49:59.520<br>
這邊這一個 matrix, X，你可以用 SVD<br>
<br>
0:49:59.520,0:50:02.620<br>
把它拆成一個<br>
<br>
0:50:02.620,0:50:09.200<br>
matrix, U 乘上一個 matrix, Σ 乘上一個 matrix, V<br>
<br>
0:50:09.200,0:50:12.540<br>
這個 U 是 m*k 維<br>
<br>
0:50:12.540,0:50:16.280<br>
這個 Σ 是 k*k 維，這個 V 是 k*n 維<br>
<br>
0:50:16.280,0:50:19.620<br>
這個 k，它就是 component 的數目<br>
<br>
0:50:19.620,0:50:24.060<br>
所以，我們把這個 X 分解成 U, Σ 跟 V<br>
<br>
0:50:24.060,0:50:27.900<br>
這一個 U 就是這一個<br>
<br>
0:50:27.900,0:50:32.540<br>
那這個 Σ*V 就是這一個<br>
<br>
0:50:32.540,0:50:36.440<br>
那我們知道說，如果我們今天的<br>
<br>
0:50:36.440,0:50:39.040<br>
我們用 SVD 的方法<br>
<br>
0:50:39.040,0:50:42.300<br>
把 X 拆成這三個 matrix 相乘<br>
<br>
0:50:42.300,0:50:45.460<br>
那右邊這三個 matrix 相乘的結果<br>
<br>
0:50:45.460,0:50:48.660<br>
跟左邊這一個 matrix 它們之間<br>
<br>
0:50:48.660,0:50:51.960<br>
的這個 Frobenius 的 norm 呢<br>
<br>
0:50:51.960,0:50:54.700<br>
是會被 minimize 的，也就是說<br>
<br>
0:50:54.700,0:50:58.980<br>
我們用 SVD 提供給我們的一個 matrix 的拆解方法<br>
<br>
0:50:58.980,0:51:01.160<br>
你拆出來的這 3 個 matrix 相乘，它跟<br>
<br>
0:51:01.160,0:51:03.320<br>
左邊這一個 matrix，是最接近的<br>
<br>
0:51:03.320,0:51:06.400<br>
那解出來結果是怎麼樣<br>
<br>
0:51:06.400,0:51:08.980<br>
如果你還記得的話，解出來的結果是這樣子<br>
<br>
0:51:08.980,0:51:12.320<br>
U 這個 matrix，它的 k 個 column<br>
<br>
0:51:12.320,0:51:17.640<br>
其實就是一組 orthonormal 的 vector<br>
<br>
0:51:17.640,0:51:20.180<br>
這一組 orthonormal 的 vector<br>
<br>
0:51:20.180,0:51:23.100<br>
它們是 x 乘上 x 的 transpose<br>
<br>
0:51:23.100,0:51:27.800<br>
它們是 X*(X^T) 的 eigenvector<br>
<br>
0:51:27.800,0:51:31.840<br>
它們這邊總共有 k 個 vector<br>
<br>
0:51:31.840,0:51:34.420<br>
這邊總共有 k 個 orthonormal 的 vector<br>
<br>
0:51:34.420,0:51:37.580<br>
這 k 個 orthonormal 的 vector 它們就對應到<br>
<br>
0:51:37.580,0:51:43.760<br>
X*(X^T) 最大的 k 個  eigenvalue 的 eigenvector<br>
<br>
0:51:43.760,0:51:45.960<br>
這個大家聽得懂嗎？<br>
<br>
0:51:45.960,0:51:48.040<br>
講到這邊大家有問題嗎？<br>
<br>
0:51:48.740,0:51:50.960<br>
那你會發現說<br>
<br>
0:51:50.960,0:51:53.740<br>
這個 X*(X^T) 是什麼呢？<br>
<br>
0:51:53.740,0:51:55.680<br>
這個 X*(X^T) 是什麼<br>
<br>
0:51:55.680,0:52:00.540<br>
這個 X*(X^T) 不就是 covariance matrix 嗎？<br>
<br>
0:52:00.540,0:52:04.400<br>
那我們說之前 PCA 找出來的那一些<br>
<br>
0:52:04.400,0:52:08.220<br>
w 就是 covariance matrix 的 eigenvector<br>
<br>
0:52:08.220,0:52:11.340<br>
而我們在這邊要說，我們做 SVD<br>
<br>
0:52:11.340,0:52:13.360<br>
你解出來的 U 的每一個 column<br>
<br>
0:52:13.360,0:52:16.760<br>
就是 covariance matrix 的 eigenvector<br>
<br>
0:52:16.760,0:52:19.800<br>
所以 U 這個解，其實就是<br>
<br>
0:52:19.800,0:52:25.300<br>
就是 PCA 得出來的解<br>
<br>
0:52:25.300,0:52:28.960<br>
所以，我們知道說 PCA 現在在做的事情<br>
<br>
0:52:28.960,0:52:34.160<br>
你找出來的，你從根據 PCA，你找出來的那些 w<br>
<br>
0:52:34.160,0:52:37.380<br>
你找出來的 Dimension Reduction 的 transform<br>
<br>
0:52:37.380,0:52:41.260<br>
其實，就是在 minimize 這個 Reconstruction error<br>
<br>
0:52:41.260,0:52:43.580<br>
那 Dimension Reduction 的結果<br>
<br>
0:52:43.580,0:52:45.520<br>
你得到其實就是這些 vector<br>
<br>
0:52:45.520,0:52:47.360<br>
你得到的就是這些 vector<br>
<br>
0:52:47.360,0:52:49.360<br>
PCA 裡面你得到的那些 W<br>
<br>
0:52:49.360,0:52:51.820<br>
其實就是 component<br>
<br>
0:52:54.800,0:52:57.880<br>
如果大家知道這些的話<br>
<br>
0:52:57.880,0:53:02.640<br>
我們等一下會看說 PCA 跟 neural network 有什麼樣的關係<br>
<br>
0:53:02.640,0:53:05.940<br>
那我們現在已經知道說<br>
<br>
0:53:05.940,0:53:09.380<br>
從用 PCA 找出來的 w^1 到 w^K<br>
<br>
0:53:09.380,0:53:14.700<br>
就是 K 個 component，u^1, u^2 到 u^K<br>
<br>
0:53:14.700,0:53:18.760<br>
那我們說我們有一個<br>
<br>
0:53:18.760,0:53:22.740<br>
根據 component linear combination 的結果叫做 x\head<br>
<br>
0:53:22.740,0:53:27.860<br>
它是 (w^K)*ck 做 linear combination 的結果<br>
<br>
0:53:27.860,0:53:30.080<br>
那我們會希望說，這個 x\head<br>
<br>
0:53:30.080,0:53:34.080<br>
跟 (x - x\bar)<br>
<br>
0:53:34.080,0:53:37.940<br>
(x - x\bar)，它的平均的是越小越好<br>
<br>
0:53:37.940,0:53:40.540<br>
你要 minimize 這個 Reconstruction error<br>
<br>
0:53:40.540,0:53:43.500<br>
那我們現在已經根據 SVD<br>
<br>
0:53:43.500,0:53:46.640<br>
找出來的 W，W 已經找出來了<br>
<br>
0:53:46.640,0:53:50.860<br>
W 已經找出來了，那 ck 的值到底應該是多少呢？<br>
<br>
0:53:50.860,0:53:55.560<br>
這個 ck 是每一個 example<br>
<br>
0:53:55.560,0:53:58.340<br>
如果是 image recognition 的話，就每一個 image<br>
<br>
0:53:58.340,0:54:00.380<br>
都有一組自己的 ck<br>
<br>
0:54:00.380,0:54:03.900<br>
所以，你要找這個 ck 就每一個 image 各自找就好<br>
<br>
0:54:03.900,0:54:06.000<br>
每一個 image 各自找就好了<br>
<br>
0:54:06.000,0:54:07.980<br>
那這個問題，其實就是問說<br>
<br>
0:54:07.980,0:54:11.880<br>
我現在有 K 維的 vector<br>
<br>
0:54:11.880,0:54:14.360<br>
它們做 span 以後，得到一個 space<br>
<br>
0:54:14.360,0:54:17.000<br>
如果我現在<br>
<br>
0:54:17.000,0:54:21.020<br>
要用 c1 到 cK 對它做 linear combination<br>
<br>
0:54:21.020,0:54:24.780<br>
怎麼樣才能夠最接 (x-x\bar)<br>
<br>
0:54:24.780,0:54:26.660<br>
怎麼樣才能夠最接 (x-x\bar) 呢<br>
<br>
0:54:26.660,0:54:31.120<br>
因為現在這 K 個 vector 它們是 orthonormal 的<br>
<br>
0:54:31.120,0:54:34.420<br>
所以你要得到這個 ck，其實是很簡單的，你只要把<br>
<br>
0:54:34.420,0:54:37.880<br>
(x-x\bar) 跟 w^k 做 inner product<br>
<br>
0:54:37.880,0:54:39.760<br>
你要找一組 ck 可以<br>
<br>
0:54:39.760,0:54:41.400<br>
那這個性質是來自於說<br>
<br>
0:54:41.400,0:54:44.020<br>
minimize 左邊這個跟右邊這個的 error<br>
<br>
0:54:44.020,0:54:46.740<br>
你只需要把<br>
<br>
0:54:46.740,0:54:50.320<br>
這個 (x-x\bar) 跟 w^k 做 inner product 就好了<br>
<br>
0:54:50.320,0:54:52.060<br>
那這個性質是來自於說<br>
<br>
0:54:52.060,0:54:55.000<br>
這 K 個 vector 是 orthonormal 的<br>
<br>
0:54:55.000,0:54:56.860<br>
這個如果你有困惑的話<br>
<br>
0:54:56.860,0:55:00.880<br>
就回去 check 一下線性代數的課本<br>
<br>
0:55:00.880,0:55:04.720<br>
那我們現在已經知道了這些事情<br>
<br>
0:55:04.720,0:55:06.420<br>
我們已經知道說<br>
<br>
0:55:06.420,0:55:08.540<br>
c^k 就是長成這個樣子<br>
<br>
0:55:08.540,0:55:11.200<br>
那這件事情呢<br>
<br>
0:55:11.200,0:55:13.280<br>
這個做 linear combination 的事情<br>
<br>
0:55:13.280,0:55:16.620<br>
其實你可以想成用 neural network 來表示它<br>
<br>
0:55:16.620,0:55:19.380<br>
什麼意思呢？<br>
<br>
0:55:19.380,0:55:23.840<br>
假設我們的 (x-x\bar) 就是一個 vector<br>
<br>
0:55:23.840,0:55:28.200<br>
這邊寫作一個三維的 vector<br>
<br>
0:55:28.200,0:55:32.540<br>
那我們假設，現在 K 只有兩個 component<br>
<br>
0:55:32.540,0:55:34.360<br>
K 等於 2<br>
<br>
0:55:34.360,0:55:38.240<br>
那我們先算出 c^1 跟 c^2<br>
<br>
0:55:38.240,0:55:39.740<br>
怎麼算 c^1 呢？<br>
<br>
0:55:39.740,0:55:43.900<br>
c^1 就是 (x-x\bar) 跟 w^1 的 inner product<br>
<br>
0:55:43.900,0:55:46.480<br>
所謂的 inner product 就是 element-wise 的相乘<br>
<br>
0:55:46.480,0:55:49.040<br>
也就是把 (x-x\bar) 的每一個 component<br>
<br>
0:55:49.040,0:55:51.460<br>
乘上 w^1 的每一個 component<br>
<br>
0:55:51.460,0:55:54.360<br>
接下來，你就得到 c^1<br>
<br>
0:55:54.360,0:55:56.140<br>
這件事情就好像是說<br>
<br>
0:55:56.140,0:55:58.760<br>
這個是 neural network 的 input<br>
<br>
0:55:58.760,0:56:02.300<br>
這是一個 neuron，這是 neuron 的 weight<br>
<br>
0:56:02.300,0:56:05.620<br>
這個 neuron 它是 linear 的 neuron<br>
<br>
0:56:05.620,0:56:09.400<br>
它沒有 activation function，它是 linear 的<br>
<br>
0:56:09.400,0:56:12.980<br>
那這個 neuron，你把這個東西 input<br>
<br>
0:56:12.980,0:56:15.640<br>
乘上這個 weight，你就得到 c^1<br>
<br>
0:56:15.640,0:56:17.520<br>
那 c^2 也是一樣<br>
<br>
0:56:17.520,0:56:21.500<br>
c^2，我們這邊<br>
<br>
0:56:21.500,0:56:24.880<br>
這邊是這樣子，那我們接下來要把 c^1<br>
<br>
0:56:25.320,0:56:27.280<br>
我這邊犯了一個錯<br>
<br>
0:56:27.280,0:56:30.140<br>
這個應該是下標<br>
<br>
0:56:30.140,0:56:32.540<br>
這個應該是下標<br>
<br>
0:56:32.540,0:56:35.940<br>
如果統一起來的話，這個 K 應該是下標<br>
<br>
0:56:35.940,0:56:39.640<br>
那我們把這個 c1 乘上 w^1<br>
<br>
0:56:39.640,0:56:43.240<br>
所謂的 c1*(w^1)是什麼意思呢？<br>
<br>
0:56:43.240,0:56:46.160<br>
你就把 c1*(w^1)<br>
<br>
0:56:46.160,0:56:51.180<br>
把 c1 乘上 w^1 的第一維，得到一個 value<br>
<br>
0:56:51.180,0:56:55.100<br>
乘上 w^1 的第二維，得到一個 value；<br>
乘上 w^1 的第三維，得到一個 value<br>
<br>
0:56:55.100,0:56:58.400<br>
這一項，就是 c1*(w^1)<br>
<br>
0:56:58.400,0:57:01.400<br>
接下來，我們再算一下 c^2<br>
<br>
0:57:01.400,0:57:04.860<br>
c^2 一樣就是這個 input 一樣乘上<br>
<br>
0:57:04.860,0:57:07.040<br>
跟這個 w^2 做 inner product<br>
<br>
0:57:07.040,0:57:11.160<br>
得到 c^2，然後再把這個 c^2<br>
<br>
0:57:11.160,0:57:15.760<br>
w^2 的三個 element<br>
<br>
0:57:15.760,0:57:19.680<br>
再跟原來 w^1 的三個 element 加起來<br>
<br>
0:57:19.680,0:57:23.300<br>
得到最後的 output，這一項就是 x\head<br>
<br>
0:57:23.300,0:57:25.260<br>
這一項就是 x\head<br>
<br>
0:57:25.260,0:57:28.520<br>
接下來，我們 training 的 criteria<br>
<br>
0:57:28.520,0:57:30.680<br>
就是 minimize<br>
<br>
0:57:30.680,0:57:34.180<br>
我們要讓這個 x\head  跟 (x-x\bar) 越接近越好<br>
<br>
0:57:34.180,0:57:36.620<br>
你所以，我們就是希望這個 neural network 的 output<br>
<br>
0:57:36.620,0:57:41.080<br>
跟 (x-x\bar) 越接近越好<br>
<br>
0:57:41.080,0:57:43.620<br>
這是我們的 input, (x-x\bar)<br>
<br>
0:57:43.620,0:57:44.960<br>
它乘上一組 weight<br>
<br>
0:57:44.960,0:57:47.160<br>
再 hidden layer 的 output 是 c^1, c^2<br>
<br>
0:57:47.160,0:57:49.140<br>
再乘上另外一組 weight，得到 x\head<br>
<br>
0:57:49.140,0:57:52.960<br>
那我們希望 (x-x\bar) 越接近越好<br>
<br>
0:57:52.960,0:57:55.700<br>
那你就會發現說<br>
<br>
0:57:55.700,0:57:59.380<br>
其實 PCA 可以表示成一個 neural network<br>
<br>
0:57:59.380,0:58:01.320<br>
它可以表示成一個 neural network，然後<br>
<br>
0:58:01.320,0:58:04.300<br>
這個 neural network 它只有一個 hidden layer<br>
<br>
0:58:04.300,0:58:07.120<br>
然後，這個 hidden layer 是 linear 的 activation function<br>
<br>
0:58:07.120,0:58:11.100<br>
然後，我們現在 train 這個 neural network 的 criterion<br>
<br>
0:58:11.100,0:58:15.460<br>
是要讓 input 一個東西，得到 output<br>
<br>
0:58:15.460,0:58:18.780<br>
結果這個 output 要跟 input 越接近越好<br>
<br>
0:58:18.780,0:58:20.500<br>
這個 output 要跟 input 越接近越好<br>
<br>
0:58:20.500,0:58:22.760<br>
這個 output 要跟 input 越接近越好<br>
<br>
0:58:22.760,0:58:26.220<br>
這個東西就叫做 Autoencoder<br>
<br>
0:58:26.220,0:58:31.640<br>
那這邊我們就有一個問題<br>
<br>
0:58:31.640,0:58:34.100<br>
我們這邊就有一個問題<br>
<br>
0:58:34.100,0:58:36.540<br>
假設我們現在這個 weight<br>
<br>
0:58:36.540,0:58:38.920<br>
不是用 PCA 的方法<br>
<br>
0:58:38.920,0:58:42.020<br>
也就不是用找 eigenvector 的方法<br>
<br>
0:58:42.020,0:58:46.240<br>
去找出這些 w^1, w^2 ......w^K<br>
<br>
0:58:46.240,0:58:47.620<br>
而是，兜一個 neural network<br>
<br>
0:58:47.620,0:58:51.100<br>
直接用我們要 minimize 這個 error 的 criterion<br>
<br>
0:58:51.100,0:58:55.120<br>
然後用 Gradient Descent 去 train 一發的話<br>
<br>
0:58:55.120,0:58:56.960<br>
那你覺得你得到的結果<br>
<br>
0:58:56.960,0:59:01.040<br>
會跟用 PCA 解出來的結果一樣嗎？<br>
<br>
0:59:01.840,0:59:04.480<br>
給大家一秒鐘的時間想一想<br>
<br>
0:59:04.480,0:59:06.860<br>
你覺得會一樣的同學舉手<br>
<br>
0:59:06.860,0:59:08.980<br>
有些同學同學會覺得一樣，好<br>
<br>
0:59:08.980,0:59:10.900<br>
手放下來 ，你覺得會不一樣的同學舉手<br>
<br>
0:59:10.900,0:59:12.700<br>
也有一些同學覺得會不一樣的<br>
<br>
0:59:12.700,0:59:14.560<br>
覺得不一樣的稍微多一點<br>
<br>
0:59:14.560,0:59:16.320<br>
其實，是會不一樣的<br>
<br>
0:59:16.320,0:59:18.940<br>
你仔細想看看 PCA 解出來這些 W<br>
<br>
0:59:19.360,0:59:22.280<br>
它們是 orthonormal 的，它們是 orthogonal 的<br>
<br>
0:59:22.280,0:59:24.200<br>
它們是垂直的<br>
<br>
0:59:24.200,0:59:27.100<br>
你今天如果你用 neural network<br>
<br>
0:59:27.100,0:59:29.780<br>
你就兜一個 neural network，硬 learn 一發<br>
<br>
0:59:29.780,0:59:32.820<br>
你得到的結果，你沒有辦法保證，會是垂直的阿<br>
<br>
0:59:32.820,0:59:34.840<br>
你會得到一個 solution<br>
<br>
0:59:34.840,0:59:37.120<br>
但是你沒有辦法保證說<br>
<br>
0:59:37.120,0:59:41.300<br>
這組 weight，這個 w^1 跟 w^2 是垂直的<br>
<br>
0:59:41.300,0:59:45.180<br>
你得到的是另外一組解<br>
<br>
0:59:45.180,0:59:46.900<br>
這樣大家了解我的意思嗎 ?<br>
<br>
0:59:46.900,0:59:49.360<br>
我們在前面 SVD 的證明裡面<br>
<br>
0:59:49.380,0:59:53.500<br>
已經說 PCA 導出來的這組解  w^1 到 w^K<br>
<br>
0:59:53.500,0:59:58.020<br>
它可以讓我們的 Reconstruction error 被 minimize<br>
<br>
0:59:58.020,1:00:00.500<br>
那你用這個 neural network 的方法<br>
<br>
1:00:00.500,1:00:02.480<br>
去用 Gradient Descent 硬解一發<br>
<br>
1:00:02.480,1:00:04.360<br>
你其實也不可能找出來<br>
<br>
1:00:04.360,1:00:06.560<br>
你不可能讓你的 Reconstruction error 比<br>
<br>
1:00:06.560,1:00:11.340<br>
比 PCA 找出來的 Reconstruction error 還要小<br>
<br>
1:00:11.340,1:00:15.100<br>
所以，如果是在 linear 的情況下<br>
<br>
1:00:15.100,1:00:16.460<br>
或許你就會想要用<br>
<br>
1:00:16.460,1:00:20.040<br>
直接用 PCA 來找這個 w，是比較快的<br>
<br>
1:00:20.040,1:00:24.160<br>
你用 neural network，或許是比較麻煩的<br>
<br>
1:00:24.160,1:00:27.020<br>
但是，用 neural network 的好處就是<br>
<br>
1:00:27.020,1:00:28.760<br>
它可以是 deep 的<br>
<br>
1:00:28.760,1:00:31.760<br>
這邊為什麼，只可以有一個 hidden layer 呢<br>
<br>
1:00:31.760,1:00:34.280<br>
它的一個 hidden layer，就要改成很多的 hidden layer<br>
<br>
1:00:34.280,1:00:37.000<br>
所以這個，就是我們等一下會講的<br>
<br>
1:00:37.000,1:00:40.980<br>
下一堂課會講的 Deep Autoencoder<br>
<br>
1:00:42.200,1:00:46.920<br>
那 PCA 其實有一些很明顯的弱點<br>
<br>
1:00:46.920,1:00:48.760<br>
一個就是，因為它是 unsupervised<br>
<br>
1:00:48.760,1:00:50.000<br>
它是 unsupervised<br>
<br>
1:00:50.000,1:00:54.320<br>
所以今天假如給它一大堆點，沒有 label<br>
<br>
1:00:54.320,1:00:58.700<br>
那對 PCA 來說，假設你把它 project 到一維上<br>
<br>
1:00:58.720,1:01:01.760<br>
PCA 會找一個可以讓 data variance 最大的<br>
<br>
1:01:01.760,1:01:02.680<br>
那一個 dimension<br>
<br>
1:01:02.680,1:01:03.880<br>
比如說，在這個case 裡面<br>
<br>
1:01:03.880,1:01:06.240<br>
或許，它就把它 project 到這一維上<br>
<br>
1:01:06.240,1:01:09.260<br>
把每一個綠色點，都 project 到這一維上<br>
<br>
1:01:09.260,1:01:12.280<br>
但是，有一個可能是，或許其實<br>
<br>
1:01:12.280,1:01:15.200<br>
這兩組 data point，它們分別代表了<br>
<br>
1:01:15.200,1:01:17.560<br>
兩個 class，代表了兩個 class<br>
<br>
1:01:17.560,1:01:20.800<br>
如果你用 PCA 這個方法來做 Dimension Reduction 的話<br>
<br>
1:01:20.800,1:01:24.180<br>
你就會使得這兩個藍色跟橙色的 class<br>
<br>
1:01:24.180,1:01:25.660<br>
被 merge 在一起<br>
<br>
1:01:25.660,1:01:28.640<br>
它們在 PCA 找出來的 single dimension 上<br>
<br>
1:01:28.640,1:01:30.920<br>
完全被混在一起，就無法分別<br>
<br>
1:01:30.920,1:01:32.960<br>
這個時候怎麼辦呢<br>
<br>
1:01:32.960,1:01:36.520<br>
你可能會需要引入 labeled data<br>
<br>
1:01:36.520,1:01:42.160<br>
那 LDA，是考慮這個 labeled data 的一個降維的方法<br>
<br>
1:01:42.160,1:01:46.780<br>
不過它是 supervised，所以這邊就不是我們要講的對象<br>
<br>
1:01:46.780,1:01:51.420<br>
這個 LDA 是 linear discriminant analysis 的縮寫<br>
<br>
1:01:51.420,1:01:55.500<br>
另外一個 PCA 的弱點，就是它是 linear 的<br>
<br>
1:01:55.500,1:01:58.100<br>
我們剛才在一開始舉例子的時候，我們會說<br>
<br>
1:01:58.100,1:02:01.100<br>
這一邊有一個 S 形的這個 manifold<br>
<br>
1:02:01.100,1:02:04.820<br>
這邊有一堆的點，它的分佈像是一個 S 形<br>
<br>
1:02:04.820,1:02:07.820<br>
那我們期待做 dimension reduction 以後<br>
<br>
1:02:07.820,1:02:10.100<br>
可以把這個 S 形的曲面<br>
<br>
1:02:10.100,1:02:12.280<br>
把它拉直，但是這一件事情<br>
<br>
1:02:12.280,1:02:14.060<br>
對 PCA 來說是做不到的<br>
<br>
1:02:14.060,1:02:17.120<br>
你要它做這一件事情，其實是強人所難<br>
<br>
1:02:17.120,1:02:19.920<br>
因為你要把這一個 S 形的曲面拉直<br>
<br>
1:02:19.920,1:02:21.660<br>
是一個 non-linear 的 transformation<br>
<br>
1:02:21.660,1:02:23.400<br>
PCA 做不到這件事情<br>
<br>
1:02:23.400,1:02:25.000<br>
如果你做 PCA 的話<br>
<br>
1:02:25.000,1:02:26.200<br>
你得到的結果<br>
<br>
1:02:26.200,1:02:28.180<br>
就是把這個 S 形的曲面做 PCA<br>
<br>
1:02:28.180,1:02:30.620<br>
你得到的結果就是這樣<br>
<br>
1:02:30.620,1:02:36.040<br>
你就是把它從藍色這一邊把它打扁這樣<br>
<br>
1:02:36.040,1:02:37.980<br>
你會發現說藍色跟紅色的點<br>
<br>
1:02:37.980,1:02:41.520<br>
那就被壓在一起，然後綠色的點在這邊<br>
<br>
1:02:41.520,1:02:42.960<br>
黃色的點在這一邊<br>
<br>
1:02:42.960,1:02:45.480<br>
你會把它打扁而不是把它拉開<br>
<br>
1:02:45.480,1:02:47.480<br>
因為把它拉開這件事情<br>
<br>
1:02:47.480,1:02:48.960<br>
是 PCA 辦不到的<br>
<br>
1:02:48.960,1:02:53.020<br>
那我們等一下會講 non-linear 的 transformation<br>
<br>
1:02:53.440,1:02:57.500<br>
再來，我們就把 PCA 用在一些實際的問題上<br>
<br>
1:02:57.500,1:03:00.440<br>
比如說，我們用它來分析寶可夢的 data<br>
<br>
1:03:00.440,1:03:03.560<br>
我們這一門課的例子都是寶可夢的例子<br>
<br>
1:03:03.560,1:03:05.760<br>
寶可夢，我們都知道說<br>
<br>
1:03:05.760,1:03:08.380<br>
它總共有 800 種寶可夢<br>
<br>
1:03:08.380,1:03:11.840<br>
那其實有人說是 721 種，因為在 800 種裡面<br>
<br>
1:03:11.840,1:03:13.820<br>
有一些是重複的<br>
<br>
1:03:13.820,1:03:15.620<br>
我也不知道怎麼解釋<br>
<br>
1:03:15.620,1:03:18.360<br>
反正就是假設有 800 種好了<br>
<br>
1:03:18.360,1:03:21.340<br>
那每一種寶可夢，你可以用 6 個 feature<br>
<br>
1:03:21.340,1:03:23.340<br>
來表示它，分別是<br>
<br>
1:03:23.340,1:03:25.180<br>
生命值、攻擊力、防禦力<br>
<br>
1:03:25.180,1:03:29.860<br>
特殊攻擊力、特殊防禦力還有它的速度等等<br>
<br>
1:03:29.860,1:03:31.760<br>
所以，它們就是<br>
<br>
1:03:31.760,1:03:34.820<br>
每一個寶可夢就是一個六維度的 data point<br>
<br>
1:03:34.820,1:03:36.580<br>
就是一個六維的 vector<br>
<br>
1:03:36.580,1:03:40.740<br>
那我們現在，要用 PCA 來分析它<br>
<br>
1:03:40.740,1:03:44.200<br>
那在用 PCA 的時候，常常會有人問的問題就是<br>
<br>
1:03:44.200,1:03:46.860<br>
我需要有多少個 component<br>
<br>
1:03:46.860,1:03:49.460<br>
我到底要把它 project 到一維<br>
<br>
1:03:49.460,1:03:52.340<br>
還是二維、還是三維<br>
<br>
1:03:52.340,1:03:56.040<br>
這個資訊量才足夠呢，這個 depend on 你想要做甚麼<br>
<br>
1:03:56.040,1:03:57.580<br>
假設你想要做 Visualization<br>
<br>
1:03:57.580,1:04:01.660<br>
因為現在每一個寶可夢都是 6 維<br>
<br>
1:04:01.660,1:04:04.540<br>
你沒有辦法了解這個寶可夢<br>
<br>
1:04:04.540,1:04:06.760<br>
它們之間的特性有什麼樣的關係<br>
<br>
1:04:06.760,1:04:08.160<br>
因為 6 維你沒有辦法看<br>
<br>
1:04:08.160,1:04:10.880<br>
所以，你可能就會想把它 project 到二維<br>
<br>
1:04:10.880,1:04:12.780<br>
就比較容易分析<br>
<br>
1:04:13.380,1:04:17.200<br>
那其實要用多少的 principle component，就好像是<br>
<br>
1:04:17.200,1:04:19.700<br>
neural network 要有幾個 layer，每個 layer 要有多少個<br>
<br>
1:04:19.700,1:04:23.400<br>
hidden variable，要有幾個 neuron 一樣<br>
<br>
1:04:23.400,1:04:25.780<br>
所以，這個是你要自己決定的<br>
<br>
1:04:25.780,1:04:27.560<br>
那一個常見的方法是這樣<br>
<br>
1:04:27.560,1:04:29.300<br>
一個常見的方法是說<br>
<br>
1:04:29.300,1:04:36.560<br>
我們去計算每一個 principle component 的 λ<br>
<br>
1:04:36.560,1:04:38.480<br>
我們知道每一個 principle component 就是一個<br>
<br>
1:04:38.480,1:04:41.220<br>
eigenvector，然後這個 eigenvector 又對應到一個<br>
<br>
1:04:41.260,1:04:44.280<br>
eigenvalue，就是這邊的 λ<br>
<br>
1:04:44.280,1:04:46.380<br>
那這個 eigenvalue 代表什麼意思呢？<br>
<br>
1:04:46.380,1:04:48.280<br>
這個 eigenvalue 代表說<br>
<br>
1:04:48.280,1:04:51.520<br>
我們用這個 principle component<br>
<br>
1:04:51.520,1:04:54.120<br>
去做 dimension reduction 的時候<br>
<br>
1:04:54.180,1:04:57.140<br>
在 principle component 的 dimension 上<br>
<br>
1:04:57.140,1:05:01.540<br>
它的 variance 有多大，那個 variance 就是 λ<br>
<br>
1:05:01.540,1:05:05.700<br>
今天這個寶可夢的 data 總共有 6 維<br>
<br>
1:05:05.700,1:05:08.140<br>
所以它 covariance matrix 是六維<br>
<br>
1:05:08.140,1:05:12.020<br>
所以，你可以找出 6 個 eigenvector<br>
<br>
1:05:12.020,1:05:15.620<br>
你可以找出 6 個 eigenvalue<br>
<br>
1:05:15.620,1:05:17.780<br>
我們現在來計算一下<br>
<br>
1:05:17.780,1:05:20.700<br>
每一個 eigenvalue 的 ratio<br>
<br>
1:05:20.700,1:05:22.200<br>
每一個 eigenvalue 的 ratio<br>
<br>
1:05:22.200,1:05:26.040<br>
我們就把每一個 eigenvalue  除掉 6 個 eigenvalue 的總和<br>
<br>
1:05:26.040,1:05:28.300<br>
你得到的結果，會是這個樣子<br>
<br>
1:05:28.300,1:05:29.840<br>
第一個 eigenvalue<br>
<br>
1:05:29.840,1:05:33.060<br>
它佔全部的 eigenvalue 的 0.45<br>
<br>
1:05:33.060,1:05:35.440<br>
第二個是 0.18，第三個是 0.13<br>
<br>
1:05:35.440,1:05:36.960<br>
以此類推<br>
<br>
1:05:36.960,1:05:38.640<br>
現在看到這個結果<br>
<br>
1:05:38.640,1:05:41.560<br>
我們可以從這個結果看出來說<br>
<br>
1:05:41.560,1:05:47.080<br>
這一邊是 0.45, 0.18, 0.13, 0.12，再來是 0.07, 0.04<br>
<br>
1:05:47.080,1:05:49.780<br>
所以，第 5 個 principle component<br>
<br>
1:05:49.780,1:05:52.760<br>
和第 6 個 principle component，它們的作用<br>
<br>
1:05:52.760,1:05:54.000<br>
其實是比較小的<br>
<br>
1:05:54.000,1:05:55.940<br>
你用這兩個 dimension<br>
<br>
1:05:55.940,1:05:57.840<br>
來做 projection 的時候<br>
<br>
1:05:57.840,1:06:00.320<br>
你 project 出來的 variance 是很小的<br>
<br>
1:06:00.320,1:06:03.460<br>
代表說，現在寶可夢的這這些特性<br>
<br>
1:06:03.460,1:06:06.740<br>
在第五個和第六個 principle component 上<br>
<br>
1:06:06.740,1:06:09.460<br>
是沒有太多的 information<br>
<br>
1:06:09.460,1:06:11.780<br>
它 variance 很小，所以它沒有太多的information<br>
<br>
1:06:11.780,1:06:14.800<br>
所以，如果我們今天要分析寶可夢的 data 的話<br>
<br>
1:06:14.800,1:06:19.220<br>
感覺只需要前面 4 個 principle component 就好了<br>
<br>
1:06:19.220,1:06:21.920<br>
所以，我們就實際來分析一下<br>
<br>
1:06:21.920,1:06:23.560<br>
實際來分析一下<br>
<br>
1:06:23.560,1:06:25.620<br>
如果你做 PCA 以後<br>
<br>
1:06:25.620,1:06:28.020<br>
我們得到的四個 principle component<br>
<br>
1:06:28.020,1:06:29.920<br>
就是這個樣子<br>
<br>
1:06:29.920,1:06:32.360<br>
然後，每一個 principle component 就是一個 vector<br>
<br>
1:06:32.360,1:06:36.240<br>
現在每一個寶可夢，它是有用六維的 vector 來描述它<br>
<br>
1:06:36.240,1:06:38.540<br>
所以呢，每一個 principle component<br>
<br>
1:06:38.540,1:06:40.640<br>
就是一個六維的 vector<br>
<br>
1:06:40.640,1:06:43.840<br>
這 4 個 principle component 就是 4 個六維的 vector<br>
<br>
1:06:43.840,1:06:46.000<br>
那我們來看一下每一個 principle component<br>
<br>
1:06:46.000,1:06:48.440<br>
它在做的事情是什麼<br>
<br>
1:06:48.440,1:06:50.960<br>
如果我們看第一個 principle component<br>
<br>
1:06:50.960,1:06:55.060<br>
第一個 principle component，它的每一個 dimension<br>
<br>
1:06:55.060,1:06:57.720<br>
每一個 dimension 都是正的<br>
<br>
1:06:57.720,1:07:01.560<br>
每一個 dimension 都是正的<br>
<br>
1:07:01.560,1:07:04.380<br>
那這個東西是什麼意思，這個東西其實就<br>
<br>
1:07:04.380,1:07:07.460<br>
代表寶可夢的強度<br>
<br>
1:07:07.460,1:07:11.920<br>
知道嗎？就是如果你要產生一隻寶可夢的時候<br>
<br>
1:07:11.920,1:07:13.560<br>
每一個寶可夢都是<br>
<br>
1:07:13.560,1:07:18.760<br>
由這 4 個 vector 做 linear combination<br>
<br>
1:07:18.760,1:07:20.580<br>
每一個寶可夢都可以想成是<br>
<br>
1:07:20.580,1:07:22.820<br>
它的數值可以想成是，這 4 個 vector<br>
<br>
1:07:22.820,1:07:25.160<br>
做 linear combination 的結果<br>
<br>
1:07:25.160,1:07:27.660<br>
combine 的 weight，每一隻寶可夢是不一樣的<br>
<br>
1:07:27.660,1:07:31.740<br>
所以，如果你在選第一個<br>
<br>
1:07:31.740,1:07:34.460<br>
principle component 的時候，你給它的 weight 比較大<br>
<br>
1:07:34.460,1:07:37.440<br>
那這個寶可夢它的六維都是強的<br>
<br>
1:07:37.440,1:07:40.920<br>
那如果選擇的值小，它的六維都是弱的<br>
<br>
1:07:40.920,1:07:42.540<br>
所以，在 第一個 principle component<br>
<br>
1:07:42.540,1:07:45.240<br>
就代表了這一隻寶可夢強度<br>
<br>
1:07:45.240,1:07:47.720<br>
那第二個 principle component 是什麼呢？<br>
<br>
1:07:47.720,1:07:49.520<br>
第二個 principle component 它在<br>
<br>
1:07:49.520,1:07:53.960<br>
它在防禦力的地方是正值<br>
<br>
1:07:53.960,1:07:56.700<br>
它在速度的地方是負值<br>
<br>
1:07:56.700,1:07:59.620<br>
也就是說，如果你選<br>
<br>
1:08:01.460,1:08:04.100<br>
這個防禦力跟速度<br>
<br>
1:08:04.100,1:08:05.900<br>
是成反比的<br>
<br>
1:08:05.900,1:08:07.080<br>
是成反比的<br>
<br>
1:08:07.080,1:08:09.220<br>
你今天選，你今天給這個<br>
<br>
1:08:09.220,1:08:11.160<br>
第二個 component 一個 weight 的時候<br>
<br>
1:08:11.160,1:08:13.880<br>
你會增加那隻寶可夢的防禦力<br>
<br>
1:08:13.880,1:08:16.000<br>
但是，會減低它的速度<br>
<br>
1:08:16.000,1:08:19.120<br>
所以，寶可夢防禦力提升的時候<br>
<br>
1:08:19.120,1:08:20.860<br>
它的速度會下降<br>
<br>
1:08:20.860,1:08:22.640<br>
如果我們把<br>
<br>
1:08:22.640,1:08:25.720<br>
第一個和第二個 principle component 畫出來的話<br>
<br>
1:08:25.720,1:08:27.240<br>
你發現是這個樣子<br>
<br>
1:08:27.240,1:08:30.840<br>
這個圖上，有 800 個點，每一個點<br>
<br>
1:08:30.840,1:08:32.300<br>
就對應到一隻寶可夢<br>
<br>
1:08:32.300,1:08:34.080<br>
那你把它畫到二維的平面上<br>
<br>
1:08:34.080,1:08:36.500<br>
那這樣我們很難知道每一隻寶可夢是甚麼<br>
<br>
1:08:36.500,1:08:38.580<br>
所以，我就做了一件瘋狂的事<br>
<br>
1:08:38.580,1:08:42.380<br>
如果我們看第三、第四個 component 的話<br>
<br>
1:08:42.380,1:08:46.220<br>
會發現說第三個 component<br>
<br>
1:08:46.220,1:08:48.980<br>
它是 special defense<br>
<br>
1:08:48.980,1:08:50.900<br>
是正的<br>
<br>
1:08:50.900,1:08:52.420<br>
special defense 是正的<br>
<br>
1:08:52.420,1:08:56.660<br>
然後，攻擊力跟 HP 是負的<br>
<br>
1:08:56.660,1:08:58.880<br>
也就是說，這是用<br>
<br>
1:08:58.880,1:09:01.780<br>
特殊防禦力來換取<br>
<br>
1:09:01.780,1:09:07.220<br>
用攻擊力跟 HP 來換取特殊防禦力的寶可夢<br>
<br>
1:09:07.220,1:09:09.460<br>
最後一個呢？最後一個是<br>
<br>
1:09:09.460,1:09:11.200<br>
它的 HP 是正的<br>
<br>
1:09:11.200,1:09:13.680<br>
然後，攻擊力和防禦率是負的<br>
<br>
1:09:13.680,1:09:17.620<br>
也就是說，它是用攻擊力和防禦來換取生命力的寶可夢<br>
<br>
1:09:17.620,1:09:19.160<br>
那這些分別是什麼呢<br>
<br>
1:09:19.160,1:09:21.400<br>
如果我們把第三個和第四個 principle component<br>
<br>
1:09:21.400,1:09:22.580<br>
畫在圖上的話<br>
<br>
1:09:22.580,1:09:24.360<br>
其實它們也是橢圓的形狀<br>
<br>
1:09:24.360,1:09:25.820<br>
其實也是橢圓的形狀<br>
<br>
1:09:25.820,1:09:29.540<br>
不過基本上考慮的是統計的結果，所以會有一些 outlier<br>
<br>
1:09:29.540,1:09:31.860<br>
不過，統計的結果最後算出來<br>
<br>
1:09:31.860,1:09:34.260<br>
它是 decorrelation 的<br>
<br>
1:09:34.260,1:09:37.520<br>
其實，特殊的防禦力用<br>
<br>
1:09:37.520,1:09:40.280<br>
攻擊力和生命值換特殊防禦力<br>
<br>
1:09:40.280,1:09:41.148<br>
其實也是普普<br>
<br>
1:09:41.148,1:09:45.380<br>
它不只是一個防禦力特別高的寶可夢<br>
<br>
1:09:45.380,1:09:47.700<br>
它也是一個特殊防禦力特別高的寶可夢<br>
<br>
1:09:47.700,1:09:52.380<br>
第二名是冰柱機器人這樣子<br>
<br>
1:09:54.720,1:09:57.060<br>
然後，如果我們看生命力的話<br>
<br>
1:09:57.060,1:09:59.420<br>
第四個 component 就是生命力特別強的<br>
<br>
1:09:59.420,1:10:02.220<br>
那這個其實跟我們預期是一樣的，就是<br>
<br>
1:10:02.220,1:10:05.940<br>
這個是吉利蛋跟幸福蛋，對不對<br>
<br>
1:10:10.280,1:10:14.560<br>
所以，今天我們至少回答到一個問題<br>
<br>
1:10:14.560,1:10:17.580<br>
你知道最強的寶可夢其實是超夢，還有<br>
<br>
1:10:17.580,1:10:20.400<br>
那三隻神獸是特別強的<br>
<br>
1:10:20.400,1:10:23.180<br>
那如果我們可以拿它來做其他 class<br>
<br>
1:10:23.180,1:10:25.060<br>
比如說，我們拿它來做<br>
<br>
1:10:25.060,1:10:29.320<br>
影像的手寫數字的辨識的話<br>
<br>
1:10:29.320,1:10:33.480<br>
那會怎麼樣呢？我們可以把每一張<br>
<br>
1:10:33.480,1:10:36.540<br>
數字都拆成<br>
<br>
1:10:36.540,1:10:38.880<br>
component 的 weight 乘上 component<br>
<br>
1:10:38.880,1:10:41.380<br>
加上 component 的 weight 乘上另一個 component<br>
<br>
1:10:41.380,1:10:46.300<br>
其實，每一個 component 都是一張 image，對不對<br>
<br>
1:10:46.300,1:10:49.440<br>
每一個 component 都是一個 28*28 維的 vector<br>
<br>
1:10:49.440,1:10:53.920<br>
所以，你可以把它畫在圖上，把它變成一張 image<br>
<br>
1:10:53.920,1:10:59.920<br>
我們如果畫前 30 個 component 的話<br>
<br>
1:10:59.920,1:11:02.620<br>
我們畫 PCA 得到前 30 個 component 的話<br>
<br>
1:11:02.620,1:11:04.820<br>
你得到的結果，其實是這個樣子的<br>
<br>
1:11:04.820,1:11:11.140<br>
白色的地方代表是有筆劃，所以這個是 1<br>
<br>
1:11:11.140,1:11:13.580<br>
這個看起來有點像 9<br>
<br>
1:11:13.580,1:11:17.660<br>
這個看起來是 0，中間接一條線<br>
<br>
1:11:17.660,1:11:21.780<br>
這個看起來不知道，像是加了勾勾<br>
<br>
1:11:21.780,1:11:25.540<br>
後面這很複雜，看起來像是馬雅文字一樣複雜<br>
<br>
1:11:25.540,1:11:31.560<br>
你用這些 component<br>
<br>
1:11:31.560,1:11:33.060<br>
做 linear combination<br>
<br>
1:11:33.060,1:11:36.600<br>
你就可以得到所有的 digit，就可以得到 0~9<br>
<br>
1:11:36.600,1:11:40.440<br>
所以，這些 component 就叫做 Eigen-digit<br>
<br>
1:11:40.440,1:11:42.080<br>
之所以叫 Eigen-digit 就是說<br>
<br>
1:11:42.080,1:11:44.940<br>
Eigen 就是說，這些 component 其實都是<br>
<br>
1:11:44.940,1:11:47.540<br>
covariance matrix 的 eigenvector<br>
<br>
1:11:47.540,1:11:49.280<br>
所以，叫它 Eigen-digit<br>
<br>
1:11:49.280,1:11:51.140<br>
所以，Eigen-digit 做 linear combination<br>
<br>
1:11:51.140,1:11:54.440<br>
就可以得到各種不同的 digit<br>
<br>
1:11:54.440,1:11:57.780<br>
如果做人臉辨識的話<br>
<br>
1:11:57.780,1:12:00.400<br>
處理人臉的話，得到的結果大概是這樣<br>
<br>
1:12:00.400,1:12:04.940<br>
這邊有一大堆的人臉<br>
<br>
1:12:04.940,1:12:09.560<br>
然後，你就找它的 principle component<br>
<br>
1:12:09.560,1:12:16.160<br>
你就找它的 principle component，你就找前 30 個<br>
<br>
1:12:16.160,1:12:17.620<br>
principle component<br>
<br>
1:12:17.620,1:12:21.760<br>
你就會發現說找出來是這樣，每一個都是哀怨的臉<br>
<br>
1:12:21.760,1:12:24.820<br>
這叫 Eigen-face<br>
<br>
1:12:27.400,1:12:31.500<br>
你看每一個都是一個臉，每一張都是一個臉<br>
<br>
1:12:31.500,1:12:34.260<br>
那你把這些臉做 linear combination 以後<br>
<br>
1:12:34.260,1:12:37.060<br>
就可以得到所有的臉<br>
<br>
1:12:37.060,1:12:42.080<br>
但是，這邊你有沒有覺得有些地方<br>
<br>
1:12:42.080,1:12:43.880<br>
跟你預期的不太一樣呢？<br>
<br>
1:12:43.880,1:12:46.000<br>
我第一次看到的時候，覺得這跟預期的結果不太一樣<br>
<br>
1:12:46.000,1:12:47.320<br>
是不是程式有 bug 阿<br>
<br>
1:12:47.320,1:12:49.940<br>
因為，我們說P CA 找出來的是<br>
<br>
1:12:49.940,1:12:51.740<br>
是 component，對不對<br>
<br>
1:12:51.740,1:12:54.040<br>
我們把很多 component linear combine 以後<br>
<br>
1:12:54.040,1:12:57.360<br>
它會變成一個 face 或一個 digit<br>
<br>
1:12:57.360,1:12:59.640<br>
但我們現在找出來的不是 component 阿<br>
<br>
1:12:59.640,1:13:03.600<br>
我們找出來的每一個圖，幾乎都是完整的臉<br>
<br>
1:13:03.600,1:13:05.220<br>
幾乎都是完整的臉<br>
<br>
1:13:05.220,1:13:07.840<br>
我們剛才前一個數字<br>
<br>
1:13:07.840,1:13:11.400<br>
你找出來每一個 Eigen-digit 看起來都是馬雅文字<br>
<br>
1:13:11.400,1:13:14.440<br>
它們不是 component，不是<br>
<br>
1:13:14.440,1:13:18.420<br>
一筆劃這種東西，不是圈圈、一豎這種東西<br>
<br>
1:13:18.420,1:13:20.140<br>
那為甚麼會這樣呢？<br>
<br>
1:13:20.140,1:13:22.120<br>
如果你仔細想想看 PCA 的特性<br>
<br>
1:13:22.120,1:13:25.220<br>
你就會發現說，會得到這個結果是可能的<br>
<br>
1:13:25.220,1:13:28.840<br>
因為在 PCA 裡面，你的這個 weight<br>
<br>
1:13:28.840,1:13:30.940<br>
你的這個 linear combination 的 weight<br>
<br>
1:13:30.940,1:13:32.500<br>
它可以是任何值<br>
<br>
1:13:32.500,1:13:35.400<br>
它可以是正的，也可以是負的<br>
<br>
1:13:35.400,1:13:37.580<br>
所以，當我們用這些 principal component<br>
<br>
1:13:37.580,1:13:39.480<br>
組成一張 image 的時候<br>
<br>
1:13:39.480,1:13:42.080<br>
你可以把這些 component 相加<br>
<br>
1:13:42.080,1:13:45.080<br>
也可以把這些 component 相減<br>
<br>
1:13:45.080,1:13:48.860<br>
所以，這會導致說你找出來的 component<br>
<br>
1:13:48.860,1:13:54.860<br>
不見得是一個圖的 basic 的東西<br>
<br>
1:13:54.860,1:13:57.760<br>
舉例來說，假設我想要畫一個 9<br>
<br>
1:13:58.120,1:14:00.420<br>
那我可以先畫一個 8<br>
<br>
1:14:00.420,1:14:02.720<br>
然後，再把下面的圈圈減掉<br>
<br>
1:14:02.720,1:14:05.060<br>
再把一槓加上去<br>
<br>
1:14:05.060,1:14:08.120<br>
這樣大家了解我意思嗎？因為現在我們<br>
<br>
1:14:08.120,1:14:11.200<br>
不一定是把這些 component 加起來<br>
<br>
1:14:11.200,1:14:13.040<br>
我們可以把這些 component 相減<br>
<br>
1:14:13.040,1:14:15.920<br>
所以就變成說，你可以先畫一個很複雜的圖<br>
<br>
1:14:15.920,1:14:18.520<br>
然後，再把多餘的東西減掉<br>
<br>
1:14:18.520,1:14:21.680<br>
這就是為甚麼我們剛才會看到一堆馬雅文字的關係<br>
<br>
1:14:21.680,1:14:26.120<br>
這些 component 其實不見得是類似筆劃這種東西<br>
<br>
1:14:26.120,1:14:29.560<br>
如果你想要得到類似筆劃的東西的話<br>
<br>
1:14:29.560,1:14:32.320<br>
怎麼辦呢？你要用另外一個技術叫做<br>
<br>
1:14:32.320,1:14:36.200<br>
Non-negative matrix 的 factorization (NMF)<br>
<br>
1:14:36.200,1:14:40.220<br>
那在 NMF 裡面<br>
<br>
1:14:40.220,1:14:45.040<br>
我們剛才講說，PCA 它可以看成是對 matrix, X<br>
<br>
1:14:45.040,1:14:48.300<br>
做 SVD，SVD 就是一種<br>
<br>
1:14:48.300,1:14:51.280<br>
matrix factorization 的技術，就是一種矩陣分解的技術<br>
<br>
1:14:51.280,1:14:54.400<br>
那它分解出來的兩個 matrix 的值<br>
<br>
1:14:54.400,1:14:56.180<br>
可以是正的，可以是負的<br>
<br>
1:14:56.180,1:14:59.080<br>
現在如果你用 NMF 的話<br>
<br>
1:14:59.080,1:15:01.560<br>
我們沒有打算要講它的細節<br>
<br>
1:15:01.560,1:15:03.360<br>
我等一下列 reference 給大家參考就好<br>
<br>
1:15:03.360,1:15:05.560<br>
簡單來說的話，精神就是<br>
<br>
1:15:05.560,1:15:07.520<br>
如果我們現在用 NMF 的話<br>
<br>
1:15:07.520,1:15:11.760<br>
我們會強迫所有的 component 都是正的<br>
<br>
1:15:11.760,1:15:15.140<br>
首先，我會強迫所有 component 的 weight<br>
<br>
1:15:15.140,1:15:16.320<br>
都是正的<br>
<br>
1:15:16.320,1:15:17.920<br>
是正的好處就是<br>
<br>
1:15:17.920,1:15:21.300<br>
現在一張 image 必須由 component 疊加得到<br>
<br>
1:15:21.300,1:15:22.840<br>
而你不能說，我先畫一個圖<br>
<br>
1:15:22.840,1:15:27.620<br>
很複雜的東西，再把複雜的東西去掉一些部分<br>
<br>
1:15:27.620,1:15:28.840<br>
得到一個 digit<br>
<br>
1:15:28.840,1:15:33.140<br>
現在因為每一個 weight 都一定要是正的，所以<br>
<br>
1:15:33.140,1:15:35.540<br>
你只能相加，再來就是<br>
<br>
1:15:35.540,1:15:38.020<br>
所有的 component<br>
<br>
1:15:38.020,1:15:40.520<br>
它的每一個 dimension，也都必須要是正的<br>
<br>
1:15:40.520,1:15:43.560<br>
如果你用 NMF 的話，會讓你的每一個 dimension<br>
<br>
1:15:43.560,1:15:45.220<br>
都是正的<br>
<br>
1:15:45.220,1:15:46.640<br>
如果你用 PCA 的話<br>
<br>
1:15:46.640,1:15:48.840<br>
你得到的 dimension 不見得每一維都是正的<br>
<br>
1:15:48.840,1:15:50.840<br>
你找出來的 principal component 裡面<br>
<br>
1:15:50.840,1:15:53.980<br>
它會有一些負值<br>
<br>
1:15:53.980,1:15:56.560<br>
那你知道，你今天如果要畫一張 image 的話<br>
<br>
1:15:56.560,1:15:59.940<br>
其實，負值你是有點不知道該怎麼處理的<br>
<br>
1:15:59.940,1:16:05.220<br>
如果我今天把負值都當作是<br>
<br>
1:16:05.220,1:16:07.200<br>
就沒有筆劃，就是 0 的話<br>
<br>
1:16:07.200,1:16:08.940<br>
你可能整張圖都看起來黑漆漆<br>
<br>
1:16:08.940,1:16:11.060<br>
你大部分的圖都黑漆漆，就很怪<br>
<br>
1:16:11.060,1:16:13.740<br>
我前面那幾張圖的作法是<br>
<br>
1:16:13.740,1:16:16.360<br>
如果有負值又有正值，會把它 normalize<br>
<br>
1:16:16.360,1:16:18.800<br>
把它做一個平移，讓負的也都變成正的<br>
<br>
1:16:18.800,1:16:21.440<br>
你看起來的圖才會比較好看<br>
<br>
1:16:21.440,1:16:24.640<br>
那這個就比較麻煩，如果你用 NMF 就沒有這個問題了<br>
<br>
1:16:24.640,1:16:26.720<br>
你找出來的 component 都是正的<br>
<br>
1:16:26.720,1:16:31.060<br>
所以，那些 component 就自然地會形成一張 image<br>
<br>
1:16:31.060,1:16:33.900<br>
下面這是 reference 給大家參考<br>
<br>
1:16:33.900,1:16:36.340<br>
所以，如果在同樣的 test 上<br>
<br>
1:16:36.340,1:16:39.480<br>
比如在手寫數字的 test 上，一樣 apply NMF 的時候<br>
<br>
1:16:39.480,1:16:43.120<br>
這個時候，你找出來的那些 principal component<br>
<br>
1:16:43.120,1:16:45.900<br>
它就會長這樣<br>
<br>
1:16:45.900,1:16:51.020<br>
它就會清楚很多，你就會發現說，這顯然<br>
<br>
1:16:51.020,1:16:55.380<br>
都是筆劃，這個是縱的，這個是 0 的兩邊<br>
<br>
1:16:55.380,1:16:58.420<br>
這個是斜的，這個是一小點<br>
<br>
1:16:58.420,1:17:00.400<br>
一直線、一撇這樣子<br>
<br>
1:17:00.400,1:17:02.060<br>
一橫線、一撇這樣子<br>
<br>
1:17:02.060,1:17:03.840<br>
你會發現，你找出來的每一個東西<br>
<br>
1:17:03.840,1:17:07.260<br>
就都變成是筆劃了，這跟我們<br>
<br>
1:17:07.260,1:17:09.840<br>
本來想要找的東西，是比較像的<br>
<br>
1:17:09.840,1:17:12.240<br>
如果你看臉的話<br>
<br>
1:17:12.240,1:17:16.260<br>
你會發現說，它長的是這個樣子，它比較像是<br>
<br>
1:17:16.260,1:17:18.620<br>
臉的一些部分<br>
<br>
1:17:18.620,1:17:22.180<br>
比如說，這個是人中的地方<br>
<br>
1:17:22.180,1:17:28.660<br>
這個是眉毛，這個是下巴，這個是嘴唇<br>
<br>
1:17:28.660,1:17:31.580<br>
這個也是嘴唇等等<br>
<br>
1:17:31.580,1:17:33.280<br>
如果，你今天是用 NMF<br>
<br>
1:17:33.280,1:17:37.240<br>
對這個人臉圖片做 NMF 的話<br>
<br>
1:17:37.240,1:17:40.780<br>
你得到的結果會比較像是<br>
<br>
1:17:40.780,1:17:45.140<br>
你期待要找的像是 component 一樣的東西<br>
<br>
1:17:46.060,1:17:50.040<br>
那接下來，剩下來的時間<br>
<br>
1:17:50.040,1:17:53.640<br>
我們講一下 Matrix Factorization<br>
<br>
1:17:53.640,1:17:56.740<br>
Matrix Factorization 是這樣子，有時候<br>
<br>
1:17:56.740,1:18:00.320<br>
你會有兩種東西<br>
<br>
1:18:00.320,1:18:02.160<br>
你會有兩種 object<br>
<br>
1:18:02.160,1:18:09.800<br>
它們之間是受到某種共通的 latent factor 去操控的<br>
<br>
1:18:09.800,1:18:13.800<br>
什麼意思呢，假設我們現在做一下調查<br>
<br>
1:18:13.800,1:18:17.720<br>
調查每一個人手上有，每一個人有買公仔的數目<br>
<br>
1:18:17.720,1:18:21.180<br>
然後，A B C D E 代表 5 個人<br>
<br>
1:18:21.180,1:18:24.040<br>
我們調查一下，每個人手上有的公仔的數目<br>
<br>
1:18:24.040,1:18:28.160<br>
比如說，調查 A 有 5 個涼宮春日的公仔<br>
<br>
1:18:28.180,1:18:30.280<br>
B 有 4個，C 有 1 個， D有 1 個<br>
<br>
1:18:30.280,1:18:36.080<br>
然後這個是御坂美琴，A有 3 個，B有 3 個，C有 1 個<br>
<br>
1:18:36.080,1:18:41.820<br>
這個是小野寺小咲 (人名)<br>
<br>
1:18:41.820,1:18:44.240<br>
我要想看看人名到底是什麼<br>
<br>
1:18:44.240,1:18:46.740<br>
小野寺，對不對<br>
<br>
1:18:46.740,1:18:49.560<br>
D有4個，E有5個<br>
<br>
1:18:49.560,1:18:53.660<br>
然後這個，這個是小唯，是平澤唯<br>
<br>
1:18:53.660,1:18:56.660<br>
然後，C有5個，D有4個，E有4個，<br>
<br>
1:18:56.660,1:18:58.320<br>
你會發現說<br>
<br>
1:18:58.320,1:19:00.240<br>
在這個 matrix 上面<br>
<br>
1:19:00.240,1:19:03.620<br>
每一個 table 裡面的 block<br>
<br>
1:19:03.620,1:19:05.120<br>
並不是隨機出現<br>
<br>
1:19:05.120,1:19:09.540<br>
你會發現，如果有買涼宮春日公仔的人<br>
<br>
1:19:09.540,1:19:11.900<br>
就比較有可能會有御坂美琴的公仔<br>
<br>
1:19:11.900,1:19:16.180<br>
有這個姐寺公仔的人，就比較有可能有小唯的公仔<br>
<br>
1:19:16.180,1:19:17.780<br>
那是因為說<br>
<br>
1:19:17.780,1:19:22.480<br>
這每一個人跟每一個人<br>
<br>
1:19:22.480,1:19:27.500<br>
跟角色背後是有一些共同的特性<br>
<br>
1:19:27.500,1:19:31.140<br>
有一些個共同的 factor 來操控<br>
<br>
1:19:31.140,1:19:32.640<br>
這些事情發生<br>
<br>
1:19:32.640,1:19:36.580<br>
otaku 這個是御宅族的意思這樣<br>
<br>
1:19:36.580,1:19:40.300<br>
每一個御宅族和每一個角色後面，是有一些<br>
<br>
1:19:40.300,1:19:44.040<br>
共同的 factor在操控它們的<br>
<br>
1:19:44.040,1:19:46.180<br>
什麼意思呢？<br>
<br>
1:19:46.180,1:19:51.100<br>
其實這個動漫宅，或許可以分成兩種<br>
<br>
1:19:51.100,1:19:53.340<br>
有一種是萌傲嬌的<br>
<br>
1:19:53.340,1:19:55.700<br>
有一種是萌天然呆的<br>
<br>
1:19:55.700,1:19:59.820<br>
每一個人，其實就是在萌傲嬌和萌天然呆<br>
<br>
1:19:59.820,1:20:01.780<br>
這個平面上的一個點<br>
<br>
1:20:01.780,1:20:06.780<br>
所以，如果比較偏，A 是比較萌傲嬌的<br>
<br>
1:20:06.780,1:20:09.960<br>
那每一個角色，他也是<br>
<br>
1:20:09.960,1:20:14.360<br>
他可能是有傲嬌屬性，或者是有天然呆的屬性<br>
<br>
1:20:14.360,1:20:17.720<br>
所以每一個角色，也都是平面上的一個點<br>
<br>
1:20:17.720,1:20:20.380<br>
每一個角色，你也都可以用一個 vector 來描述它<br>
<br>
1:20:20.380,1:20:22.920<br>
如果某一個人萌的屬性<br>
<br>
1:20:22.920,1:20:26.660<br>
跟某一個角色，他本身所具有的屬性是 match 的話<br>
<br>
1:20:26.660,1:20:28.700<br>
所謂的屬性 match 是說<br>
<br>
1:20:28.700,1:20:30.300<br>
他們背後的這個 vector<br>
<br>
1:20:30.300,1:20:33.240<br>
他們背後的這個 vector 很像<br>
<br>
1:20:33.240,1:20:34.760<br>
他們背後的這個 vector<br>
<br>
1:20:34.760,1:20:37.800<br>
比如說，在做 inner product 的時候，值很大<br>
<br>
1:20:37.800,1:20:39.460<br>
那這個人<br>
<br>
1:20:39.460,1:20:41.600<br>
就會買很多涼宮春日的公仔<br>
<br>
1:20:41.600,1:20:44.600<br>
所以，這個他們有沒有 match 呢<br>
<br>
1:20:44.600,1:20:48.440<br>
這個 degree，他們這個匹配的程度<br>
<br>
1:20:48.440,1:20:52.840<br>
就取決於他們背後的這個的 latent factor 是不是匹配的<br>
<br>
1:20:52.840,1:20:55.860<br>
所以，只可能 ABC 他們背後的這個<br>
<br>
1:20:55.860,1:20:58.440<br>
他們萌的角色，大概是這個樣子<br>
<br>
1:20:58.440,1:21:00.900<br>
A 是萌傲嬌的， B也是萌傲嬌的<br>
<br>
1:21:00.900,1:21:03.460<br>
但沒有 A 那麼強 ，C 是萌天然呆的<br>
<br>
1:21:03.460,1:21:06.100<br>
然後每一個動漫人物角色的後面<br>
<br>
1:21:06.100,1:21:09.020<br>
也都有傲嬌和天然呆，這兩種屬性<br>
<br>
1:21:09.020,1:21:12.620<br>
每一角色都有他不同的屬性<br>
<br>
1:21:12.620,1:21:14.300<br>
然後，如果他們的屬性<br>
<br>
1:21:14.300,1:21:16.100<br>
它們背後的屬性 match 的話<br>
<br>
1:21:16.100,1:21:19.520<br>
那這一個人，就會買這個公仔<br>
<br>
1:21:19.520,1:21:22.360<br>
這個世界操控的邏輯是這個樣子<br>
<br>
1:21:22.360,1:21:24.340<br>
但是這些 factor<br>
<br>
1:21:24.340,1:21:27.620<br>
就一個人到底是萌傲嬌還是萌天然呆，這一件事情<br>
<br>
1:21:27.620,1:21:30.700<br>
是沒有辦法直接被觀察的<br>
<br>
1:21:30.700,1:21:32.140<br>
因為，其實沒有人在意一個<br>
<br>
1:21:32.140,1:21:34.240<br>
阿宅他心裡想什麼<br>
<br>
1:21:34.960,1:21:38.340<br>
所以這些事情，是沒有人知道的<br>
<br>
1:21:38.780,1:21:42.060<br>
你也沒有辦法直接知道說<br>
<br>
1:21:42.060,1:21:46.400<br>
每一個動漫人物他背後的屬性是甚麼<br>
<br>
1:21:46.400,1:21:48.140<br>
這也是沒有辦法直接觀察到的<br>
<br>
1:21:48.140,1:21:50.380<br>
所以我們有的東西是什麼呢？<br>
<br>
1:21:50.380,1:21:55.580<br>
我們有的是，這個動漫人物跟阿宅中間的關係<br>
<br>
1:21:55.580,1:21:57.500<br>
他們中間的關係，也就是<br>
<br>
1:21:57.500,1:22:01.080<br>
也就是他手上有的公仔的數目，他們之間的關係<br>
<br>
1:22:01.080,1:22:02.820<br>
那我們要憑著這個關係<br>
<br>
1:22:02.820,1:22:08.520<br>
去推論出每一個人，跟每一個動漫人物<br>
<br>
1:22:08.520,1:22:11.660<br>
他們背後的 latent factor<br>
<br>
1:22:11.660,1:22:14.680<br>
他們每一個人背後都有一個二維的 vector<br>
<br>
1:22:14.680,1:22:22.080<br>
分別代表他，就每一個這個阿宅背後都有一個 vector<br>
<br>
1:22:22.080,1:22:24.840<br>
代表了他萌傲嬌或是萌天然呆的程度<br>
<br>
1:22:24.840,1:22:26.580<br>
他都是用一個 vector來表示<br>
<br>
1:22:26.580,1:22:29.420<br>
每一個人物，他背後也都有一個 vector<br>
<br>
1:22:29.420,1:22:32.600<br>
代表他傲嬌的屬性和天然呆的屬性<br>
<br>
1:22:32.600,1:22:38.880<br>
而我們知道的每一個阿宅，他手上有的這個公仔的數目<br>
<br>
1:22:38.880,1:22:43.020<br>
你就可以把他們合起來看作是一個 matrix, X<br>
<br>
1:22:43.020,1:22:46.780<br>
我們可以把合起來看做是一個 matrix, X<br>
<br>
1:22:46.780,1:22:51.760<br>
那這一個 matrix, X，它的 row 的數目就是<br>
<br>
1:22:51.760,1:22:53.700<br>
Otaku 的數目<br>
<br>
1:22:53.700,1:22:56.040<br>
它的這個 column 的數目<br>
<br>
1:22:56.040,1:22:58.540<br>
就是動漫角色的數目<br>
<br>
1:22:58.540,1:23:02.420<br>
那我們現在要做的事情，就是做一個假設<br>
<br>
1:23:02.420,1:23:04.140<br>
這一個假設是這個樣子<br>
<br>
1:23:04.140,1:23:07.240<br>
每一個，這個 matrix 裡面的 element<br>
<br>
1:23:07.240,1:23:11.460<br>
它都來自於兩個 vector 的 inner product<br>
<br>
1:23:11.460,1:23:13.900<br>
都來自於兩個 vector 的內積<br>
<br>
1:23:13.900,1:23:18.460<br>
這一個 5，為什麼 A 會有 5 個涼宮春日的公仔呢<br>
<br>
1:23:18.460,1:23:22.620<br>
是因為 r^A 跟 r^1 的 inner product 很大<br>
<br>
1:23:22.620,1:23:24.040<br>
它們的 inner product 是 5<br>
<br>
1:23:24.040,1:23:27.880<br>
所以他就會買，他們就會有 5 個涼宮春日的公仔<br>
<br>
1:23:27.880,1:23:31.400<br>
如果 r^B 跟 ^r1，它的 inner product 是 4 的話<br>
<br>
1:23:31.400,1:23:34.580<br>
那 B 就會有 4個涼宮春日的公仔<br>
<br>
1:23:34.580,1:23:35.520<br>
以此類推<br>
<br>
1:23:35.520,1:23:38.560<br>
這個世間運作的邏輯就是這個樣子<br>
<br>
1:23:38.560,1:23:41.900<br>
那這件事情，如果你用數學式來表示它的話<br>
<br>
1:23:41.900,1:23:43.920<br>
你可以寫成這樣子<br>
<br>
1:23:43.920,1:23:46.960<br>
我們把 r^A 到 r^E 排成一排<br>
<br>
1:23:46.960,1:23:48.660<br>
我們把 r^A 到 r^E 排成一排<br>
<br>
1:23:48.660,1:23:53.400<br>
把 r^1 到 r^4<br>
<br>
1:23:53.400,1:23:59.360<br>
也當作是另外一個 matrix 的 row 把它排起來<br>
<br>
1:23:59.360,1:24:03.360<br>
那這個 K 是 latent factor 的數目<br>
<br>
1:24:03.360,1:24:06.360<br>
這個東西，我們是沒有辦法知道的<br>
<br>
1:24:06.360,1:24:10.040<br>
我們把人分成只有萌傲嬌和萌天然呆，這樣是一個<br>
<br>
1:24:10.040,1:24:12.780<br>
不精確的分析方式<br>
<br>
1:24:12.780,1:24:14.440<br>
如果我們有更多的 data 的話<br>
<br>
1:24:14.440,1:24:18.180<br>
我們應該可以更準確知道應該要有多少 factor<br>
<br>
1:24:18.180,1:24:20.700<br>
但是，實際上要有多少 factor 這件事情<br>
<br>
1:24:20.700,1:24:23.960<br>
你必須要試出來，就像 principal component 的數目<br>
<br>
1:24:23.960,1:24:27.000<br>
或者是這個 neural network 的陳述一樣<br>
<br>
1:24:27.000,1:24:28.900<br>
這個是你要事先決定好的<br>
<br>
1:24:28.900,1:24:30.440<br>
我們現在就假設說<br>
<br>
1:24:30.440,1:24:32.740<br>
latent factor 的數目<br>
<br>
1:24:32.740,1:24:34.940<br>
latent factor的數目就是K<br>
<br>
1:24:34.940,1:24:37.560<br>
所以， 這個 r^A 到 r^E<br>
<br>
1:24:37.560,1:24:39.100<br>
你把它當作是 matrix row<br>
<br>
1:24:39.100,1:24:42.480<br>
這邊就有 N*K，這是一個 N*K 的 matrix<br>
<br>
1:24:42.480,1:24:47.140<br>
你把 r^1 到 r^4 當作 column，那就是 K*N 的 matrix<br>
<br>
1:24:47.140,1:24:50.320<br>
你把這個 N*K  matrix、 N*K matrix 乘起來<br>
<br>
1:24:50.320,1:24:52.640<br>
你就會得到一個<br>
<br>
1:24:52.640,1:24:56.200<br>
我寫錯了，這應該是 M，不好意思，這應該是 M<br>
<br>
1:24:56.200,1:25:02.260<br>
這個應該是 M 個 Otaku，所以是有 M 個人<br>
<br>
1:25:02.260,1:25:05.820<br>
所以，把這個 (M*K) * (K*N)<br>
<br>
1:25:05.820,1:25:07.620<br>
得到一個 M*N 的 matrix<br>
<br>
1:25:07.620,1:25:10.800<br>
那它的每一個 dimension 分別是甚麼呢？<br>
<br>
1:25:10.800,1:25:13.720<br>
我們知道說，最左上角這個 dimension<br>
<br>
1:25:13.720,1:25:15.440<br>
最左上角的這個 dimension<br>
<br>
1:25:15.440,1:25:17.620<br>
如果你現在熟的話<br>
<br>
1:25:17.620,1:25:22.240<br>
它就是 r^A * r^1，對不對<br>
<br>
1:25:23.060,1:25:27.800<br>
那這第一個 row，第二個 column 的 element<br>
<br>
1:25:27.800,1:25:31.760<br>
n(下標 A2)，就是 r^A * r^2<br>
<br>
1:25:31.760,1:25:34.700<br>
n(下標 B1)，就是 r^B * r^1<br>
<br>
1:25:34.700,1:25:38.560<br>
n(下標 B2)，就是 r^B * r^2<br>
<br>
1:25:38.560,1:25:41.380<br>
所以這一個 matrix 是什麼呢？<br>
<br>
1:25:41.380,1:25:44.380<br>
這一個 matrix 其實就是這一個 matrix<br>
<br>
1:25:44.380,1:25:45.720<br>
其實就是這一個 matrix<br>
<br>
1:25:46.360,1:25:50.200<br>
假設我們說，這一個 matrix 就是這一個 matrix 的話<br>
<br>
1:25:50.200,1:25:53.340<br>
那我們要做的事情就是找一組<br>
<br>
1:25:53.340,1:25:56.360<br>
r^A 到 r^E，找一組 r^A 到 r^E<br>
<br>
1:25:56.360,1:26:00.560<br>
找一組 r^1 到 r^4<br>
<br>
1:26:00.560,1:26:02.000<br>
把這兩個 matrix 相乘以後<br>
<br>
1:26:02.000,1:26:05.240<br>
跟這個 matrix, X 越接近越好<br>
<br>
1:26:05.240,1:26:08.840<br>
minimize 這兩個 matrix 相乘以後<br>
<br>
1:26:08.840,1:26:13.420<br>
跟這個 matrix, X 之間的 Reconstruction error<br>
<br>
1:26:13.420,1:26:15.760<br>
那這個東西，我們剛才講過<br>
<br>
1:26:15.760,1:26:18.180<br>
你就可以用 SVD 來解<br>
<br>
1:26:18.180,1:26:21.220<br>
那你可能說 SVD 不是解完以後，有 3 個 matrix 嗎？<br>
<br>
1:26:21.220,1:26:22.800<br>
中間還會有 Σ 呢<br>
<br>
1:26:22.800,1:26:25.520<br>
你就看你要把 Σ 併到左邊或是併到右邊都可以<br>
<br>
1:26:25.520,1:26:30.460<br>
你就把這個 matrix, X 拆成兩個 matrix<br>
<br>
1:26:30.460,1:26:34.720<br>
它們可以 minimize Reconstruction error，然後就結束了<br>
<br>
1:26:34.720,1:26:36.680<br>
但有時候你可能會遇到這個問題<br>
<br>
1:26:36.680,1:26:43.780<br>
就是有一些 information，是 missing，是你不知道的<br>
<br>
1:26:43.780,1:26:47.980<br>
這個 information，比如說 ABC 他們<br>
<br>
1:26:47.980,1:26:54.240<br>
你並不知道 ABC 手上有沒有，有沒有小野寺的公仔<br>
<br>
1:26:54.240,1:26:58.380<br>
有可能就只是在他所在的地區，沒有發行這個公仔而已<br>
<br>
1:26:58.380,1:27:01.080<br>
所以，不知道如果發行的話，他到底會不會買<br>
<br>
1:27:01.080,1:27:04.020<br>
所以這邊，其實是個問號，這個是問號<br>
<br>
1:27:04.020,1:27:07.300<br>
假設這個 table 上有一些問號的話，怎麼辦呢？<br>
<br>
1:27:07.300,1:27:09.660<br>
這 table 上有一些問號的話<br>
<br>
1:27:09.660,1:27:11.740<br>
你用剛才那個 SVD 的方法<br>
<br>
1:27:11.740,1:27:15.040<br>
就會有點卡<br>
<br>
1:27:15.040,1:27:16.540<br>
你可能說我把這個值<br>
<br>
1:27:16.540,1:27:18.360<br>
都當作是零的話<br>
<br>
1:27:18.360,1:27:20.240<br>
做一發，這樣子也可以啦<br>
<br>
1:27:20.240,1:27:22.820<br>
不過總覺得有些怪怪的<br>
<br>
1:27:22.820,1:27:24.280<br>
可以描述得更精確一點<br>
<br>
1:27:24.280,1:27:26.900<br>
所以，如果今天在 matrix 上面<br>
<br>
1:27:26.900,1:27:29.300<br>
有一些 missing value 的話，怎麼辦呢？<br>
<br>
1:27:29.300,1:27:30.980<br>
有一些 missing value 的話<br>
<br>
1:27:30.980,1:27:32.680<br>
我們還是可以做的<br>
<br>
1:27:32.680,1:27:35.440<br>
我們就用 Gradient Descent 的方法來做它<br>
<br>
1:27:35.440,1:27:38.020<br>
就我們寫一個 loss function<br>
<br>
1:27:38.020,1:27:41.020<br>
這一個 loss function 是這樣子<br>
<br>
1:27:41.020,1:27:43.360<br>
我們要讓 r^i<br>
<br>
1:27:43.360,1:27:48.200<br>
r^i 指的是每一個 Otaku 背後的 latent factor<br>
<br>
1:27:48.200,1:27:52.320<br>
r^j 指的是每一個動漫角色背後的 latent factor<br>
<br>
1:27:52.320,1:27:54.480<br>
你要讓 i 這個人<br>
<br>
1:27:54.480,1:27:57.120<br>
他的 latent factor 乘上 j 這個角色<br>
<br>
1:27:57.120,1:27:59.160<br>
他的 latent factor，這個 inner product<br>
<br>
1:27:59.160,1:28:02.700<br>
跟他購買的數量，n(下標ij) 越接近越好<br>
<br>
1:28:02.700,1:28:04.340<br>
那重點是說<br>
<br>
1:28:04.340,1:28:07.640<br>
我今天在 summation over 這些 element 的時候<br>
<br>
1:28:07.640,1:28:10.560<br>
我們可以避開這一些 missing 的 data<br>
<br>
1:28:10.560,1:28:12.880<br>
我們可以避開這些 missing data<br>
<br>
1:28:12.880,1:28:15.340<br>
我們在 summation over 這些 ij 的時候<br>
<br>
1:28:15.340,1:28:17.340<br>
如果今天這個值是沒有的<br>
<br>
1:28:17.340,1:28:18.520<br>
我就不算它<br>
<br>
1:28:18.520,1:28:23.300<br>
我們只算這個值是有定義的部分<br>
<br>
1:28:23.300,1:28:25.200<br>
那接下來怎麼辦呢？<br>
<br>
1:28:25.200,1:28:27.580<br>
接下來，你都把 loss function 寫出來了<br>
<br>
1:28:27.580,1:28:30.320<br>
你要找 r^i 跟 r^j<br>
<br>
1:28:30.320,1:28:33.240<br>
就用 Gradient Descent，就好了<br>
<br>
1:28:33.240,1:28:35.900<br>
用 Gradient Descent 運算一發<br>
<br>
1:28:35.900,1:28:37.680<br>
然後就結束了<br>
<br>
1:28:37.680,1:28:39.520<br>
所以，根據剛才那個例子<br>
<br>
1:28:39.520,1:28:43.700<br>
我們就可以實際地用 Gradient Descent 來做一下<br>
<br>
1:28:43.700,1:28:47.340<br>
那我們假設 latent factor 的數目等於 2<br>
<br>
1:28:47.340,1:28:49.480<br>
假設 latent factor 的數目等於 2<br>
<br>
1:28:49.480,1:28:52.900<br>
那每一個這個 A 到 E 呢<br>
<br>
1:28:52.900,1:28:57.920<br>
它們就都會對應到一個二維的 vector<br>
<br>
1:28:57.920,1:28:59.780<br>
A 的 vector是這樣， B 是這樣<br>
<br>
1:28:59.780,1:29:01.260<br>
C 是這樣，以此類推<br>
<br>
1:29:01.260,1:29:04.080<br>
每一個人都會對應到一個 vector<br>
<br>
1:29:04.080,1:29:06.860<br>
那每一個這個角色呢<br>
<br>
1:29:06.860,1:29:09.700<br>
他也都可以對應到一個 vector<br>
<br>
1:29:09.700,1:29:11.760<br>
我怕大家，如果我只寫這個字的話<br>
<br>
1:29:11.760,1:29:13.100<br>
會不知道什麼意思<br>
<br>
1:29:13.100,1:29:14.960<br>
所以特別把編號列出來<br>
<br>
1:29:14.960,1:29:16.960<br>
這個是春日，這個是炮姐<br>
<br>
1:29:16.960,1:29:18.360<br>
這個是姐寺，這個是小唯<br>
<br>
1:29:18.360,1:29:20.440<br>
那你就會找出說<br>
<br>
1:29:20.440,1:29:23.100<br>
每一個角色，都得到一個 vector<br>
<br>
1:29:23.100,1:29:27.200<br>
也都得到一個 latent factor，這代表他的屬性<br>
<br>
1:29:27.200,1:29:28.160<br>
代表他的屬性<br>
<br>
1:29:28.160,1:29:34.980<br>
這個每一個人的 latent factor 就代表他萌哪一種屬性<br>
<br>
1:29:34.980,1:29:37.420<br>
所以，如果我們把他在兩個維度裡面<br>
<br>
1:29:37.420,1:29:40.000<br>
比較大的維度挑出來的話<br>
<br>
1:29:40.000,1:29:43.520<br>
你就會發現說，A 跟 B 是萌同一組屬性的<br>
<br>
1:29:43.520,1:29:45.540<br>
C，D，E 是萌同一組屬性的<br>
<br>
1:29:45.540,1:29:48.120<br>
1 跟 2 他們有同樣的屬性<br>
<br>
1:29:48.120,1:29:50.740<br>
然後 3 跟 4 他們有同樣的屬性<br>
<br>
1:29:50.740,1:29:52.560<br>
你沒有辦法真的知道說，每一個屬性<br>
<br>
1:29:52.560,1:29:54.100<br>
分別代表的是甚麼<br>
<br>
1:29:54.100,1:29:57.920<br>
你不知道說，這個維度是代表是傲嬌，還是天然呆<br>
<br>
1:29:57.920,1:30:00.480<br>
你不知道這個維度是代表傲嬌，還是天然呆<br>
<br>
1:30:00.580,1:30:03.460<br>
你會需要先找出這些 latent factor 以後<br>
<br>
1:30:03.460,1:30:05.420<br>
再去分析它的結果<br>
<br>
1:30:05.420,1:30:06.520<br>
你就可以知道說<br>
<br>
1:30:06.520,1:30:08.260<br>
因為我們事先已經知道說<br>
<br>
1:30:08.260,1:30:10.780<br>
姐寺跟小唯是有天然呆屬性<br>
<br>
1:30:10.780,1:30:14.160<br>
所以第一個維度代表的就是天然呆屬性<br>
<br>
1:30:14.160,1:30:17.140<br>
而第二個維度代表的就是傲嬌屬性<br>
<br>
1:30:17.140,1:30:19.360<br>
接下來，有這些 data 以後<br>
<br>
1:30:19.360,1:30:22.420<br>
你就可以，預測你的 missing value<br>
<br>
1:30:22.420,1:30:24.340<br>
你就可以預測問號的值<br>
<br>
1:30:24.340,1:30:26.020<br>
怎麼預測問號的值呢<br>
<br>
1:30:26.020,1:30:27.820<br>
我們如果已經知道 r^3<br>
<br>
1:30:27.820,1:30:29.680<br>
我們已經知道 r^a<br>
<br>
1:30:29.680,1:30:32.480<br>
那我們知道說，一個人會購買公仔的數量<br>
<br>
1:30:32.480,1:30:34.380<br>
其實是他背後的<br>
<br>
1:30:34.380,1:30:38.020<br>
這個動漫角色背後的 latent factor<br>
<br>
1:30:38.020,1:30:40.860<br>
那個人背後的 latent factor 做 inner product 的結果<br>
<br>
1:30:40.860,1:30:42.740<br>
所以，我們只要把 r^3 跟 r^a<br>
<br>
1:30:42.740,1:30:44.080<br>
做 inner product，你就可以預測說<br>
<br>
1:30:44.080,1:30:47.060<br>
這個人會買多少的公仔<br>
<br>
1:30:47.060,1:30:49.880<br>
所以，做完 inner product 以後的結果<br>
<br>
1:30:49.880,1:30:50.960<br>
就是這樣子<br>
<br>
1:30:50.960,1:30:52.100<br>
這告訴我們說<br>
<br>
1:30:52.100,1:30:56.140<br>
這個 C，如果說<br>
<br>
1:30:56.140,1:31:00.360<br>
r^C 跟 r^3 他們的 latent factor，其實是 match 的<br>
<br>
1:31:00.360,1:31:02.820<br>
所以 C 這個人呢<br>
<br>
1:31:02.820,1:31:04.600<br>
如果他可以買的話<br>
<br>
1:31:04.600,1:31:09.240<br>
預期他會買，他會有 2.2 個姐寺的公仔<br>
<br>
1:31:09.240,1:31:11.180<br>
所以，你就可以推薦姐寺公仔給他<br>
<br>
1:31:11.180,1:31:13.020<br>
你就可以推薦他入坑<br>
<br>
1:31:13.020,1:31:15.200<br>
所以，這個方法<br>
<br>
1:31:15.200,1:31:17.600<br>
常用在這種推薦系統裡面<br>
<br>
1:31:17.600,1:31:20.720<br>
大家可能都有聽過 Netflix 的比賽<br>
<br>
1:31:20.720,1:31:25.220<br>
如果你今天把動漫角色換成電影<br>
<br>
1:31:25.220,1:31:29.200<br>
你把這個中間的這個數值換成 rating 的話<br>
<br>
1:31:29.200,1:31:30.940<br>
你就可以預測某一個人<br>
<br>
1:31:30.940,1:31:33.980<br>
會不會喜歡某一部電影，等等<br>
<br>
1:31:33.980,1:31:38.700<br>
線上推薦系統，現在很多都會使用這樣的技術<br>
<br>
1:31:38.700,1:31:43.480<br>
其實，剛才那個 model 可以做更精緻一點<br>
<br>
1:31:43.480,1:31:47.160<br>
我們剛才說，A 背後的 latent factor<br>
<br>
1:31:47.160,1:31:49.720<br>
乘上 1 背後的 latent factor<br>
<br>
1:31:49.720,1:31:52.660<br>
得到的結果，就是 table 上面的數值<br>
<br>
1:31:52.660,1:31:55.040<br>
但是事實上，可能還有別的因素<br>
<br>
1:31:55.040,1:31:58.300<br>
會操控他的數值<br>
<br>
1:31:58.300,1:32:01.820<br>
所以更精確的寫法，或許是我們可以寫成<br>
<br>
1:32:01.820,1:32:04.380<br>
r^A 跟 的 r^1 inner product<br>
<br>
1:32:04.380,1:32:07.440<br>
加上某一個跟 A 有關的 scalar<br>
<br>
1:32:07.440,1:32:09.920<br>
再加上某一個跟 1 有關的 scalar<br>
<br>
1:32:09.920,1:32:11.300<br>
其實才等於 5<br>
<br>
1:32:11.300,1:32:14.140<br>
這跟 A 有關的 scalar, b(下標A) 指的是甚麼呢？<br>
<br>
1:32:14.140,1:32:20.100<br>
它代表的是說，A 他有多喜歡買公仔<br>
<br>
1:32:20.100,1:32:21.860<br>
有的人，他就只是<br>
<br>
1:32:21.860,1:32:23.560<br>
其實他也不特別喜歡某一個角色<br>
<br>
1:32:23.560,1:32:25.120<br>
他就只是想要買公仔而已<br>
<br>
1:32:25.120,1:32:29.860<br>
所以這個 b(下標A)，代表他本身有多想要買公仔<br>
<br>
1:32:29.860,1:32:32.020<br>
本身有多想要買公仔<br>
<br>
1:32:32.020,1:32:33.880<br>
這個 b1 代表說<br>
<br>
1:32:33.880,1:32:36.980<br>
這個角色，他本質上會有<br>
<br>
1:32:36.980,1:32:38.580<br>
多想讓人家購買<br>
<br>
1:32:38.580,1:32:40.580<br>
這件事情是跟屬性無關的<br>
<br>
1:32:40.580,1:32:42.940<br>
就是他本來就會想要購買這個角色<br>
<br>
1:32:42.940,1:32:49.240<br>
比如說，最近涼宮春日慶祝動畫十周年<br>
<br>
1:32:49.240,1:32:51.160<br>
就出現藍光 DVD，甚麼的<br>
<br>
1:32:51.160,1:32:53.660<br>
所以大家就可能會比較想要買<br>
<br>
1:32:53.660,1:32:55.400<br>
涼宮春日的公仔，其實也不會啦<br>
<br>
1:32:55.400,1:32:58.460<br>
所以就會有這個 b1<br>
<br>
1:32:58.460,1:33:02.800<br>
所以，你今天就改一下 minimize 的式子<br>
<br>
1:33:02.800,1:33:06.740<br>
就變成說，把 r^i 跟 r^j 的 inner product<br>
<br>
1:33:06.740,1:33:10.160<br>
加上 bi 再加上 bj<br>
<br>
1:33:10.160,1:33:12.180<br>
然後，你會希望這個值<br>
<br>
1:33:12.180,1:33:15.180<br>
跟 n(下標ij) 越接近越好<br>
<br>
1:33:15.180,1:33:17.680<br>
你會希望說這兩個 latent factor 的 inner product<br>
<br>
1:33:17.680,1:33:23.020<br>
加上這個代表 i 本身的 scalar<br>
<br>
1:33:23.020,1:33:24.980<br>
還有代表 j 本身的 scalar<br>
<br>
1:33:24.980,1:33:29.360<br>
要跟在 table 上面看到的值，越接近越好<br>
<br>
1:33:29.360,1:33:30.800<br>
那這個怎麼解呢<br>
<br>
1:33:30.800,1:33:32.460<br>
這沒有什麼好講的<br>
<br>
1:33:32.460,1:33:35.540<br>
你就用 Gradient Descent 硬解一發就好了<br>
<br>
1:33:35.540,1:33:38.520<br>
你也可以加 Regularization<br>
<br>
1:33:38.520,1:33:41.340<br>
如果你覺得應該要 Regularization 的話<br>
<br>
1:33:41.340,1:33:45.140<br>
那你也可以在這個 r^i，r^j，bi，bj 上面<br>
<br>
1:33:45.140,1:33:47.980<br>
加上 Regularization，比如說你會希望說<br>
<br>
1:33:47.980,1:33:51.620<br>
r^i, r^j 是 sparse 的<br>
<br>
1:33:51.620,1:33:55.260<br>
每個人要麼就是萌傲嬌，要麼就是萌天然呆<br>
<br>
1:33:55.260,1:33:57.000<br>
不會有模糊的人<br>
<br>
1:33:57.000,1:34:01.040<br>
你可能就會想要加上 L1 的 Regularization<br>
<br>
1:34:01.920,1:34:04.480<br>
如果你想要知道更多的話<br>
<br>
1:34:04.480,1:34:07.340<br>
以下是 Matrix Factorization 精簡的 paper<br>
<br>
1:34:07.340,1:34:11.160<br>
講的是在這個 Netflix 上面，是怎麼做的<br>
<br>
1:34:11.160,1:34:14.080<br>
那 Matrix Factorization 還有很多其他的應用<br>
<br>
1:34:14.080,1:34:16.000<br>
比如說，它可以用在 Topic analysis 上面<br>
<br>
1:34:16.000,1:34:17.660<br>
這個，應該是大家<br>
<br>
1:34:17.660,1:34:22.080<br>
等一下，11 點多的時候，助教會回來講一下作業 4<br>
<br>
1:34:22.080,1:34:24.920<br>
作業 4 要做的是跟文字有關的<br>
<br>
1:34:24.920,1:34:28.540<br>
裡面你可能會需要用到 Topic analysis 的技術<br>
<br>
1:34:28.540,1:34:32.360<br>
如果是把剛才所講的 Matrix Factorization 的方式<br>
<br>
1:34:32.360,1:34:34.720<br>
用在 Topic analysis 的上面的話<br>
<br>
1:34:34.720,1:34:38.180<br>
就叫做 Latent semantic analysis (LSA)<br>
<br>
1:34:38.180,1:34:42.220<br>
它的技術跟我們剛才所講的是一模一樣的<br>
<br>
1:34:42.220,1:34:44.180<br>
就只是換一個名詞而已<br>
<br>
1:34:44.180,1:34:48.300<br>
就是把剛才的動漫人物，通通換成文章<br>
<br>
1:34:48.300,1:34:52.900<br>
把剛才的人都換成詞彙<br>
<br>
1:34:52.900,1:34:55.160<br>
那 table 裡面的值呢<br>
<br>
1:34:55.160,1:34:56.980<br>
把人物換成文章<br>
<br>
1:34:56.980,1:34:58.480<br>
把人都換成詞彙<br>
<br>
1:34:58.480,1:34:59.800<br>
那 table 裡面的值呢<br>
<br>
1:34:59.800,1:35:02.100<br>
就是 Term frequency<br>
<br>
1:35:02.100,1:35:04.140<br>
就是說，投資這個 word<br>
<br>
1:35:04.140,1:35:06.580<br>
在 Doc 1 出現 5 次<br>
<br>
1:35:06.580,1:35:08.520<br>
股票這個 word 在 Doc1 出現 4 次<br>
<br>
1:35:08.520,1:35:09.800<br>
以此類推<br>
<br>
1:35:09.800,1:35:12.300<br>
有時候，我們不只會用 Term frequency<br>
<br>
1:35:12.300,1:35:13.900<br>
你會把 Term frequency<br>
<br>
1:35:13.900,1:35:17.620<br>
再乘上一個 weight，代表說<br>
<br>
1:35:17.620,1:35:20.180<br>
這個 term 本身有多重要<br>
<br>
1:35:20.180,1:35:23.340<br>
如果你把一個 term 乘上比較大的 weight 的話<br>
<br>
1:35:23.340,1:35:26.400<br>
今天你在做 matrix factorization 的時候<br>
<br>
1:35:26.400,1:35:30.540<br>
那一個 term 它就比較會被考慮到<br>
<br>
1:35:30.540,1:35:34.980<br>
你就會比較想要讓那一個 term 的 Reconstruction error<br>
<br>
1:35:34.980,1:35:38.320<br>
你就比較想要讓那個 term 的 Reconstruction error<br>
<br>
1:35:38.320,1:35:40.600<br>
比較小<br>
<br>
1:35:40.600,1:35:43.840<br>
通常怎麼 evaluate 一個 term 重不重要呢<br>
<br>
1:35:43.840,1:35:44.960<br>
有很多方法<br>
<br>
1:35:44.960,1:35:48.300<br>
比如說，常用的就是 inverse document frequency<br>
<br>
1:35:48.300,1:35:50.560<br>
這個我想等一下，助教會講<br>
<br>
1:35:50.560,1:35:52.000<br>
或是說你不知道的話<br>
<br>
1:35:52.000,1:35:54.220<br>
也可以自己 google 一下<br>
<br>
1:35:54.220,1:35:56.260<br>
所以，inverse document frequency 簡單來講<br>
<br>
1:35:56.260,1:35:58.900<br>
就是計算某一個詞彙<br>
<br>
1:35:58.900,1:36:03.300<br>
在整個 corpus 裡面，有多少比例的 document<br>
<br>
1:36:03.300,1:36:04.780<br>
有涵蓋這個詞彙<br>
<br>
1:36:04.780,1:36:07.120<br>
假如，某一個詞彙是每個 document 都有的<br>
<br>
1:36:07.120,1:36:08.820<br>
比如說，"的 " 每一個 document 都有<br>
<br>
1:36:08.820,1:36:12.080<br>
它的 inverse document frequency 就很小<br>
<br>
1:36:12.080,1:36:14.540<br>
代表說這個詞彙的重要性是低的<br>
<br>
1:36:14.540,1:36:16.840<br>
假如某一個詞彙，是只有某一篇 document 有<br>
<br>
1:36:16.840,1:36:19.140<br>
那它的 inverse document frequency 就很大<br>
<br>
1:36:19.140,1:36:21.760<br>
就代表這個詞彙的重要性是高的<br>
<br>
1:36:21.760,1:36:24.740<br>
那在這一個 task 裡面<br>
<br>
1:36:24.740,1:36:27.740<br>
如果你今天把這個 matrix 做分解的話<br>
<br>
1:36:27.740,1:36:31.460<br>
你就會找到每一個 document 背後的 latent factor<br>
<br>
1:36:31.460,1:36:35.720<br>
你就會找到每一個詞彙背後的 latent factor<br>
<br>
1:36:35.720,1:36:37.800<br>
這邊這個 latent factor 是什麼呢<br>
<br>
1:36:37.800,1:36:40.120<br>
今天這個 task 裡面，你的 latent factor 可能指的是<br>
<br>
1:36:40.120,1:36:42.360<br>
topic<br>
<br>
1:36:42.360,1:36:46.340<br>
可能指的是這個主題<br>
<br>
1:36:46.340,1:36:50.860<br>
某一個 document，它背後要談的主題<br>
<br>
1:36:50.860,1:36:53.180<br>
有多少部分是跟財經有關的<br>
<br>
1:36:53.180,1:36:55.420<br>
有多少部分是跟政治有關的<br>
<br>
1:36:55.420,1:36:57.900<br>
那某一個詞彙，它跟<br>
<br>
1:36:57.900,1:36:59.480<br>
它有多少部分是跟財經有關的<br>
<br>
1:36:59.480,1:37:01.740<br>
它有多少部分是跟政治有關的<br>
<br>
1:37:01.740,1:37:04.020<br>
比如說，可能<br>
<br>
1:37:04.020,1:37:06.480<br>
可能今天在這個例子裡面<br>
<br>
1:37:06.480,1:37:09.780<br>
投資跟股票是跟財經有關的<br>
<br>
1:37:09.780,1:37:14.260<br>
那 Doc1 跟 Doc2 又都有<br>
<br>
1:37:14.260,1:37:16.700<br>
比較多的投資跟股票這兩個詞彙<br>
<br>
1:37:16.700,1:37:18.240<br>
那 Doc1 跟 Doc 2<br>
<br>
1:37:18.240,1:37:21.620<br>
它就有比較高可能性<br>
<br>
1:37:21.620,1:37:25.240<br>
它背後的 latent factor 是偏向於財經的<br>
<br>
1:37:26.260,1:37:30.020<br>
其實 Topic analysis 的方法多如牛毛<br>
<br>
1:37:30.020,1:37:32.860<br>
所以，它們基本的精神是差不多的<br>
<br>
1:37:32.860,1:37:35.320<br>
但是有很多各種各樣的變化<br>
<br>
1:37:35.320,1:37:39.960<br>
常見的是 Probability latent semantic analysis (PLSA)<br>
<br>
1:37:39.960,1:37:43.920<br>
另外一個 latent Dirichlet allocation (LDA)<br>
<br>
1:37:43.920,1:37:46.780<br>
那這一邊就把 reference 列在這邊，給大家參考<br>
<br>
1:37:46.780,1:37:49.100<br>
前面我們有講過另外一個 LDA<br>
<br>
1:37:49.100,1:37:52.000<br>
在 machine learning 裡面，有兩個 LDA<br>
<br>
1:37:52.000,1:37:54.240<br>
然後是完全不一樣的東西<br>
<br>
1:37:54.240,1:37:56.040<br>
就是這麼回事<br>
<br>
1:37:56.040,1:38:00.300<br>
這邊就是一些 reference 給大家參考<br>
<br>
1:38:00.300,1:38:02.620<br>
其實這種 Dimension Reduction 的方式<br>
<br>
1:38:02.620,1:38:03.541<br>
這邊還沒有列<br>
<br>
1:38:03.541,1:38:06.920<br>
這邊只列一些跟 PCA 比較像、有比較關係的<br>
<br>
1:38:06.920,1:38:11.440<br>
這個 Dimension Reduction 的方法，多如牛毛<br>
<br>
1:38:11.440,1:38:12.760<br>
比如說，像 MDS<br>
<br>
1:38:12.760,1:38:14.080<br>
MDS，它特別的地方是<br>
<br>
1:38:14.080,1:38:18.120<br>
它不需要把每一個 data 都表示成 feature vector<br>
<br>
1:38:18.120,1:38:19.160<br>
它要知道的只有<br>
<br>
1:38:19.160,1:38:21.380<br>
feature vector 跟 feature vector 之間的 distance<br>
<br>
1:38:21.380,1:38:23.920<br>
你只要有這個 distance，你就可以做 Dimension Reduction<br>
<br>
1:38:23.920,1:38:26.420<br>
那一般教科書舉的例子，都會舉說<br>
<br>
1:38:26.420,1:38:27.740<br>
我現在有一堆程式<br>
<br>
1:38:27.740,1:38:29.980<br>
你不知道怎麼把程式描述成一個 vector<br>
<br>
1:38:29.980,1:38:32.180<br>
但你知道程式和程式之間的距離<br>
<br>
1:38:32.180,1:38:35.520<br>
你就有每一筆 data 之間的距離<br>
<br>
1:38:35.520,1:38:39.100<br>
就可以把它畫在一個二維平面上<br>
<br>
1:38:39.100,1:38:42.740<br>
那其實 MDS 跟 PCA ，是有一些關係的<br>
<br>
1:38:42.740,1:38:43.860<br>
你如果用某些<br>
<br>
1:38:43.860,1:38:49.060<br>
如果用某一些特定的這個 distance<br>
<br>
1:38:49.060,1:38:51.980<br>
來衡量兩個 data point 之間的距離的話<br>
<br>
1:38:51.980,1:38:55.800<br>
你又做 MDS 就等於是做 PCA<br>
<br>
1:38:55.800,1:38:57.840<br>
所以，其實 PCA 有一個特性<br>
<br>
1:38:57.840,1:38:59.500<br>
是它保留了<br>
<br>
1:38:59.500,1:39:01.220<br>
原來在高維空間中的距離<br>
<br>
1:39:01.220,1:39:04.380<br>
如果兩個點在高維空間中的距離是遠的<br>
<br>
1:39:04.380,1:39:06.620<br>
在低維的空間中，它的距離也是遠的<br>
<br>
1:39:06.620,1:39:08.820<br>
在高維空間中是接近的<br>
<br>
1:39:08.820,1:39:10.560<br>
在低維空間中也是接近的<br>
<br>
1:39:10.560,1:39:13.060<br>
那 PCA 有機率的版本<br>
<br>
1:39:13.060,1:39:14.760<br>
叫做 Probabilistic PCA<br>
<br>
1:39:14.760,1:39:18.160<br>
PCA 有非線性的版本叫做 Kernel PCA<br>
<br>
1:39:18.160,1:39:20.020<br>
另外一個東西叫  CCA<br>
<br>
1:39:20.020,1:39:22.600<br>
CCA 是說如果你有兩種不同的 source<br>
<br>
1:39:22.600,1:39:24.800<br>
這個時候你會想要用 CCA<br>
<br>
1:39:24.800,1:39:27.000<br>
比如說，如果你做語音辨識<br>
<br>
1:39:27.000,1:39:29.760<br>
你有兩個 source ，一個是聲音訊號<br>
<br>
1:39:29.760,1:39:31.080<br>
另外一個是<br>
<br>
1:39:31.080,1:39:32.920<br>
假設你今天其實是有螢幕的<br>
<br>
1:39:32.920,1:39:36.460<br>
那你可以看到這個人的唇形<br>
<br>
1:39:36.460,1:39:38.400<br>
你可以讀他的唇語<br>
<br>
1:39:38.400,1:39:40.220<br>
那你現在有聲音訊號<br>
<br>
1:39:40.220,1:39:42.700<br>
有那一個人嘴巴的 image<br>
<br>
1:39:42.700,1:39:45.400<br>
你把這兩種不同的 source<br>
<br>
1:39:45.400,1:39:47.120<br>
都做 Dimension Reduction<br>
<br>
1:39:47.120,1:39:48.540<br>
這個是 CCA<br>
<br>
1:39:48.540,1:39:51.520<br>
還有另外一種東西叫做 ICA<br>
<br>
1:39:51.520,1:39:53.640<br>
它們的名字聽起來都很像<br>
<br>
1:39:53.640,1:39:55.840<br>
ICA 比較常用在 source separation 上面<br>
<br>
1:39:55.840,1:39:57.320<br>
它要做的事情是<br>
<br>
1:39:57.320,1:39:58.820<br>
原來在 PCA 裡面<br>
<br>
1:39:58.820,1:40:00.800<br>
我們找出來的 component 是 orthogonal<br>
<br>
1:40:00.800,1:40:03.080<br>
那 ICA 裡面，我們不見得要找<br>
<br>
1:40:03.080,1:40:05.780<br>
ICA 說我們不見得要找 orthogonal 的 component<br>
<br>
1:40:05.780,1:40:08.260<br>
我們找 independent component 就好<br>
<br>
1:40:08.260,1:40:10.360<br>
然後，它用一套很複雜的方法來定義什麼叫做<br>
<br>
1:40:10.360,1:40:11.760<br>
independent component<br>
<br>
1:40:11.760,1:40:14.360<br>
還有剛才有提到的 LDA<br>
<br>
1:40:14.360,1:40:16.120<br>
它是 supervise 的方式<br>
<br>
1:40:16.120,1:40:18.200<br>
那以上給大家參考，我們在這邊休息10分鐘，謝謝<br>
<br>
1:40:18.200,1:40:20.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
