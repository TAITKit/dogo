0:00:00.320,0:00:02.960
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:03.220,0:00:06.720
接下來要講 ensemble

0:00:16.000,0:00:24.220
那 ensemble 這種方法其實就是團隊合作

0:00:24.220,0:00:27.500
好幾個模型一起上的方法

0:00:28.020,0:00:31.280
在做 ensemble 的時候通常狀況是這樣

0:00:31.720,0:00:37.275
有一打的 classifier，假設現在要做分類的問題

0:00:39.160,0:00:41.120
f1(x), f2(x), f3(x)

0:00:41.340,0:00:44.020
你想把這一把的 classifier

0:00:44.155,0:00:47.225
集合起來讓他們發揮原來

0:00:47.225,0:00:51.460
一個 classifier 所沒有辦法發揮的更強大的力量

0:00:51.460,0:00:55.040
這些 classifier 通常會希望他們是 diverse 的

0:00:55.040,0:00:59.820
每一個 classifier 有不同的屬性

0:00:59.860,0:01:02.400
有不同的作用

0:01:04.580,0:01:08.040
如果今天大家一起出團去打王的時候

0:01:08.140,0:01:13.155
每一個人都會有自己需要做的工作

0:01:13.160,0:01:16.620
需要一個團隊裡面有各種不同的角色

0:01:16.620,0:01:19.880
有人扮演坦、有人扮演補、有人扮演DD

0:01:19.980,0:01:23.600
DD就是輸出類似

0:01:24.420,0:01:31.320
你要把不同的 classifier 把它集合在一起

0:01:31.440,0:01:37.600
集合的時候必須要用比較好的方法來把他們集合在一起

0:01:37.600,0:01:39.960
就好像是在打王的時候

0:01:39.960,0:01:44.240
坦、補、DD他們有不同要站的位子

0:01:44.600,0:01:48.335
ensemble 最適合在期末的時候講

0:01:48.340,0:01:52.700
為甚麼呢？因為假設現在已經開始做 final

0:01:52.720,0:01:54.640
我相信其實你很累了

0:01:54.860,0:02:01.980
你可能沒什麼時間為 final 寫甚麼 fancy、新的程式

0:02:02.080,0:02:04.940
也許你想要摳一下手上現有的程式

0:02:04.940,0:02:07.400
然後調調參數看可以做到多好

0:02:07.600,0:02:12.360
而且有一個大招可以迅速 improve 你的 performance

0:02:12.360,0:02:14.160
就是 ensemble

0:02:14.160,0:02:18.660
如果你今天已經想不到新招術了

0:02:18.660,0:02:22.340
你現在做一做已經卡住了不知道怎麼進步的話

0:02:22.340,0:02:27.960
通常用 ensemble 可以讓你的 performance 再提升一個 level

0:02:27.960,0:02:31.380
你會發現在 machine learning 比賽的時候

0:02:31.380,0:02:34.080
在 Kaggle 上的比賽的時候

0:02:34.080,0:02:37.560
你有一個好的模型，你可以拿到前幾名

0:02:37.700,0:02:43.360
但你要奪得冠軍你通常會需要 ensemble

0:02:43.360,0:02:47.720
都會需要群毆的方式才能得到冠軍

0:02:47.720,0:02:51.620
今天就是要講怎麼來做群毆

0:02:52.340,0:02:57.800
群毆的方式有幾種不同的方法

0:02:57.840,0:03:00.915
我們先講 Bagging 這個方法

0:03:00.920,0:03:07.000
注意一下等一下除了會講 Bagging 之外還會講 Boosting

0:03:07.040,0:03:12.700
Bagging 和 Boosting 使用的場合是不太一樣的

0:03:13.360,0:03:15.860
這個大家要特別注意一下

0:03:15.860,0:03:19.700
複習一下我們在開學講過的東西

0:03:19.700,0:03:26.040
我們開學講過做 Machine Learning 的時候有 Bias 和 Variance 的 trade-off

0:03:26.140,0:03:29.120
如果有一個很簡單的 Model

0:03:29.120,0:03:32.580
我們會有很大的 Bias、比較小的 Variance

0:03:32.580,0:03:37.680
如果我們有複雜的 Model 可能是小的 Bias、大的 Variance

0:03:37.780,0:03:48.480
在這兩者的組合下，我們會看到我們的 Error rate 隨著 Model 的複雜度增加逐漸下降再逐漸上升

0:03:51.140,0:03:56.280
之前也有舉過說假設現在在不同的世界裡面

0:03:56.280,0:03:58.640
我們都在抓寶可夢

0:03:58.640,0:04:03.080
在不同的世界裡面我們都會得到一個模型

0:04:03.120,0:04:07.740
假設我們用的是很複雜的模型

0:04:07.740,0:04:10.160
我們會有很大的 Variance

0:04:10.160,0:04:17.280
在不同的世界，我們所預測出來可以預測寶可夢 CP 值的模型會非常的不一樣

0:04:17.720,0:04:22.920
但是這些結果雖然 Variance 不大

0:04:22.940,0:04:27.200
但是他們的 Bias 是小的

0:04:27.520,0:04:31.240
所以我們可以把不同的模型

0:04:31.240,0:04:39.440
我們可以把不同的模型通通集合起來

0:04:39.440,0:04:44.940
我們可以把不同模型的輸出做一個平均，得到一個新的模型 f hat

0:04:44.940,0:04:50.980
這個新的模型 f hat 可能就會跟正確答案是接近的

0:04:51.120,0:04:55.740
那 Bagging 其實就是要體現這件事情

0:04:56.420,0:04:59.160
Bagging 要做的事情就是

0:04:59.200,0:05:04.460
雖然我們不可能到不同的宇宙蒐集 data

0:05:04.460,0:05:08.380
但是我們可以創造出不同的 data set

0:05:08.380,0:05:11.900
再用不同的 data set 各自訓練一個複雜的 Model

0:05:12.000,0:05:16.200
雖然每一個 Model 獨自拿出來看可能 Variance 很大

0:05:16.200,0:05:19.740
把不同的 Variance 很大的 Model 集合起來以後

0:05:19.740,0:05:22.080
他的 Variance 就不會那麼大

0:05:22.080,0:05:24.400
他的 Bias 會是小的

0:05:24.800,0:05:27.200
怎麼自己製造不同的 data 呢

0:05:27.200,0:05:30.340
假設現在有 N 筆 Training Data

0:05:31.140,0:05:35.380
對這 N 筆 Training Data 做 Sampling

0:05:35.380,0:05:41.340
從這 N 筆 Training Data 裡面每次取 N' 筆 data

0:05:41.480,0:05:43.900
組成一個新的 Data Set

0:05:44.000,0:05:48.180
通常在做 Sampling 的時候會做 replacement

0:05:48.180,0:05:51.480
抽出一筆 data 以後會再把它放到 pool 裡面去

0:05:52.300,0:05:56.980
那所以通常 N' 可以設成 N

0:05:57.360,0:05:59.380
所以把 N' 設成 N

0:05:59.380,0:06:01.200
從 N 這個 Data Set 裡面

0:06:01.620,0:06:04.700
做 N 次的 Sample with replacement

0:06:04.800,0:06:10.400
得到的 Data Set 跟原來的這 N 筆 data 並不會一樣

0:06:10.440,0:06:14.200
因為你可能會反覆抽到同一個 example

0:06:14.200,0:06:19.360
總之我們就用 sample 的方法建出好幾個 Data Set

0:06:19.360,0:06:22.840
每一個 Data Set 都有 N' 筆 Data

0:06:23.020,0:06:27.040
每一個 Data Set 裡面的 Data 都是不一樣的

0:06:27.040,0:06:28.240
接下來

0:06:28.520,0:06:31.260
你再用一個複雜的模型

0:06:31.260,0:06:34.820
去對這四個 Data Set 做 Learning

0:06:34.820,0:06:38.080
就找出了四個 function

0:06:38.200,0:06:40.900
接下來在 testing 的時候

0:06:41.000,0:06:44.600
就把一筆 testing data 丟到這四個 function 裡面

0:06:44.600,0:06:48.220
再把得出來的結果作平均

0:06:48.220,0:06:50.300
或者是作 Voting

0:06:50.300,0:06:54.080
通常就會比只有一個 function 的時候

0:06:54.080,0:06:55.860
performance 還要好

0:06:55.860,0:06:58.320
performance 還要好是指說你的

0:06:58.440,0:07:00.360
Variance 會比較小

0:07:00.360,0:07:04.340
所以你得到的結果會是比較 robust 的

0:07:04.340,0:07:07.175
比較不容易 Overfitting

0:07:07.180,0:07:10.100
如果做的是 regression 方法的時候

0:07:10.100,0:07:15.140
你可能會用 average 的方法來把四個不同 function 的結果組合起來

0:07:15.140,0:07:21.240
如果是分類問題的話可能會用 Voting 的方法把四個結果組合起來

0:07:21.360,0:07:25.180
就看這四個 function 裡面哪一個類別

0:07:25.220,0:07:31.420
最多 classifier 投票給他，就選那個 class 當作 Model 的 output

0:07:31.860,0:07:34.740
注意一下，甚麼時候做 Bagging

0:07:34.740,0:07:39.980
當你的 model 很複雜的時候、擔心它 Overfitting 的時候

0:07:40.040,0:07:42.120
才做 Bagging

0:07:42.600,0:07:47.560
做 Bagging 的目的是為了要減低 Variance

0:07:48.180,0:07:51.980
你的 model Bias 已經很小但 Variance 很大

0:07:51.980,0:07:54.320
想要減低 Variance 的時候

0:07:54.320,0:07:55.360
你才做 Bagging

0:07:55.360,0:07:57.920
所以適用做 Bagging 的情況是

0:07:57.920,0:08:02.400
你的 Model 本身已經很複雜在 Training Data 上很容易就 Overfit

0:08:02.400,0:08:04.720
這個時候你會想要用 Bagging

0:08:04.720,0:08:06.700
甚麼樣的 Model 會很容易 Overfit 呢

0:08:06.700,0:08:09.760
有人會說 NN 很容易 Overfit

0:08:09.760,0:08:11.920
沒有，其實 NN 沒有那麼容易 Overfit

0:08:11.920,0:08:13.860
相較於看你跟誰比

0:08:13.860,0:08:20.960
很多人就憑著直覺說：一個 Neural Network 參數那麼多應該很容易 Overfit 吧

0:08:21.060,0:08:24.075
如果你有實作過 Neural Network 的話

0:08:24.080,0:08:27.780
我想你其實是不會這麼想

0:08:27.780,0:08:30.620
做 Neural Network 的時候常常遇到的問題是

0:08:30.620,0:08:34.980
你沒辦法在 Training Set 上 Overfit
而不是你非常容易 Overfit

0:08:35.080,0:08:36.080
對不對

0:08:36.900,0:08:39.480
甚麼樣的 Model 非常容易 Overfit 呢

0:08:39.480,0:08:42.060
舉例來說 Decision Tree

0:08:42.340,0:08:44.980
就是一個非常容易 Overfit 的方法

0:08:44.980,0:08:48.420
Decision Tree 你只要想的話，把樹長得很深

0:08:48.420,0:08:54.080
他在 Training data 上只要夠深都能可以拿到 100% 的正確率

0:08:54.300,0:08:56.540
NN 很難拿到 100% 的正確率

0:08:56.540,0:09:01.220
要拿到 100% 的正確率就要在 MNIST 上好好調參數
才能拿到 100% 正確率

0:09:01.220,0:09:06.000
但像 Decision Tree 這種方法只要他想他可以拿到 100% 正確率

0:09:06.000,0:09:10.760
但在 Training data 上拿到 100% 正確率不見得有甚麼特別厲害的地方

0:09:10.780,0:09:13.540
其實就只是 Overfitting 而已

0:09:13.860,0:09:17.360
所以甚麼時候要做 Bagging

0:09:17.360,0:09:20.600
就是 Model 很容易 Overfitting 的時候要做 Bagging

0:09:20.600,0:09:23.140
所以 Decision Tree 很需要做 Bagging

0:09:23.280,0:09:29.120
Random Forest 就是 Decision Tree 做 Bagging 的版本
就是 Random Forest

0:09:29.120,0:09:31.340
我們沒有講過 Decision Tree

0:09:31.340,0:09:34.440
那其實我覺得也不見得需要講

0:09:34.500,0:09:36.920
你們每個人都知道 Decision Tree 對不對

0:09:36.920,0:09:40.800
我看大家在作業裡面都能夠用到 Decision Tree

0:09:40.800,0:09:44.840
都用得很爽，所以就好像是不太需要講

0:09:44.840,0:09:48.420
我們就秒講過去

0:09:48.740,0:09:53.980
現在假設每個 object，有兩個 feature x1 跟 x2

0:09:53.980,0:10:03.520
Decision Tree 就是根據 Training data 建出一棵樹

0:10:03.660,0:10:08.900
這棵樹告訴我們

0:10:08.900,0:10:15.580
如果輸入的 object x1 < 0.5 就是 yes
x1 > 0.5 就是 no

0:10:15.580,0:10:19.860
所以就是在 x1 = 0.5 的地方切一刀

0:10:19.860,0:10:26.640
以左就走到左邊這條路上去
往右就走到右邊這條路上去

0:10:26.640,0:10:30.980
接下來再看 x2 < 0.3 的時候

0:10:30.980,0:10:34.800
那就說是 Class 1，藍色

0:10:34.860,0:10:39.240
x2 > 0.3 的時候就說是 Class 2，就紅色

0:10:39.340,0:10:42.280
那如果在右邊呢

0:10:42.280,0:10:45.820
右邊如果 x2 < 0.7 的時候就塗紅色

0:10:45.820,0:10:49.360
x2 > 0.7 的時候就塗藍色

0:10:49.780,0:10:54.820
那這邊這個 Decision Tree 的問題是比較簡單的

0:10:54.820,0:10:58.820
它只看一個 dimension
其實可以同時看兩個 dimension

0:10:58.880,0:11:00.600
其實可以問更複雜的問題

0:11:00.600,0:11:04.740
要問甚麼問題是人自己決定的

0:11:05.620,0:11:10.120
所以做 Decision Tree 的時候會有很多需要注意的地方

0:11:10.120,0:11:17.040
舉例來說，在每個節點做多少分支

0:11:17.040,0:11:21.240
要用甚麼樣的 criterion 來做分支

0:11:21.240,0:11:24.040
要甚麼時候停止分支

0:11:24.080,0:11:30.040
有可以問的問題在集合裡面，有那些問題等等

0:11:32.880,0:11:39.160
也是有很多參數要調，跟 NN 一樣有一些東西是需要調整的

0:11:39.820,0:11:42.400
我們就來舉一個 Decision Tree 的例子吧

0:11:42.400,0:11:45.880
我們把 Decision Tree 實作在以下這個 task 上面

0:11:45.880,0:11:48.740
這個 task 叫做初音 task

0:11:48.860,0:11:52.720
這個 task 是這樣的，有一個分類的問題

0:11:52.720,0:12:01.920
這個分類的問題是說輸入的 feature 就是二維

0:12:01.920,0:12:06.260
這個紅色的部分是屬於 Class 1

0:12:06.260,0:12:11.100
在藍色的部分是屬於另外一個 Class，Class 2

0:12:11.100,0:12:17.200
Class 1 分佈的樣子正好就跟初音是一樣的

0:12:17.200,0:12:22.040
如果你要用這個 data 我放在這邊
你可以載這個 data 來用

0:12:22.040,0:12:24.280
這個是初音的 task

0:12:24.280,0:12:28.880
一般教科書都會用方型、圈圈那個都太弱了

0:12:28.880,0:12:31.280
這個要用初音的 task

0:12:32.020,0:12:35.860
Class 1 的分佈就跟初音一樣

0:12:35.860,0:12:44.020
現在 Decision Tree 能不能夠在這個 task 裡面把 Class 1 和 Class 2 進行正確的分類呢

0:12:44.040,0:12:45.680
我們來看一下結果

0:12:45.680,0:12:48.055
現在用一棵 Decision Tree

0:12:48.060,0:12:51.900
那這個 Decision Tree 的深度是 5

0:12:51.960,0:12:54.860
他沒有辦法把 Class 1 和 Class 2 分開

0:12:54.880,0:12:59.880
它只能框說這個方塊的地方就是 Class 1

0:12:59.880,0:13:05.080
如果更深的 Decision Tree 呢
如果 Decision Tree 的深度深達 10 的話

0:13:05.080,0:13:08.020
看起來就有點初音的樣子

0:13:08.020,0:13:14.300
不過它有很明顯的鋸齒狀
看起來像是在 Minecraft 世界裡面看到的初音

0:13:14.780,0:13:18.700
如果 Depth = 15 的話就看起來更好

0:13:18.700,0:13:21.115
這個樣子看起來就滿對的

0:13:21.120,0:13:26.000
有一些地方還是有點怪怪的比如說這裡
這邊凸起來一塊

0:13:26.520,0:13:29.460
如果 Decision Tree 的深度是 20 的話

0:13:29.460,0:13:36.320
那你就可以完美的把 Class 1 的位置跟 Class 2 的位置區別開來

0:13:36.320,0:13:40.660
就可以完美的把初音的樣子勾勒出來

0:13:40.880,0:13:45.280
這個其實沒有甚麼，Decision Tree 只要你想的話

0:13:45.280,0:13:47.785
永遠可以做到 Error Rate 是 0

0:13:47.785,0:13:50.060
永遠可以做到正確率是 100

0:13:50.060,0:13:54.180
因為你想想看最極端的 case 就是這個 tree 一直長下去

0:13:54.180,0:13:59.980
每一筆 data point 就是一個很深的樹的其中一個節點

0:13:59.980,0:14:04.760
的其中一片葉子

0:14:04.800,0:14:07.380
這樣正確率就一定是 100%

0:14:07.380,0:14:12.720
所以這個沒有甚麼
樹夠深，Decision Tree 可以做出任何 function

0:14:14.200,0:14:19.340
但是因為 Decision Tree 太容易 Overfitting

0:14:19.340,0:14:23.080
所以單用一棵 Decision Tree 你往往不見得可以得到好的結果

0:14:23.120,0:14:28.580
所以要對 Decision Tree 做 Bagging 這個方法就是 Random Forest

0:14:28.700,0:14:31.820
我們可以用傳統 Bagging 的方法

0:14:31.820,0:14:36.780
來做 Random Forest
可以用傳統的剛才講的 sample 那個方法來做 Bagging

0:14:36.780,0:14:42.520
但是如果用那種方法你得到的 tree 通常每棵都沒有差太多

0:14:42.520,0:14:46.440
所以光用 sample 的方法看起來是不太夠的

0:14:46.940,0:14:50.120
在做 Random Forest 比較 typical 的方法是

0:14:50.120,0:14:54.400
在每一次要產生 Decision Tree 的 branch 的時候

0:14:54.400,0:15:03.800
都 random 的決定哪一些 feature 或哪一些問題是不能用

0:15:03.940,0:15:07.920
你 random 的決定現在要做 split 的時候

0:15:07.920,0:15:11.420
那些 question 或那些 feature 不能用就可以促使

0:15:11.420,0:15:13.900
就算你用的是一模一樣的 dataset

0:15:13.900,0:15:17.540
每一次你產生的 Decision Tree 也會是不一樣的

0:15:17.620,0:15:20.840
最後再把所有 Decision Tree 的結果通通集合起來

0:15:20.840,0:15:23.640
就得到 Random Forest

0:15:24.240,0:15:28.100
如果是用 Bagging 的方法有一個

0:15:28.160,0:15:32.040
有一個叫 Out-of-bag 的方法可以幫你做 Validation

0:15:32.040,0:15:37.000
一般我們在做 validation 都是把手上的 training set 切成兩塊

0:15:37.000,0:15:41.320
手上原來有 label 的 data 切成兩塊
training set 跟 validation set

0:15:41.380,0:15:44.140
如果是用 Bagging 的方法的話

0:15:44.140,0:15:48.260
你可以不要把你的 labeled data 切成 training set 跟 validation set

0:15:48.260,0:15:54.420
但一樣有 validation 的效果，怎麼做呢

0:15:54.420,0:15:57.420
因為我們知道在做 Bagging 的時候

0:15:57.420,0:16:03.280
每一個 function，train 出來的每一個 model 他都只用到部分的 data

0:16:03.280,0:16:08.540
假設現在 training data 裡面有 x1 到 x4 總共有四筆 data

0:16:08.660,0:16:12.720
f1 只用第一筆和第二筆 data train

0:16:12.720,0:16:15.200
f2 只用第三筆、第四筆 data train

0:16:15.200,0:16:19.800
f3 只用 1、3筆 data train
f4 只用 2、4筆 data train

0:16:21.160,0:16:30.180
那我們就會知道實際上在 train f2 跟 f4 的時候

0:16:30.180,0:16:32.355
其實沒有用到 x1

0:16:32.360,0:16:41.060
所以可以用 f2 加 f4 Bagging 的結果去在 x1 上面 testing 他的 performance

0:16:41.060,0:16:42.520
同理我們可以用

0:16:42.520,0:16:45.960
x2 跟 x3 做 Bagging 的結果去 test x2

0:16:45.960,0:16:48.940
用 f1 跟 f4 Bagging 的結果 test x3

0:16:48.940,0:16:54.260
用 f1 跟 f3 Bagging 的結果 test x4

0:16:54.860,0:16:58.780
接下來再把 x1 跟 x4 的結果

0:16:58.780,0:17:00.740
把它做平均

0:17:00.740,0:17:05.500
算一下 error rate 就得到 Out-of-bag 的 error

0:17:05.500,0:17:09.200
雖然這邊沒有明確的切出一個 Validation Set

0:17:09.220,0:17:11.940
但是在做 testing 的時候所用的 model

0:17:11.940,0:17:14.680
並沒有看過那些 testing 的 data

0:17:14.680,0:17:17.600
在 test x1 到 x4 的時候

0:17:17.600,0:17:27.560
這些 model 並沒有看過 x1 到 x4

0:17:27.560,0:17:37.460
所以這個 Out-of-bag error 其實也是一個可以在 Testing set 上可以反應 testing set 結果的 estimation

0:17:37.620,0:17:44.780
那我們看一下 Random Forest 在初音的 task 上的結果

0:17:47.900,0:17:51.320
這邊是做一百棵樹

0:17:51.320,0:17:54.820
你會發現如果是一百棵 Depth = 5 的樹

0:17:54.820,0:17:57.080
做出來的結果是這個樣子

0:17:57.220,0:18:02.740
這邊要強調一下做 Bagging 並不會使你的 model 更能夠 fit data

0:18:03.080,0:18:06.460
所以 Depth 是 5 的樹沒有辦法 fit 出那個 function

0:18:06.460,0:18:09.760
用 Random Forest 還是沒有辦法 fit 出那個 function

0:18:09.760,0:18:14.400
可以得到的結果只是現在因為是把五棵樹平均起來

0:18:14.400,0:18:20.560
所以得到整體的 function 他是比較平滑的而已

0:18:20.620,0:18:25.560
所以比如說 Depth = 10
看起來就比較不像 Minecraft 的世界就是了

0:18:25.880,0:18:28.460
如果 Depth = 15 得到的結果是這樣

0:18:28.460,0:18:31.980
看起來很好但其實他是有一個瑕疵的

0:18:31.980,0:18:36.180
他有一些地方沒有做好
我記得這邊有一條頭髮垂下來

0:18:36.180,0:18:39.760
他沒有把那條頭髮框出來的樣子

0:18:39.760,0:18:42.680
我看一下，喔對沒錯
如果是 Depth = 20

0:18:42.680,0:18:47.360
把一棵 Depth = 20 的樹
就可以完美的把初音框出來

0:18:47.360,0:18:49.880
這邊其實是有一條頭髮

0:18:49.880,0:18:52.380
要把這個做出來才是真的正確

0:18:54.240,0:18:58.420
接下來要講 Boosting

0:18:58.420,0:19:00.780
Boosting 跟剛才的 Bagging 是不一樣的

0:19:00.780,0:19:02.960
Bagging 是用在很強的 model

0:19:02.960,0:19:08.040
Boosting 是用在弱的 model 上面

0:19:08.040,0:19:11.100
當你有一些弱的 model

0:19:11.140,0:19:17.060
但問題是你沒有辦法讓他們去 fit data 的時候

0:19:17.120,0:19:20.420
這個時候你就會想要用 Boosting

0:19:20.520,0:19:24.380
Boosting 是這樣
Boosting 他

0:19:24.760,0:19:29.540
它有一個很 powerful 的 guarantee

0:19:29.540,0:19:31.860
這個很 powerful 的 guarantee 是這樣說的

0:19:31.860,0:19:34.320
假設有一個 ML 的 algorithm

0:19:34.560,0:19:40.580
它可以給你一個錯誤率高過 50% 的 classifier

0:19:40.580,0:19:42.700
假設要做分類的問題

0:19:42.740,0:19:45.560
那錯誤率高過 50% 的 classifier

0:19:46.100,0:19:47.700
假設這二元分類的問題

0:19:47.700,0:19:49.820
用 random 猜都高過 50%

0:19:49.820,0:19:54.420
很爛的模型都可以辦到

0:19:54.420,0:19:58.540
只要能夠做到這件事

0:19:58.680,0:20:08.880
Boosting 這個方法可以保證最後把這些錯誤率僅略高於 50% 的 classifier 組合起來以後

0:20:08.880,0:20:12.160
它可以讓錯誤率達到 0%

0:20:12.160,0:20:16.160
有沒有聽起來非常神奇

0:20:16.260,0:20:18.780
聽起來就是非常的強

0:20:18.840,0:20:21.460
Boosting 的 framework

0:20:21.640,0:20:25.360
整個大架構大概是這樣

0:20:25.740,0:20:29.760
首先找一個 classifier f1

0:20:29.760,0:20:33.280
這個 classifier f1 很弱沒有關係

0:20:33.280,0:20:38.960
接下來再找一個 classifier f2 他去輔助 f1

0:20:38.960,0:20:45.440
但要注意 f2 跟 f1 不可以很像
他們要是互補

0:20:45.540,0:20:47.800
f2 跟 f1 的特性是互補

0:20:47.900,0:20:50.815
f2 要去彌補 f1 的缺失

0:20:50.815,0:20:53.995
f2 要去做 f1 沒有辦法做到的事情

0:20:54.020,0:20:57.580
這樣進步量才大
那 Boosting

0:20:57.640,0:21:00.500
等一下就會講怎麼樣找到一個 f2

0:21:00.500,0:21:03.300
它跟 f1 是最互補的

0:21:03.300,0:21:05.680
然後就得到第二個 classifier f2

0:21:05.860,0:21:08.880
接下來呢 你再找說 我先找 classifier f2

0:21:08.880,0:21:11.480
再找一個 f3 跟 f2 是互補的

0:21:11.480,0:21:14.220
接下來再找一個 f4 跟 f3 是互補的

0:21:14.220,0:21:17.940
這個 process 就繼續下去，找到一把的 classifier

0:21:17.940,0:21:22.040
再把這把 classifier 及合起來就可以得到很低的 error rate

0:21:22.040,0:21:24.735
就算是每個 classifier 他們都很弱

0:21:24.740,0:21:26.960
也沒有關係

0:21:27.240,0:21:30.940
要注意的是在做 Boosting 的時候

0:21:31.260,0:21:35.560
classifier 的訓練是有順序的

0:21:35.560,0:21:37.940
要先找出 f1

0:21:37.940,0:21:40.800
才找得出 f2 才找得出 f3

0:21:40.800,0:21:42.780
它是 sequential 的

0:21:42.780,0:21:46.740
要先找 f1 才知道怎麼找 f2 跟 f1 是互補的

0:21:46.800,0:21:49.160
所以它是有順序的找

0:21:49.160,0:21:51.440
那前面在 Bagging 的時候

0:21:51.440,0:21:54.140
每一個 classifier 是沒有順序的

0:21:56.380,0:21:59.120
在做 Random Forest 要 train 一百棵 Decision Tree

0:21:59.120,0:22:01.620
這一百棵 Decision Tree 可以一起做

0:22:01.620,0:22:07.620
但這邊如果是要把一百個 Decision Tree 用 Boosting 的方法把它變得很強的話

0:22:07.660,0:22:12.340
要按順序做，它不是平行做的方法

0:22:12.420,0:22:17.000
這邊假設我們考慮的一個 task 是一個 Binary Classification 的 task

0:22:17.000,0:22:19.600
就是有一堆 training data，x 跟 y hat

0:22:19.640,0:22:22.620
y hat 就等於 +-1

0:22:33.540,0:22:41.680
接下來要講的是怎麼得到不同的 classifier

0:22:41.680,0:22:45.140
剛剛在 Bagging 的時候講過

0:22:45.360,0:22:47.340
要得到不同的 classifier

0:22:47.340,0:22:50.700
可以用製造不同的 training set 的方式

0:22:50.700,0:22:54.140
來得到不同的 classifier

0:22:54.240,0:22:59.020
在 Boosting 的時候也可以這麼做

0:22:59.020,0:23:07.720
可以用 resample 的方式來製造不同的 training data
然後得到不同的 classifier

0:23:07.980,0:23:12.820
但是有另外一種方法可以幫你製造出不同的 dataset

0:23:12.820,0:23:17.840
你可以給你 training data 裡面的每一筆 data 一個 weight

0:23:17.960,0:23:19.340
舉例來說

0:23:19.340,0:23:25.020
這邊用 u 來代表每一筆 data 的 weight

0:23:25.420,0:23:27.100
一開始

0:23:27.100,0:23:34.040
可以藉由改變這個 weight 來製造不同的 dataset

0:23:34.040,0:23:36.760
舉例來說現在本來有三筆 data

0:23:36.760,0:23:39.120
每一筆 data 的 weight 都是 1

0:23:39.140,0:23:43.420
你可以把它改成第一筆 data weight 是 0.4

0:23:43.420,0:23:47.040
第二筆 data weight 是 2.1
第三筆 data weight 是 0.7

0:23:47.100,0:23:53.100
這樣就等於製造出了一個新的 dataset

0:23:53.220,0:23:57.100
其實 sampling 也可以視同是改了 weight

0:23:57.100,0:23:59.760
只是 sampling 比如說某一筆 data 被 sample 兩次

0:23:59.760,0:24:02.275
就代表他的 weight 變成 2

0:24:02.275,0:24:05.665
如果用 sampling 的方法 weight 只能是整數

0:24:06.315,0:24:09.395
直接調一個 weight u 的話可以給小數就是了

0:24:11.620,0:24:14.460
就算是改變了 weight

0:24:14.460,0:24:16.920
對 training 也不會有太大的影響

0:24:16.920,0:24:19.200
我們知道在 training 的時候

0:24:19.200,0:24:24.060
原來 Objective Function 是寫成這個樣子

0:24:24.180,0:24:26.805
有一個 Loss Function 要去 minimize 他

0:24:26.805,0:24:32.120
這個 Loss Function 是 summation over 所有的 training data

0:24:32.120,0:24:35.580
對每一筆 training data x n

0:24:35.600,0:24:38.940
都把他帶到 function f 裡面去得到 f(x n)

0:24:38.940,0:24:42.320
計算 f(x n) 跟 y hat n 的差距

0:24:42.320,0:24:45.520
這個差距就用 Loss Function 來表示

0:24:45.520,0:24:48.700
這個小 L 可以是各種不同的 function

0:24:48.700,0:24:54.040
反正能夠量 f(x n) 跟 y hat n 之間的差異就行了

0:24:54.040,0:24:56.160
然後就用 Gradient Descent 的方法

0:24:56.160,0:25:01.580
去找一個 function f 來 minimize 大 L 這個 Total Loss Function

0:25:01.580,0:25:04.300
如果加上 weight 的話有甚麼不同呢

0:25:04.300,0:25:10.220
沒有甚麼不同，唯一的不同只有會在每一個小 L 的 function 前面

0:25:10.220,0:25:12.680
乘上 u

0:25:12.680,0:25:18.060
會在每一個小 L 的 function 前面乘上那筆 data 的 weight

0:25:18.060,0:25:20.240
代表那筆 data 的權重

0:25:20.240,0:25:23.080
如果有一筆 data 它的權重比較重

0:25:23.080,0:25:26.200
他的 u 比較大，那在 training 的時候

0:25:26.200,0:25:29.380
他就會被多考慮一點

0:25:29.960,0:25:32.980
那有了這個概念以後

0:25:32.980,0:25:35.880
Adaboost 的精神是甚麼

0:25:37.500,0:25:44.340
Boosting 有很多的方法等一下我們要介紹其中最經典的 Adaboost

0:25:46.800,0:25:52.180
Adaboost 他的想法是

0:25:54.040,0:25:58.940
先訓練好一個 classifier f1

0:25:59.040,0:26:05.560
要去找一組新的 training data

0:26:05.560,0:26:10.860
所謂找一組新的 training data 其實就是 reweight training 的 example

0:26:10.860,0:26:13.040
要去找一組新的 training data

0:26:17.400,0:26:22.140
讓 f1 在這組新的 training data 上結果是會爛掉的

0:26:22.140,0:26:25.480
會 fail 掉，正確率會變成只有 50%

0:26:25.480,0:26:29.880
要找一組新的 training data
f1 在這組新的 training data 是做不好的

0:26:29.880,0:26:35.140
然後再用 f2 在這組新的 training data 上面訓練

0:26:35.280,0:26:39.580
接下來怎麼找 f1

0:26:39.680,0:26:48.820
怎麼找一個新的 training data 可以讓 f1 壞掉
假設給你一個 f1

0:26:49.100,0:26:55.960
先來看一下 f1 在 training data 上的 Error Rate 怎麼計算

0:26:56.040,0:27:01.500
f1 在 training data 上的 Error Rate 這邊寫成 epsilon 1

0:27:01.500,0:27:03.880
epsilon 1 的計算方法就是

0:27:03.880,0:27:11.280
summation over 所有的 training example n

0:27:11.280,0:27:20.720
然後去計算每一筆 training example 的結果是不是對的

0:27:20.720,0:27:23.120
如果是對的話就是 0

0:27:23.120,0:27:25.040
如果是錯的話就是 1

0:27:25.040,0:27:32.280
每一筆 training example 還要乘上他的 weight u n

0:27:32.340,0:27:36.860
然後再做一下 normalization
因為這個 u n 的值

0:27:37.040,0:27:40.620
合起來不見得是 1
所以要做一個 normalization

0:27:40.620,0:27:43.640
這個 normalization 就是 summation over

0:27:43.640,0:27:45.480
所有的 u1

0:27:45.480,0:27:47.540
summation over 所有的 weight

0:27:47.540,0:27:50.320
就是這個 normalization 的 term

0:27:50.320,0:27:53.660
那 epsilon 1 一定會小於 0.5

0:27:53.660,0:27:57.580
因為我們假設 classifier 是還可以的

0:27:58.820,0:28:01.620
不是一個完全 random 的 classifier

0:28:01.620,0:28:04.620
所以 Error Rate 總是可以小於 0.5

0:28:06.620,0:28:10.160
其實沒有辦法製造一個 classifier 他的 Error Rate > 0.5

0:28:10.160,0:28:14.720
你知道嗎，因為 classifier 他的 Error Rate > 0.5只要把它的 output 反過來

0:28:14.720,0:28:17.620
它的 Error Rate 就小於 0.5 了

0:28:26.520,0:28:29.160
原來 training data 的 weight 是 u1

0:28:29.160,0:28:32.880
要給一組新的 training data 的 weight u2

0:28:32.880,0:28:37.600
這組新的 training data 的 weight 會使得

0:28:37.600,0:28:41.580
如果把上面這個算 epsilon 1 的式子的 u1

0:28:41.700,0:28:44.640
換成 u2 得到的結果

0:28:44.640,0:28:46.480
會變成 0.5

0:28:46.480,0:28:49.480
本來 epsilon 1 是小於 0.5

0:28:49.520,0:28:53.920
在 u1 作為 weight 做計算的時候小於 0.5

0:28:53.920,0:29:00.140
把 u1 換成 u2 weight 就變成 0.5

0:29:00.480,0:29:05.480
這個就好像是假如重新 weight 我們的 training data

0:29:05.480,0:29:07.800
本來用 u1 作為 training data 的 weight

0:29:07.800,0:29:09.380
現在用 u2 作為 training data 的 weight

0:29:09.380,0:29:11.560
在這組新的 weight 上面

0:29:11.560,0:29:16.340
f1 的 performance 就像是隨機的一樣

0:29:16.340,0:29:22.080
接下來再拿這組新的 training data

0:29:22.080,0:29:26.660
用 u2 當作 weight 的 training data 再去訓練 f2

0:29:26.660,0:29:30.140
f2 就會跟 f1 是互補的

0:29:30.900,0:29:35.240
這樣講也許有點抽象舉一個實際的例子

0:29:35.240,0:29:37.380
現在有四筆 training data

0:29:37.800,0:29:41.780
這四筆 training data 的 weight 就是 u1 到 u4

0:29:41.780,0:29:47.940
假設 u1, u2, u3, u4 通通等於 1
這四筆 training data 的 weight 是一樣的

0:29:47.940,0:29:52.020
用這四筆 training data 去訓練一個模型

0:29:52.020,0:29:54.920
去訓練一個 classifier f1

0:29:54.920,0:29:58.700
假設 f1 它不是一個特別 powerful 的 algorithm

0:29:58.700,0:30:03.300
所以就算是 training data 他也沒有辦法每一筆 training data 都分類正確

0:30:03.300,0:30:07.760
假設他指正確分類三筆 training data
一筆 training data 是分類是錯的

0:30:08.560,0:30:11.680
所以它的 Error Rate 是 0.25

0:30:11.680,0:30:16.620
四筆 training data 它分錯一筆所以它的 Error Rate 是 0.25

0:30:16.720,0:30:19.520
接下來要改變 data 的 weight

0:30:19.520,0:30:22.860
要把 u 值變一下

0:30:22.860,0:30:26.980
讓 f1 在新的 training dataset 上

0:30:26.980,0:30:29.080
它的 error 變成 0.5

0:30:29.080,0:30:30.900
怎麼改
其實有不同的改法

0:30:33.200,0:30:36.340
假設 u1 的 weight 是 1/√3

0:30:36.340,0:30:41.840
現在要讓 f1 的 error 變大

0:30:41.840,0:30:46.480
怎麼讓 f1 的 error 變大
就是看它答對哪幾題

0:30:46.480,0:30:48.460
那幾題的配分就變小

0:30:48.500,0:30:51.840
答錯哪幾題，那幾題的配分就變大

0:30:51.840,0:30:56.780
考試的時候先把考卷寫完

0:30:56.780,0:31:00.220
老師也改完以後再重新去計算配分

0:31:00.220,0:31:03.940
答錯的配分就比較高
答對的配分就比較低

0:31:03.940,0:31:05.960
你就會發狂生氣

0:31:06.280,0:31:09.280
今天要做的事情就是要讓 f1 生氣

0:31:11.040,0:31:13.560
我們先看它答對那些答錯那些

0:31:13.560,0:31:18.880
它答對的
本來跟他說好每一題的配分都是一樣的

0:31:18.880,0:31:21.600
但是你騙它，它答完以後

0:31:21.600,0:31:23.640
再改一下題目的配分

0:31:23.640,0:31:27.560
第一題它答對了
所以配分就變成 1/√3

0:31:27.620,0:31:31.220
第二題它答錯了
所以配分就增加變成 √3

0:31:31.220,0:31:35.340
第三題跟第四題都答對了
所以配分都減少變成 1/√3

0:31:35.340,0:31:39.760
如果在這筆新的 training data 情況下就會變成

0:31:40.160,0:31:43.540
f1 就會變得很糟
因為你想想看

0:31:43.620,0:31:48.440
他答錯的題目 weight 是 √3

0:31:48.440,0:31:51.980
它答對的題目 weight 是 1/√3 有三題

0:31:51.980,0:31:55.900
1/√3 * 3 也是 √3

0:31:55.900,0:31:58.880
所以答錯的題目跟答對的題目的 weight 是一樣

0:31:59.000,0:32:06.220
所以 f1 的 Error Rate 就變成 0.5

0:32:06.220,0:32:10.020
接下來在這組新的 training data 上面

0:32:10.020,0:32:12.280
這組新的 training data 可以讓 f1 整個爛掉

0:32:12.280,0:32:15.940
在這組新的 training data 上面再去訓練 f2

0:32:15.980,0:32:24.660
那 f2 因為它是看著這組新的 weight、新的配分去做練習的

0:32:24.660,0:32:30.900
所以新的 Error Rate 在這組 weight 上它的 error 會是小於 0.5

0:32:30.900,0:32:33.280
所以 f2 可以跟 f1 是互補

0:32:33.280,0:32:42.580
更詳細的證明之後會有
今天都是講個精神

0:32:45.460,0:32:56.400
接下來講一下實際怎麼做 reweight 這件事情

0:33:00.320,0:33:04.440
如果某一筆 data x n

0:33:04.440,0:33:06.760
他會被 f1 分類錯

0:33:06.760,0:33:14.080
就把第 n 筆 data 的 weight u1 乘上一個值 d1 變成 u2

0:33:14.080,0:33:16.640
這個 d1 是大於 0 的值

0:33:16.640,0:33:22.800
也就是 x n 如果分類錯誤的話就那一個題目、那筆 data 的權重提高

0:33:22.800,0:33:24.600
乘上 d1 把它提高

0:33:25.080,0:33:32.780
如果 x n 是正確的被 f1 分類的話

0:33:32.840,0:33:39.360
就把 u1 除掉 d1，把它變小

0:33:41.020,0:33:47.780
所以對錯的就增加，對的就變小

0:33:48.780,0:33:52.680
f2 會在新的 weight u2 上面

0:33:52.680,0:33:54.700
進行訓練

0:33:54.700,0:33:59.320
再來的問題就是 d1 的值要設多少

0:33:59.320,0:34:01.500
這邊沒有甚麼高深的數學

0:34:01.500,0:34:02.900
就是推一下

0:34:02.900,0:34:05.040
要設甚麼樣的 d1

0:34:05.040,0:34:10.040
可以讓 u1 變成 u2 以後可以讓 f1 的 Error Rate 是 0.5

0:34:10.040,0:34:16.520
這邊就只是數學式比較繁瑣但其實很簡單的數學

0:34:16.520,0:34:19.280
這個數學是這樣

0:34:19.280,0:34:23.740
已經計算出 epsilon 1
epsilon 1 的式子是這個樣子

0:34:23.740,0:34:26.800
現在希望把 u1 換成 u2

0:34:26.800,0:34:30.800
得到的 weight 是 0.5

0:34:34.660,0:34:44.240
原則就是如果第二筆 data 的分類是錯誤的那就乘上 d1

0:34:44.240,0:34:48.420
如果是正確的就除掉 d1

0:34:48.420,0:34:52.980
先看一下上面這邊

0:34:52.980,0:34:59.160
上面這邊是指 summation over 分類錯誤的那些 data

0:34:59.160,0:35:04.500
上面這邊是指先 summation over 分類錯誤的那些 data

0:35:04.500,0:35:09.640
所以上面的這些 u2 都是分類錯誤的，所以都會乘上 d1

0:35:09.640,0:35:12.680
所以上面分子的地方

0:35:12.680,0:35:19.440
可以寫成 summation over u1 乘上 d1

0:35:19.440,0:35:26.300
上面這些 u 每一筆都是 u1 乘上 d1

0:35:26.300,0:35:29.040
因為他們都是分類錯的

0:35:29.040,0:35:31.800
再來看分子的地方

0:35:31.805,0:35:34.815
分子的地方是 summation over u2

0:35:34.820,0:35:36.300
u2 有兩個 case

0:35:36.300,0:35:41.560
一個是如果 f1 會把這筆 data 分類錯誤的話

0:35:41.560,0:35:44.800
那 u2 是來自於 u1 * d1

0:35:44.800,0:35:46.580
如果是分類正確的話

0:35:46.620,0:35:51.500
那 u2 就是來自 u1/d1

0:35:51.500,0:35:57.060
所以這整個式子列出來的話就是這個樣子

0:35:57.160,0:36:00.720
然後把分子的地方

0:36:00.720,0:36:02.040
帶進去

0:36:02.040,0:36:06.940
分母的地方，這一項帶進去得到這個式子

0:36:06.940,0:36:12.080
這個式子是等於 0.5

0:36:20.280,0:36:24.280
然後把分子和分母倒過來

0:36:24.280,0:36:27.340
所以左邊分子和分母倒過來

0:36:27.340,0:36:31.880
右邊就從 0.5 變成 2

0:36:32.240,0:36:40.500
接下來發現分子和分母都有共同的這一項

0:36:40.500,0:36:45.420
所以知道這一項除以這一項

0:36:45.420,0:36:48.560
這一項除以這一項

0:36:48.640,0:36:50.880
會等於 1

0:36:50.880,0:36:59.180
這告訴我們 u1 除以 d1

0:36:59.180,0:37:04.280
把所有那些 f1 會答對的 data x n 拿出來

0:37:04.280,0:37:07.500
把他們的 u1 除以 d1

0:37:07.500,0:37:14.380
要等於所有 f1 會答錯的那些 x n 他們的 u1 乘以 d1

0:37:14.380,0:37:21.500
也不用剛才推導其實也可以很直覺地寫出這個式子

0:37:21.500,0:37:26.740
如果要讓 f1 在新的 weight 的 Error Rate 是 0.5 的話

0:37:26.740,0:37:32.620
當然他答對的部分的新的 weight

0:37:32.640,0:37:36.280
要等於答錯的部分的新的 weight

0:37:36.700,0:37:50.380
接下來就把 d1 提出去

0:37:59.100,0:38:05.120
epsilon 1 可以寫成這個樣子

0:38:05.120,0:38:12.280
epsilon 1 分子的地方是對那些答錯的 example x n 的 weight 的總和

0:38:12.520,0:38:15.480
然後再做 normalization，那這一項

0:38:15.600,0:38:18.040
出現在這個地方

0:38:18.040,0:38:24.660
所以可以把這一項用 epsilon 1 把它代換掉

0:38:24.660,0:38:30.020
所以這一項 = z1 * epsilon 1

0:38:30.020,0:38:33.080
那這一項呢

0:38:33.080,0:38:39.980
這一項是 Z1 *( 1 - epsilon 1 )

0:38:39.980,0:38:48.460
因為這一項加這一項會是 Z1

0:38:48.460,0:38:50.420
既然它是 Z1 * epsilon 1

0:38:50.420,0:38:52.660
它就是 Z1 *( 1 - epsilon 1)

0:38:52.660,0:38:56.980
總之經過一翻推導以後

0:38:56.980,0:39:06.800
你會算出來 d1 = 更號 1 除以 epsilon 1 除以 epsilon 1

0:39:06.800,0:39:11.620
拿這個 d1 去乘或者是除 u1

0:39:11.620,0:39:18.200
就可以製造一個 training dataset 它是會讓 f1 fail 掉的 training dataset

0:39:18.260,0:39:23.500
這個 d1 的值一定會大於 1

0:39:23.540,0:39:26.275
因為 epsilon 1 一定小於 0.5

0:39:26.280,0:39:29.300
所以在 d1 的更號項裡面

0:39:29.300,0:39:37.840
分子會大於分母所以 d1 都會大於 1

0:39:42.580,0:39:49.140
整個 Adaboost 的演算法可以講完這一頁就好

0:39:49.140,0:39:53.340
整個 Adaboost 的演算法看起來就是這個樣子

0:39:53.340,0:40:04.240
現在有一堆 training data，每一筆 training data 給它的初始的 weight 都是 1

0:40:04.380,0:40:08.580
接下來要跑大 T 個 iteration

0:40:08.580,0:40:15.320
每一個 iteration 都會給一個 weight 的 classifier ft

0:40:15.320,0:40:21.260
最後再把所有的 ft 集合起來就變成一個強的 classifier

0:40:21.560,0:40:27.320
在每一個 iteration 每一筆 training data 都有自己的 weight

0:40:27.320,0:40:32.460
這邊寫成 u 上標 1 下標 t 
到 u 上標 N 下標 t

0:40:32.460,0:40:40.900
用下標 t 代表那一個 iteration 的 weight

0:40:44.700,0:40:49.320
用這個 weight 訓練出 ft

0:40:49.320,0:40:56.960
然後計算 ft 在原來 weight 上面的 error epsilon t

0:40:56.960,0:40:59.000
計算 epsilon t 以後

0:40:59.000,0:41:05.000
就可以 reweight 每一筆 training data

0:41:05.000,0:41:11.360
如果 x n 它被 ft 分類錯誤的話

0:41:11.360,0:41:18.080
就把 u 上標 n 下標 t 乘上 d 下標 t

0:41:18.280,0:41:22.360
就把 u 上標 n 下標 t 乘上一個大於 1 的值

0:41:22.360,0:41:27.920
然後得到一組新的 weight
這組新的 weight 會在下一個 iteration 的時候被使用

0:41:27.920,0:41:32.000
反之就把原來的 weight 除掉 dt

0:41:32.000,0:41:37.020
然後得到一組新的 weight
這組新的 weight 要在下一個 iteration 的時候被使用

0:41:38.780,0:41:46.460
這個 dt 就等於 √ ( 1 - epsilon t ) / epsilon t

0:41:46.500,0:41:54.640
或者是可以寫成另外有一個變數叫 alpha t

0:41:54.640,0:41:58.820
這個 alpha t = ln√ ( 1 - epsilon t ) / epsilon t

0:41:58.820,0:42:02.140
這麼做是有涵義的
這麼做的話

0:42:02.140,0:42:06.240
可以把 dt 換成 exp ( alpha t )

0:42:06.260,0:42:10.120
把除 dt 換成 乘以 exp ( - alpha t )

0:42:10.120,0:42:11.560
本來是有乘有除

0:42:11.560,0:42:15.000
現在變成一個是  * exp ( alpha t )

0:42:15.000,0:42:17.920
一個是 * exp ( - alpha t )

0:42:17.920,0:42:22.220
之所以這麼做是為了要表達式子的時候

0:42:22.220,0:42:24.860
可以更簡便一點

0:42:24.860,0:42:26.580
怎麼樣更簡便?

0:42:26.580,0:42:30.400
可以把這兩個式子合成一個式子

0:42:32.740,0:42:37.200
這兩個式子差的只有一個負號而已

0:42:37.200,0:42:40.940
都是要把原來的 weight 乘上 exp ( alpha t )

0:42:40.940,0:42:45.660
只是這個 alpha t 前面有時候是 +1 有時候是 -1

0:42:45.660,0:42:49.700
怎麼用一條式子決定 alpha t 前面應該是 +1 還是 -1

0:42:49.820,0:42:57.480
只需要把 y n hat * ft ( x n )

0:42:57.600,0:43:01.880
如果是 misclassified 的情況下

0:43:02.320,0:43:06.920
y hat 跟 ft( x ) 它是不一樣的

0:43:06.920,0:43:09.440
這兩個值是不一樣的所以它是 -1

0:43:09.440,0:43:15.800
-1 * -1，alpha t 前面就變成 1

0:43:15.960,0:43:20.160
如果是分類正確的情況下，這兩項是一樣的

0:43:20.160,0:43:22.440
這兩項相乘就是 +1

0:43:22.440,0:43:26.960
所以再乘上 -1 這一項就變成 -1

0:43:26.960,0:43:34.660
總之可以直接用這一個式子來表示這兩個式子

0:43:35.380,0:43:45.180
經過剛才的訓練以後就得到一把 classifier f1 到 fT

0:43:45.180,0:43:50.260
再來就是怎麼把這把 classifier 集合在一起

0:43:50.380,0:43:56.680
你可以用 uniform 的 weight
現在有大 T 個 classifier

0:43:56.780,0:43:59.680
叫這大 T 個 classifier 都得到一個 output

0:43:59.680,0:44:04.880
把大 T 個 classifier 的 output 就加起來看是正的還是負的

0:44:04.880,0:44:09.340
如果是正的話就代表是 class 1
如果是負的話就代表是 class 2

0:44:09.400,0:44:14.380
就把這大 T 個 classifier 的值通通加起來然後取正負號

0:44:14.660,0:44:17.820
這樣雖然可以但這樣不是最好的方法

0:44:17.820,0:44:21.740
因為這大 T 個 classifier 有好有壞

0:44:21.740,0:44:26.180
所以應該要給它不同的權重

0:44:26.180,0:44:36.360
怎麼給不同的權重
在每一個 classifier output 前面都乘上一個權重 alpha t

0:44:36.360,0:44:40.640
然後在全部加起來以後

0:44:40.640,0:44:45.320
再取它的正負號這樣可以得到比較好的結果

0:44:45.440,0:44:50.960
alpha t 怎麼得到

0:44:50.960,0:44:54.060
這個 alpha t 在前一頁的式子有看過

0:44:54.060,0:45:01.120
這個 alpha t 就是拿來改變每一筆 training data 的 weight 的 alpha t

0:45:01.160,0:45:03.440
那個 alpha t 在前面看過

0:45:03.440,0:45:05.840
現在看一下 alpha t 的精神

0:45:05.840,0:45:09.920
如果某一個 classifier 的 epsilon t 是 0.1

0:45:09.920,0:45:12.800
是一個錯誤率比較低的 classifier

0:45:12.800,0:45:17.700
把 epsilon t = 0.1 帶到這個式子去算出 alpha t

0:45:17.740,0:45:20.280
它的 alpha t 就是 1.1

0:45:20.280,0:45:24.420
錯誤率低的 classifier 會有比較大的 alpha t

0:45:24.420,0:45:31.060
如果有另外一個 classifier 它的 epsilon t 是 0.4 代表它是個很爛的 classifier，錯誤率已經接近 0.5 了

0:45:31.060,0:45:36.240
把 epsilon t = 0.4 帶到這個式子裡面去算 alpha t 得到 alpha t 是 0.2

0:45:36.240,0:45:39.160
如果有一個比較正確的 classifier

0:45:39.160,0:45:42.540
錯誤率比較低的 classifier 它得到的 alpha t 的值是大的

0:45:42.540,0:45:45.860
如果爛的 classifier 它得到的 alpha t 的值是小的

0:45:45.860,0:45:48.060
也就是在做 weighted sum 的時候

0:45:48.060,0:45:49.840
如果有一個 classifier 的正確率

0:45:49.900,0:45:53.020
它當初訓練的時候錯誤率是比較大的

0:45:53.020,0:45:54.720
它的 weight 就比較小

0:45:54.720,0:45:59.680
它當初訓練的時候錯誤率比較小它的 weight 就比較大

0:45:59.780,0:46:06.780
這件事情是非常有道理的

0:46:06.820,0:46:10.120
這個 alpha t 是 make sense 的

0:46:10.240,0:46:13.820
我們很快把後面這個例子講過好了

0:46:13.820,0:46:20.000
如果這邊你覺得太快就回去自己看投影片
我相信這個對大家非常容易

0:46:20.000,0:46:24.340
我講的這一段就請助教來講一下作業六

0:46:24.420,0:46:27.775
這個很簡單

0:46:27.780,0:46:32.260
剛剛的演算法如果沒有聽懂就看這個例子
就知道它的意思了

0:46:32.260,0:46:34.580
假設大 T = 3
0:46:34.580,0:46:39.980
現在 weak 的 classifier 很 weak
它不是 Decision Tree 也不是 Neural Network

0:46:39.980,0:46:41.800
它叫做 decision stump

0:46:41.820,0:46:45.920
decision stump 沒什麼好講的它太簡單了

0:46:45.920,0:46:47.600
它做的事情就是

0:46:47.600,0:46:50.640
假設 feature 都分佈在二維平面上

0:46:50.680,0:46:54.360
在二維平面上選一個 dimension 切一刀

0:46:54.420,0:46:57.840
其中一邊當作 class 1 另外一邊當作 class 2

0:46:57.840,0:47:00.600
結束，這個就叫做 decision stump

0:47:00.780,0:47:03.700
要做 Boosting 一定要找個 weak 的 classifier

0:47:03.700,0:47:07.320
decision stump 它夠 weak 所以把它用在這裡

0:47:07.580,0:47:13.720
一開始每一筆 training data 的 weight 都是一模一樣的都是 1.0

0:47:16.160,0:47:19.420
用 decision stump 找一個 function

0:47:19.420,0:47:21.920
這個 function 是 f1

0:47:21.920,0:47:24.600
它的 bounder 就切在這個地方

0:47:24.600,0:47:27.300
以左就說是 positive 的 example

0:47:27.300,0:47:29.440
就算是 positive 的

0:47:29.440,0:47:31.820
一邊 class 1 是 positive 的

0:47:31.820,0:47:35.560
往右就是粉紅色就是 negative 的

0:47:35.560,0:47:40.120
發現這邊有三筆 data 它的分類是錯的

0:47:40.180,0:47:45.800
計算一下有三筆 data 總共有十筆 data

0:47:45.800,0:47:47.400
有三筆 data 分類錯

0:47:47.400,0:47:49.400
所以 Error Rate 是 0.3

0:47:49.400,0:47:54.500
Error Rate 是 0.3 的話 d1 算出來就是 1.53 alpha 算出來就是 0.42

0:47:54.500,0:48:00.840
就帶前一頁投影片的公式就可以輕易地求出來了

0:48:01.420,0:48:05.640
現在已經算出 epsilon 1, d1, alpha 1 以後

0:48:05.640,0:48:09.440
接下來就是改變每一筆 training data 的 weight

0:48:09.440,0:48:13.380
分類正確的 weight 就要變小

0:48:13.680,0:48:16.040
分類錯誤的 weight 就要變大

0:48:16.040,0:48:18.560
分類錯誤的要乘 1.53

0:48:18.560,0:48:20.660
分類對的就要除 1.53

0:48:20.700,0:48:25.420
這三筆分類錯的 weight 就變大

0:48:25.440,0:48:28.000
分類對的 weight 就變小

0:48:28.280,0:48:31.540
有了一組新的 weight 以後

0:48:31.540,0:48:37.460
就可以再去找一次另外一個 decision stump

0:48:37.500,0:48:41.120
有一組新的 weight 找出來的 decision stump 就不一樣了

0:48:41.120,0:48:44.840
在新的 decision stump 切一刀切在這個地方

0:48:44.840,0:48:49.080
往左是 positive 往右是 negative

0:48:49.080,0:48:52.115
往左是藍色往右是紅色

0:48:52.120,0:48:56.020
會發現有三筆 data 分類是錯的

0:48:56.020,0:48:59.200
現在 f2 的 Error Rate 是多少

0:48:59.200,0:49:02.360
會根據每一筆 data 的 weight 進行計算

0:49:02.360,0:49:07.060
就會發現第二個 classifier 的 Error Rate 是 0.21

0:49:07.060,0:49:11.140
它的 d2 = 1.94, alpha 2 = 0.66

0:49:11.260,0:49:14.880
接下來這三筆 data 分類錯所以給他 weight 比較大

0:49:14.880,0:49:17.800
這三筆 data 要把它乘上 1.94

0:49:17.860,0:49:21.480
剩下的 data 把他除掉 1.94

0:49:22.000,0:49:26.480
就找到了第二個 classifier

0:49:26.820,0:49:29.840
每一個 classifier 的 weight 就是它 alpha 的值

0:49:29.840,0:49:33.160
把 alpha 的值寫再 classifier 的旁邊

0:49:33.160,0:49:35.800
接下來找第三個 classifier

0:49:36.800,0:49:41.820
第三個 classifier 上面是藍色下面是紅色

0:49:42.140,0:49:47.240
它這麼講會導致有三筆 data 錯誤

0:49:47.240,0:49:49.700
計算一下它的 Error Rate = 0.13

0:49:49.700,0:49:53.660
可以計算它的 d3 可以計算它的 alpha 3

0:49:53.860,0:49:59.360
如果有更多 iteration 的話會去重新 weight data

0:49:59.440,0:50:02.980
但現在只跑三個 iteration 跑完就結束了

0:50:02.980,0:50:06.540
得到三個 classifier 還有他們的 weight 就結束了

0:50:06.540,0:50:10.720
最後怎麼把這三個 classifier 組合起來

0:50:10.720,0:50:15.180
把每個 classifier 都乘上對應的 weight

0:50:15.180,0:50:19.020
通通加起來再取它的正負號

0:50:19.280,0:50:25.480
這個加起來的結果到底是怎麼回事

0:50:25.480,0:50:29.600
有三個 decision stump 這三個 decision stump

0:50:29.680,0:50:34.300
把整個二維的平面切成六塊

0:50:34.300,0:50:39.560
左上角三個 classifier 都覺得是藍的
所以就藍色

0:50:39.560,0:50:46.060
中間這一塊他們兩個覺得是藍的，第一個覺得是紅的

0:50:46.060,0:50:49.480
但是他們兩個合起來的 weight 比較大

0:50:49.480,0:50:52.220
所以上面這組就是藍的

0:50:52.220,0:50:56.840
右上角第一個覺得是紅的第二個覺得是紅的第三個覺得是藍的

0:50:56.840,0:51:03.340
這兩個紅的 weight 合起來比藍的 weight 大
所以又是紅的

0:51:03.420,0:51:08.560
左下角是第一個藍的第二個藍的第三個紅的

0:51:08.620,0:51:11.740
兩個藍的合起來比紅的大所以是藍的

0:51:11.740,0:51:15.100
下面這個紅的藍的紅的

0:51:15.100,0:51:17.600
兩個紅的加起來比藍的大所以是紅的

0:51:17.620,0:51:24.520
右下角三個 decision stump 都說是紅的所以是紅的

0:51:24.520,0:51:32.340
這三個 decision stump 沒有一個是 0% 的 Error Rate

0:51:32.340,0:51:35.900
他們都有犯一些錯

0:51:35.900,0:51:39.700
但把這三個 decision stump 組合起來的時候

0:51:39.700,0:51:43.780
它告訴我們這三個區塊屬於藍色、這三個區塊屬於紅色

0:51:43.840,0:51:46.640
而它的正確率是 100%

0:51:46.640,0:51:51.500
三個 weak 的 classifier 把它組合起來可以得到好的結果

0:51:51.640,0:51:57.000
接下來就請助教來講一下作業六

0:52:05.760,0:52:09.400
我們來繼續講 Adaboost

0:52:09.460,0:52:16.020
上次講的是 Adaboost 的 algorithm

0:52:29.980,0:52:33.800
現在要講的是理論上的證明

0:52:33.800,0:52:40.200
這邊要證明假設我們按照 Adaboost 的 algorithm

0:52:40.200,0:52:44.260
來產生最後的 classifier

0:52:44.260,0:52:48.700
這最後的 classifier 這邊寫成 H(x)

0:52:48.740,0:52:57.440
這個最後的 classifier H(x) 是由一堆 weak 的 classifier ft 所組成的

0:52:57.500,0:53:01.700
如果 Adaboost 的 algorithm 跑大 T 個 iteration 的話

0:53:01.700,0:53:07.400
就會得到大 T 個 weak 的 classifier f1 到 fT

0:53:07.400,0:53:11.440
每一個 weak 的 classifier 還有 weight 權重

0:53:11.440,0:53:18.000
這樣就可以知道哪一些 weak classifier 應該被參考多一點
哪一些應該被參考少一點

0:53:18.000,0:53:21.960
這個權重就是 alpha t

0:53:21.980,0:53:24.740
把所有 weak classifier 的 output

0:53:24.740,0:53:27.080
假設切到 classifier 某一個 object x

0:53:27.100,0:53:31.140
就把 x 分別丟到每一個 weak 的 classifier ft 裡面

0:53:31.140,0:53:34.900
再把 x 的 output 乘上它的 weight alpha t

0:53:34.900,0:53:39.040
再 summation over 所有 weak 的 classifier 再取它的正負號

0:53:39.040,0:53:41.880
就可以得到最終的分類的結果

0:53:41.940,0:53:44.900
這個 alpha t 是甚麼

0:53:44.900,0:53:49.700
這個 alpha t 跟 epsilon t 有關

0:53:49.700,0:53:52.020
epsilon t 組成的 alpha t

0:53:52.020,0:53:56.120
epsilon t 是甚麼
epsilon t 是 Error Rate

0:55:35.100,0:55:43.220
我想先講另外一件事情就是我們接下來的規劃

0:55:43.720,0:55:53.340
這周我們講 Boosting 跟 Structure Learning 的開頭

0:55:53.340,0:55:57.560
下周有一位 NVIDIA 的外賓要來

0:55:57.560,0:56:00.460
它要來告訴我們一些它做的研究

0:56:00.460,0:56:05.420
相比於我自己授課我比較喜歡請外賓來講

0:56:05.420,0:56:08.040
為甚麼? 因為我上課的內容都是有錄影的

0:56:08.040,0:56:13.720
所以請外賓來講比較能夠聽到不一樣的東西

0:56:13.720,0:56:17.540
如果我再講一次同樣的內容其實笑話都是差不多的

0:56:19.460,0:56:24.120
所以如果有外賓的話就盡量請外賓來講

0:56:24.120,0:56:25.900
沒有講的內容怎麼辦

0:56:25.900,0:56:30.080
還沒講 SVM 阿，那個上課都有錄影

0:56:30.080,0:56:34.920
都有上學期的錄影再把它放到課程網站就好

0:56:34.920,0:56:37.880
所以不用太擔心

0:56:37.880,0:56:43.480
在下下周講 Reinforcement Learning

0:56:43.480,0:56:47.420
上學期其實沒有講完 Reinforcement Learning
這學期會把它講完

0:56:47.500,0:56:53.280
接下來就是 final，祝大家 final 做的順利

0:56:53.340,0:56:59.980
有人可能會想說沒講到機器學習理論的部分

0:57:05.240,0:57:12.520
剛才講到 alpha t 是跟 epsilon t 有關的

0:57:14.600,0:57:17.460
epsilon t 是 Error rate

0:57:17.460,0:57:20.620
是 classifier ft 的 Error Rate

0:57:22.460,0:57:28.520
現在要證明如果 weak 的 classifier 越多

0:57:28.520,0:57:34.360
或者換句話說 Adaboost 的 algorithm 跑越多的 iteration

0:57:34.360,0:57:37.240
在 training set 上的 error

0:57:37.240,0:57:40.080
會越來越小

0:57:40.240,0:57:44.280
所以這樣子就可以增加 weak 的 classifier

0:57:44.280,0:57:48.720
然後讓 model 在 training set 上的 performance 越來越好

0:57:48.880,0:57:53.300
怎麼證
其實是滿簡單的

0:57:53.300,0:57:58.480
先算一下 H(x) 的 Error Rate

0:57:58.540,0:58:01.880
先把 H(x) 的 Error Rate 的式子列出來

0:58:04.760,0:58:12.600
其實就是 summation over n
這個 x n 代表 training data

0:58:12.600,0:58:16.700
如果 H(x n) 不等於 y n hat

0:58:16.700,0:58:19.660
H(x n) 的 output 跟正確解答不一樣的話

0:58:19.660,0:58:23.480
那就有一筆 error，得到的 error 就是 1

0:58:23.540,0:58:26.680
反之如果 H 等於 y n hat 的話

0:58:26.680,0:58:29.080
那得到的 error 就是 0

0:58:29.080,0:58:30.880
然後再做一下平均

0:58:30.880,0:58:33.720
假設有大 N 筆 training data

0:58:36.680,0:58:41.160
這邊是先把大 T 個 weak classifier weighted sum 起來

0:58:41.160,0:58:43.680
再取它的正負號

0:58:43.680,0:58:48.720
括號裡面這一項用 g(x) 來表示

0:58:48.720,0:58:54.900
g(x) 代表大 T 個 classifier 的 weighted sum

0:58:54.980,0:58:57.465
所以 Error Rate 這一項

0:58:57.465,0:59:00.380
也可以寫成是 y hat

0:59:00.380,0:59:04.140
乘上 g(x) 看它是小於 0 還是大於 0

0:59:04.140,0:59:15.820
小於 0 代表 y hat 跟 g 異號所以是錯誤的

0:59:15.840,0:59:17.840
得到的 error 就是 1

0:59:17.840,0:59:23.700
如果他們是同號代表是正確的得到的 error 就是 0

0:59:23.700,0:59:26.140
這都沒有甚麼特別難的

0:59:26.220,0:59:32.780
最後一項我們說這個 Error Rate 有一個 upper bound

0:59:32.780,0:59:35.320
這個 upper bound 寫作這樣

0:59:35.320,0:59:42.220
這個 upper bound 是 exp ( - y hat * g(x))

0:59:42.340,0:59:47.580
如果把 y hat * g(x) 的這個值

0:59:47.580,0:59:50.720
把它畫出來就一目瞭然了

0:59:52.100,1:00:05.220
這個圖的橫軸是 y hat * g(x)

1:00:05.320,1:00:14.860
綠色的線代表的是 delta ( y hat * g(x) < 0) 這個 function 的值

1:00:14.860,1:00:17.900
所以 y hat * g(x) < 0 的話

1:00:17.900,1:00:22.000
這個 output 是 1 反之 delta output 是 0

1:00:22.060,1:00:27.520
綠色的 function 有一個 upper bound 就是藍色的 function

1:00:27.520,1:00:31.240
藍色的 function 是 exp ( - y hat * g(x))

1:00:31.360,1:00:35.660
exp ( - y hat * g(x)) 畫起來就是這個樣子

1:00:35.740,1:00:39.600
藍色的 function 是綠色的 function 的 upper bound

1:00:39.600,1:00:42.820
應該是沒有甚麼特別的問題

1:00:44.960,1:00:51.260
再來是要證明這個 upper bound 會越來越小

1:00:51.260,1:00:57.620
怎麼證 upper bound 會越來越小
在直接證它之前來算另外一個式子

1:00:57.620,1:01:02.080
我們來算 Zt，甚麼是 Zt

1:01:02.080,1:01:09.060
在每一個 iteration 都會給 training data 一個 weight

1:01:09.060,1:01:11.500
每一筆 training data 都有一個 weight

1:01:11.500,1:01:15.140
用這些 weight 來算 ft

1:01:15.320,1:01:20.060
所謂的 Zt 就是把所有 training data 的 weight 加總起來

1:01:20.060,1:01:22.380
就是 Zt

1:01:22.380,1:01:26.700
等一下會說明 Zt 跟上面 upper bound 的關係

1:01:26.700,1:01:30.840
先不要管 upper bound，先算 Zt

1:01:31.080,1:01:36.840
要先來算的是 Z T+1

1:01:36.840,1:01:41.420
也就是當 T 個 iteration 跑完以後

1:01:41.420,1:01:50.620
假設接下來要算、要學 F T+1

1:01:50.620,1:01:52.900
第 T + 1 個 weak classifier

1:01:52.900,1:01:59.400
那 train 第 T + 1 個 weak classifier 的時候

1:01:59.480,1:02:03.040
那些 training data 的 weight 把它總合起來

1:02:03.040,1:02:06.360
應該是多少

1:02:06.360,1:02:16.680
Z T + 1 = summation over 所有的 training data 它的每一筆 training data 的 weight 總和

1:02:16.720,1:02:20.020
每一筆 training data 的 weight 又是多少

1:02:20.020,1:02:24.940
假設初始的時候、train 第一個  weak classifier 的時候

1:02:24.940,1:02:29.200
這個時候每一筆 training data 裡面的 weight 都是一樣
都是 1

1:02:29.200,1:02:32.540
這是非常合理的假設

1:02:32.540,1:02:37.180
接下來在第 T + 1 個 iteration

1:02:37.360,1:02:40.160
要 train 第 T + 1 個 classifier 的時候

1:02:40.260,1:02:42.100
會把原來的 weight

1:02:42.100,1:02:50.700
在第 t 個 iteration 的 weight u t 乘上
exp ( - y n hat * (ft) * (alpha t))

1:02:51.740,1:02:56.580
這件事情其實之前有講過了

1:02:56.880,1:03:02.040
如果第 n 筆 classifier

1:03:02.360,1:03:04.160
它被 classified 是正確的

1:03:04.160,1:03:06.400
它的 weight 就會被下降

1:03:06.400,1:03:10.600
如果它 classified 是錯誤的它的 weight 就會被上升

1:03:10.600,1:03:16.160
怎麼增加和減少它的 weight 靠的是乘後面這一項

1:03:16.240,1:03:20.220
前面在講 Adaboost 的 algorithm 的時候

1:03:20.220,1:03:24.280
有解釋過為甚麼這個式子長這個樣子

1:03:24.280,1:03:27.280
這 alpha t 跟 epsilon t 有關係

1:03:27.280,1:03:29.600
把它寫在右上角

1:03:29.620,1:03:35.100
總之第 t + 1 個時間點的 weight

1:03:35.120,1:03:40.140
跟第 t 個時間點的 weight 之間有甚麼樣的關係

1:03:40.360,1:03:48.740
如果要算第 T + 1 的 iteration 的時候的 weight

1:03:48.740,1:03:52.460
要 train 第 T + 1 個 weak classifier 的 weight

1:03:52.460,1:03:54.740
會不會算呢

1:03:54.740,1:03:57.640
因為第一項是 1

1:03:57.640,1:04:01.660
一開始是 1，接下來就一直乘 exponential 這一項

1:04:01.660,1:04:04.400
所以其實我們只是把

1:04:04.400,1:04:11.660
這些 exponential 這些項乘上 T 次而已

1:04:11.740,1:04:14.940
這個大家應該沒有問題吧

1:04:15.200,1:04:18.200
知道 t 跟 t + 1 的關係就是乘這一項

1:04:18.300,1:04:22.300
從 u1 到 u (T+1) 中間

1:04:22.300,1:04:28.560
就是乘了這個 exponential 項乘了 T 次

1:04:29.740,1:04:34.120
如果要算 Z 的話

1:04:34.120,1:04:38.380
Z 就是把每一筆 training data 的 u 通通 summation 起來

1:04:38.400,1:04:44.280
所以只是在這個式子前面、這個式子前面加了 summation 而已

1:04:49.940,1:04:57.680
接下來可以把連乘這一項放到 exponential 裡面

1:04:57.800,1:05:03.400
有一大堆 exponential 相乘等於指數項相加

1:05:03.400,1:05:08.680
所以可以把連乘這一項放到 exponential 裡面

1:05:08.820,1:05:15.440
那 y hat 是指第 n 筆 training data 正確答案

1:05:15.520,1:05:18.880
它跟 iteration 是完全沒有關係

1:05:18.880,1:05:21.300
label 它跟 iteration 完全沒有關係

1:05:21.300,1:05:24.980
所以 y hat 這一項可以被提出來

1:05:25.140,1:05:31.660
總之 Z ( T + 1 ) 會寫成右下角這個式子

1:05:31.660,1:05:36.480
右下角這個式子是甚麼

1:05:37.860,1:05:44.240
這一項其實就是 g(x)

1:05:44.400,1:05:50.660
所以這一項其實就是這一項

1:05:50.940,1:05:55.760
所以這 Z ( T + 1 ) 它的 upper bound 是非常有關係的

1:05:55.760,1:06:02.300
其實 training 的時候，error 的 upper bound 就是
Z(T+1) / N

1:06:02.440,1:06:10.060
你會發現 training data 的 weight 的 summation 居然是跟 error 的 upper bound 是有關係的

1:06:10.340,1:06:23.840
接下來要證 weight 的 summation 會越來越小

1:06:23.880,1:06:28.760
所有的 training data 的 weight 的 summation 會越來越小

1:06:28.760,1:06:33.020
如果可以證明這件事的話

1:06:33.300,1:06:35.920
遊戲就結束了

1:06:36.140,1:06:37.800
Z1 是甚麼

1:06:37.800,1:06:41.560
Z1 在第一次 train 第一個 classifier 的時候

1:06:41.560,1:06:47.260
每一筆 training data 的 weight 都是 1
總共有 N 筆 training data 所以 weight 是 N

1:06:47.440,1:06:51.680
所以 Z1 = N

1:06:52.060,1:06:55.580
那 Zt 呢

1:06:55.960,1:06:59.860
Zt 跟 Zt - 1 中間

1:06:59.860,1:07:02.220
有以下的這個關係

1:07:02.220,1:07:08.660
要從 Z( t - 1 ) 變到 Zt 只要做以下這個運算就好

1:07:08.860,1:07:10.940
這個運算是甚麼意思

1:07:10.940,1:07:17.460
這個運算是先找出 Z ( t - 1 ) 裡面

1:07:17.900,1:07:19.880
misclassified 的部分

1:07:19.880,1:07:23.020
misclassified 的部分、分類錯誤的部分

1:07:23.020,1:07:26.120
會被乘上 exp( alpha )

1:07:26.480,1:07:32.600
分類正確的部分會被乘上 exp( - alpha )

1:07:34.060,1:07:38.360
分類錯誤的部分有多少

1:07:38.360,1:07:43.580
假設 Error Rate 叫做 epsilon t

1:07:43.580,1:07:48.880
那分類錯誤的部分就是 Z ( t - 1 ) * epsilon t

1:07:48.880,1:07:53.200
分類正確的部分就是 Z ( t - 1 ) * ( 1 - epsilon t )

1:07:53.200,1:07:56.940
分類錯誤的部分會被乘上 exp( alpha t )

1:07:56.940,1:08:00.800
分類正確的部分會被乘上 exp ( - alpha t )

1:08:00.800,1:08:05.340
把這兩項加起來就得到 Zt

1:08:05.420,1:08:10.660
現在知道 alpha t 是多少
alpha t 的式子就寫在這邊

1:08:10.660,1:08:13.780
把 alpha t 帶進去

1:08:13.800,1:08:31.860
就得到 Zt = Z( t - 1 ) * (εt) * √( 1 - εt) /εt
+ Z ( t - 1 )(1 - εt)* √ εt / ( 1 - εt)

1:08:34.660,1:08:36.940
合起來就是這個太容易了

1:08:37.060,1:08:41.440
就把分子跟分母消一下

1:08:41.440,1:08:47.740
得到 Z(t - 1) * 2 √ εt ( 1 - εt )

1:08:48.040,1:08:53.380
其實從這一項就可以看出

1:08:53.400,1:08:58.280
Zt 會比 Z(t - 1) 還要小

1:08:59.800,1:09:03.180
εt 是 Error Rate

1:09:03.180,1:09:07.220
Error Rate 一定小於 0.5

1:09:07.220,1:09:10.260
最大就是 0.5

1:09:10.300,1:09:15.120
所以 Z(t - 1) 後面乘的這一項是多少

1:09:15.120,1:09:19.380
如果 εt = 0.5 的時候它最大

1:09:19.400,1:09:27.740
所以 2√ εt ( 1 - εt ) 最大就是 1
它沒有辦法比 1 還要更大

1:09:27.740,1:09:35.240
Z( t - 1 ) 會乘上一個比 1 小的值變成 Zt
所以 Zt 會小於 Z( t - 1)

1:09:37.880,1:09:41.520
Z ( T + 1 ) 算出來的話是多少

1:09:41.520,1:09:45.720
Z ( T + 1 ) 就是 Z1 的 N 乘上 T 項

1:09:45.720,1:09:50.940
每一項都是 2 √  εt ( 1 - εt )

1:09:50.940,1:09:55.380
所以 training 的 error 會越來越小

1:09:55.380,1:09:59.580
因為 2 √ εt ( 1 - εt ) 是小於 1 的

1:09:59.580,1:10:02.220
所以 Zt 會越來越小

1:10:02.220,1:10:05.940
Zt 就是 upper bound 所以 upper bound 會越來越小

1:10:05.940,1:10:10.520
所以 Error Rate 可能也會是越來越小

1:10:13.135,1:10:15.925
這個證明就到這邊

1:10:15.925,1:10:20.100
接下來講 Adaboost 神秘的現象

1:10:20.100,1:10:23.800
這個神祕的現象是這個樣子

1:10:23.860,1:10:27.740
這邊橫軸是 training 的 iteration

1:10:27.740,1:10:31.320
找多少個 weak 的 classifier 來幫忙

1:10:31.320,1:10:34.940
縱軸是 Error Rate

1:10:37.640,1:10:44.240
比較低的這條線是在 training data 上的 Error Rate

1:10:44.280,1:10:49.180
比較高的這條線是在 testing data 上的 Error Rate

1:10:49.200,1:10:55.380
但神奇的是 training data 的 Error Rate

1:10:55.380,1:10:57.860
其實很快就變成 0

1:10:57.900,1:11:02.800
大概在 5 個 iteration 之後
找五個 weak 的 classifier combine 在一起以後

1:11:02.800,1:11:06.240
Error Rate 其實就已經是 0

1:11:06.720,1:11:13.220
雖然 Error Rate 是 0

1:11:13.220,1:11:16.300
五個 weak classifier 的 Error Rate 合起來是 0

1:11:16.300,1:11:20.660
這邊要強調一下是五個 weak classifier 的 Error Rate 合起來是 0

1:11:20.660,1:11:24.760
並不是單一一個 weak classifier 的 Error Rate 是 0

1:11:24.760,1:11:29.120
單一一個 weak classifier 都很弱
要五個合起來以後 Error Rate 才是 0

1:11:29.120,1:11:33.840
事實上在 Adaboost 演算法裡面如果你想一下

1:11:33.840,1:11:39.600
如果 weak classifier 的 Error Rate train 在 training data 上就已經是 0 了

1:11:39.640,1:11:44.100
其實這演算法是會有問題的

1:11:44.100,1:11:47.940
算一下那個 alpha 會發現是 undefine

1:11:48.020,1:11:58.700
Adaboost 假設你的 train weak classifier 的 algorithm 沒有辦法讓 Error Rate 變 0

1:11:58.700,1:12:03.400
如果變 0 的話這個演算法是會有問題的

1:12:10.020,1:12:14.440
雖然加了更多 weak classifier 以後

1:12:14.440,1:12:17.900
整體的 Error Rate 在 training data 上沒已下降

1:12:17.900,1:12:22.420
但是在 testing data 上仍然是有下降

1:12:22.420,1:12:26.120
這又是一件還頗神奇的事情

1:12:26.120,1:12:29.680
在 training data 上的 error 已經沒有再下降了

1:12:29.680,1:12:34.360
但是在 testing data 上的 error 仍然可以繼續下降

1:12:34.360,1:12:40.100
classifier 已經可以把 training data 的每一筆都 classify 正確

1:12:40.100,1:12:48.120
感覺已經沒有可以學的東西了
它可以把 training data 都 classify 正確

1:12:48.220,1:12:51.055
但是在加了更多的 weak classifier 以後

1:12:51.060,1:12:55.340
居然 testing data error 還可以再下降

1:12:55.520,1:12:59.460
為甚麼

1:12:59.700,1:13:01.920
我們來看一下這個式子

1:13:04.040,1:13:07.540
最後找到的 classifier 叫 H(x)

1:13:07.540,1:13:10.380
它是一大堆 weak classifier combine 後的結果

1:13:10.380,1:13:15.160
把 weak classifier combine 後的 output 叫作 g(x)

1:13:15.160,1:13:18.300
把 g(x) 乘上 y hat

1:13:18.300,1:13:20.745
定義為 margin

1:13:20.745,1:13:25.020
我們希望 g(x) 跟 y hat 是同號

1:13:25.020,1:13:26.780
如果是同號分類才正確

1:13:26.780,1:13:32.180
不只希望它同號，希望它相乘以後越大越好

1:13:33.480,1:13:36.300
不只是希望這個 g(x)

1:13:36.300,1:13:39.560
如果 x 是 positive

1:13:39.560,1:13:41.320
如果 y hat 是正的

1:13:41.320,1:13:45.240
不只希望 g(x) 就是稍微大於 0
0.000001

1:13:45.240,1:13:50.280
希望它比 0 大的越多越好

1:13:50.480,1:13:56.400
如果 y hat 是正的，g(x) 是 0.00001

1:13:56.400,1:13:59.920
那一點的 error 就會讓分類錯誤

1:13:59.920,1:14:03.980
只要一點 training data、testing data mismatch 就會讓分類錯誤

1:14:03.980,1:14:09.240
但如果 y hat 是正的，而 g(x) 是一個非常大的正值

1:14:09.280,1:14:11.640
那 error 的影響就會比較小

1:14:11.820,1:14:20.140
如果從現象上來看一下 Adaboost margin 變化的話

1:14:20.180,1:14:24.740
會發現如果只有五個 weak classifier 合在一起

1:14:24.740,1:14:29.780
margin 的分佈是這個樣子

1:14:29.780,1:14:33.700
如果有一百個甚至一千個 weak classifier 結合在一起的時候

1:14:33.700,1:14:37.320
它的分佈就是黑色的實線

1:14:37.320,1:14:42.180
會發現雖然 training data 上的 error 已經不會再下降

1:14:43.460,1:14:46.220
五個 weak classifier 的時候就已經不會再下降

1:14:46.220,1:14:51.200
因為所有的 training data 它的 g(x) * y hat 都是大於 0

1:14:51.200,1:14:54.340
會發現 margin 的分布都是在右邊

1:14:54.340,1:14:57.560
也就是 y hat 跟所有的 g(x) 同號

1:14:57.560,1:15:05.280
但在加上 weak classifier 以後可以增加 margin

1:15:05.340,1:15:09.420
增加 margin 的好處是讓你的方法比較 robust

1:15:09.480,1:15:13.160
可以在 testing set 上得到比較好的 performance

1:15:13.160,1:15:17.340
其實 SVM 也有類似的效果

1:15:17.340,1:15:20.620
Adaboost 也有這個效果

1:15:20.880,1:15:24.320
為甚麼可以讓 margin 增加

1:15:24.320,1:15:30.920
這邊就是要說明一下為甚麼 Adaboost 可以讓 margin 增加

1:15:30.920,1:15:35.160
剛才已經把 Error Rate 的式子列出來
這是 Error Rate 的式子

1:15:35.160,1:15:37.320
它是綠色這條線

1:15:37.520,1:15:54.180
這個 Error Rate 的式子有個 upper bound 是紅色這條線

1:15:54.180,1:15:58.480
這一項是紅色這條線

1:16:04.860,1:16:08.460
這個 upper bound 其實會越來越小

1:16:08.460,1:16:11.960
剛才證明對每一個 iteration 而言

1:16:11.960,1:16:14.100
這個 upper bound 會越來越小

1:16:14.100,1:16:19.040
雖然並沒有對 upper bound 做微分之類的事情

1:16:19.040,1:16:23.740
並沒有對 upper bound 做微分、做 Gradient Descent 等等的事情

1:16:23.740,1:16:26.200
但是會讓這個 upper bound 越來越小

1:16:26.200,1:16:36.980
所以可以把 upper bound 想成就是 Adaboost 的 Objective Function

1:16:36.980,1:16:43.660
所以 Adaboost 做的事情是 minimize 一個 Objective Function

1:16:43.660,1:16:47.620
而這個 Objective Function 是紅色的這條線

1:16:47.640,1:16:52.220
這邊還畫了別的方法的 Objective Function

1:16:52.220,1:16:55.560
黃色這條線是 SVM 的 Objective Function

1:16:55.560,1:17:00.640
綠色這條線是 Logistic Regression 的 Objective Function

1:17:00.720,1:17:04.920
Adaboost 的 Objective Function 是紅色這條線

1:17:04.920,1:17:07.880
紅色這條線有甚麼樣的特性

1:17:10.880,1:17:13.120
如果是考慮這條線

1:17:13.120,1:17:20.600
只要讓 y hat * g(x) 到這個圖的右邊 error 就是 0

1:17:20.620,1:17:25.460
到右邊以後如果讓 y hat * g(x) 再更靠右

1:17:25.460,1:17:27.960
也沒什麼好處 error 不會下降

1:17:27.960,1:17:32.860
但你看 Adaboost，其實 SVM 跟 Logistic Regression 也有同樣的效果

1:17:32.860,1:17:38.940
看 Adaboost 這條線，當 y hat、g(x) 同號在右邊的時候

1:17:38.940,1:17:40.800
error 並不是 0

1:17:40.800,1:17:47.020
可以把 y hat 跟 g(x) 繼續再往右還是可以得到越來越小的 error

1:17:47.040,1:17:52.420
所以就算是現在 Error Rate 算出來已經是 0 了

1:17:52.420,1:17:55.420
對 Adaboost 來說還沒有結束

1:17:55.420,1:17:57.560
還可以再做得更好

1:17:57.560,1:18:04.160
因為它可以把 g(x) * y hat 再更往右邊推然後得到更小的 error

1:18:05.080,1:18:12.460
這個是 Adaboost 為甚麼會 increase 這個 margin

1:18:12.780,1:18:16.880
最後這一頁是個實作

1:18:16.880,1:18:20.460
實作一下 Adaboost + Decision Tree

1:18:20.460,1:18:23.600
Decision Tree 的深度就設為 5

1:18:23.600,1:18:27.240
把很多深度只有 5 的 Decision Tree 集合起來

1:18:27.260,1:18:30.520
看看他們可以變甚麼樣子，之前有講過

1:18:30.520,1:18:34.520
深度是 5 的 Decision Tree 沒有辦法

1:18:34.520,1:18:36.800
我們之前用的是初音的 function

1:18:36.800,1:18:39.620
它沒有辦法 fit 一個初音的 function

1:18:39.620,1:18:42.280
就算做 Bagging、做 Random Forest

1:18:42.280,1:18:44.660
也沒有用 Random Forest 它本來要做的事情

1:18:44.660,1:18:49.380
就並不是要讓不同 weak classifier 之間可以互補

1:18:49.380,1:18:52.520
它只要讓強的 classifier varience 不要那麼大而已

1:18:52.520,1:18:57.560
但是 Adaboost 不一樣它可以讓 weak classifier 彼此之間是互補

1:18:57.580,1:19:01.120
所以就算是深度是 5 的 Decision Tree

1:19:01.120,1:19:04.460
一棵沒有辦法 fit 出一個 function

1:19:04.460,1:19:08.960
如果找了十棵，這邊 T = 10 代表 Adaboost iteration 跑了十次

1:19:08.960,1:19:11.820
有十棵深度是 5 的 Decision Tree

1:19:11.820,1:19:16.620
這些 Decision Tree 互相之間是互補的

1:19:16.620,1:19:18.420
跟 Random Forest 不一樣

1:19:18.560,1:19:20.920
這十棵 Decision Tree 是互補的

1:19:20.920,1:19:25.040
如果是 Random Forest 找十棵一百棵都 fit 不了初音這個 function

1:19:25.040,1:19:31.240
但是如果找十棵 tree 彼此之間是互補的就可以得到比較好的結果

1:19:31.240,1:19:37.560
這是個初音的 function，你可以看到初音的樣子，可是他的腳是歪的

1:19:37.560,1:19:41.000
如果有二十棵樹就可以做得好很多了

1:19:41.000,1:19:43.920
只是這邊腳的地方還是有點奇怪的東西

1:19:43.920,1:19:47.560
如果五十棵樹，幾乎就可以 fit 初音的 function

1:19:47.560,1:19:50.580
但其實這樣還沒有結束因為這邊其實要有一根毛

1:19:50.580,1:19:52.420
要把那根毛做出來才行

1:19:52.420,1:19:58.880
如果用一百棵樹的話就可以幾乎完美的 fit 初音的 function

1:19:58.960,1:20:08.340
從這個例子可以看到 Boosting 跟 Bagging 是不一樣的

1:20:10.420,1:20:15.700
接下來講的是 Gradient Boosting

1:20:15.820,1:20:24.480
Gradient Boosting 它是剛才那個 Boosting 演算法更 general 的版本

1:20:24.480,1:20:27.980
整個 Boosting 演算法 in general 可以看成是

1:20:27.980,1:20:30.340
以下這樣的 algorithm

1:20:30.340,1:20:34.800
現在跑 T 個 iteration

1:20:34.940,1:20:38.420
每次在 T 個 iteration 裡面

1:20:38.420,1:20:43.440
找一個 function ft 跟 alpha t

1:20:43.440,1:20:47.340
找一個 weak classifier ft 跟它的 weight alpha t

1:20:47.440,1:20:53.140
這些人合在一起會 improve 一個 g t-1

1:20:53.140,1:20:54.620
g t-1 是甚麼

1:20:54.620,1:21:00.220
g t-1 是把過去所有的已經找出來的 function

1:21:01.600,1:21:06.120
把過去所有已經找出來的 function 根據 weighted sum 的結果

1:21:06.120,1:21:08.160
就是這個 g t-1

1:21:08.340,1:21:10.600
已經有一個 g t-1 了

1:21:10.600,1:21:15.840
要找一個 ft 跟 alpha t 跟 g t-1 是互補的

1:21:15.900,1:21:20.720
把這個 ft 跟 alpha t 加到 g t-1 以後

1:21:20.720,1:21:22.160
變成了 gt 了

1:21:22.160,1:21:24.700
會比原來的 g t-1 更好

1:21:24.700,1:21:29.100
最後跑完 T 個 iteration 就得到 H(x)

1:21:29.300,1:21:37.120
問題是怎麼找到比較好的 g(x)

1:21:39.000,1:21:47.720
怎麼樣找到一個 ft 把它加到 g t-1以後得到的 gt 是比較好的

1:21:48.000,1:21:53.200
要為 g 設一個目標

1:21:53.940,1:21:59.420
在做 Machine Learning 的時候要設一個 Objective Function

1:21:59.420,1:22:07.240
接下來就是調整參數去 maximize Objective Function 或是 minimize Cost Function

1:22:07.260,1:22:13.720
現在要做的就是 minimize Cost Function

1:22:14.220,1:22:16.140
這 Cost Function 怎麼寫

1:22:16.140,1:22:20.220
對某一個 function g 它的 Cost Function 怎麼寫

1:22:20.220,1:22:23.740
寫成 summation over 所有的 training data n

1:22:23.740,1:22:27.560
小 L 是 Loss Function

1:22:27.560,1:22:33.280
小 L 這個 function 是算 y hat 跟 g(x) 他們之間的差異

1:22:33.280,1:22:40.740
比如說可以用 Cross Entropy 或 Mean Square Error 等等

1:22:40.740,1:22:44.700
來計算 y hat 和 g 之間的差異

1:22:44.820,1:22:56.860
把小 L 定為 exp( - y hat * g(x))

1:22:56.880,1:22:58.640
這個定義合不合理

1:22:58.640,1:23:04.000
這個定義應該是合理的，因為如果要 minimize 它的話

1:23:04.000,1:23:07.280
minimize exp( - y hat * g(x))

1:23:07.280,1:23:11.080
希望 y hat 跟 g(x) 儘量同號

1:23:11.080,1:23:16.780
而且他們同號相乘的時候要越大要越好

1:23:17.720,1:23:22.880
那怎麼 minimize 這個 function g

1:23:22.900,1:23:26.840
這一步可能需要稍微地想一下

1:23:26.840,1:23:32.440
這件事情比較抽象可是其實要用 Gradient Descent

1:23:32.440,1:23:36.040
來找一個新的 function gt

1:23:36.100,1:23:41.280
它可以 minimize Loss Function L

1:23:46.440,1:23:53.640
要把 g 這個 function 對 L 做微分

1:23:53.640,1:23:56.720
算出它的 Gradient

1:24:02.540,1:24:05.540
這邊 notation 好像沒有用的很好

1:24:05.540,1:24:09.040
這邊我應該是寫三角形比較對

1:24:12.480,1:24:15.480
沒關係大家知道我的意思

1:24:15.480,1:24:23.280
要把 function g 對 L 算它的 gradient

1:24:23.340,1:24:29.500
然後再用這個 gradient 去 update g t-1 得到 gt

1:24:29.500,1:24:35.280
這樣新的 gt 跟原來的 g t-1 比起來它會讓 Loss Function 比較小

1:24:35.280,1:24:37.380
這樣大家知道我的意思嗎

1:24:37.380,1:24:41.480
我猜這邊你一定一下又卡住了

1:24:41.480,1:24:48.100
甚麼叫拿一個 function 對 L 做 gradient

1:24:48.100,1:24:52.080
function g 又不是參數

1:24:52.200,1:24:55.800
對不對，如果是 Neural Network 參數 theta

1:24:55.800,1:24:58.720
你知道怎麼對 L 算 gradient

1:24:58.720,1:25:01.640
但是如果是一個 function g

1:25:01.840,1:25:05.940
它要怎麼對 L 做 gradient
這個地方

1:25:06.060,1:25:11.660
你可以這樣想
其實一個 function g(x)

1:25:11.660,1:25:14.420
假設橫坐標就是 x

1:25:14.420,1:25:18.440
它其實是高維不過這邊就意思一下

1:25:18.580,1:25:23.600
其實一個 function 比如說它長這個樣子就是 g(x)

1:25:23.740,1:25:29.600
你可以想成它的每一點就是一個參數

1:25:29.840,1:25:32.200
這樣大家可以想像嗎

1:25:32.280,1:25:35.040
取一個 x1

1:25:35.640,1:25:38.880
得到一個 g(x1)

1:25:39.160,1:25:41.740
取一個 x2

1:25:41.740,1:25:45.160
得到一個 g(x2)

1:25:45.160,1:25:47.880
假設 x 取的非常非常的密

1:25:47.880,1:25:51.440
那其實 g(x) 就是一個 vector

1:25:51.540,1:25:55.340
這個 g 就是一個 vector g(x1)

1:25:55.460,1:25:57.260
g(x2)

1:25:57.520,1:25:59.220
......

1:25:59.300,1:26:04.380
它就是一個 vector
這個 vector 就是這個 function 的參數

1:26:04.380,1:26:07.880
可以調整參數就改變了這個 function 的形狀

1:26:07.880,1:26:10.020
你可以決定這個 function 的形狀是甚麼

1:26:10.020,1:26:13.660
你就調整這些參數這個 function 的形狀就變了

1:26:13.660,1:26:19.480
其實可以把一個 function 想成它其實就是有無窮多的參數

1:26:19.540,1:26:21.360
大家可以想像嗎

1:26:25.720,1:26:33.640
反正我既然可以接受它是參數的話
就可以把它對 L 做偏微分

1:26:33.660,1:26:38.900
還是可以說我如果改變 g 在某一個點的位置

1:26:38.900,1:26:41.620
它對 L 的影響有多小、有多大

1:26:41.620,1:26:47.280
所以還是可以算出 g 對 L 的偏微分

1:26:49.420,1:26:54.760
這個是如果從 Gradient Descent 的角度來考慮的話是這樣子

1:26:55.040,1:27:01.160
如果從 Boosting 角度來看得話
Boosting 做的事情是找一個 ft 跟 alpha t

1:27:01.160,1:27:03.920
加到 g t-1 後變 gt

1:27:03.920,1:27:06.580
怎麼找這個 ft 跟 alpha t

1:27:06.580,1:27:11.560
就會希望 ft 跟 alpha t 這一項

1:27:11.720,1:27:14.700
其實就是這一項

1:27:14.780,1:27:18.240
或者是至少他們的方向

1:27:18.400,1:27:23.480
要是一樣的，因為前面還有乘一個 Learning Rate

1:27:23.480,1:27:27.040
這兩個式子一模一樣其實沒有必要但是

1:27:27.220,1:27:30.060
希望他們的方向是一樣的

1:27:30.060,1:27:36.400
如果 ft 的方向跟這個微分的方向一致的話

1:27:36.400,1:27:40.780
把這個 ft 加給 g t-1

1:27:40.860,1:27:45.080
就可以讓新的 gt loss 變小

1:27:45.260,1:27:48.300
這個是比較抽象的部分

1:27:50.880,1:28:02.120
假設定義 L 就是長樣子的話那對 g 做偏微分得到的值是多少

1:28:02.220,1:28:05.820
把 L 對 g 做偏微分

1:28:05.860,1:28:08.260
這邊是 exp( - y n * g(x))

1:28:08.260,1:28:12.280
這邊是 exp (  - y n *g (t-1) )

1:28:12.520,1:28:18.100
把它對 g 做偏微分得到的值是多少

1:28:18.100,1:28:23.140
exponential 的部分做偏微分以後是不變的

1:28:27.420,1:28:29.280
g 其實是我們的參數

1:28:29.280,1:28:32.660
對 exponential 的指數項做微分的話

1:28:32.660,1:28:35.080
這邊得到的是 - y hat

1:28:35.080,1:28:39.620
前面這邊有一項符號我把它拿下來，這邊省略掉了 Learning Rate

1:28:39.620,1:28:42.820
負號是可以消掉的

1:28:42.820,1:28:45.140
所以得到了這樣的式子

1:28:45.140,1:28:58.800
會希望 ft 跟這個式子的方向越一致越好

1:28:58.860,1:29:01.320
所謂的方向越一致越好是甚麼意思

1:29:01.320,1:29:07.680
每一個 function 都可以把它想成是一個 vector
只是這個 vector 有無窮多維

1:29:07.680,1:29:12.840
ft 是一個 vector 它有無窮多維

1:29:12.920,1:29:17.960
這個也是一個 vector 這個 vector 有無窮多維

1:29:19.480,1:29:25.860
如果你覺得無窮多維很難想像的話，可以只考慮 training data 有出現的 x

1:29:25.980,1:29:31.640
那它的維度就是有限的，training data 一百萬筆 data 它就是一百萬維

1:29:31.640,1:29:38.020
希望 ft 跟這個式子他們的的方向越一致越好

1:29:38.020,1:29:40.060
怎麼讓它越一致越好

1:29:40.060,1:29:43.100
因為 ft 是我們要找的目標

1:29:43.100,1:29:45.920
我們要找出 ft

1:29:45.920,1:29:48.140
要怎麼找 ft

1:29:48.140,1:30:00.120
要找這個 ft 希望如果把 ft 乘上這一項

1:30:00.120,1:30:02.340
這個值要越大越好

1:30:02.340,1:30:07.200
如果這個值越大越好就代表 ft 跟這個式子

1:30:07.200,1:30:12.960
他們的方向越一致

1:30:13.680,1:30:16.960
這個式子要怎麼看

1:30:16.960,1:30:29.960
這個式子可以想成對每一筆 training data 都希望 y hat 跟ft 他們是同號的

1:30:29.960,1:30:35.440
然後每一筆 training data 前面都乘上了一個 weight

1:30:35.440,1:30:38.760
這個 weight 是 u 上標 n 下標 t

1:30:38.760,1:30:48.420
都乘上一個 weight 這個 weight 是 exp( - y hat * g (t-1))

1:30:51.640,1:31:02.920
這個 weight exp ( - y hat * g t-1(x n )) 到底是甚麼

1:31:02.920,1:31:06.040
把 g t-1 的式子帶進去

1:31:06.040,1:31:12.480
g t-1 是一堆小 f 乘上它的 weight 的 summation

1:31:12.480,1:31:16.460
然後再把相加的這一項提出來

1:31:16.460,1:31:18.480
變成連乘

1:31:18.480,1:31:30.340
就會發現這個 weight exactly 就是 Adaboost 的 weight

1:31:31.360,1:31:37.200
所以找出來的這個 ft

1:31:37.240,1:31:41.920
其實就是 Adaboost 裡面找出來的 f

1:31:41.920,1:31:47.380
所以 Adaboost 裡面找一個 weak classifier ft 的時候

1:31:47.400,1:31:51.620
可以想成好像在做 Gradient Descent 一樣

1:31:51.620,1:31:56.580
有了這個 ft 把這個 ft 加到 g 裡面

1:31:56.580,1:32:04.060
會讓 g 的 loss 變小

1:32:04.380,1:32:11.700
再來的問題就是怎麼決定 alpha t

1:32:12.300,1:32:18.340
這個 alpha t 的作用就很像是 Learning Rate 一樣

1:32:18.340,1:32:22.800
在一般做 Gradient Descent、train Neural Network 的時候

1:32:22.800,1:32:29.820
Learning Rate 就是設個 fix 的值或者是用種種比如說

1:32:29.820,1:32:34.200
adaptive Learning Rate 的設法來設它

1:32:35.860,1:32:40.880
但是這邊做的事情是給定了 ft 以後

1:32:40.880,1:32:45.920
窮舉各種不同可能的 alpha、去試不同可能的 alpha

1:32:45.920,1:32:49.860
看看哪一個 alpha 可以讓 gt 最小

1:32:49.860,1:32:53.880
找完 ft 以後把 ft 固定下來試不同的 alpha

1:32:53.880,1:33:04.540
看哪一個 alpha 可以讓 gt 的 loss 更小

1:33:04.720,1:33:08.280
為甚麼這邊會選擇這樣的做法

1:33:08.280,1:33:12.640
因為在做 Gradient Descent、在 train Neural Network 的時候

1:33:12.640,1:33:16.540
算參數的 gradient 都是比較快的

1:33:16.540,1:33:21.820
所以你可能不會稀罕說你的 Learning Rate 設得好不好

1:33:21.820,1:33:24.540
如果 Learning Rate 設得太小

1:33:24.540,1:33:27.640
反正就多算幾次 gradient、多跑幾次就行

1:33:27.700,1:33:31.020
但是 Gradient Boosting 的方法裡面

1:33:31.060,1:33:35.040
ft 是一個 classifier

1:33:35.040,1:33:39.840
在找 ft 的過程中它的運算量可能就是很大的

1:33:39.840,1:33:42.020
甚至如果 ft 是個 Neural Network

1:33:42.020,1:33:51.900
要把 ft 找出來的時候本身就需要很多次的 Gradient Descent 的 iteration

1:33:51.900,1:33:56.340
既然找出了 ft 以後就要好好的珍惜它

1:33:56.340,1:34:00.080
把它的利用價值發揮到最大

1:34:00.080,1:34:03.180
這邊 Gradient Boosting 採取的方式是

1:34:03.180,1:34:06.960
既然已經找出 ft，固定住 ft 然後

1:34:06.960,1:34:09.900
硬調一個最好的 Learning Rate alpha t

1:34:09.900,1:34:13.740
窮舉所有的 Learning Rate alpha t

1:34:13.740,1:34:19.580
看哪一個 alpha t 可以讓 loss 掉最多

1:34:21.300,1:34:26.120
實際上不可能窮舉所有的 alpha t 一個一個去試試看

1:34:26.240,1:34:27.940
這邊做的事情其實就是

1:34:28.060,1:34:31.355
解一個 optimization 的 problem

1:34:31.360,1:34:35.540
看哪一個 alpha t 可以讓 loss 最小

1:34:36.100,1:34:41.980
怎麼做
這邊就把 equation 略過

1:34:41.980,1:34:43.720
實際上做的事情就是

1:34:43.720,1:34:48.080
計算 alpha t 跟 L(g) 的微分

1:34:48.080,1:34:53.320
然後再看 alpha t 的值是多少的時候這個微分是 0

1:34:53.320,1:34:57.500
這樣就可以把極值找出來

1:34:57.680,1:35:00.720
巧合的是找出來的 alpha t

1:35:00.720,1:35:04.360
就是 ln√( 1 - εt)/εt

1:35:04.360,1:35:08.800
就是 Adaboost 裡面的那個 weight

1:35:08.800,1:35:11.320
所以 Adaboost 整件事情

1:35:11.320,1:35:14.900
就可以想成它也是在做 Gradient Descent

1:35:14.900,1:35:18.180
只是 Gradient 是一個 function

1:35:18.180,1:35:21.900
Learning Rate 有一個很好的方法

1:35:21.900,1:35:24.240
可以決定 Learning Rate

1:35:27.420,1:35:30.420
Gradient Boosting 有一個想法好的地方是

1:35:30.500,1:35:34.780
可以任意更改 Objective Function

1:35:34.780,1:35:37.260
我們剛才定了一個 Objective Function

1:35:37.260,1:35:40.780
是 exp(- y hat * g(x))

1:35:40.780,1:35:44.800
你永遠可以定其它的 Objective Function

1:35:44.800,1:35:49.700
就可以創造出不一樣的新的方法

1:35:52.720,1:35:58.720
最後一個我要講的 ensemble 方法是 Stacking

1:35:58.720,1:36:02.660
Stacking 是甚麼
Stacking 非常實用我覺得

1:36:02.660,1:36:05.560
final project 裡面非常實用的方法

1:36:05.620,1:36:09.020
現在到了期末大家都很忙

1:36:09.020,1:36:11.140
一組其實有四個人

1:36:11.140,1:36:16.920
可能就四個人每個人都弄了一個自己的 model

1:36:17.080,1:36:23.280
選好一個 final project 題目後四個人都弄了一個自己的 model

1:36:23.300,1:36:27.500
但最後要怎麼讓 performance 再提升

1:36:27.500,1:36:30.580
就要把四個人的 model combine 起來

1:36:30.580,1:36:35.720
比如說把一把 data x 丟到四個 model 裡面

1:36:35.720,1:36:38.740
然後每一個 model 都會給你一個 output

1:36:38.740,1:36:44.660
再把這四個 output 想辦法把它合併起來得到最終的答案

1:36:44.660,1:36:50.940
假設是個分類的問題的話可以用 Majority Vote

1:36:50.940,1:36:55.920
最多系統選擇哪個 class 那那個 class 就是正確答案

1:36:56.080,1:37:04.380
今天會遇到的問題是並不是所有的系統都是好
並不是所有的 model 都是好

1:37:04.400,1:37:07.480
有一些 model 可能是爛的，比如說可能小毛特別弱

1:37:07.480,1:37:10.320
它做的系統是跟 random 一樣

1:37:10.340,1:37:14.960
所以如果把它系統權重跟其它系統設一樣

1:37:15.000,1:37:17.500
那這樣不行這樣整個 performance 會壞掉

1:37:17.500,1:37:22.500
但如果你本來就知道小毛特別弱，把他的權重設的很低的話

1:37:22.500,1:37:24.340
就傷了他的自尊心

1:37:24.600,1:37:29.160
所以怎麼辦
要去 learn 一個 classifier

1:37:29.360,1:37:33.860
這個 classifier 是這樣，它把前面這些 system 的 output

1:37:33.920,1:37:36.900
當作 input，也就是說這些 system 的 output

1:37:36.980,1:37:42.140
對最後這個 classifier 來說就好像是個 feature 一樣

1:37:42.140,1:37:48.220
它把這些 system 的 output 當作 feature 再決定最終的結果是甚麼

1:37:48.260,1:37:51.400
這個最終的 classifier 就不需要太複雜

1:37:51.400,1:37:56.320
最前面如果都已經用好幾個 Hidden Layer 的 Neural Network 了

1:37:56.320,1:38:00.580
也許 final classifier 就不需要再好幾個 Hidden Layer 的 Neural Network

1:38:00.580,1:38:04.420
它可以只是 Logistic Regression 就行了

1:38:10.255,1:38:13.385
那在做這個實驗的時候要注意

1:38:13.385,1:38:18.240
我們會把有 label 的 data 分成 training set 跟 validation set

1:38:18.240,1:38:22.340
在做 Stacking 的時候要把 training set 再分成兩部分

1:38:22.340,1:38:27.260
一部份的 training set 拿來 learn 這些 classifier

1:38:27.260,1:38:32.460
另外一部分的 training data 拿來 learn 這個 final classifier

1:38:32.580,1:38:36.120
為甚麼要這麼做? 因為有的 classifier

1:38:36.120,1:38:38.940
有的要來做 Stacking 的前面 classifier

1:38:38.940,1:38:41.460
它可能只是 fit training data

1:38:41.460,1:38:45.560
舉例來說可能小明的 code 就是亂寫的

1:38:45.560,1:38:49.540
其實它的 classifier 甚麼事都不會做
它的 classifier 就是

1:38:49.540,1:38:54.840
如果有一筆 data 進來跟 training data 一樣它就把它 label 找出來

1:38:54.840,1:38:57.480
不然就甚麼事都沒有做

1:38:57.600,1:39:02.620
它可能寫一個很奇怪、很爛的、很異常 overfitting 的 code

1:39:02.620,1:39:05.915
如果 final classifier 的 training data

1:39:05.920,1:39:09.860
跟這些 system 用的 training data 是同一組的話

1:39:09.860,1:39:12.800
就會發現喔小明的 classifier 好強 Error Rate 是 100%

1:39:12.800,1:39:14.960
都參考小明的 classifier 就好

1:39:15.040,1:39:18.000
但其實小明的 classifier 其實甚麼事都沒有做

1:39:18.000,1:39:20.840
它只是硬把 training data 記起來而已

1:39:20.840,1:39:27.860
所以在 train final classifier 的時候必須要用另外一筆 training data 來 train final classifier

1:39:27.860,1:39:30.880
不能跟前面 train 系統的 classifier 一樣

1:39:30.880,1:39:35.600
如果有 final classifier 就可以給不同的系統不同的權重

1:39:35.600,1:39:37.800
如果小毛的系統特別差的話

1:39:37.800,1:39:40.040
那 final classifier 就會給它比較小的權重

1:39:40.040,1:39:41.460
比如說是 0 這樣

1:39:41.460,1:39:44.360
這樣小毛的自尊心其實還是會被傷害

1:39:44.360,1:39:49.280
只是它是被機器傷害所以就可以維護團隊和諧

1:39:51.280,1:39:54.900
最後這一頁講過了

1:39:54.900,1:39:58.640
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
