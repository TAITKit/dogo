0:00:00.000,0:00:02.860
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:03.120,0:00:05.560
好，我們來講 Logistic Regression

0:00:05.560,0:00:08.020
在 Logistic Regression 裡面呢

0:00:08.220,0:00:11.480
我們在上一份投影片裡面，我們都已經知道說

0:00:11.480,0:00:14.740
我們要找的東西呢，是一個機率

0:00:14.740,0:00:16.720
是一個 Posterior probability

0:00:16.720,0:00:20.640
如果這個 Posterior probability > 0.5 的話

0:00:20.640,0:00:23.900
就 output C1，否則呢，就 output C2

0:00:23.900,0:00:26.360
我們知道這個 posterior probability

0:00:26.360,0:00:29.900
假設你覺得你想要用 Gaussian 的話

0:00:29.900,0:00:32.080
其實很多其他的 probability，化簡以後

0:00:32.080,0:00:34.820
也都可以得到同樣的結果

0:00:34.920,0:00:37.260
假設你覺得你想要用 Gaussian 的話

0:00:37.260,0:00:39.600
那你就說，你可以說這個

0:00:39.600,0:00:44.340
posterior probability 就是 σ(z)

0:00:44.340,0:00:48.580
它的 function 長的是右邊這個樣子

0:00:48.580,0:00:51.940
那這個 z 呢，是 w 跟 x 的 inner product 加上 b

0:00:52.110,0:00:54.720
所謂 w 跟 x 的 inner product 呢，是說

0:00:54.720,0:00:57.860
這個 w，它是一個 vector

0:00:57.860,0:01:02.100
它的每一個 dimension 我們就用下標 i 來表示

0:01:02.440,0:01:04.560
那這個 w 呢，是一個 vector

0:01:04.560,0:01:09.520
每一個 x，都有一個對應的 w 下標 i

0:01:09.520,0:01:11.920
你把所有的 xi 跟 (w 下標 i) 相乘

0:01:12.220,0:01:14.700
summation 起來再加上 b，就得到 z

0:01:14.700,0:01:17.040
代進這個 sigmoid function

0:01:17.210,0:01:18.660
就得到機率

0:01:18.660,0:01:22.600
所以，我們的 function set 是長這樣子

0:01:22.600,0:01:26.720
我們的 function set,  f(下標 w, b) (x)

0:01:26.760,0:01:28.540
這邊加下標 w, b 的意思就是說

0:01:28.540,0:01:34.680
我們現在的這個 function set 是受 w 和 b 所控制的

0:01:34.680,0:01:38.500
就是你可以選不同的 w 和 b，你就會得到不同 function

0:01:38.500,0:01:41.920
所有 w 和 b 可以產生的 function 集合起來

0:01:42.140,0:01:44.500
就是一個 function set，那這一項

0:01:44.500,0:01:48.980
它的涵意呢，就是一個 posterior probability

0:01:48.980,0:01:52.620
given x，它是屬於 C1 的機率

0:01:53.460,0:01:56.180
如果我們用圖像化的方式來表它的話呢

0:01:56.180,0:01:57.700
它長這樣

0:01:57.700,0:02:03.520
我們的 function 裡面，有兩組參數

0:02:03.520,0:02:06.740
一組是 w，我們稱之為 weights

0:02:06.740,0:02:08.160
weights 呢，有一整排

0:02:08.160,0:02:10.620
然後有一個 constant b

0:02:10.620,0:02:12.580
這個我們稱之為 bias

0:02:12.580,0:02:15.060
然後有一個 sigmoid function

0:02:16.060,0:02:23.180
如果我們今天的 input 是 x1, xi 到 xI

0:02:23.180,0:02:28.820
你就把 x1, xi, xI 分別乘上 w1, wi, wI

0:02:28.820,0:02:32.900
然後再加上 b 呢，你就得到 z，這個 z

0:02:34.760,0:02:39.700
我發現我寫錯了一個地方，後面這邊呢，應該是要 + b

0:02:39.860,0:02:46.560
你把  x1*w1 + xi*wi 加到 xI*wI，再加上 b，就得到 z

0:02:46.560,0:02:49.340
z 通過我們剛才得到的 sigmoid function

0:02:49.380,0:02:54.460
它 output 的值，就是機率，就是 posterior probability

0:02:54.650,0:02:59.500
這個是整個模型，長這個樣子

0:02:59.730,0:03:03.940
這件事呢，叫做 Logistic Regression

0:03:03.940,0:03:06.920
那我們可以把 Logistic Regression

0:03:06.920,0:03:10.920
跟我們在第一堂課就講的 Linear Regression 做一下比較

0:03:10.940,0:03:14.900
Logistic Regression，把每一個 feature 乘上一個 w

0:03:14.900,0:03:18.080
summation 起來再加上 b，再通過 sigmoid function

0:03:18.080,0:03:20.120
當作 function 的 output

0:03:20.200,0:03:22.580
那它的 output 因為有通過 sigmoid function

0:03:22.580,0:03:25.700
所以，一定是界於 0~1 之間

0:03:25.800,0:03:30.260
那 Linear Regression 呢？它就把 feature * w 再加上 b

0:03:30.840,0:03:32.540
它沒有通過 sigmoid function

0:03:32.540,0:03:36.040
所以，它的 output 就可以是任何值

0:03:36.040,0:03:39.260
可以是正的，可以是負的，負無窮大到正無窮大

0:03:39.260,0:03:42.960
那等一下呢，我們說 machine learning 就是 3 個 step

0:03:42.960,0:03:44.320
等一下我們會一個一個 step

0:03:44.520,0:03:49.600
比較 Logistic Regression 跟 Linear Regression 的差別

0:03:50.340,0:03:55.200
接下來呢，我們要決定一個 function 的好壞

0:03:55.500,0:03:57.980
那我們的 training data 呢

0:03:57.980,0:04:00.340
因為我們現在要做的是 Classification

0:04:00.580,0:04:04.860
所以我們的 training data，就是假設有 N 筆 training data

0:04:04.960,0:04:08.140
那每一筆 training data，你都要標說它屬於哪一個 class

0:04:08.140,0:04:12.100
比如說，x^1 屬於 C1，x^2 屬於 C1，x^3 屬於 C2

0:04:12.100,0:04:15.200
x^N 屬於 C1......等等，那接下來呢

0:04:15.360,0:04:20.400
我們假設這筆 training data 是從

0:04:20.600,0:04:26.000
我們的 function 所定義出來的這個 posterior probability

0:04:26.000,0:04:29.180
所產生的，就是這組 training data

0:04:29.180,0:04:35.260
是根據這個 posterior probability 所產生的

0:04:35.880,0:04:39.940
那給我們一個 w 和 b

0:04:39.940,0:04:43.860
我們就決定了這個 posterior probability

0:04:43.860,0:04:47.760
那我們就可以去計算，某一組 w 和 b

0:04:47.760,0:04:52.360
產生 N 筆 training data 的機率

0:04:53.060,0:04:57.380
某一組 w 和 b 產生 N 筆 training data 的機率怎麼算呢？

0:04:57.880,0:05:01.700
這個很容易，就是假設 x1 是屬於 C1

0:05:01.700,0:05:06.140
那它根據某一組 w 和 b，產生的機率就是

0:05:06.140,0:05:08.240
f(下標w, b) (x^1)

0:05:08.300,0:05:12.680
假設 x2 是屬於 C1，那它被產生的機率就是

0:05:12.680,0:05:13.860
f(下標w, b) (x^2)

0:05:14.040,0:05:16.620
假設 x3 是屬於 C2

0:05:16.620,0:05:20.080
我們知道 x3 如果屬於 C1 的機率

0:05:20.080,0:05:23.720
就是 f(下標w, b) (x^3)，因為我們這邊算的是 C1 的機率

0:05:23.820,0:05:26.420
那這個 x3 屬於 C2，所以他的機率就是

0:05:26.420,0:05:30.880
1 - f(下標w, b) (x^3)，以此類推

0:05:30.880,0:05:36.040
那最有可能的參數 w 跟 b

0:05:36.040,0:05:39.060
我們覺得最好的參數 w 跟 b

0:05:39.060,0:05:43.820
就是那個有最大的可能性、最大的機率

0:05:43.820,0:05:49.360
可以產生這個 training data 的 w 跟 b

0:05:49.360,0:05:51.960
我們把它叫做 w* 跟 b*

0:05:51.960,0:05:59.420
w* 跟 b* 就是那個可以最大化這一個機率的 w 跟 b

0:06:00.740,0:06:05.060
那我們在這邊，做一個數學式上的轉換

0:06:05.120,0:06:07.500
我們原來是要找一組 w 跟 b

0:06:07.500,0:06:11.640
最大化 L(w , b)

0:06:11.640,0:06:13.520
最大化這個 function

0:06:13.520,0:06:16.240
但是，這件事情等同於呢

0:06:16.240,0:06:18.020
我們找一個 w 跟 b

0:06:18.020,0:06:20.940
minimize 負 ln 這個 function

0:06:20.980,0:06:24.440
我們知道取 ln，它的這個 order 是不會變的

0:06:24.500,0:06:29.700
加上一個負號，就從找最大的，變成找最小的

0:06:29.800,0:06:32.160
所以，我們就是要找一組 w 跟 b

0:06:32.160,0:06:35.760
最小化 -ln L(w, b)

0:06:35.760,0:06:39.340
這個可以讓計算變得容易一點，左式跟右式是一樣的

0:06:39.340,0:06:41.880
根據左式跟右式找出來的 w 跟 b 呢

0:06:41.880,0:06:45.900
w* 跟 b* 呢，是同個 w* 跟 b*

0:06:46.440,0:06:50.320
那 -ln (這一項) 怎麼做呢？

0:06:50.320,0:06:54.580
你知道取 -ln 的好處就是，本來相乘，現在變成相加

0:06:54.580,0:06:59.700
然後，你就把它展開，所以這一項就是 -ln f(x^1)

0:06:59.700,0:07:03.360
-ln f(x^2), -ln (1 - f(x^3))

0:07:03.560,0:07:05.180
以此類推

0:07:05.180,0:07:09.360
那這件事情讓你，寫式子有點難寫

0:07:09.360,0:07:12.520
就是你沒有辦法寫一個 summation over

0:07:12.520,0:07:16.160
因為對不同的 x，如果屬於不同 class

0:07:16.160,0:07:19.260
我們就要用不同的方法來處理它

0:07:19.260,0:07:22.020
而且你沒有辦法 summation over x，那怎麼辦呢？

0:07:22.020,0:07:25.060
我們做一個符號上的轉換

0:07:25.060,0:07:29.720
我們說，如果某一個 x 它屬於 class 1

0:07:29.720,0:07:32.800
我們就說它的 target 是 1

0:07:32.800,0:07:36.560
如果它屬於 class 2，我們就說它的 target 是 0

0:07:36.560,0:07:39.560
我們之前在做 Linear Regression 的時候

0:07:39.720,0:07:43.420
每一個 x，它都有一個對應的 y\head，對不對？

0:07:43.420,0:07:45.540
然後那個對應的 y\head 是一個 real number

0:07:45.540,0:07:49.420
在這邊呢，每一個 x 也都有一個對應的 y\head

0:07:49.540,0:07:52.260
這個對應的 y\head，它的 number 就代表說

0:07:52.260,0:07:54.220
現在這個 x，屬於哪一個 class

0:07:54.310,0:07:56.310
如果屬於 class 1

0:07:57.200,0:08:02.280
欸，我怎麼會犯這麼弱智的錯誤

0:08:02.800,0:08:05.600
大家有發現嗎？這個投影片上，有一個錯啊

0:08:06.880,0:08:09.840
對，它應該是 1, 1 , 0，麼會犯這麼弱智的錯誤

0:08:09.840,0:08:14.040
這個屬於 class 1 就應該是 1，所以這個應該是

0:08:14.040,0:08:18.920
1, 1, 0 這樣，沒關係，你無視這邊

0:08:18.920,0:08:20.380
你就看這裡就好

0:08:20.380,0:08:23.200
屬於 class 1 就是 1，屬於 class 2 就是 0

0:08:23.640,0:08:30.060
如果你做這件事的話

0:08:30.060,0:08:35.160
那你就可以把這邊的每一個式子

0:08:35.160,0:08:36.600
都寫成這樣

0:08:36.600,0:08:41.140
這看起來有一點複雜，但你仔細算一下就會發現說

0:08:41.140,0:08:43.740
左邊和右邊，是相等的

0:08:43.740,0:08:47.660
每一個 -ln f(x)，你都可以寫成

0:08:47.660,0:08:51.860
負的中括號、負的括號

0:08:51.860,0:08:58.260
它的 y1\head、它的 y\head 乘上 ln f(x)

0:08:58.260,0:09:02.680
加上 (1 - y\head) * ln(1 - f(x))

0:09:02.680,0:09:05.600
你就實際上算一下，比如說

0:09:05.600,0:09:07.840
x1, x2 都是屬於 C1

0:09:07.840,0:09:10.880
所以呢，它對應的 y\head 是 1

0:09:10.900,0:09:14.200
所以這個 y1\head 跟 y2\head 是 1

0:09:14.200,0:09:17.000
(1 -  y1\head) 跟 (1 - y2\head) 就是 0

0:09:17.000,0:09:21.460
0 的話呢，它乘上後面那一項，你就不要管它，把它拿掉

0:09:21.520,0:09:24.540
所以，你會發現它等於它，它等於它這樣

0:09:24.540,0:09:27.680
因為空間的關係，我就把 w 跟 b 省略掉了

0:09:27.680,0:09:30.520
有時候放 w 跟 b，只是為了強調說

0:09:30.520,0:09:33.700
這個 f 是 w 跟 b 的 function

0:09:33.880,0:09:37.100
那因為這個寫不下，所以把它省略掉

0:09:37.100,0:09:39.440
好，那這個 y3 呢？

0:09:39.840,0:09:42.740
這個 x3 它屬於 C2，C2 是 0

0:09:42.740,0:09:46.800
所以 y3\head 是 0，(1 - y3\head) 就是 1

0:09:46.800,0:09:49.160
那前面這個部分可以拿掉，你會發現

0:09:49.500,0:09:52.860
右邊這個也是等於左邊這個

0:09:52.860,0:09:55.500
有了這些以後，我們把

0:09:55.800,0:10:02.820
這個 likelihood 的 function，取 -ln

0:10:03.440,0:10:06.200
然後呢，再假設說

0:10:06.220,0:10:08.200
class 1 就是 1，class 2 就是 0 以後

0:10:08.200,0:10:12.780
我們就可以把，我們要去 minimize 的對象

0:10:12.780,0:10:14.040
寫成一個 function

0:10:14.040,0:10:16.040
我們就可以把我們要去 minimize 的對象

0:10:16.060,0:10:18.000
寫成 summation over N

0:10:18.000,0:10:28.380
- [ y^n\head * ln f(x^n) + (1-y^n\head) * ln (1-f(x^n))]

0:10:28.380,0:10:33.400
那其實 summation over 的這一項阿

0:10:33.400,0:10:38.140
這個 Σ 後面的這一整項阿

0:10:38.140,0:10:43.880
它其實是兩個 Bernoulli distribution 的 Cross entropy

0:10:45.400,0:10:48.700
這一項其實是一個 Cross entropy

0:10:48.700,0:10:52.180
所以，等一下我們就會說它是 Cross entropy

0:10:52.180,0:10:57.580
雖然它的來源跟 information theory 沒有太直接的關係

0:10:57.580,0:11:02.280
但是，我們剛才看過它推導的過程

0:11:02.280,0:11:04.920
但是，如果你把

0:11:05.800,0:11:08.460
你假設有兩個 distribution，p 跟 q

0:11:08.460,0:11:13.660
這個 p 的 distribution，它是說 p(x =1) = y^n\head

0:11:13.660,0:11:16.560
p(x=0) =1 – y^n\head

0:11:16.820,0:11:20.700
q 的 distribution，它 q(x =1) 是 f(x^n)

0:11:20.740,0:11:25.020
q(x =0) 是 1 - f(x^n)

0:11:25.020,0:11:28.820
那你把這兩個 distribution 算 cross entropy

0:11:29.640,0:11:32.120
如果你不知道甚麼是 cross entropy 的話，沒有關係

0:11:32.120,0:11:35.400
反正就是，代一個式子

0:11:35.400,0:11:39.520
summation over 所有的 x (-Σ p(x)*ln(q(x))

0:11:39.520,0:11:42.780
前面有個負號，這個就是 cross entropy

0:11:42.780,0:11:47.820
如果你把這兩個 distribution，
算他們之間的 cross entropy

0:11:47.820,0:11:53.080
cross entropy 的涵義是這兩個 distribution 有多接近

0:11:53.080,0:11:57.060
如果今天這兩個 distribution 一模一樣的話

0:11:57.240,0:12:00.420
那他們算出來的 cross entropy 就是 0

0:12:00.420,0:12:03.680
所以，你把這兩個 distribution 算一下 cross entropy

0:12:03.680,0:12:08.040
你把 y^n\head 乘上 ln(f(x^n))

0:12:08.040,0:12:11.240
(1 - y^n\head) 乘上 ln (1 - f(x^n))

0:12:11.240,0:12:13.560
你得到的，就是這項

0:12:13.560,0:12:17.680
如果你有修過 information theory 的話呢

0:12:17.680,0:12:21.960
它這個式子寫出來，跟 cross entropy 是一樣的

0:12:22.980,0:12:26.480
所以在 Logistic Regression 裡面

0:12:26.480,0:12:30.100
我們怎麼定義一個 function，它的好壞呢？

0:12:30.100,0:12:31.720
我們定義的方式是這樣

0:12:31.720,0:12:36.780
有一堆 training data，我們有(x^n, y^n\head)

0:12:36.840,0:12:39.520
有這樣的 pair，如果屬於 class 1 的話呢

0:12:39.520,0:12:43.740
y^n\head 就等於 1，如果屬於 class 2 的話，
y^n\head 就等於 0

0:12:43.900,0:12:49.640
那我們定義的 loss function，我們要去 minimize 的對象

0:12:49.640,0:12:54.280
是所有的 example

0:12:54.340,0:12:57.380
它的 cross entropy 的總和

0:12:57.380,0:13:02.300
也就是說，假設你把 f(x^n) 
當作一個 Bernoulli distribution

0:13:02.300,0:13:05.260
把 y^n\head 當作另一個 Bernoulli distribution

0:13:05.360,0:13:09.960
它們的 cross entropy，你把它算出來

0:13:09.960,0:13:13.860
這個東西，是我們要去 minimize 的對象

0:13:13.860,0:13:16.340
所以，就直觀來講，我們要做的事情是

0:13:16.380,0:13:21.880
我們希望 function 的 output 跟它的 target

0:13:21.880,0:13:24.860
如果你都把它看作是 Bernoulli distribution 的話

0:13:24.860,0:13:29.160
這兩個 Bernoulli distribution，他們越接近越好

0:13:30.120,0:13:32.840
如果我們比較一下 Linear Regression 的話

0:13:32.840,0:13:37.620
Linear Regression 這邊，這個你大概很困惑啦

0:13:37.620,0:13:39.140
如果你今天是第一次聽到的話

0:13:39.140,0:13:41.520
你應該聽得一頭霧水，想說這個

0:13:41.520,0:13:43.940
哇，這個這麼複雜，到底是怎麼來的

0:13:43.940,0:13:48.820
如果你是看 Linear Regression 的話，這個很簡單

0:13:48.820,0:13:53.760
減掉它的 target，y^n\head的平方

0:13:53.880,0:13:56.380
就是我們要去 minimize 的對象

0:13:56.380,0:13:58.180
這個，比較單純

0:13:58.260,0:14:00.420
這個，不知道怎麼來的

0:14:00.420,0:14:02.540
因為你可能就會有一個想法說

0:14:02.540,0:14:07.340
為甚麼在 Logistic Regression 裡面

0:14:07.440,0:14:10.780
我們不跟 Linear Regression 一樣

0:14:10.780,0:14:13.740
用 square error 就好了呢？

0:14:13.740,0:14:16.520
這邊其實也可以用 square error 阿

0:14:16.520,0:14:20.160
沒有甚麼理由，你不能用 square error 不是嗎？

0:14:20.160,0:14:24.180
對不對，因為你完全可以算說

0:14:24.180,0:14:28.980
這個 f(x^n) 跟 (y^n\head) 的 square error

0:14:28.980,0:14:33.520
你就把這個 f(x^n) 跟 (y^n\head) 代到右邊去

0:14:33.520,0:14:36.820
你一樣可以定一個 loss function

0:14:36.820,0:14:39.540
這個 loss function 聽起來也是頗合理的

0:14:39.540,0:14:43.400
為甚麼不這麼做呢？

0:14:43.400,0:14:47.420
等一下，我們會試著給大家一點解釋

0:14:48.920,0:14:54.180
那到目前為止呢，這個東西，反正就是很複雜

0:14:54.260,0:14:56.180
你就先記得說，必須要這麼做

0:14:57.160,0:15:00.140
接下來呢，我們要做的事情就是

0:15:00.140,0:15:03.380
找一個最好的 function

0:15:03.380,0:15:08.440
就是要去 minimize，我們現在要 minimize 的對象

0:15:08.440,0:15:12.100
那怎麼做呢？你就用 Gradient Descent 就好了

0:15:12.100,0:15:17.920
很簡單，接下來都是一些數學式無聊的運算而已

0:15:17.920,0:15:20.360
我們就算

0:15:20.360,0:15:25.360
它對某一個 w 這個 vector，
裡面的某一個 element 的微分

0:15:25.360,0:15:30.020
我們就算這個式子，對 wi 的微分就好

0:15:30.020,0:15:33.480
剩下的部分呢，其實就可以交給大家自己來做

0:15:33.680,0:15:38.040
那我們要算這個東西對 w 的偏微分

0:15:38.040,0:15:39.860
那我們只需要能夠算

0:15:39.860,0:15:42.920
ln(f(x^n) 對 w 的偏微分

0:15:42.920,0:15:46.680
跟 ln(1 - f(x^n)) 對 w 的偏微分就行了

0:15:46.680,0:15:50.940
那 ln(f(x^n) 對 w 的偏微分，怎麼算呢？

0:15:50.940,0:15:54.980
我們知道說，我們把這個 f 寫在下面

0:15:54.980,0:15:56.160
把 f 寫在下面

0:15:56.160,0:16:01.740
f 它受到 z 這個 variable 的影響

0:16:01.740,0:16:03.420
然後 z 這個 variable 呢？

0:16:03.420,0:16:08.740
是從 w, x, b 所產生的

0:16:08.880,0:16:11.920
所以，你就知道說，我們可以把

0:16:11.920,0:16:14.380
這個偏微分拆開

0:16:14.380,0:16:18.360
把 ∂(ln(f(x)) / ∂(wi) 拆解成呢

0:16:18.360,0:16:22.000
∂(ln(f(x)) / ∂(z)

0:16:22.000,0:16:27.040
乘上 ∂(z) / ∂(wi)

0:16:28.240,0:16:31.140
那這個 ∂(z) / ∂(wi) 是甚麼？

0:16:31.140,0:16:34.600
∂(z) / ∂(wi)，這個 z 的式子我寫在這邊了

0:16:34.600,0:16:37.680
只有一項是跟 wi 有關

0:16:37.680,0:16:40.360
只有 wi * xi 那一項是跟 wi 有關

0:16:40.360,0:16:43.900
所以 ∂(z) / ∂(wi) 就是 xi

0:16:44.500,0:16:48.360
那這一項是甚麼呢？這一項太簡單了

0:16:48.360,0:16:51.020
我們把 f(x) 換成 σ(z)

0:16:51.020,0:16:52.740
把這個換成 σ(z)

0:16:52.740,0:16:54.640
然後做一下微分

0:16:54.640,0:16:59.920
這個  ∂(ln σ(z)) / ∂(z) 做微分以後呢，1/σ(z)

0:17:00.060,0:17:03.360
然後，再算  ∂(σ(z))/σ(z)

0:17:03.360,0:17:06.100
那  ∂(σ(z))/σ(z) 是甚麼呢？

0:17:06.100,0:17:08.260
這個 σ(z) 是 sigmoid function

0:17:08.260,0:17:11.600
sigmoid function 的微分呢，其實你可以直接背起來

0:17:11.600,0:17:14.560
就是 σ(z) * (1 - σ(z))

0:17:14.560,0:17:16.860
如果你要看比較直觀的結果的話

0:17:16.860,0:17:18.200
你就把它的圖畫出來

0:17:18.200,0:17:21.820
σ(z) 這邊顏色可能有一點淡

0:17:21.880,0:17:25.200
是綠色這條線，σ(z) 是綠色這條線

0:17:25.200,0:17:30.740
橫軸是 z，那如果對 z 做偏微分的話

0:17:30.740,0:17:36.440
在接近頭跟尾的地方

0:17:36.440,0:17:38.120
它的斜率很小

0:17:38.120,0:17:40.960
所以對 z 做微分的時候，是接近於 0 的

0:17:40.960,0:17:43.980
在中間的地方，斜率最大

0:17:43.980,0:17:45.320
所以這個地方，斜率最大

0:17:45.320,0:17:48.360
所以把這一項對 z 做偏微分的話

0:17:48.440,0:17:52.760
你得到的結果是長得像這樣

0:17:52.760,0:17:56.300
那這一項，其實就是 σ(z) * (1 - σ(z))

0:17:56.320,0:17:59.420
那你就把 σ(z) 消掉

0:17:59.420,0:18:06.060
那你就得到說，這項就是 (1 - σ(z)) * xi

0:18:06.060,0:18:09.320
那 σ(z) 其實就是 f(x)

0:18:09.320,0:18:16.640
所以這一項就是 [1 - f(x^n)] * xi^n

0:18:16.640,0:18:20.860
右邊這一項呢，這個也是 trivial 阿

0:18:20.860,0:18:25.740
你把 ln(1-f(x)) 對 wi 做偏微分

0:18:25.820,0:18:30.760
那就可以拆成先對 z 做偏微分，wi 再對 z 做偏微分

0:18:30.840,0:18:35.320
右邊這一項， ∂(z)/∂(wi) 我們已經知道它就是 xi

0:18:35.400,0:18:38.800
左邊這一項

0:18:38.880,0:18:43.000
你就把 ln 裡面的值，放到分母

0:18:43.000,0:18:47.940
然後呢，這邊是 -σ(z)

0:18:47.940,0:18:52.060
前面有個負號，然後這邊要算 σ(z) 的偏微分

0:18:53.100,0:18:56.960
那 σ(z) 做偏微分以後，得到的結果是這樣

0:18:56.960,0:19:00.800
把 (1 - σ(z)) 消掉，就只剩下 σ(z)

0:19:00.800,0:19:05.160
所以，這一項就是 xi * σ(z)，把它放上來

0:19:05.160,0:19:07.880
就是這個，ok

0:19:09.260,0:19:15.680
那我們就把這一項放進來

0:19:15.680,0:19:17.480
把這一項放進來

0:19:17.480,0:19:20.740
整理一下以後，你得到的結果就是這樣

0:19:21.840,0:19:26.840
接下來呢，你整理一下，把 xi

0:19:26.840,0:19:29.700
提到右邊去

0:19:29.700,0:19:34.780
把 xi 提到右邊去，把括弧的部分展開

0:19:34.780,0:19:37.580
那裡面有一樣的，把它拿掉

0:19:37.580,0:19:39.940
最後，你得到一個直觀的結果

0:19:39.940,0:19:43.980
這個式子看起來有點複雜、有點崩潰

0:19:43.980,0:19:46.880
但是，你對它做偏微分以後

0:19:46.880,0:19:52.240
得到的值的結果，卻是容易理解的

0:19:52.240,0:19:56.620
你得到的值，它的結果呢，每一項都是負的

0:19:56.620,0:20:00.560
y^\head - f(x^n)

0:20:00.560,0:20:06.900
再乘上 x^n 的第 i 個 component

0:20:08.040,0:20:10.640
如果你用 Gradient Descent update 它的話

0:20:10.640,0:20:12.320
那你的式子就很單純，就這樣

0:20:12.320,0:20:17.540
wi 是原來的 wi - learning rate

0:20:17.540,0:20:20.380
乘上 summation over 所有的 training sample

0:20:20.380,0:20:25.240
[y^n\head - f(x^n)] * x^n 在 i 維的地方

0:20:25.240,0:20:29.240
這件事情，它代表了甚麼意思呢？

0:20:29.240,0:20:34.680
它代表甚麼涵義呢？如果你看括號內的式子的話

0:20:34.680,0:20:36.460
括號內的這個式子的話

0:20:36.460,0:20:41.820
現在，你的 w 的 update 取決於三件事

0:20:41.820,0:20:44.900
一個是 learning rate，這個是你自己調的

0:20:44.900,0:20:50.400
一個是 xi，這個是來自於 data

0:20:50.400,0:20:55.220
第三項呢，就是這個 y^n\head

0:20:55.220,0:20:58.640
y^n\head - f(x^n) 是甚麼意思呢？

0:20:58.640,0:21:04.280
y^n\head - f(x^n) 代表說你現在這個 f 的 output

0:21:04.540,0:21:10.680
跟理想的這個目標

0:21:10.680,0:21:12.580
它的差距有多大

0:21:12.580,0:21:14.820
y^n\head 是目標

0:21:14.840,0:21:19.320
f(x^n) 是現在你的 model 的 output

0:21:19.320,0:21:23.960
這兩項之間的差距，這兩個相減的差呢

0:21:23.960,0:21:30.500
就代表說，他們的差距有多大

0:21:30.500,0:21:33.380
那如果今天，你離目標越遠

0:21:33.380,0:21:37.760
那你 update 的量就應該越大

0:21:37.760,0:21:42.460
所以，這個結果看起來是匹頗為合理的

0:21:44.580,0:21:48.160
那接下來呢

0:21:48.340,0:21:53.820
我們就來比較一下 Linear Regression 跟 Logistic Regression

0:21:53.820,0:21:56.460
Logistic Regression 跟 Linear Regression

0:21:56.460,0:21:59.680
他們在做 Gradient Descent 的時候，參數 update 的方式

0:21:59.680,0:22:01.920
我們已經看到 Logistic Regression

0:22:01.920,0:22:03.700
它 update 的式子長這樣子

0:22:03.700,0:22:05.880
那神奇的是 Linear Regression

0:22:05.880,0:22:09.280
大家應該都順利做完作業一了

0:22:09.280,0:22:11.060
Linear Regression 的這個

0:22:11.060,0:22:14.040
Gradient Descent update 的式子，你應該是很熟

0:22:14.040,0:22:18.400
他們其實是一模一樣的

0:22:18.920,0:22:22.560
你看哦，他們都算

0:22:22.560,0:22:25.380
y^n\head - f(x^n)

0:22:25.380,0:22:27.780
他們都算 y^n\head - f(x^n)

0:22:27.780,0:22:29.340
唯一不一樣的地方是

0:22:29.340,0:22:32.840
Logistic Regression 你的 target 一定是 0 或 1

0:22:32.840,0:22:34.480
你的 target 一定是 0 或 1

0:22:34.480,0:22:37.640
你的這個 f 呢。一定是介於 0~1 之間

0:22:37.640,0:22:40.460
但是如果是 Linear Regression 的話，你的 target y\head

0:22:40.460,0:22:43.600
它可以是任何 real number

0:22:43.600,0:22:46.400
而你這個 output 也可以是任何 value

0:22:46.400,0:22:50.000
但是他們 update 的這個方式，是一樣的

0:22:50.000,0:22:52.620
作業二我們需要做 Logistic Regression

0:22:52.620,0:22:55.720
你甚至八成都不用改 code

0:22:55.720,0:22:58.760
秒做就可以把它做出來這樣子

0:22:58.900,0:23:00.980
大家作業一做的還順利嗎？

0:23:00.980,0:23:04.680
我相信你應該是遇到了種種特別的問題啦

0:23:04.680,0:23:07.540
比如說，如果你在做 Gradient Descent 的話

0:23:07.540,0:23:08.660
你會發現說

0:23:08.660,0:23:10.940
雖然教科書上跟你講 Gradient Descent 的時候

0:23:10.940,0:23:13.000
你對他是不屑一顧的

0:23:13.000,0:23:16.900
然後，你覺得說一個 complex 的這個 surface

0:23:16.900,0:23:19.940
我應該用 Gradient Descent 可以輕易地找到它的最佳解

0:23:19.940,0:23:21.980
但是，你會發現說實際上做起來

0:23:21.980,0:23:23.620
是沒有那麼容易的

0:23:23.680,0:23:28.280
對不對，我其實可以出一個那種教科書上的問題
讓你們來做 Linear Regression

0:23:28.280,0:23:31.600
但是，我們用真實的 example 就可以讓你知道說

0:23:31.600,0:23:34.580
在真實的世界，你會碰到怎麼樣的問題

0:23:34.580,0:23:37.200
事實上，因為今天我們做的是 Linear Regression

0:23:37.200,0:23:40.060
我看你八成可以用這個

0:23:40.060,0:23:44.680
解 least square error 的方式，偷偷找一下最佳解

0:23:44.700,0:23:48.100
然後再從那個最佳解當作 initialization 開始找對吧

0:23:48.100,0:23:50.120
大家聽得懂我在說甚麼嗎？

0:23:50.120,0:23:50.740
呵呵

0:23:50.740,0:23:53.280
有這麼做的人舉手一下

0:23:53.280,0:23:55.960
沒有人這麼做，還是不敢舉手這樣子

0:23:57.340,0:24:00.560
要是我就這麼做

0:24:00.560,0:24:03.460
大家知道我意思嗎？

0:24:03.460,0:24:06.740
好，但是如果你做到 deep learning 的時候

0:24:06.740,0:24:07.940
你就不能這麼做啦

0:24:07.940,0:24:10.900
因為 deep learning 你沒有任何方法可以去

0:24:10.900,0:24:14.000
找它的最佳解，到時候你才會真正的卡翻

0:24:16.560,0:24:19.680
那在下課之前，我想要講一下

0:24:20.260,0:24:21.980
我們今天的計畫是這樣子啦

0:24:21.980,0:24:24.660
我們講完 Logistic Regression 以後

0:24:24.660,0:24:26.140
我們就會進入 deep learning

0:24:26.140,0:24:29.700
然後等一下第三堂課，助教就會來講一下作業二

0:24:30.880,0:24:33.500
那我們現在要問的問題是這樣

0:24:33.500,0:24:36.800
為什麼 Logistic Regression 不能加 square error？

0:24:36.800,0:24:39.800
我為甚麼不能用 square error，
當然可以用 square error 阿

0:24:39.800,0:24:42.080
我們如果用 square error 的話會怎樣？

0:24:42.180,0:24:45.420
我們做 Logistic Regression 的時候，我們的式子長這樣

0:24:45.420,0:24:47.580
我當然可以做 square error 阿

0:24:47.580,0:24:50.580
我把我的 function 的 output 減掉 y^n\head 的平方

0:24:50.580,0:24:52.820
summation 起來當作我的 loss function

0:24:52.820,0:24:55.480
我一樣用 Gradient Descent 去 minimize 它

0:24:55.480,0:24:59.540
有什麼不可以呢？當然沒什麼不可以這樣子

0:24:59.980,0:25:02.940
如果我們算一下這個微分的話

0:25:02.940,0:25:05.640
你會發現說，如果我們把括號裡面

0:25:05.640,0:25:07.620
summation 後面這個式子

0:25:07.800,0:25:10.100
對 wi 做偏微分的話

0:25:10.100,0:25:12.560
它得到的結果呢，是這樣子

0:25:12.560,0:25:14.920
然後這個 2 呢，提到前面去

0:25:14.920,0:25:17.560
所以，2(f(x) - y\head)

0:25:17.560,0:25:20.500
然後，把 f(x) 對 z 做偏微分

0:25:20.500,0:25:22.960
把 w 對 z 做偏微分

0:25:22.960,0:25:25.000
把他們都乘起來

0:25:25.000,0:25:28.740
然後，這一項

0:25:30.340,0:25:34.700
欸，這個地方

0:25:34.770,0:25:37.100
沒有寫錯，好，就是

0:25:37.100,0:25:40.720
這一項，就是這一項

0:25:40.720,0:25:43.620
把 z 對 f(x) 做偏微分

0:25:43.620,0:25:45.780
因為 f(x) 是 sigmoid function

0:25:45.780,0:25:49.120
所以，做偏微分以後，就是 f(x) * (1 - f(x))

0:25:49.640,0:25:52.640
∂(z) / ∂(wi) 就是 xi

0:25:52.640,0:25:55.160
當然你可以

0:25:55.160,0:25:58.260
就用 Gradient Descent 去 update 你的參數

0:25:58.580,0:26:01.980
但是，你現在會發現你遇到一個問題

0:26:01.980,0:26:04.220
假設 y^n\head  = 1

0:26:04.220,0:26:07.420
假設第 n 筆 data 是 class 1

0:26:07.860,0:26:12.020
當我的 f(x) 已經等於 1 的時候

0:26:12.020,0:26:16.580
當我的第 n 筆 data 是 class 1

0:26:16.580,0:26:18.460
而我的 f(x) 已經等於 1 的時候

0:26:18.520,0:26:20.780
我已經達到 perfect 的狀態了

0:26:20.780,0:26:23.280
這個時候，沒有甚麼問題

0:26:23.280,0:26:26.000
因為你 f(x) = 1、y^n\head = 1 的時候

0:26:26.000,0:26:30.040
你把這兩個數值代進這個 function 裡面

0:26:30.040,0:26:32.500
你會發現說，至少這一項

0:26:32.500,0:26:34.840
f(x) - y^n\head 是 0

0:26:34.840,0:26:37.400
所以你的微分，會變成 0

0:26:37.400,0:26:39.440
這件事情是很合理

0:26:39.440,0:26:41.620
但是，如果今天是另一個狀況

0:26:41.620,0:26:43.620
f(x^n) = 0

0:26:44.200,0:26:47.180
意味著說，你現在離你的目標

0:26:47.180,0:26:49.240
仍然非常的遠

0:26:49.240,0:26:52.480
因為你的目標是希望 f(x^n) 的目標是 1

0:26:52.480,0:26:53.960
但你現在 output 是 0

0:26:53.960,0:26:55.800
你離目標還很遠

0:26:55.800,0:27:00.080
但是，如果你把這個式子代到這裡面的話

0:27:00.080,0:27:03.460
你會發現說，這邊有乘一個 f(x^n)，而 f(x^n)  = 0

0:27:03.460,0:27:07.000
這時候，你會變成你微分的結果算出來也是 0

0:27:07.000,0:27:09.900
所以，如果你離目標很近

0:27:09.900,0:27:12.560
微分算出來是 0 沒有問題，但是，如果你離目標很遠

0:27:12.560,0:27:14.640
微分算出來也是 0

0:27:14.640,0:27:16.840
這個是 class 1 的例子

0:27:16.840,0:27:20.120
如果我們舉 class 2 的例子，看起來結果也是一樣

0:27:20.120,0:27:22.100
假設 y^n\head  = 0

0:27:22.640,0:27:25.120
假設現在距離目標很遠

0:27:25.120,0:27:27.600
假設距離目標很遠的時候

0:27:27.600,0:27:30.600
f(x^n) = 1 你代進去，至少最後這個式子是 0

0:27:30.600,0:27:32.300
你微分算出來也是 0

0:27:32.300,0:27:35.180
距離目標很近的時候，微分算出來也是 0

0:27:35.180,0:27:37.020
這會造成甚麼問題呢？

0:27:37.020,0:27:40.140
如果我們把

0:27:40.140,0:27:44.940
參數的變化對 total loss 作圖的話

0:27:44.940,0:27:47.580
你會發現說，如果你選擇 cross entropy

0:27:47.580,0:27:50.740
跟你選擇 square error

0:27:50.740,0:27:54.260
參數的變化跟 loss 的變化

0:27:54.260,0:27:55.960
串起來是這樣子的

0:27:55.960,0:27:58.900
黑色的是 cross entropy

0:27:58.900,0:28:01.940
紅色的是 square error

0:28:01.940,0:28:07.480
我們剛才講說 cross entropy 在距離目標很近的地方

0:28:07.540,0:28:11.080
假設現在中心最低的點就是

0:28:11.080,0:28:13.760
距離目標很近的地方

0:28:13.760,0:28:16.300
你的微分值就很小

0:28:16.300,0:28:19.260
但是，距離目標很遠的地方

0:28:19.260,0:28:21.280
你的微分值也是很小的

0:28:21.280,0:28:23.740
所以，在距離目標很遠的地方呢

0:28:23.740,0:28:26.800
你會非常的平坦

0:28:26.800,0:28:28.560
這會造成甚麼問題呢？

0:28:28.560,0:28:31.800
如果是 cross entropy 的話

0:28:31.800,0:28:35.160
你距離目標越遠，你的微分值就越大

0:28:35.160,0:28:37.700
那沒有問題，所以你距離目標越遠

0:28:37.700,0:28:40.660
你參數 update 的時候就越快

0:28:40.660,0:28:43.100
你參數更新的速度就越快

0:28:43.180,0:28:45.840
你參數 update 的時候，變化量就越大

0:28:45.840,0:28:48.620
這個沒有問題，距離你的目標越遠

0:28:48.620,0:28:50.960
你的步伐當然要踏越大一點

0:28:50.960,0:28:54.080
但是，如果你選 square error 的話，你就會很卡

0:28:54.080,0:28:56.760
因為，當你距離目標遠的時候

0:28:56.840,0:29:00.740
你的微分是非常非常小的

0:29:00.740,0:29:03.480
就變成說，你離目標遠的時候

0:29:03.480,0:29:07.500
你移動的速度是非常慢

0:29:07.500,0:29:10.400
所以，如果你用隨機

0:29:10.400,0:29:13.600
你 random 找一個初始值

0:29:13.600,0:29:16.720
那通常你離目標的距離，是非常遠的

0:29:16.720,0:29:20.260
那如果你今天是用 square error

0:29:20.260,0:29:21.920
你其實可以自己在作業裡面試試看

0:29:21.920,0:29:24.120
如果你用 square error，你選一個起始值

0:29:24.120,0:29:26.160
你算出來的微分很小，你一開始就卡住了

0:29:26.160,0:29:29.060
它的參數都不 update，你就永遠卡在那邊

0:29:29.060,0:29:32.300
它的參數 update 的速度很慢

0:29:32.300,0:29:35.060
你等了好幾個小時了，它都跑不出來這樣

0:29:35.060,0:29:37.280
那你可能會想說

0:29:37.400,0:29:39.920
那我們可以說看到這個

0:29:39.920,0:29:43.260
微分值很小的時候，就把它的

0:29:43.320,0:29:45.500
那個 learning rate 設大一點阿

0:29:45.500,0:29:47.180
可使問題是微分值很小的時候

0:29:47.180,0:29:49.960
你也有可能距離你的目標很近阿

0:29:49.960,0:29:51.360
如果距離目標很近的時候

0:29:51.360,0:29:53.620
這個時候你應該把它的微分值設小一點

0:29:53.620,0:29:56.000
但是，你現在搞不清楚說

0:29:56.000,0:29:59.680
到底 Gradient 小的時候，微分值算出來小的時候

0:29:59.680,0:30:02.960
你是距離目標很近，還是距離目標很遠

0:30:02.960,0:30:04.320
因為做 Gradient Descent 的時候

0:30:04.320,0:30:06.260
你是在玩世紀帝國這個遊戲

0:30:06.260,0:30:08.860
你不知道你距離目標是很近，還是很遠

0:30:08.860,0:30:09.960
所以你就會卡翻了

0:30:09.960,0:30:12.420
不知道你的 learning rate 應該設大還是設小

0:30:12.420,0:30:14.500
所以你選 square error

0:30:14.500,0:30:16.140
在實做上，你當然可以這麼做

0:30:16.140,0:30:17.460
那你可以在作業裡面試試看

0:30:17.460,0:30:20.320
你是不容易得到好的結果的

0:30:20.320,0:30:24.260
用 cross entropy 可以讓你的 training 順很多

0:30:24.260,0:30:27.080
我們在這邊呢，休息 10 分鐘

0:30:29.400,0:31:19.620
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:31:21.060,0:31:23.320
我們來上課吧

0:31:23.320,0:31:25.820
那我們接下來要講的是

0:31:25.820,0:31:28.780
這個 Logistic Regression 的方法阿

0:31:28.780,0:31:32.440
我們稱它為 discriminative 的方法

0:31:32.440,0:31:36.460
而剛才呢，我們用 Gaussian

0:31:36.540,0:31:40.320
來描述 posterior probability 這件事呢

0:31:40.320,0:31:43.220
我們稱之為 generative 的方法

0:31:43.220,0:31:47.120
實際上，他們的 function

0:31:47.380,0:31:51.820
他們的這個 model、function set 呢，是一模一樣的

0:31:51.820,0:31:56.740
不管你是用我們在這份投影片講的 Logistic Regression

0:31:56.740,0:32:00.860
還是前一份投影片講的機率模型

0:32:00.860,0:32:03.220
只要你在做機率模型的時候

0:32:03.220,0:32:05.840
你把 covariance matrix 設成是 share 的

0:32:05.840,0:32:08.920
那他們的 model 其實是一模一樣的

0:32:08.920,0:32:12.560
都是 σ(w * x + b)

0:32:12.560,0:32:17.620
那你可以找不同的 w 跟不同的 b，
就得到不同的 function

0:32:18.220,0:32:20.980
那我們

0:32:22.680,0:32:27.300
我們可以直接去把 w 跟 b 找出來

0:32:27.300,0:32:30.560
如果你今天是用 Logistic Linear Regression 的話

0:32:30.560,0:32:33.380
你可以直接把 w 跟 b 找出來

0:32:33.440,0:32:35.440
就用 Gradient Descent 的方法

0:32:35.440,0:32:38.400
如果今天是 generative model 的話

0:32:38.440,0:32:40.640
那首先呢，我們會去算

0:32:40.640,0:32:45.300
μ^1, μ^2 跟 Σ 的 inverse

0:32:45.300,0:32:47.340
然後呢，我們就去把

0:32:47.340,0:32:51.060
我們一樣把 w 算出來、把 b 算出來

0:32:51.060,0:32:56.180
你算出 μ^1, μ^2 跟 covariance matrix

0:32:56.180,0:32:59.620
接下來呢，你就把這些項代到

0:32:59.620,0:33:01.980
這裡面，代到這裡面

0:33:01.980,0:33:05.820
這邊這個 Σ^1, Σ^2 呢，應該都是等於 Σ

0:33:05.820,0:33:07.420
這邊應該都把它代成 Σ

0:33:07.420,0:33:11.600
那就把它算出來就可以得到 w 跟 b

0:33:11.600,0:33:13.460
現在的問題來了

0:33:13.460,0:33:18.260
如果我們比較左邊跟右邊求 w 跟求 b 的方法

0:33:20.960,0:33:25.520
我們找出來的 w 跟 b 會是同一組嗎？

0:33:25.520,0:33:27.760
會是同一組嗎？

0:33:28.380,0:33:31.340
你覺得它是同一組的同學舉手一下

0:33:32.100,0:33:35.780
你覺得它是不同的同學舉手一下

0:33:35.780,0:33:37.080
好，謝謝，手放下

0:33:37.080,0:33:39.140
多數同學覺得它是不同

0:33:39.140,0:33:42.680
沒錯，你找出來的結果不會是一樣的

0:33:42.680,0:33:46.960
所以，今天當我們用 Logistic Regression

0:33:46.960,0:33:51.360
還是用剛才的 probabilistic 的 generative model

0:33:51.360,0:33:55.180
我們用的其實是同一個 model

0:33:55.180,0:33:57.360
其實是同一個 function set

0:33:57.360,0:33:59.540
也就是我們的 function 的那個 pool 阿

0:33:59.540,0:34:01.900
我們可以挑的那個 function 的 candidate 阿

0:34:01.900,0:34:03.480
其實是同一個 set

0:34:03.480,0:34:07.580
但是 ，因為我們做了不同的假設

0:34:07.580,0:34:10.140
所以，我們最後找出來的

0:34:10.140,0:34:13.680
根據同一組 training data 找出來的參數

0:34:13.680,0:34:14.920
會是不一樣的

0:34:14.920,0:34:17.340
在這個 Logistic Regression 裡面

0:34:17.340,0:34:19.480
其實我們就沒有做任何假設

0:34:19.480,0:34:24.280
我們沒有對這個 probability distribution 有任何的描述

0:34:24.280,0:34:27.200
我們就是單純去找一個 w 跟 b

0:34:27.200,0:34:30.720
那在 generative model 裡面

0:34:30.720,0:34:33.960
我們對 probability distribution 是有假設的

0:34:33.960,0:34:36.320
比如說，假設它是 Gaussian

0:34:36.320,0:34:38.600
假設它是 Bernoulli

0:34:38.600,0:34:43.300
假設它是不是 Naive Bayes...... 等等

0:34:43.300,0:34:45.420
我們做了種種的假設

0:34:45.420,0:34:49.580
根據這些假設，我們可以找到另外一組 w 跟 b

0:34:49.580,0:34:55.320
左右兩邊找出來的 w 跟 b 呢，不會是同一組

0:34:55.320,0:35:01.040
那問題就是，哪一組找出來的 w 跟 b 是比較好的呢？

0:35:01.040,0:35:02.440
如果我們比較

0:35:02.840,0:35:06.980
Generative model 跟 Discriminative model 的話

0:35:07.020,0:35:09.100
那我們先看一下我們之前講的

0:35:09.100,0:35:12.240
defense 跟 special defense 的例子

0:35:12.240,0:35:14.420
如果用 generative model 的話

0:35:14.420,0:35:16.360
我們的這兩個 class

0:35:16.360,0:35:21.020
就藍色的是水系的神奇寶貝，紅色的是一般系的寶可夢

0:35:21.020,0:35:23.140
他們之間 boundary 是這一條

0:35:23.400,0:35:25.760
如果你是用 Logistic Regression 的話呢

0:35:25.760,0:35:28.020
你找出來的 boundary 是這一條

0:35:28.140,0:35:31.280
其實從這個結果上，你很難看出來說

0:35:31.280,0:35:32.900
誰比較好啦

0:35:33.060,0:35:34.940
但是，如果我們比較說

0:35:34.940,0:35:38.760
我們都用 7 個 feature 的這個 class

0:35:38.760,0:35:41.960
我們會發現說，如果用 generative model 的話

0:35:41.960,0:35:45.600
我們剛才說，我們得到的正確率呢，是 73%

0:35:45.600,0:35:48.520
如果是用 discriminative model 的話

0:35:48.620,0:35:52.620
在同樣的 data set 上面，我們只是用

0:35:52.620,0:35:56.480
不同的假設，所以找了不同的 w 跟 b

0:35:56.480,0:35:58.340
但是我們找出來的結果比較好的

0:35:58.420,0:36:01.540
我們找出來的正確率有 79% 這樣

0:36:01.540,0:36:05.160
那我相信在文獻上，會常常聽到有人說

0:36:05.160,0:36:09.640
discriminative model 會比 generative model

0:36:09.640,0:36:12.220
常常會 performance 的更好

0:36:12.220,0:36:13.680
為甚麼會這樣呢？

0:36:13.680,0:36:16.600
我們來舉一個 toy 的 example

0:36:16.600,0:36:20.280
現在假設呢，你有一筆 training data

0:36:20.280,0:36:22.320
你有兩個 class

0:36:22.320,0:36:24.360
那你這筆 training data 裡面呢

0:36:24.360,0:36:25.780
總共有

0:36:27.650,0:36:31.280
每一筆 data 呢，有兩個 feature

0:36:31.280,0:36:32.610
然後呢

0:36:32.660,0:36:39.260
你總共有 1 + 4 + 4 + 4，總共有 13 筆 data

0:36:39.260,0:36:42.740
第一筆 data 是兩個 feature 的 value 都是 1

0:36:42.740,0:36:44.980
接下來呢，有 4 筆 data

0:36:44.980,0:36:47.360
第一個 feature 是 1，第二個 feature 是 0

0:36:47.360,0:36:50.100
接下來 4 筆 data，是第一個 feature 是 0，
第二個 feature 是 1

0:36:50.100,0:36:52.740
接下來 4 筆 data，是兩個 feature 都是 0

0:36:53.020,0:36:57.120
然後呢，我們給第一筆 data 的 label 是 1

0:36:57.120,0:37:01.060
我們給剩下 12 筆 data 的 label 呢

0:37:01.060,0:37:03.900
都是 class 2

0:37:03.900,0:37:09.240
那假設你現在不做機器學習，做人類的學習

0:37:09.240,0:37:11.120
給你一個 testing data

0:37:11.120,0:37:15.260
它的兩個 feature 都是 1，
你覺得它是 class 1 還是 class 2 呢？

0:37:16.220,0:37:18.160
我們來問一下大家的意見吧

0:37:18.160,0:37:20.540
如果你覺得它是 class 1 的同學，舉手一下

0:37:21.480,0:37:25.000
手放下，你覺得它是 class 2 的同學，舉手一下

0:37:25.000,0:37:28.540
沒有人覺得是 class 2，大家都覺得是 class 1

0:37:28.540,0:37:31.660
那如果我們來問一下 Naive Bayes

0:37:31.660,0:37:35.160
它覺得是 class 1 還是 class 2，它會怎麼說呢？

0:37:35.160,0:37:38.680
所謂的 Naive Bayes 就是，我們假設

0:37:38.680,0:37:42.260
所有的 feature，它產生的機率是 independent

0:37:42.260,0:37:48.560
所以 P(x|Ci)，P of x 從某一個 class 產生出來的機率

0:37:48.560,0:37:51.840
等於從某一個 class 產生 x1 的機率

0:37:51.840,0:37:55.460
乘上從某一個 class 產生 x2 的機率

0:37:55.920,0:38:00.720
那我們用 Naive Bayes 來算一下

0:38:00.720,0:38:03.920
首先，算一下 prior 的 probability

0:38:04.520,0:38:06.840
class 1 它出現的 probability 是多少？

0:38:06.840,0:38:09.640
總共 13 筆 data，只 sample 到一次是 class 1

0:38:09.640,0:38:11.220
所以是 1/13

0:38:11.220,0:38:12.960
class 2 的機率是多少呢？

0:38:12.960,0:38:16.380
總共 13 筆 data，有 12 筆是 class 2

0:38:16.380,0:38:19.140
所以它是 12/13，它比較多

0:38:19.140,0:38:21.760
那接下來呢，我們算說

0:38:21.760,0:38:25.660
在 class 1 裡面，x1 = 1 的機率

0:38:25.660,0:38:30.180
在 class 1 裡面，x1 = 1 的機率 就是 1

0:38:30.180,0:38:34.780
在 class 2 裡面，x1 = 1 的機率也是 1

0:38:34.780,0:38:35.860
class 1 就這筆 data 嘛

0:38:35.860,0:38:37.980
它 x1 是 1，x2 是 1

0:38:37.980,0:38:40.340
所以，如果你用機率來統計的話

0:38:40.340,0:38:44.080
那在 class 1 裡面，x1 = 1 的機率是 1

0:38:44.080,0:38:47.240
在 class 1 裡面，x2 = 1 的機率也是 1

0:38:47.240,0:38:49.040
接下來我們看

0:38:49.740,0:38:53.640
我發現我犯了一個錯，這邊應該是 C2，不好意思

0:38:53.640,0:38:56.400
這邊應該是 C2，這邊也應該是 C2

0:38:56.400,0:39:00.160
如果我們看 class 2 的話

0:39:00.160,0:39:03.720
如果我們看右邊這 12 筆 class 2 的 data

0:39:04.460,0:39:08.060
在 class 2 裡面，x1 = 1的機率是多少呢？

0:39:08.060,0:39:14.020
是 1/3 對不對，只有 1/3 的 data 是 x1(老師講錯) = 1 的

0:39:14.300,0:39:16.600
是 x1 = 1 的

0:39:16.600,0:39:20.580
那再來我們看 x2

0:39:20.580,0:39:23.360
x2= 1 的機率在 class 2 裡面有多少呢？

0:39:23.360,0:39:25.520
在 class 2 裡面只有 1/3 的 data

0:39:25.520,0:39:29.460
x2 = 1，所以它的機率是 1/3

0:39:29.460,0:39:32.380
如果我們把這些機率通通算出來以後

0:39:32.380,0:39:35.060
給你一個 testing data

0:39:35.060,0:39:39.120
你就可以去初估測它是來自 class 1 的機率

0:39:39.240,0:39:41.300
你就可以初估測它是來自 class 2 的機率

0:39:41.300,0:39:44.080
我們就算這筆 training data x 呢

0:39:44.080,0:39:45.900
它來自 class 1 的機率是多少

0:39:45.900,0:39:48.040
那你就把它代到這個

0:39:48.040,0:39:53.260
Bayesian 的 function 裡面，算一下

0:39:53.680,0:39:56.420
C1 的 prior probability 是 1/3

0:39:56.420,0:40:01.000
P(x|C1) 的機率是 1*1

0:40:01.000,0:40:02.020
甚麼意思呢？

0:40:02.020,0:40:05.340
這筆 data x，從 C1 裡面 generate 出來的機率

0:40:05.340,0:40:09.140
等於這個機率乘上這個機率

0:40:09.660,0:40:13.400
下面這項你算過，這是 1/13，這是 1*1

0:40:13.520,0:40:16.640
那這一項呢，P(C2) 是 12/13

0:40:16.640,0:40:20.740
P(x|C2)，從 C2 裡面 sample 出

0:40:20.740,0:40:24.620
根據 C2 的 distribution ，
sample 出這筆 data 的機率是多少呢？

0:40:24.760,0:40:27.020
是 1/3 * 1/3

0:40:27.020,0:40:29.780
因為 x1 = 1 的機率在 C2 裡面是 1/3

0:40:29.780,0:40:35.520
x2 = 1 的機率在 C2 裡面是 1/3

0:40:35.660,0:40:38.820
所以這一項是 1/3 * 1/3

0:40:38.820,0:40:41.880
如果你實際去做一下運算

0:40:41.880,0:40:45.160
實際算一發就知道說

0:40:45.160,0:40:48.740
這一個是小於 0.5 的

0:40:48.740,0:40:51.300
所以對 Naive Bayes 來說

0:40:51.300,0:40:55.040
給它這樣子的 training data

0:40:55.040,0:40:58.040
它認為這一筆 testing data

0:40:58.040,0:41:00.640
應該是屬於 class 2

0:41:00.640,0:41:02.740
而不是 class 1

0:41:02.740,0:41:09.040
所以這跟大家的直覺比起來，是相反的

0:41:09.040,0:41:10.260
是相反的

0:41:10.260,0:41:13.940
其實我們很難知道說

0:41:13.940,0:41:16.800
怎麼知道說這筆 data 的產生到底是來自

0:41:16.800,0:41:18.900
class 1 還是 class 2

0:41:18.900,0:41:20.260
比較合理的假設

0:41:20.260,0:41:22.280
你會覺得說，因為 class 1 裡面

0:41:22.300,0:41:25.820
x1 和 x2 通通都是等於 1 的

0:41:25.820,0:41:29.380
所以，這筆 data 應該是要來自 class 1 才對吧

0:41:29.380,0:41:31.320
可是對 Naive Bayes 來說

0:41:31.320,0:41:35.240
它不考慮不同 dimension 之間的 correlation

0:41:35.240,0:41:37.400
所以，對 Naive Bayes 來說

0:41:37.400,0:41:39.820
這兩個 dimension 是 independent 所產生的

0:41:39.820,0:41:43.220
在 class 2 裡面，之所以沒有

0:41:43.220,0:41:46.920
sample 到這樣的 data，觀察到這樣的 data

0:41:46.920,0:41:48.940
是因為你 sample 的不夠多

0:41:48.940,0:41:50.460
如果你 sample 的夠多

0:41:50.460,0:41:54.920
搞不好就有都是 1 的 data

0:41:56.020,0:41:57.440
就都是 1 的 data

0:41:57.440,0:41:59.540
也是有機率被產生出來的

0:41:59.540,0:42:02.180
只是，因為我們 data 不夠多

0:42:02.280,0:42:05.040
所以，沒有觀察到這件事而已

0:42:05.600,0:42:08.840
所以，今天這個 generative model

0:42:08.840,0:42:12.680
跟 discriminative model 的差別就在於

0:42:12.720,0:42:14.840
這個 generative model

0:42:14.840,0:42:17.300
它有做了某些假設

0:42:17.420,0:42:20.880
假設你的 data 來自於一個機率模型

0:42:20.880,0:42:22.580
它做了某些假設

0:42:22.580,0:42:26.200
也就是說，它其實做了腦補這件事情

0:42:26.200,0:42:28.300
腦補是什麼，大家知道嗎？

0:42:28.300,0:42:30.720
就是，如果你看了一部動漫

0:42:30.720,0:42:33.320
那裡面沒有發生某一些事情

0:42:33.320,0:42:37.340
比如說，兩個男女主角其實沒有在一起

0:42:37.400,0:42:40.560
但是，你心裡想像他們是在一起，這個就是腦補

0:42:41.420,0:42:45.680
所以，這個 generative model 它做的事情

0:42:45.680,0:42:47.120
就是腦補

0:42:47.160,0:42:49.540
如果，我們在 data 裡面明明沒有觀察到

0:42:49.540,0:42:51.180
在 class 2 裡面

0:42:51.180,0:42:54.460
有都是 1 的這樣的 example 出現

0:42:54.460,0:42:58.020
但是，對 Naive Bayes 來說

0:42:58.020,0:43:00.200
它想像它看到了這件事情

0:43:00.300,0:43:04.720
所以，它就會做出一個跟我們人類直覺想法不一樣的

0:43:04.720,0:43:06.760
判斷的結果

0:43:06.760,0:43:10.120
那到底腦補是不是一件好的事情呢？

0:43:10.120,0:43:12.540
通常腦補可能不是一件好的事情

0:43:12.540,0:43:14.540
因為你的 data 沒有告訴你這一件事情

0:43:14.540,0:43:17.040
你卻腦補出這樣的結果

0:43:17.040,0:43:20.360
但是，如果今天在 data 很少的情況下

0:43:20.360,0:43:22.760
腦補有時候也是有用的

0:43:22.760,0:43:27.080
如果你得到的情報很少，腦補可以給你更多的情報

0:43:27.080,0:43:28.480
所以，其實

0:43:28.860,0:43:33.040
discriminative model 並不是在所有的情況下

0:43:33.040,0:43:35.840
都可以贏過 generative model 

0:43:35.840,0:43:38.640
有些時候 generative model 也是有優勢的

0:43:38.640,0:43:40.120
甚麼時候會有優勢呢？

0:43:40.120,0:43:42.600
如果你今天的 training data 很少

0:43:42.600,0:43:46.020
你可以比較說，在同一個 problem 下

0:43:46.020,0:43:50.420
你給 discriminative model 和 generative model 

0:43:50.420,0:43:52.600
不同量的 training data

0:43:52.600,0:43:56.260
你會發現說，這個 discriminative model

0:43:56.260,0:43:58.120
因為它完全沒有做任何假設

0:43:58.120,0:44:00.080
它是看著 data 說話

0:44:00.080,0:44:04.560
所以它的 performance 的變化量，
會受你的 data 量影響很大

0:44:04.560,0:44:07.600
假設，現在由左到右是 data 越來越多

0:44:07.600,0:44:10.900
然後，縱軸是 error rate

0:44:10.900,0:44:14.380
discriminative model 它受到 data 影響很大

0:44:14.380,0:44:18.420
所以 data 越來越多，它的 error 就越來越小

0:44:18.420,0:44:21.320
如果你是看 generative model 的話

0:44:21.320,0:44:25.820
它受 data 的影響是比較小的

0:44:25.820,0:44:28.280
因為它有一個它自己的假設

0:44:28.280,0:44:32.580
它有時候會無視那個 data，而遵從它內心自己的假設

0:44:32.580,0:44:34.480
自己內心腦補的結果

0:44:34.480,0:44:36.340
所以如果你看 data 量

0:44:36.340,0:44:40.140
所以如果你看 data 量的影響的話，在 data 少的時候

0:44:40.140,0:44:44.520
generative model 有時候是可以
贏過 discriminative model 的

0:44:44.520,0:44:46.940
只有在 data 慢慢增加的時候

0:44:46.940,0:44:50.280
generative model 才會輸給 discriminative model

0:44:50.280,0:44:52.180
這個其實是 case by case

0:44:52.180,0:44:58.680
你可以在作業裡面做做實驗，
看看你能不能觀察到這樣的現象

0:44:59.560,0:45:01.940
那有時候 generative model 是有用的

0:45:01.940,0:45:04.160
可能是

0:45:04.160,0:45:06.980
你今天的 data 是 noise 的

0:45:06.980,0:45:09.580
你的 label 本身就有問題

0:45:09.580,0:45:12.140
因為你的 label 本身就有問題

0:45:12.140,0:45:14.220
你自己做一些腦補、做一些假設

0:45:14.220,0:45:17.960
反而可以把 data 裡面有問題的部分呢

0:45:17.960,0:45:19.600
忽視掉

0:45:20.300,0:45:23.500
那我們在做 discriminative model 的時候

0:45:23.500,0:45:27.540
我們是直接假設一個 posterior probability

0:45:27.540,0:45:30.320
然後去找 posterior probability 裡面的參數

0:45:30.320,0:45:32.820
但是我們在做 generative model 的時候

0:45:32.820,0:45:36.200
我們把整個 formulation 裡面拆成

0:45:36.200,0:45:39.900
prior 跟 class-dependent 的 probability 這兩項，對不對

0:45:39.900,0:45:43.160
那這樣做有時候是有好處的

0:45:43.160,0:45:46.880
如果你把你的整個 function 拆成

0:45:47.280,0:45:52.700
prior 跟 class-dependent 的 probability 這兩項的話

0:45:52.700,0:45:54.360
有時候會有幫助

0:45:54.360,0:45:57.360
因為，這個 prior 跟 class-dependent 的 probability 

0:45:57.400,0:46:01.460
它們可以是來自於不同的來源

0:46:01.500,0:46:05.420
舉例來說，以語音辨識為例

0:46:05.420,0:46:08.280
大家可能都知道說呢

0:46:08.280,0:46:10.900
語音辨識現在都是用

0:46:10.900,0:46:14.240
neural network，它是一個discriminative 的方法

0:46:14.240,0:46:16.640
但事實上，整個語音辨識的系統

0:46:16.640,0:46:18.600
是一個 generative 的 system

0:46:18.600,0:46:20.520
DNN 只是其中一塊而已

0:46:20.520,0:46:23.160
所以說，該怎麼說呢

0:46:24.740,0:46:27.880
所以說，就全部都是用 DNN 這件事情呢

0:46:27.880,0:46:31.140
並不是那麼的精確，它整個 model 其實是 discriminative

0:46:31.140,0:46:32.580
為甚麼會這樣呢

0:46:32.580,0:46:34.960
因為它還是要去算一個 prior probability

0:46:34.960,0:46:38.300
因為 prior probability 是某一句話

0:46:38.300,0:46:39.940
被說出來的機率

0:46:39.940,0:46:42.860
而你要 estimate 某一句話被說出來的機率

0:46:42.860,0:46:46.100
你並不需要有聲音的 data

0:46:46.100,0:46:49.340
你只要去網路上爬很多很多的文字

0:46:49.340,0:46:52.260
你就可以計算某一段文字出現的機率

0:46:52.260,0:46:54.260
你不需要聲音的 data

0:46:54.260,0:46:56.220
這個就是 language model

0:46:56.220,0:46:57.740
所以在語音辨識裡面

0:46:57.740,0:47:00.000
我們整個 model 反而是 generative 的

0:47:00.000,0:47:04.080
因為你可以把 class-dependent 的部分跟 prior 的部分 

0:47:04.080,0:47:05.940
拆開來考慮，而 prior 的部分

0:47:05.940,0:47:07.520
你就用文字的 data 來處理

0:47:07.520,0:47:10.620
而 class-dependent 的部分，才需要聲音和文字的配合

0:47:10.620,0:47:13.660
這樣你可以把 prior estimate 的更精確

0:47:13.660,0:47:16.420
這一件事情在語音辨識裡面是很關鍵的

0:47:16.500,0:47:20.600
現在幾乎沒有辦法擺脫這個架構

0:47:21.400,0:47:25.760
那現在我們要講，我們剛剛舉的例子通通都是

0:47:25.760,0:47:28.460
只有兩個 class 的例子

0:47:28.460,0:47:30.340
接下來我們要講的是

0:47:30.340,0:47:34.180
如果是有兩個以上的 class，我們等一下舉的例子是

0:47:34.180,0:47:37.400
3 個 class 的，那應該要怎麼做呢？

0:47:37.400,0:47:41.660
那我們等一下就只講過程、不講原理

0:47:41.660,0:47:44.760
如果你想要知道原理的話

0:47:44.760,0:47:46.800
你可以看一下 Bishop 的教科書

0:47:46.940,0:47:49.900
那這個原理跟我們剛才從

0:47:49.900,0:47:54.640
只有兩個Class的情況呢

0:47:54.640,0:47:56.680
幾乎是一模一樣的

0:47:56.800,0:47:58.560
我相信你自己就可以推導出來

0:47:58.560,0:48:02.040
所以我就不想重複一個你覺得很 trivial 的東西

0:48:02.040,0:48:04.300
那我們就直接看它的操作怎麼做

0:48:04.300,0:48:06.900
假設我有三個 class

0:48:06.900,0:48:08.800
C1, C2 跟 C3

0:48:08.800,0:48:13.840
每一組 class 都有自己的 weight

0:48:13.840,0:48:15.840
和自己的 bias

0:48:15.840,0:48:21.280
這邊 w^1, w^2, w^3 分別代表 3 個 vector

0:48:21.280,0:48:25.200
b1, b2, b3 呢，代表 3 個 scalar

0:48:26.540,0:48:27.880
那接下來呢

0:48:27.880,0:48:31.540
input  一個 x，這個是你要分類的對象

0:48:31.620,0:48:34.640
你把 x 跟 w^1 做 inner product 加上 b1

0:48:34.640,0:48:37.060
x 跟 w^2 做 inner product 加上 b2

0:48:37.060,0:48:40.200
x 跟 w^3 做 inner product 加上 b3

0:48:40.200,0:48:42.960
你得到 z1, z2 跟 z3

0:48:42.960,0:48:47.440
這個 z1, z2 跟 z3 呢，它可以是任何值

0:48:47.440,0:48:51.640
它可以是負無窮大到正無窮大的任何值

0:48:51.840,0:48:56.740
接下來呢，我們把 z1, z2, z3 

0:48:56.820,0:48:59.620
丟進一個 Softmax 的 function

0:48:59.620,0:49:02.900
這個 Softmax function 它做的事情是這樣

0:49:02.900,0:49:06.600
把 z1, z2, z3 都取 exponential

0:49:06.720,0:49:10.320
得到 e^(z1), e^(z2), e^(z3)

0:49:10.320,0:49:14.300
接下來，把 e^(z1), e^(z2), e^(z3)

0:49:14.300,0:49:16.460
summation 起來

0:49:16.570,0:49:19.340
你得到它們的 total sum

0:49:19.340,0:49:23.020
然後你再把這個 total sum 分別除掉這 3 項

0:49:23.020,0:49:25.100
把 total sum 分別除掉這 3 項

0:49:25.440,0:49:29.540
得到 Softmax function 的 output, y1, y2 跟 y3

0:49:29.540,0:49:33.280
如果覺得有一點複雜的話，我舉一個數字

0:49:33.280,0:49:38.320
假設 z1 = 3, z2 = 1, z3 = -3

0:49:39.360,0:49:40.780
做完 exponential 以後

0:49:40.780,0:49:43.200
e^3 是很大的，是 20

0:49:43.200,0:49:44.980
e^1 是 2.7

0:49:45.100,0:49:47.900
e^(-3) 很小，是 0.05

0:49:47.900,0:49:52.200
接下來呢，你把這 3 項合起來

0:49:52.200,0:49:56.720
再分別去除掉，也就是做 normalization

0:49:56.720,0:50:00.360
那你得到的結果呢，20 就變成 0.88

0:50:00.360,0:50:04.380
2.7 就變成 0.12，0.05 就趨近於 0

0:50:04.380,0:50:07.580
那當你做完 Softmax 以後

0:50:07.580,0:50:11.420
原來 input z1, z2, z3 它可以是任何值

0:50:11.420,0:50:15.200
但是，做完 Softmax 以後，你的 output 會被限制住

0:50:15.200,0:50:18.960
第一個，你的 output 的值一定是介於 0~1 之間

0:50:18.960,0:50:20.660
首先，你 output 值一定是正的

0:50:20.660,0:50:24.580
不管你 z1, z2, z3 是正的還是負的，開exponential 以後

0:50:24.580,0:50:26.800
都變成是正的

0:50:28.100,0:50:30.980
那今天它的 total sum 

0:50:30.980,0:50:33.080
一定是 1，你 output 的和一定是 1

0:50:33.260,0:50:37.240
因為在這個地方做了一個 normalization，
所以你的 total sum 一定是 1

0:50:37.240,0:50:40.700
為甚麼這個東西叫 Softmax 呢？因為如果你做

0:50:40.700,0:50:43.920
如果是 max 的話，就是取最大值嘛

0:50:44.040,0:50:45.940
但是，你做 Softmax 的意思呢

0:50:45.940,0:50:50.380
是說你會對最大的值做強化

0:50:50.380,0:50:53.920
因為今天，你有取了 exponential

0:50:53.920,0:50:56.120
你取了 exponential 以後呢

0:50:56.120,0:50:59.460
大的值和小的值，他們之間的差距呢

0:50:59.460,0:51:01.560
會被拉得更開

0:51:01.560,0:51:05.980
強化它的值，這件事情呢，叫做 Softmax

0:51:05.980,0:51:10.460
那你就可以把這邊的每一個 y 呢

0:51:10.460,0:51:13.920
yi 呢，當作 input x

0:51:13.920,0:51:18.100
input 這個 x 是第 i 個 class 的 posterior probability

0:51:18.100,0:51:20.720
所以今天假設說，你 y1 是 0.88

0:51:20.720,0:51:25.980
也就是說，你 input x 屬於 class 1 的機率是 88%

0:51:25.980,0:51:27.960
屬於 class 2 的機率是 12%

0:51:27.960,0:51:30.380
屬於 class 3 的機率是趨近於 0

0:51:30.380,0:51:32.520
這個 Softmax 的 output

0:51:32.520,0:51:35.660
就是拿來當 z 的posterior probability

0:51:35.660,0:51:37.600
那你可能會問說

0:51:37.680,0:51:40.600
為甚麼會這樣呢？

0:51:40.600,0:51:42.640
事實上這件事情

0:51:42.920,0:51:44.980
是有辦法推導的

0:51:44.980,0:51:48.580
如果有人在外面演講，問我說為什麼是用 exponential

0:51:48.580,0:51:50.980
我就會回答說，你也可以用別的

0:51:50.980,0:51:53.980
因為我用別的，你也會問同樣的問題這樣子

0:51:55.860,0:52:00.260
但是，這個事情是有辦法講的

0:52:00.260,0:52:02.280
你可以去翻一下 Bishop 的教科書

0:52:02.280,0:52:03.880
這件事情是可以解釋的

0:52:03.880,0:52:06.240
如果，你今天有 3 個 class

0:52:06.240,0:52:10.360
假設這 3 個 class，通通都是 Gaussian distribution

0:52:10.360,0:52:13.700
他們共用同一個 covariance matrix

0:52:13.860,0:52:17.460
在這個情況下，你做一般推導以後

0:52:17.460,0:52:21.620
你得到的就會是這個 Softmax 的 function

0:52:21.620,0:52:25.840
這個就留給大家，自己做

0:52:26.660,0:52:29.440
如果你想要知道更多，你還可以 google 一個叫做

0:52:29.640,0:52:32.100
 maximum entropy 的東西

0:52:32.100,0:52:35.260
maximum entropy 也是一種 classify

0:52:35.260,0:52:38.300
但它其實跟 Logistic Regression 是一模一樣的東西

0:52:38.300,0:52:40.280
你只是換個名字而已

0:52:40.280,0:52:42.640
那它是從另外一個觀點呢

0:52:42.640,0:52:47.860
來切入為甚麼我們的 classifier 長這樣子

0:52:47.860,0:52:50.640
我們剛才說，我們可以從機率的觀點

0:52:50.640,0:52:54.860
假設我們用的是 Gaussian distribution，
經過一般推導以後

0:52:54.860,0:52:56.780
你可以得到 Softmax 的 function

0:52:56.780,0:52:58.780
那你可以從另外一個角度

0:52:58.780,0:53:01.780
從 information theory 的角度去推導

0:53:01.780,0:53:03.620
你會得到 Softmax 這個 function

0:53:03.840,0:53:06.080
這個就留給大家

0:53:06.080,0:53:10.000
自己去研究，google maximum entropy 你會找到答案

0:53:10.620,0:53:14.660
所以，我們複習一下剛才做的事情

0:53:14.680,0:53:16.660
你就有一個 x 當作 input

0:53:16.660,0:53:20.080
所以你乘上 3 組不同的 weight

0:53:20.080,0:53:24.200
加上 3 組不同的 bias，得到 3 個不同的 z

0:53:24.200,0:53:26.000
通過 Softmax function 

0:53:26.000,0:53:27.460
你就得到

0:53:27.460,0:53:32.220
y1, y2, y3 分別是這 3 個 class 的 posterior probability

0:53:32.220,0:53:35.800
可以把它合起來呢，當作是 y

0:53:36.000,0:53:39.620
那你在訓練呢，你要有一個 target

0:53:39.620,0:53:41.140
它的 target 是甚麼呢？

0:53:41.240,0:53:44.220
它的 target 是 y\head

0:53:44.220,0:53:48.220
每一維、你用 3 個 class，你 output 就是 3 維

0:53:48.220,0:53:52.800
這 3 維分別對應到 y1\head, y2\head 跟 y3\head

0:53:52.800,0:53:55.000
我們要去 minimize 的對象

0:53:55.000,0:53:58.840
是 y 所形成的這個 probability distribution

0:53:58.840,0:54:00.500
它是一個 probability distribution 嘛

0:54:00.500,0:54:02.620
在做完 Softmax 的時候

0:54:02.620,0:54:03.880
它就變成了

0:54:03.880,0:54:06.480
你把它當一個 probability distribution 來看待

0:54:06.560,0:54:12.260
你可以去計算這個 y 跟 y\head 他們之間的 cross entropy

0:54:12.260,0:54:15.040
它的這個 cross entropy 的式子呢

0:54:15.040,0:54:17.360
我就發現我寫錯了

0:54:17.360,0:54:21.440
這邊前面應該要有一個負號，不好意思

0:54:21.570,0:54:24.800
這前面應該要有一個負號

0:54:24.800,0:54:26.220
好

0:54:27.220,0:54:30.220
所以，這兩個 probability 的 cross entropy

0:54:30.220,0:54:33.200
它們的式子就是 y1\head

0:54:33.200,0:54:35.120
乘上 ln(y1)

0:54:35.120,0:54:38.120
y2\head * ln(y2) + y3\head * ln(y3)

0:54:38.120,0:54:40.220
前面再加一個負號

0:54:40.220,0:54:43.040
就是它們之間的 cross entropy

0:54:43.080,0:54:46.660
如果我們要計算 y 跟 y\head 的 cross entropy 的話

0:54:46.660,0:54:50.340
y\head 顯然也要是一個 probability distribution

0:54:50.340,0:54:52.140
我們才能夠算 cross entropy

0:54:52.140,0:54:54.140
怎麼算呢？

0:54:55.100,0:54:58.640
假設 x 是屬於 class 1 的話

0:54:58.640,0:55:01.740
在 training data 裡面，我們知道 x 是屬於 class 1 的話

0:55:01.740,0:55:04.740
它的 target 就是 [1 0 0]

0:55:04.740,0:55:08.460
如果是屬於 class 2 的話，它的 target 就是 [0 1 0]

0:55:08.460,0:55:12.840
如果是屬於 class 3 的話，它的 target 就是 [0 0 1]

0:55:12.840,0:55:15.000
我們之前有講過說

0:55:15.000,0:55:17.300
如果你設 class 1 的 target 是 1

0:55:17.300,0:55:19.200
class 2 的 target 是 2，class 3 的 target 是 3

0:55:19.200,0:55:20.460
這樣會有問題

0:55:20.500,0:55:23.280
你是假設說，1 跟 2 比較近、2 跟 3 比較近

0:55:23.280,0:55:26.080
1 跟 3 比較遠，這樣做會有問題

0:55:26.080,0:55:28.740
但是，如果你今天是

0:55:28.740,0:55:31.720
換一個假設

0:55:31.720,0:55:33.880
你今天是假設

0:55:33.880,0:55:37.140
如果 x 是屬於 class 1 的話，它的目標是 [1 0 0]

0:55:37.140,0:55:39.860
屬於 class 2是 [0 1 0]，屬於 class 3是 [0 0 1]

0:55:39.860,0:55:42.280
那你就不用假設 class 和 class 之間

0:55:42.280,0:55:44.680
誰跟誰比較近，誰跟誰比較遠

0:55:44.680,0:55:46.360
的問題這樣

0:55:46.740,0:55:50.020
至於這個式子，哪來的呢？

0:55:50.020,0:55:54.020
其實這個式子也是去 maximum likelihood

0:55:54.020,0:55:58.800
我們剛才在講這個 binary 的 case 的時候

0:55:58.800,0:56:02.020
我們講說，我們的這個 cross entropy 這個 function

0:56:02.020,0:56:04.420
minimize cross entropy 這件事情

0:56:04.420,0:56:07.940
其實是來自 maximize likelihood

0:56:07.940,0:56:10.460
那在有多個 head 的情況下

0:56:10.460,0:56:13.400
也是一模一樣的

0:56:13.440,0:56:15.940
它是一模一樣的

0:56:15.940,0:56:19.000
你就把 max likelihood 那個 function 列出來

0:56:19.000,0:56:22.660
那經過一番整理，你也會得到 minimize cross entropy

0:56:22.660,0:56:26.040
那這件事呢，就交給大家自己做

0:56:26.040,0:56:28.360
接下來，我要講的是

0:56:28.480,0:56:31.840
這個 Logistic Regression 阿，其實它是有

0:56:31.840,0:56:34.420
非常強的限制

0:56:34.420,0:56:36.220
怎麼樣的限制呢？

0:56:36.240,0:56:38.380
我們今天假設這樣一個 class

0:56:38.380,0:56:40.300
假設這樣一個 class

0:56:40.300,0:56:41.960
現在有 4 筆 data

0:56:42.120,0:56:44.660
它們每一筆 data 都有兩個 feature

0:56:44.660,0:56:47.020
那它們都是 binary 的 feature

0:56:47.020,0:56:49.060
那 class 2 呢，有兩筆 data

0:56:49.060,0:56:51.600
分別是 (0, 0)、(1, 1)

0:56:51.600,0:56:54.960
class 1 有兩筆 data，分別是 (0, 1)、(1, 0)

0:56:54.960,0:56:56.620
如果把它畫出來的話

0:56:56.740,0:56:59.220
class 1 的兩筆 data 是在這裡、在這裡

0:56:59.220,0:57:02.960
class 2 的兩筆 data 是在這跟這

0:57:02.960,0:57:07.080
如果我們想要用 Logistic Regression

0:57:07.080,0:57:10.100
對它做分類的話

0:57:10.100,0:57:12.700
我們能做到這件事情嗎？

0:57:12.700,0:57:14.540
我們能做到這件事情嗎？

0:57:14.540,0:57:17.440
你會發現說

0:57:17.440,0:57:19.740
這件事情

0:57:19.740,0:57:21.420
我們是辦不到的

0:57:21.420,0:57:24.760
Logistic Regression 的話，我們會希望說

0:57:24.760,0:57:27.680
對 Logistic Regression 的 output 而言

0:57:27.680,0:57:31.960
這兩個屬於 class 1 的 data，它的機率

0:57:31.960,0:57:34.020
要大於 0.5

0:57:34.020,0:57:36.400
另外兩個屬於 class 2 的 data

0:57:36.400,0:57:38.740
它的機率要小於 0.5

0:57:38.740,0:57:42.800
但這件事情，對 Logistic Regression 來說呢

0:57:42.800,0:57:45.300
它卡翻了，它沒有辦法做到這件事

0:57:45.300,0:57:48.880
因為 Logistic Regression 兩個 class 之間 boundary

0:57:48.880,0:57:51.000
就是一條直線

0:57:51.200,0:57:53.080
它的 boundary 就是一條直線

0:57:53.080,0:57:56.240
所以，你要分兩個 class 的時候，你只能在你的

0:57:56.330,0:57:59.840
feature 的平面上，畫一條直線

0:57:59.840,0:58:02.460
要馬畫這邊，要馬畫這邊

0:58:02.460,0:58:03.880
那不管你怎麼畫

0:58:03.880,0:58:05.260
你都沒有辦法把

0:58:05.260,0:58:07.740
紅色的放一邊、藍色的放一邊

0:58:07.740,0:58:10.720
不管你怎麼畫，你都沒有辦法把
紅色的放一邊、藍色的放一邊

0:58:10.720,0:58:12.760
這直線可以隨便亂畫、你可以調整

0:58:12.760,0:58:15.800
w 跟 b，你的 weight 跟 bias

0:58:16.060,0:58:19.920
使得你的某些 Regression，兩個 class 之間的 boundary

0:58:20.020,0:58:23.140
可以是任何樣，可以是這樣、是這樣，怎麼畫都可以

0:58:23.140,0:58:26.160
這個 boundary 是一條直線，怎麼畫都可以

0:58:26.160,0:58:28.000
但你永遠沒有辦法把

0:58:28.000,0:58:32.460
今天這個 example 的紅色點跟藍色點分成兩邊

0:58:33.300,0:58:35.200
怎麼辦呢？

0:58:35.200,0:58:38.060
假設你還是堅持要用 Logistic Regression 的話

0:58:38.060,0:58:40.620
有一招叫做 Feature Transformation

0:58:40.620,0:58:43.340
就是你剛才的 feature 定的不好

0:58:43.340,0:58:46.520
原來 x1, x2 的 feature 定的不好

0:58:46.520,0:58:50.260
我們可以做一些轉化以後

0:58:50.390,0:58:53.840
找一個比較好的 feature space

0:58:53.840,0:58:57.920
這一個比較好的 feature space，
讓 Logistic Regression 是可以處理

0:58:57.920,0:59:00.840
我們把 x1 跟 x2 呢

0:59:00.840,0:59:03.360
轉到另外一個 space 上面

0:59:03.360,0:59:05.940
轉到 x1' 跟 x2' 上面

0:59:05.940,0:59:07.480
x1' 是

0:59:07.480,0:59:10.100
怎麼做 feature transformation 是

0:59:10.100,0:59:12.860
這是很 heuristic and ad hoc的東西

0:59:12.930,0:59:15.260
想一個你喜歡的方式

0:59:15.260,0:59:20.400
舉例來說，我這邊定 x1' 就是某一個點到 (0,0) 的距離

0:59:21.020,0:59:25.680
x2' 就是某一個點到 (1,1) 的距離

0:59:26.020,0:59:32.980
如果我們把它畫出來的話

0:59:33.920,0:59:37.640
如果我們把它畫出來，我們先看

0:59:37.640,0:59:40.060
先看左下角這個點好了

0:59:40.060,0:59:42.320
如果我們看左下角這個點

0:59:42.320,0:59:44.720
它的 x1' 應該是 0

0:59:44.720,0:59:47.420
因為它跟 (0,0) 的距離就是 0

0:59:47.840,0:59:50.860
它跟 (1,1) 的距離，(0,0) 跟 (1,1) 的距離是

0:59:50.860,0:59:54.140
根號 2，所以 x2' 是 sqrt(2)

0:59:54.140,0:59:56.900
所以，經過這個 transformation，(0,0) 這個點

0:59:56.900,0:59:58.240
跑到這邊

0:59:58.330,1:00:01.680
那經過這個 transformation，(1,1) 這個點

1:00:01.680,1:00:03.340
跑到右下角

1:00:03.380,1:00:07.240
因為它跟 (0,0) 的距離是 sqrt(2)，跟 (1,1) 的距離是 0

1:00:07.240,1:00:09.480
經過這個 transformation，(0,1)

1:00:09.680,1:00:14.220
這邊又寫錯了，這邊應該是 (1,0)

1:00:14.220,1:00:19.160
(0,1) 和 (1,0)它們跟 (0,0) 和 (1,1) 之間的距離

1:00:19.160,1:00:20.280
都是一樣的

1:00:20.280,1:00:22.700
(0,1) 和 (0,0) 之間的距離是 1

1:00:22.740,1:00:25.060
(0,1) 和 (1,1) 之間的距離是 1

1:00:25.060,1:00:26.900
所以，經過這個 transform 以後

1:00:27.190,1:00:30.380
這兩個紅色的點會map，重疊在一起

1:00:30.380,1:00:32.150
都變成是 1

1:00:32.280,1:00:34.320
這個時候，對 Logistic Regression 來說

1:00:34.320,1:00:35.860
它可以處理這個問題了

1:00:35.860,1:00:38.600
因為它可以找一個 boundary，比如說，可能在這個地方

1:00:38.600,1:00:42.220
把藍色的點跟紅色的點分開

1:00:42.480,1:00:46.280
但是，麻煩的問題是這樣

1:00:46.280,1:00:47.940
麻煩的問題是

1:00:47.940,1:00:50.540
我們不知道怎麼做 feature transformation

1:00:50.540,1:00:52.600
如果我們花太多力氣在做 feature transformation

1:00:52.880,1:00:56.100
那就不是機器學習了，不是人工智慧了

1:00:56.100,1:00:57.700
就都是人的智慧了

1:00:57.700,1:01:00.760
所以，有時候我們不知道要怎麼找一個

1:01:00.760,1:01:02.340
好的 transformation

1:01:02.340,1:01:03.780
所以，我們會希望說

1:01:03.780,1:01:08.180
這個 transformation 是由機器自己產生的

1:01:08.180,1:01:11.840
怎麼讓機器自己產生這樣的 transformation 呢？

1:01:11.840,1:01:15.700
我們就把很多個 Logistic Regression 呢

1:01:15.700,1:01:17.340
cascade 起來

1:01:17.500,1:01:19.560
把很多 Logistic Regression 接起來

1:01:19.560,1:01:21.700
我們就可以做到這一的事情

1:01:21.700,1:01:26.000
假設 input 是 x1, x2

1:01:26.280,1:01:29.420
我們有一個 Logistic Regression 的 model

1:01:29.420,1:01:32.760
我們這邊就把 bias 省略掉，讓圖看起來比較簡單一點

1:01:32.760,1:01:35.260
這邊有一個 Logistic Regression 的 model

1:01:35.260,1:01:37.460
它的 x1 乘一個 weight，跟 x2 乘一個 weight

1:01:37.590,1:01:39.660
加起來以後得到 z 的 sigmoid function

1:01:39.660,1:01:42.160
它的 output 我們就說是

1:01:42.160,1:01:44.660
新的 transform 的第一維，x1'

1:01:45.680,1:01:48.580
我們有另外一個 Logistic Regression 的 model

1:01:48.580,1:01:52.340
它跟 x1 乘上一個 weight，對 x2 乘一個 weight，得到 z2

1:01:52.340,1:01:55.340
再通過 sigmoid function，得到 x2'

1:01:55.340,1:01:58.420
我們說它是 transform 後的另外一維

1:01:59.040,1:02:01.900
如果我們把 x1 跟 x2 經過

1:02:01.900,1:02:05.860
這兩個 Logistic Regression mode 的 transform

1:02:05.860,1:02:10.540
得到 x1' 跟 x2'，而在這個新的 transform 上面

1:02:10.540,1:02:13.800
class 1 和 class 2 是可以用一條直線分開的

1:02:13.800,1:02:17.740
那麼最後，只要再接
另外一個 Logistic Regression 的 model

1:02:17.740,1:02:20.220
它的 input 就是 x1' 和 x2'

1:02:20.220,1:02:22.800
對它來說， x1' 和 x2' 就是

1:02:22.800,1:02:27.300
每一個 example 的 feature，不是 x1 和 x2，是 x1' 和 x2'

1:02:27.300,1:02:30.380
那根據 x1' 和 x2' 這個新的 feature

1:02:30.380,1:02:34.500
它就可以把 class 1 和 class 2 分開

1:02:34.500,1:02:36.780
所以前面這兩個

1:02:37.660,1:02:40.280
Logistic Regression 做的事情

1:02:40.280,1:02:42.820
就是做 feature transform 這件事情

1:02:42.880,1:02:45.040
它先把 feature transform 好以後

1:02:45.040,1:02:49.380
再由後面的，紅色的 Logistic Regression 的 model 呢

1:02:49.380,1:02:52.120
來做分類

1:02:52.120,1:02:54.560
如果舉比較實際的例子的話

1:02:54.560,1:02:56.520
我們看剛才那個例子

1:02:56.520,1:03:00.800
我們在 x1 和 x2 平面上，有 4 個點

1:03:00.800,1:03:04.940
我們可以調整藍色這個 Logistic Regression

1:03:04.940,1:03:06.760
它的 weight 的參數

1:03:06.760,1:03:09.920
讓它的 posterior probability 的 output 呢

1:03:09.920,1:03:13.020
長得像是這個圖上的顏色這樣子

1:03:13.020,1:03:17.060
因為這個 boundary 一定是一條直線嘛

1:03:17.060,1:03:19.440
所以，這個 posterior probability 的 output 呢

1:03:19.440,1:03:23.220
一定是長這樣子的，它的等高線一定是直的

1:03:23.220,1:03:25.040
在左上教的地方

1:03:25.040,1:03:28.480
output 的值比較大，在右下角的地方

1:03:28.560,1:03:30.580
output 的值比較小

1:03:30.580,1:03:35.000
你可以調整參數，讓這個藍色的

1:03:35.320,1:03:37.640
這個 Logistic Regression

1:03:37.640,1:03:40.320
它 input x1, x2 的時候，對這 4 個點

1:03:40.320,1:03:44.740
它的 output 是 0.73, 0.27, 0.27, 0.05

1:03:44.740,1:03:47.220
這件事情呢，是它做得到的

1:03:47.220,1:03:49.420
對綠色這個點來說

1:03:49.420,1:03:52.320
你也可以調整它的參數

1:03:52.320,1:03:55.980
讓它對右下角紅色這個點的 output 是 0.73

1:03:55.980,1:03:58.640
對藍色的點是 0.27, 0.27

1:03:58.640,1:04:02.500
對左上角這個點，它是 0.05

1:04:02.500,1:04:04.740
Logistic Regression 它可以

1:04:04.740,1:04:07.620
它的 boundary 一定是一條直線

1:04:07.620,1:04:09.560
那這個直線可以有任何的畫法

1:04:09.560,1:04:11.560
你可以是左邊高、右邊低

1:04:11.560,1:04:15.960
你可以是左上高、右下低，也可以是右下高、左上低

1:04:16.000,1:04:19.820
這都是做得到的，只要調整參數，都做得到這些事情

1:04:19.820,1:04:21.440
所以，現在呢

1:04:21.440,1:04:24.980
有了前面這兩個 Logistic Regression 以後

1:04:24.980,1:04:27.520
我們就可以把 input 的每一筆 data

1:04:27.520,1:04:31.940
做 feature transform得到另外一組 feature

1:04:32.540,1:04:36.100
有就是說呢，原來左上角這個點

1:04:36.100,1:04:37.940
原來左上角這個點

1:04:38.000,1:04:42.200
它本來在 x1, x2 的平面上是 (0, 1)

1:04:42.200,1:04:45.740
但是在 x1', x2' 的平面上

1:04:45.740,1:04:49.100
它變成是 (0.73, 0.05)

1:04:49.140,1:04:52.640
如果我們看右下角，這個紅色的點

1:04:52.640,1:04:55.100
在 x1', x2' 的平面上

1:04:55.240,1:04:59.420
它就是 (0.05, 0.73)

1:04:59.420,1:05:05.140
呃，這個 x1 跟 x2 是不是畫反了呢？

1:05:05.500,1:05:07.820
我看一下，對，畫反了

1:05:07.820,1:05:11.800
不好意思，這個 x1 跟 x2 是畫反的

1:05:11.800,1:05:14.940
因為你看這個是 0.05

1:05:14.940,1:05:17.520
然後，這個縱軸呢

1:05:17.520,1:05:19.940
才是比較小的，才是 0.05

1:05:19.940,1:05:23.340
所以，x1 跟 x2 呢，這邊的 label

1:05:23.340,1:05:27.100
應該要是反過來的

1:05:30.580,1:05:33.520
然後，我們現在把

1:05:33.520,1:05:38.100
紅色的點變到 (0.73, 0.05) 的位置

1:05:38.100,1:05:40.980
紅色的點變到 (0.05, 0.73) 的位置

1:05:40.980,1:05:44.340
把這兩個藍色的點變到 (0.27, 0.27) 的位置

1:05:44.340,1:05:46.420
我們做了這樣的轉換以後呢

1:05:46.420,1:05:51.820
我們就可以用紅色的這個 Logistic Regression

1:05:51.820,1:05:54.600
畫一條 boundary

1:05:54.600,1:05:59.240
把藍色的點和紅色的點分開

1:05:59.240,1:06:03.460
所以，如果我們只有一個 Logistic Regression

1:06:03.460,1:06:07.280
我們沒有辦法把這個 example 處理好

1:06:07.280,1:06:10.500
但是，如果我們有 3 個 Logistic Regression

1:06:10.500,1:06:12.320
他們被接在一起的話

1:06:12.320,1:06:14.860
那我們就可以把這件事情處理好

1:06:14.860,1:06:18.620
所以，把這些 Logistic Regression 的 model 疊在一起

1:06:18.620,1:06:21.260
它還滿有用的，我們可以有

1:06:21.260,1:06:24.160
某一個 Logistic Regression 它的 model、它的 input

1:06:24.160,1:06:27.880
是來自於其他 Logistic Regression 的 output

1:06:27.880,1:06:30.440
而某一個 Logistic Regression 的 output

1:06:30.500,1:06:34.800
它也可以是其他 Logistic Regression 的 input

1:06:34.800,1:06:37.820
我們可以把它前後相連起來

1:06:37.820,1:06:40.040
就變得呢，很 powerful

1:06:40.040,1:06:42.660
那我們呢，可以給它一個新的名字

1:06:42.660,1:06:44.760
我們把每一個 Logistic Regression

1:06:44.760,1:06:46.440
叫做一個 Neuron

1:06:46.440,1:06:48.780
把這些 Logistic Regression 串起來

1:06:48.780,1:06:51.400
所成的 network，就叫做 Neural Network

1:06:51.400,1:06:53.020
就叫做類神經網路

1:06:53.020,1:06:56.220
換了一個名字以後，它整個就潮起來了

1:06:57.420,1:06:59.940
你就可以騙麻瓜

1:06:59.940,1:07:04.140
你就可以跟麻瓜講說，我們是在模擬人類大腦的運作

1:07:04.140,1:07:06.700
而麻瓜就會覺得，你做的東西實在是太棒了

1:07:06.700,1:07:08.880
這個東西就是 deep learning 這樣

1:07:08.980,1:07:10.480
所以我們呢，就進入 deep learning

1:07:10.480,1:07:14.300
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

