0:00:00.000,0:00:02.240
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.240,0:00:04.840
我把 Dimension Reduction 分成兩種

0:00:04.940,0:00:10.240
一種做的事情，叫做化繁為簡

0:00:10.240,0:00:14.300
它可以分成兩大類，一種是做 Clustering

0:00:14.300,0:00:16.780
一種是做 Dimension Reduction

0:00:16.780,0:00:20.220
所謂的化繁為簡的意思是說

0:00:20.220,0:00:23.260
你現在有很多種不同的 input

0:00:23.260,0:00:26.160
比如說，你現在找一個 function

0:00:26.160,0:00:29.120
然後它可以 input 看起來像樹的東西

0:00:29.120,0:00:31.960
output 都是抽象的樹

0:00:31.960,0:00:35.300
也就是把本來比較複雜的 input

0:00:35.300,0:00:37.940
變成比較簡單的 output

0:00:37.940,0:00:40.440
那在做 Unsupervised Learning 的時候

0:00:40.440,0:00:44.480
你通常只會有 function 的其中一邊

0:00:44.480,0:00:46.840
比如說，我們要找一個 function，它可以把

0:00:46.840,0:00:48.980
所有的樹都變成抽象的樹

0:00:48.980,0:00:52.040
但是，你所能擁有的 training data

0:00:52.040,0:00:55.400
就只有一大堆的 image

0:00:55.400,0:00:58.100
一大堆的、各種不同的 image

0:00:58.100,0:01:01.740
你不知道說它的 output，應該要是長什麼樣子

0:01:02.280,0:01:07.640
那另外一個 Unsupervised Learning 會做的事情呢

0:01:07.640,0:01:10.820
是 Generation，也就是無中生有

0:01:10.820,0:01:13.140
它做的事情是這樣，我們要找一個 function

0:01:13.140,0:01:17.120
這個 function，你隨機給它一個 input

0:01:17.120,0:01:19.860
我給它一個 random number，比如說輸入一個數字 1

0:01:19.860,0:01:21.260
然後，它就會 output 這棵樹

0:01:21.260,0:01:23.240
輸入數字 2，就 output 另外一棵樹

0:01:23.240,0:01:24.840
輸入數字 3，就 output 另外一棵

0:01:24.840,0:01:28.180
你輸入一個，給它一個 random number

0:01:28.180,0:01:30.540
它就自動畫一張圖出來

0:01:30.540,0:01:33.900
不同的 number 給它，它畫出來的圖就不一樣

0:01:33.900,0:01:37.700
在這個 task 裡面，我們要找的這個可以畫圖的 function

0:01:37.700,0:01:40.480
你只有這一個 function 的 output

0:01:40.480,0:01:43.180
但是你沒有這一個 function 的 input

0:01:43.180,0:01:44.700
你就只有一大堆的 image

0:01:44.700,0:01:46.360
但是，你不知道輸入怎麼樣的 code

0:01:46.360,0:01:48.540
才可以得到這些 image

0:01:48.540,0:01:51.640
那在這一份投影片，我們先 focus 在

0:01:51.640,0:01:53.880
Dimension Reduction 這一件事情上

0:01:53.880,0:01:59.600
而且我們只 focus 在 linear 的 Dimension Reduction

0:01:59.600,0:02:03.660
那我們現在說一下什麼是 clustering，什麼是 clustering 呢

0:02:03.660,0:02:05.880
clustering 就是說，這個太直覺了

0:02:05.880,0:02:09.520
這種地方比較沒有什麼特別好講的

0:02:09.520,0:02:13.320
有一大堆的 image，假設我們現在要做 image 的 clustering

0:02:13.320,0:02:14.680
就有一大堆 image

0:02:14.680,0:02:17.760
然後，你就把它們分成一類、一類、一類的

0:02:17.760,0:02:21.440
之後，你就可以說

0:02:21.440,0:02:24.220
這一邊所有的 image 都是屬於 cluster 1

0:02:24.220,0:02:26.640
這邊都屬於 cluster 2，這邊都屬於 cluster 3

0:02:26.640,0:02:29.020
就給它們貼標籤的意思

0:02:29.020,0:02:32.300
那這樣你就把本來有些不同的 image

0:02:32.300,0:02:34.640
都用同一個 class 來表示

0:02:34.640,0:02:37.040
就可以做到化繁為簡這一件事情

0:02:37.040,0:02:39.240
那這邊最 critical 的問題就是

0:02:39.240,0:02:42.940
到底應該要有幾個 cluster，那這個東西沒有什麼好的方法

0:02:42.940,0:02:46.480
就跟 neural network 要幾層一樣，是 empirical 地去決定

0:02:46.480,0:02:49.140
你要需要幾個 cluster，這個當然不能太多

0:02:49.140,0:02:50.100
比如說，你多到說

0:02:50.100,0:02:52.800
這邊有 9 張 image，你就有 9 個 cluster

0:02:52.800,0:02:54.380
那你乾脆就不要做 clustering

0:02:54.380,0:02:55.780
這樣 image 就自己一個 cluster

0:02:55.780,0:02:57.900
那這樣有做跟沒有做是一樣的

0:02:57.900,0:03:00.280
或者是你說，我們全部的 image

0:03:00.280,0:03:02.040
我們只有一個 cluster

0:03:02.040,0:03:03.820
全部的 image 都放在同一個 cluster 裡面

0:03:03.820,0:03:05.440
那也是有做跟沒有做一樣

0:03:05.440,0:03:08.060
但是，要怎麼樣選擇適當的 cluster

0:03:08.060,0:03:10.240
這個你就要 empirical 的來決定它

0:03:10.240,0:03:12.640
那在 clustering 的方法裡面

0:03:12.640,0:03:15.600
最常用的叫做 K-means

0:03:15.600,0:03:17.340
很快地講一下，K-means 是怎麼做的

0:03:17.340,0:03:21.580
K-means 就是這樣，我們有一大堆的 data

0:03:21.580,0:03:23.320
它們都是 unlabeled

0:03:23.320,0:03:26.980
一大堆的 data，x^1 一直到 x^N

0:03:26.980,0:03:29.980
那這邊每一個 x，它可能就代表一張 image

0:03:29.980,0:03:33.700
那我們要把它做成 K 個 cluster

0:03:33.700,0:03:37.560
怎麼做呢，我們要先

0:03:37.560,0:03:41.500
找這些 cluster 的 center

0:03:41.500,0:03:45.680
假如這一邊每一個

0:03:45.680,0:03:47.840
object 都是用一個 vector 來表示的話

0:03:47.840,0:03:52.320
這一邊這些 center，也是一樣長度的 vector

0:03:52.320,0:03:56.000
那我們每一個 cluster 都要先找一個 center

0:03:56.000,0:04:00.360
有 K 個 cluster，我們就需要 c^1 到 c^K 個 center

0:04:00.360,0:04:01.820
那這些 center

0:04:01.820,0:04:03.360
這個初始的 center 怎麼來的呢

0:04:03.360,0:04:05.500
你可以從你的 training data 裡面

0:04:05.500,0:04:09.240
隨機的找 K 個 object 出來

0:04:09.240,0:04:11.580
就是你的 K個 center

0:04:11.580,0:04:18.780
接下來，你要對所有在 training data 裡面的 x

0:04:18.780,0:04:21.540
都做以下的事情，都做什麼事情呢

0:04:21.540,0:04:25.640
你決定說現在的每一個 object

0:04:25.640,0:04:30.340
屬於 1 到 K 的哪一個 cluster

0:04:30.340,0:04:35.340
那假設現在你的 object, x^n

0:04:35.340,0:04:40.320
跟第 i 個 cluster 的 center 最接近的話

0:04:40.320,0:04:43.900
那 x^n 就屬於 c^i

0:04:43.900,0:04:48.560
那我們用一個 binary 的 value，b (上標n, 下標 i)

0:04:48.560,0:04:53.640
來代表說，第 n 個 object 有沒有屬於第 i 個 class

0:04:53.640,0:04:55.880
如果第 n 個 object 屬於第 i 個 class 的話

0:04:55.880,0:04:58.400
那這一個 binary 的 value 就是 1

0:04:58.400,0:05:00.280
反之就是 0

0:05:00.280,0:05:03.060
接下來，你要 update 你的 cluster

0:05:03.060,0:05:05.120
怎麼 update 你的 cluster 呢

0:05:05.120,0:05:07.960
這個方法也是很直覺

0:05:07.960,0:05:11.360
你就把所有屬於

0:05:11.360,0:05:14.480
假設你要 update 第 i 個 cluster 的 center

0:05:14.480,0:05:18.500
你就把所有屬於第 i 個 cluster 的 object

0:05:18.500,0:05:20.500
通通拿出來，做平均

0:05:20.500,0:05:24.540
你就得到第 i 個 cluster 的 center

0:05:24.540,0:05:28.140
然後，你要反覆的做

0:05:28.140,0:05:31.660
那這邊之所以你在做 initialization 的時候

0:05:31.660,0:05:33.520
你會想要直接從你的

0:05:33.520,0:05:36.900
這個 database 裡面去挑 K 個 example

0:05:36.900,0:05:38.800
挑 K 個 object 出來做 center 呢

0:05:38.800,0:05:41.840
有一個很重要的原因是，假設你是純粹隨機的

0:05:41.840,0:05:43.460
你不是從 data points 裡面去挑

0:05:43.460,0:05:48.000
你很有可能在第一次 assign 這個 object 的時候

0:05:48.000,0:05:52.920
就沒有任何 example 跟某一個 cluster 很像

0:05:52.920,0:05:56.940
又有可能是，有某一個 cluster，它沒有任何 example

0:05:56.940,0:05:58.240
你現在 update 的時候

0:05:58.240,0:05:59.820
程式就會 segmentation fault

0:05:59.820,0:06:03.720
所以，你最好就是從你的 training data 裡面

0:06:03.720,0:06:07.220
選 K 筆 example 出來當 initialization

0:06:09.200,0:06:11.480
clustering 有另外一個方法叫做

0:06:11.480,0:06:13.840
Hierarchical Agglomerative Clustering (HAC)

0:06:13.840,0:06:15.560
那這個方法

0:06:15.560,0:06:20.540
是先建一個 tree，假設你現在有 5 個 example

0:06:20.540,0:06:22.600
你想要把它做 cluster

0:06:22.600,0:06:24.380
那你先做成一個 tree structure

0:06:24.380,0:06:25.780
怎麼建這個 tree structure 呢

0:06:25.780,0:06:27.820
你把這 5 個 example

0:06:27.820,0:06:30.360
兩兩、兩兩去算他的相似度

0:06:30.360,0:06:33.620
然後挑最相似的那一個 pair 出來

0:06:33.620,0:06:37.560
假設現在最相似的 pair，是第一個和第二個 example

0:06:37.560,0:06:41.760
你就把第一個 example 和第二個 example merge 起來

0:06:41.760,0:06:43.120
比如說，把它們平均起來

0:06:43.120,0:06:45.260
得到一個新的 vector

0:06:45.260,0:06:48.540
這個 vector 同時代表第一個和第二個 example

0:06:48.540,0:06:50.260
接下來，你再算說

0:06:50.260,0:06:52.760
現在變成有四個 example 了

0:06:52.760,0:06:55.260
現在只剩下 4 筆 data 了

0:06:55.260,0:06:57.840
把這 4 筆 data，再兩兩去算它的相似度

0:06:57.840,0:07:00.880
然後發現說，第三筆和第四筆

0:07:00.880,0:07:02.740
最後這兩筆是最像的

0:07:02.740,0:07:05.180
那你再把它們 merge 起來，把它們平均起來

0:07:05.180,0:07:07.400
得到另外一筆 data

0:07:07.400,0:07:10.660
現在只剩三筆 data，再去兩兩算它的 singularity

0:07:10.660,0:07:11.740
然後，你發現說

0:07:11.740,0:07:14.280
黃色這一個和中間這一個最像

0:07:14.280,0:07:16.560
你就再把它們平均起來

0:07:16.560,0:07:22.400
你會發現，最後只剩紅色跟綠色，你只好把它平均起來

0:07:22.400,0:07:25.040
那你就得到這個 tree 的 root

0:07:25.040,0:07:28.060
你就根據這 5 筆 data，它們之間的相似度

0:07:28.060,0:07:31.380
建立出一個 tree structure

0:07:31.380,0:07:33.200
接下來，你要決定一個

0:07:33.200,0:07:36.360
你沒有做 clustering，你只是建了一個 tree structure

0:07:36.360,0:07:38.100
這個 tree structure 可以告訴我們說

0:07:38.100,0:07:41.620
哪些 example 是比較像的

0:07:41.620,0:07:44.380
比較早分支代表比較不像

0:07:44.380,0:07:47.480
比如說 ，這三個跟這兩個

0:07:47.480,0:07:49.580
一開始在 root 的地方就分成兩類

0:07:49.580,0:07:52.240
所以，它們這三個跟這兩個就比較不像

0:07:53.580,0:07:55.800
那接下來你要做 clustering，怎麼做呢

0:07:55.800,0:07:58.280
你要決定在這個樹上

0:07:58.280,0:08:01.060
在這個 tree structure 上面切一刀

0:08:01.060,0:08:03.020
比如說，你決定切在這一邊

0:08:03.020,0:08:05.760
如果你切在這個地方的時候

0:08:05.760,0:08:08.440
你就把你的五筆 data

0:08:08.440,0:08:10.340
變成三個 cluster

0:08:10.340,0:08:12.380
根據這一刀

0:08:12.380,0:08:14.680
這兩個 example 是一個 cluster

0:08:14.680,0:08:17.560
它自己一個 cluster，這兩個 example 是一個 cluster

0:08:17.560,0:08:20.760
如果你這一刀切在這個地方

0:08:21.400,0:08:23.920
那就變成這 3 筆是一個 cluster

0:08:23.920,0:08:26.980
這三筆是一個 cluster，這兩筆是一個 cluster

0:08:26.980,0:08:29.340
如果，你這一刀切在這邊，那你就變成

0:08:29.340,0:08:31.700
分成總共四個 cluster

0:08:31.700,0:08:35.580
這個是 HAC 的做法

0:08:35.580,0:08:39.480
那 HAC 跟 K-means，我覺得最大的差別就是

0:08:39.480,0:08:41.560
你如何決定你 cluster 的數目

0:08:41.560,0:08:43.120
在 K-means 裡面，你要

0:08:43.120,0:08:45.220
決定你那個 K 的 value 是多少

0:08:45.220,0:08:49.260
有時候你覺得說，到底有多少個 cluster 我不容易想

0:08:49.260,0:08:51.160
那你可以換成 HAC

0:08:51.160,0:08:53.040
好處就是，你現在把

0:08:53.040,0:08:55.420
你現在不直接決定幾個 cluster，而是

0:08:55.420,0:08:58.280
決定你要在樹上切

0:08:58.280,0:09:00.660
切在這個樹的 structure 的哪裡

0:09:00.660,0:09:02.160
那有人會覺得說

0:09:02.160,0:09:06.620
有時候你會覺得這樣子是比較容易的話

0:09:06.620,0:09:09.140
那學 HAC 對你來說，就有一些 benefit

0:09:09.840,0:09:12.160
但是，光只做 cluster

0:09:12.160,0:09:13.900
是非常卡的

0:09:13.900,0:09:15.800
在做 cluster 的時候

0:09:15.800,0:09:18.400
我們就是以偏概全，因為每一個 object

0:09:18.400,0:09:20.880
都必須要屬於某一個 cluster

0:09:20.880,0:09:23.020
這就好像是說，我們知道說

0:09:23.020,0:09:24.940
念能力有分成六大類

0:09:24.940,0:09:27.580
然後，每一個人

0:09:27.580,0:09:30.860
都會被 assign 成這六個念能力的其中一類

0:09:30.860,0:09:32.460
他怎麼決定他是哪一類呢？

0:09:32.460,0:09:33.900
你就做水鑑識這樣子

0:09:33.900,0:09:36.880
你拿一杯水，然後看看它會有什麼反應

0:09:36.880,0:09:38.580
然後，就把他塞成某一類

0:09:38.580,0:09:40.040
比如說，水滿出來了

0:09:40.040,0:09:43.760
就是強化，所以小傑就是強化系

0:09:43.760,0:09:46.240
那我們知道說，這樣子把每一個人

0:09:46.240,0:09:49.600
都 assign 成念能力裡面的某一個系

0:09:49.600,0:09:51.880
是不夠的

0:09:51.880,0:09:54.420
是太過粗糙武斷

0:09:54.420,0:09:56.720
比如說，像比司吉就有說

0:09:56.720,0:10:02.740
小傑其實是接近放出系的強化系念能力者

0:10:02.740,0:10:05.420
所以小傑，如果你只是說他是強化系

0:10:05.420,0:10:07.760
其實是 loss 掉很多 information

0:10:07.760,0:10:09.420
你應該這樣來表示小傑

0:10:09.420,0:10:12.360
你應該說，他強化系是 0.7

0:10:12.360,0:10:16.120
他放出系是0.2，那其實強化系跟變化系是比較接近的

0:10:16.120,0:10:19.480
所以有強化系，你也會有部分的變化系的能力

0:10:19.480,0:10:22.260
所以，變化系是 0.05，像小傑可以用剪刀

0:10:22.260,0:10:25.240
那個是變化系的能力，然後其他系是 0

0:10:25.240,0:10:27.040
所以，你應該要用

0:10:27.040,0:10:29.460
如果你只是把你手上所有的 object

0:10:29.460,0:10:32.160
分別 assign 到它屬於哪一個 cluster

0:10:32.160,0:10:34.060
這樣是以偏概全，這樣太粗了

0:10:34.060,0:10:36.840
你應該要用一個 vector

0:10:36.840,0:10:39.540
來表示你的 object

0:10:39.540,0:10:42.080
那這個 vector裡面的每一個 dimension

0:10:42.080,0:10:44.060
就代表了某一種特質

0:10:44.060,0:10:46.060
某一種 attribute

0:10:46.060,0:10:47.260
那這件事情

0:10:47.260,0:10:51.120
就叫做 Distributed 的 representation

0:10:51.120,0:10:53.500
如果你原來的 object 是一個

0:10:53.500,0:10:55.840
非常 high dimension 的東西，比如說 image

0:10:55.840,0:10:58.060
那你現在把它用它的

0:10:58.060,0:11:00.880
attribute，把它用它的特質來描述

0:11:00.880,0:11:05.340
它就會從比較高維的空間變成比較低維的空間

0:11:05.340,0:11:08.400
這一件事情就叫做 Dimension Reduction

0:11:08.400,0:11:10.560
那它們其實是一樣的事情

0:11:10.560,0:11:12.480
只是有不同的稱呼而已

0:11:12.780,0:11:17.080
那我們從另外一個角度來看一下
為什麼 Dimension Reduction

0:11:17.080,0:11:19.500
是可能是有用的

0:11:19.500,0:11:23.580
舉例來說，假設你的 data 的分布

0:11:23.580,0:11:25.120
是長這個樣子

0:11:25.120,0:11:26.880
在 3D 的空間裡面

0:11:26.880,0:11:31.380
你現在的 data 的分布是長這個螺旋的樣子

0:11:31.380,0:11:33.560
但是，用3D的空間

0:11:33.560,0:11:35.660
來描述這些 data

0:11:35.660,0:11:36.860
其實是很浪費的

0:11:36.860,0:11:38.800
你從直覺上就會知道說

0:11:38.800,0:11:41.540
其實你可以把這個

0:11:41.540,0:11:43.980
這個類似地毯捲起來的東西

0:11:43.980,0:11:46.420
把它攤開、把它攤平

0:11:46.420,0:11:47.520
就變成這樣

0:11:47.520,0:11:52.300
所以，你其實只需要在 2D 的空間

0:11:52.300,0:11:55.240
就可以描述這個 3D 的 information

0:11:55.240,0:11:58.120
你根本不需要把這個問題放到 3D 來解

0:11:58.120,0:11:59.680
這樣是把問題複雜化

0:11:59.680,0:12:02.900
你其實在 2D 就可以做這個 task

0:12:02.900,0:12:05.640
或者是，我們舉一個比較具體的例子

0:12:05.640,0:12:07.620
比如說，我們考慮 MNIST

0:12:07.620,0:12:10.820
在 MNIST 裡面，每一個 input 的 digit

0:12:10.820,0:12:14.480
它是一個 image，它都用 28*28 的 dimension

0:12:14.480,0:12:15.720
來描述它

0:12:15.720,0:12:17.380
但是，實際上

0:12:17.380,0:12:21.720
多數 28*28 的 dimension 的 vector

0:12:21.720,0:12:23.740
你把它轉成一個 image

0:12:23.740,0:12:25.600
看起來都不像是一個數字

0:12:25.600,0:12:28.120
你 random sample 一個 28*28 的 vector

0:12:28.120,0:12:29.540
轉成 image 看起來可能是這樣

0:12:29.540,0:12:31.480
它其實根本就不像數字

0:12:31.480,0:12:35.160
所以在這個 28維 * 28維的空間裡面

0:12:35.160,0:12:42.140
是 digit 的 vector，其實是很少的

0:12:42.140,0:12:46.460
所以，其實你要描述一個 digit

0:12:46.460,0:12:49.240
或許根本不需要用到 28*28 維

0:12:49.240,0:12:50.680
你要描述一個 digit

0:12:50.680,0:12:52.440
你要的 dimension 可能是遠比

0:12:52.440,0:12:55.240
28維 * 28維少

0:12:55.240,0:12:57.300
所以，如果我們舉一個很極端的例子

0:12:57.300,0:13:01.120
比如這邊有一堆 3，然後這一堆 3

0:13:01.120,0:13:03.660
如果你是從 pixel 來看待它的話

0:13:03.660,0:13:05.800
你要用 28維 * 28維

0:13:05.800,0:13:07.680
來描述每一張 image

0:13:07.680,0:13:11.600
然而，實際上這一些 3，只需要用一個維度

0:13:11.600,0:13:13.020
就可以來表示

0:13:13.020,0:13:16.300
為甚麼呢？因為這些 3 就只是說

0:13:16.300,0:13:18.360
把原來的 3 放正

0:13:18.360,0:13:20.560
是中間這張 image

0:13:20.560,0:13:22.360
右轉 10 度轉就變成它

0:13:22.360,0:13:26.200
右轉20度就變它，左轉10度變它，左轉20度就變它

0:13:26.200,0:13:28.480
所以，你唯一需要記錄的事情只有

0:13:28.480,0:13:32.680
今天這張 image，它是左轉多少度、右轉多少度

0:13:32.680,0:13:35.960
你就可以知道說，它在 28 維的空間裡面

0:13:35.960,0:13:37.860
應該長什麼樣子

0:13:37.860,0:13:40.940
你只需要抓住這一個重點

0:13:40.940,0:13:42.460
你只要抓住這個角度的變化

0:13:42.460,0:13:44.700
你就可以知道 28 維空間中的變化

0:13:44.700,0:13:47.480
所以，只需要用一維就可以描述這些 image

0:13:47.480,0:13:49.140
所以，這就像我剛才舉的例子

0:13:49.140,0:13:53.380
這個是樊一翁，這個就是他的頭這樣子

0:13:55.300,0:13:59.540
那怎麼做 Dimension Reduction 呢

0:13:59.540,0:14:01.240
在做 Dimension Reduction 的時候

0:14:01.240,0:14:02.660
我們要做的事情就是

0:14:02.660,0:14:04.200
找一個 function

0:14:04.200,0:14:08.300
這個 function 的 input 是一個 vector, x

0:14:08.300,0:14:12.100
它的 output 是另外一個 vector, z

0:14:12.100,0:14:14.440
但是，因為是 Dimension Reduction，所以

0:14:14.440,0:14:16.080
你 output 的這個 vector, z

0:14:16.080,0:14:20.660
它的 dimension 要比 input 的 x 還要小

0:14:20.660,0:14:23.980
這樣你才是做 Dimension Reduction

0:14:23.980,0:14:25.680
在做 Dimension Reduction 裡面

0:14:25.680,0:14:28.420
最簡單的方法是 Feature Selection

0:14:28.420,0:14:30.800
這個方法就沒有什麼好講的，這個方法是這樣

0:14:30.800,0:14:35.480
現在如果你把你的 data 的分布拿出來看一下

0:14:35.480,0:14:37.160
本來在二維的平面上

0:14:37.160,0:14:41.200
但是，其實你發現都集中在 x2 的 dimension 而已

0:14:41.200,0:14:44.020
所以，x1 這個 dimension 沒什麼用，把它拿掉

0:14:44.020,0:14:45.400
就只有 x2 這個 dimension

0:14:45.400,0:14:48.500
你就等於是做到 Dimension Reduction 這件事

0:14:48.500,0:14:52.320
但是這個方法不見得總是有用，因為有很多時候是

0:14:52.320,0:14:55.740
你的 case 是，你任何一個 dimension 都

0:14:55.740,0:14:58.360
其實都不能拿掉

0:14:59.660,0:15:02.440
那另外一個常見的方法叫做

0:15:02.440,0:15:04.620
Principle Component Analysis

0:15:04.620,0:15:08.040
叫做 PCA

0:15:08.040,0:15:11.580
這個 PCA 怎麼做呢？這個 PCA 做的事情是這樣

0:15:11.580,0:15:17.180
它說這個 function 是一個很簡單的 linear function

0:15:17.180,0:15:21.940
這個 input, x 跟這個 output, z 之間的關係

0:15:21.940,0:15:24.960
就是一個 linear 的 transform

0:15:24.960,0:15:28.880
也就是你把這個 x 乘上一個 matrix, W

0:15:28.880,0:15:31.620
你就得到它的 output, z

0:15:31.620,0:15:33.580
那現在要做的事情就是

0:15:33.580,0:15:36.880
根據一大堆的 x，我們現在不知道 z 長什麼樣子

0:15:36.880,0:15:38.260
只有一大堆的 x

0:15:38.260,0:15:43.840
根據一大堆的 x，我們要把這個 W 找出來

0:15:43.840,0:15:46.560
那如果你要知道比較細節的東西的話

0:15:46.560,0:15:50.080
你可以看一下 Bishop 的第十二章

0:15:50.080,0:15:53.600
接下來，我們介紹一下 PCA

0:15:53.600,0:15:55.880
我剛才講過 PCA 要做的事情

0:15:55.880,0:15:59.740
就是找這一個 W

0:15:59.740,0:16:01.640
那這個 W 怎麼找呢？

0:16:01.640,0:16:03.840
假設我們現在考慮一個

0:16:03.840,0:16:05.460
比較簡單的 case

0:16:05.460,0:16:09.000
我們考慮一個 one dimensional 的 case

0:16:09.000,0:16:10.440
什麼意思呢

0:16:10.440,0:16:14.220
我們現在假設，我們只要把我們的 data project 到

0:16:14.220,0:16:15.600
一維的空間上面

0:16:15.600,0:16:16.860
也就是我們的 z 呢

0:16:16.860,0:16:20.260
它只是一個一維的 vector，所謂一維的 vector

0:16:20.260,0:16:23.200
就是一個 scalar，我們的 z 是一個 scalar

0:16:23.200,0:16:25.300
或者是說，我們可以這樣寫

0:16:25.300,0:16:30.780
就是，如果我們這邊 z 只是一個 scalar 的話

0:16:30.780,0:16:33.200
那我們的 W 其實就是一個

0:16:33.200,0:16:35.920
一個 row 對不對，就是一個 row 而已

0:16:35.920,0:16:39.800
那我們就用 w^1 來表示

0:16:39.800,0:16:42.080
w 的第一個 row

0:16:42.080,0:16:46.760
我們把 x 跟 w 的第一個 row, w^1

0:16:46.760,0:16:52.620
做 inner product，我們就得到一個 scalar, z1

0:16:52.620,0:16:54.580
接下來，我們要問的問題是

0:16:54.580,0:16:57.500
我們要找的這個 w^1

0:16:57.500,0:17:01.880
應該要長什麼樣子

0:17:01.880,0:17:09.380
首先，我們先假設 w^1 的長度是 1

0:17:09.380,0:17:11.160
w^1 的 2-norm 是 1

0:17:11.160,0:17:14.460
這個假設是有必要的，等一下你會看得更清楚為什麼

0:17:14.460,0:17:16.120
一定要有這一個假設

0:17:16.120,0:17:19.660
如果 w^1 的 2-norm 是 1 的話

0:17:19.660,0:17:22.500
那 w^1 跟 x 做 inner product

0:17:22.500,0:17:24.520
得到的 z1 意味著什麼

0:17:24.520,0:17:27.140
它意謂著說，現在呢

0:17:27.140,0:17:32.420
w 跟 x 是高維空間中的一個點

0:17:32.420,0:17:37.420
w^1 是高維空間中的一個 vector

0:17:37.420,0:17:40.980
那所謂的 z1 就是 x

0:17:40.980,0:17:43.400
在 w^1 上面的投影

0:17:43.400,0:17:45.660
它這個投影的值

0:17:45.660,0:17:48.140
就是 w^1 跟 x 的 inner product

0:17:48.140,0:17:51.460
這個就沒有什麼問題了

0:17:51.460,0:17:54.320
這個就是大一線代裡面教過的東西

0:17:54.320,0:17:57.000
所以，我們現在要做的事情就是

0:17:57.000,0:17:59.540
把一堆 x

0:17:59.540,0:18:04.140
透過 w^1 把它投影變成 z1

0:18:04.140,0:18:07.180
我們就得到一堆 z1，每一個 x 都變成一個 z1

0:18:07.180,0:18:11.940
現在的問題就是，w^1 應該長什麼樣子呢

0:18:11.940,0:18:14.560
要選哪一個 w^1，舉例來說

0:18:14.560,0:18:17.180
假設這個是 x 的分布

0:18:17.180,0:18:18.780
這個 x 的分布是什麼呢

0:18:18.780,0:18:22.880
它是橫座標，在每一個點代表一隻寶可夢

0:18:22.880,0:18:26.120
它的橫座標是攻擊力

0:18:26.120,0:18:29.360
它的縱座標是防禦力

0:18:29.900,0:18:33.220
那今天如果我要選

0:18:33.220,0:18:37.400
我們要把這個二維的，這邊這個 x 都是二維的

0:18:37.400,0:18:39.980
我把這個二維投影到一維

0:18:39.980,0:18:43.240
我應該要選什麼樣的 w^1？

0:18:43.240,0:18:46.480
我可以選這樣的 w^1

0:18:46.480,0:18:49.520
我可以選 w^1 指向這個方向

0:18:49.520,0:18:52.880
我也可以選 w^1 指向這個方向

0:18:52.880,0:18:54.500
我選不同的方向

0:18:54.500,0:18:58.080
我最後得到的結果，得到的 projection 的結果

0:18:58.080,0:19:00.160
它會是不一樣的

0:19:00.860,0:19:03.800
那我們會想要

0:19:03.800,0:19:07.480
你總是要給我一個目標，我才知道要選什麼樣的 w^1

0:19:07.480,0:19:09.460
現在的目標是這樣

0:19:09.460,0:19:12.040
我們希望選一個 w^1

0:19:12.040,0:19:16.980
它經過 projection 以後，得到的這些 z1 的分布

0:19:16.980,0:19:19.240
是越大越好

0:19:19.240,0:19:22.540
也就是說，我們不希望通過這個 projection 以後

0:19:22.540,0:19:24.860
所有的點通通擠在一起

0:19:24.860,0:19:30.080
就變成把本來 data point 和 data point 之間的奇異度

0:19:30.080,0:19:32.160
拿掉了，我們是希望說

0:19:32.160,0:19:35.680
經過這個 projection 以後，不同的 data point

0:19:35.680,0:19:37.120
它們之間的區別

0:19:37.120,0:19:38.900
我們仍然是可以看得出來

0:19:38.900,0:19:42.460
所以，我們希望找一個 projection 的方向，它可以讓

0:19:42.460,0:19:45.900
projection 後的 variance 越大越好

0:19:45.900,0:19:49.860
如果我們看這一個例子的話

0:19:49.860,0:19:53.120
你就會覺得說，如果是選這個方向的話

0:19:53.120,0:19:55.640
經過 projection 以後，你的點可能是分布在

0:19:55.640,0:19:58.240
這個地方，大概分布在這個 range

0:19:58.240,0:20:01.820
那如果是 projection 在這個方向的話

0:20:01.820,0:20:04.200
那你點的分布可能是這個 range

0:20:04.200,0:20:07.040
所以，如果 projection 在這個方向上的話

0:20:07.040,0:20:08.980
你會有比較大的 variance

0:20:08.980,0:20:11.880
但是在這個方向上的話，你會有一個比較小的 variance

0:20:11.880,0:20:13.600
所以你要選 w^1 的時候，你可能

0:20:13.600,0:20:18.180
選 w^1 的方向是指向這個方向

0:20:18.180,0:20:20.600
其實這個 w^1 代表什麼意思呢

0:20:20.600,0:20:22.980
如果從這個圖，你可以看到說

0:20:22.980,0:20:26.940
這個 w^1 其實是代表了寶可夢的強度

0:20:26.940,0:20:32.020
寶可夢可能會有一個隱藏的 vector 代表它的強度

0:20:32.020,0:20:35.460
這個隱藏的 vector，同時影響了它的

0:20:35.460,0:20:38.240
防禦力跟攻擊力

0:20:38.240,0:20:43.400
所以，防禦力跟攻擊力是會同時上升的

0:20:44.040,0:20:47.460
如果我們要用 equation 來表示它的話

0:20:47.460,0:20:51.440
你就會說我們現在要去 maximize 的對象是

0:20:51.440,0:20:53.760
z1 的 variance

0:20:53.760,0:20:57.340
z1 的 variance 就是 summation over 所有的 z1

0:20:57.340,0:21:03.480
(z1 - z1\bar) 的平方，z1\bar 就是做 z1 的平均

0:21:05.220,0:21:07.960
那這樣子

0:21:07.960,0:21:11.460
我們等一下再講怎麼做

0:21:11.460,0:21:13.840
假設你知道怎麼做，你解一解

0:21:13.840,0:21:17.060
你找到一個 w^1，你就可以讓 z1 最大

0:21:17.060,0:21:18.880
那你就找到這個 w^1，就結束了

0:21:19.120,0:21:22.040
再來，你可能不只要投影到一維

0:21:22.040,0:21:26.180
你想要投影到更多維，比如說，你想要投影到一個

0:21:26.180,0:21:27.820
二維的平面

0:21:27.820,0:21:31.240
那如果你想要投影到二維的平面的時候

0:21:31.240,0:21:35.380
這個時候你就把 x 跟另外一個 w^2

0:21:35.380,0:21:37.340
做 inner product，得到 z2

0:21:37.340,0:21:39.780
這個 z1 跟 z2 串起來就得到這邊的 z

0:21:39.780,0:21:43.220
這個 w^1 跟 w^2 的 transpose 排起來就是

0:21:43.220,0:21:45.580
W 的第一個 row 跟第二個 row

0:21:46.400,0:21:49.180
好，那這一個 z2

0:21:49.180,0:21:51.200
我們要怎麼找這一個 w^2 呢

0:21:51.200,0:21:54.440
跟剛才找 z1 一樣

0:21:54.440,0:21:57.920
我們希望，首先 w^2 它的 2-norm 是 1

0:21:57.920,0:22:03.500
然後，接下來這個 z2 它的分佈也是越大越好

0:22:03.560,0:22:04.920
也是越大越好

0:22:04.920,0:22:06.940
但是，如果你只是要讓

0:22:06.940,0:22:10.420
讓 z2 的 variance 越大越好，讓這個式子越大越好

0:22:10.420,0:22:13.800
你找出來就不是 w^1，w^1 剛才已經找過了

0:22:13.800,0:22:16.220
對不對，所以你就等於什麼事都沒有做

0:22:16.220,0:22:19.100
所以，你要再加一個 constraint

0:22:19.100,0:22:22.540
這個 constraint 是我們剛才已經先找過 w^1 了

0:22:22.540,0:22:26.680
這個 w^2 要跟 w^1是垂直的

0:22:26.680,0:22:31.520
或者是 w^1 跟 w^2 是 orthogonal

0:22:31.520,0:22:35.600
w^1 跟 w^2，它們做 inner product 等於 0

0:22:35.600,0:22:37.800
藉由這個方法，你就可以

0:22:37.800,0:22:40.420
先找 w^1，再找 w^2，再找 w^3

0:22:40.420,0:22:42.880
就看你要 project 到幾維

0:22:42.880,0:22:45.320
你要 project 到幾維是你要自己決定的

0:22:45.320,0:22:46.780
這個就跟我們要幾個 cluster

0:22:46.780,0:22:48.760
要幾個 hidden layer 也是自己決定的

0:22:48.760,0:22:51.200
你要 project 到 K 維

0:22:51.200,0:22:53.960
那你就找 w^1, w^2 到 w^K

0:22:53.960,0:22:57.600
你就把你所有找出來的 w^1, w^2 到 w^K

0:22:57.600,0:23:00.560
排起來當作 W row

0:23:00.560,0:23:04.700
放在這一邊，就結束了

0:23:04.700,0:23:08.720
那這邊有一件事情就是，這個找出來的 W

0:23:08.720,0:23:12.000
它會是一個 orthogonal的 matrix

0:23:12.000,0:23:13.040
為什麼呢？

0:23:13.040,0:23:15.040
如果你看它的 row 的話

0:23:15.040,0:23:18.720
它的 w^1 跟 w^2 是 orthogonal的

0:23:18.720,0:23:23.080
然後 w^1 的 2-norm 跟 w^2 的 2-norm 都是 1

0:23:23.080,0:23:25.640
所以，它的 row 是

0:23:25.640,0:23:28.320
norm 是 1，而且互相之間都是 orthogonal

0:23:28.320,0:23:32.780
所以，它是一個 orthogonal的 matrix

0:23:32.780,0:23:34.700
接下來的問題就是

0:23:34.700,0:23:38.440
怎麼找 w^1 跟 w^2 呢？

0:23:38.440,0:23:40.800
怎麼解這個問題呢？

0:23:40.800,0:23:42.460
怎麼解這個問題呢？

0:23:42.460,0:23:48.120
這邊其實有一個，這邊的這個解法其實是蠻容易的

0:23:48.120,0:23:51.720
這個怎麼解呢？

0:23:51.720,0:23:55.780
你其實要用 Lagrange multiplier

0:23:55.780,0:23:59.520
這邊有一個 warning od map，如果你沒有聽懂的話就算了

0:23:59.520,0:24:02.140
這個可以都直接 call 套件

0:24:02.140,0:24:03.300
這個其實 call 套件就有了

0:24:03.300,0:24:05.520
而且就算是你不會下面這套東西的話

0:24:05.520,0:24:07.920
你其實可以用 Gradient Descent 的方法

0:24:07.920,0:24:10.100
這我們之後會講，你可以把一個

0:24:10.100,0:24:13.520
你可以把 PCA 這件事情，描述成一個 neural network

0:24:13.520,0:24:17.480
然後，就用 Gradient Descent 的方法來解它

0:24:17.480,0:24:23.340
所以不一定要用 Lagrange multiplier 來做這個 PCA

0:24:23.340,0:24:28.320
這個 PCA 可以把它看成是一個 neural network 一樣

0:24:28.320,0:24:32.180
但我們現在很快講一下 Lagrange multiplier 這個方法

0:24:32.180,0:24:35.140
這個很經典的方法，它是怎麼做的呢？它是這樣子

0:24:35.140,0:24:38.620
我們說 z1 等於 w^1 跟 x 的 inner product

0:24:38.620,0:24:42.480
那 z1 的平均值是 summation over 所有的 z1

0:24:42.480,0:24:46.160
也就是 summation over 所有 w^1 跟 x 的 inner product

0:24:47.440,0:24:49.300
這邊是 summation over 所有 data point

0:24:49.300,0:24:51.700
跟 w^1 無關，所以可以把 w^1 提出來

0:24:51.700,0:24:53.900
變成先 summation over 所有的 x

0:24:53.900,0:24:55.880
再跟 w^1 做 inner product

0:24:55.880,0:24:59.840
得到 w^1 跟 x 的平均的 inner product

0:24:59.840,0:25:03.420
接下來，我們說我們要 maximize 的對象

0:25:03.420,0:25:06.540
是 z1 的 variance

0:25:06.540,0:25:08.600
那 z1 的 variance 我們可以寫成

0:25:08.600,0:25:12.520
(z1 - 它的平均值)的平方，再 summation over 所有的 z1

0:25:12.520,0:25:13.980
我們先把這一項整理一下

0:25:13.980,0:25:16.460
這項整理一下變成什麼樣子呢

0:25:16.460,0:25:19.400
z1 是 w^1 跟 x 的 inner product

0:25:19.400,0:25:22.740
z1\bar 是 w^1 跟 x\bar 的 inner product

0:25:22.740,0:25:24.320
然後，再取平方

0:25:24.320,0:25:26.720
那都有 w^1，所以把 w^1 提出來

0:25:26.720,0:25:32.180
變成 summation over [w^1 * (x - x\bar)]^2

0:25:32.180,0:25:34.840
那這個平方

0:25:34.840,0:25:36.780
你可以把它做一下轉化

0:25:36.780,0:25:38.460
怎麼轉化呢？

0:25:38.460,0:25:42.420
w^1 是一個 vector，(x - x\bar) 是另外一個 vector

0:25:42.420,0:25:47.220
兩個 vector，比如說 w^1 就是 a，(x - x\bar) 就是 b

0:25:47.220,0:25:49.400
a 跟 b 的 inner product 的平方

0:25:49.400,0:25:52.620
可以寫成 a 的 transpose 乘 b 的平方

0:25:52.620,0:25:57.400
可以寫成 a 的 transpose 乘 b，
再乘上 a 的 transpose 乘 b

0:25:57.400,0:26:00.540
然後這一項，其實可以寫成

0:26:00.540,0:26:05.080
a 的 transpose 乘 b，然後乘上

0:26:05.080,0:26:08.000
(a 的 transpose 乘 b) 再 transpose

0:26:08.000,0:26:10.400
為甚麼這邊可以直接加 transpose 呢

0:26:10.400,0:26:13.320
因為 (a^T)*b 是一個 scalar，所以再 transpose

0:26:13.320,0:26:15.320
還是它自己

0:26:15.320,0:26:17.880
那 transpose 以後

0:26:17.880,0:26:23.200
[(a^T)*b]^T 就是把它們順序對調，再加上 transpose

0:26:23.200,0:26:26.400
所以變成 (a^T)*b*(b^T)*a

0:26:26.400,0:26:29.360
然後把 a 代回 w^1

0:26:29.360,0:26:33.900
b 代回 (x - x\bar)，你就得到這樣的式子

0:26:33.900,0:26:39.640
(w^1)^T * (x - x\bar) * (x - x\bar)^T * w^1

0:26:39.640,0:26:40.860
接下來呢

0:26:40.860,0:26:43.560
你這一邊是 summation over 所有的 data

0:26:43.560,0:26:46.260
summation over 所有的 data，所以跟 w 無關

0:26:46.260,0:26:49.940
跟 w 無關，所以把 w^1 的 transpose 拿出去

0:26:49.940,0:26:51.360
把 w^1 拿出去

0:26:51.360,0:26:53.560
注意一下，這邊 summation 是 summation over

0:26:53.560,0:26:57.400
(x - x\bar)*(x - x\bar)^T 這一項

0:26:57.400,0:27:00.320
w^1 被拿出去了

0:27:00.320,0:27:02.240
那 (x - x\bar) 的 transpose

0:27:02.240,0:27:06.560
(x - x\bar) * (x - x\bar)^T 
summation over 所有 data point 是甚麼呢

0:27:06.560,0:27:09.460
它是 x 的 covariance

0:27:09.460,0:27:13.740
對不對，它是 x 的 covariance matrix，所以這一項

0:27:13.740,0:27:18.280
其實就是 (w^1)^T * Cov(x) * w^1

0:27:18.280,0:27:22.360
我們用 S 來描述 x 的 covariance matrix

0:27:22.360,0:27:25.800
所以，現在我們要解的問題是這樣

0:27:25.800,0:27:27.120
找出一個 w^1

0:27:27.120,0:27:29.840
它可以 maximize w^1 的 transpose

0:27:29.840,0:27:31.400
乘上一個 matrix, x

0:27:31.400,0:27:33.240
再乘上 w^1

0:27:33.240,0:27:36.300
但這個 optimization 的對象是有 constraint 的

0:27:36.300,0:27:37.660
如果沒有 constraint 的話

0:27:37.660,0:27:41.100
這個問題它會有無聊的 solution

0:27:41.100,0:27:47.040
你把 w^1 的每一個值都變無窮大，就結束了

0:27:47.040,0:27:49.600
所以，你要有 constraint

0:27:49.600,0:27:51.820
它的 constraint 是說

0:27:51.820,0:27:55.540
w^1 的 2-norm 要等於 1

0:27:55.540,0:27:57.420
那有了這些以後

0:27:57.420,0:28:01.000
我們就要解這一個 optimization 的 problem

0:28:01.000,0:28:05.640
這邊這個 S，S 是 covariance matrix，
x 的 covariance matrix

0:28:05.640,0:28:06.760
它是 symmetric 的

0:28:06.760,0:28:09.280
而且因為它是 covariance matrix 的關係

0:28:09.280,0:28:11.000
它又是半正定

0:28:11.000,0:28:12.600
它是 positive-semidefinite

0:28:12.600,0:28:16.160
也就是說它所有的 eigenvalue

0:28:16.160,0:28:18.240
都是 non-negative 的

0:28:18.240,0:28:19.820
如果你對這件事情有困惑的話

0:28:19.820,0:28:23.020
你就回去翻一下線代課本，你其實可以在

0:28:23.020,0:28:27.960
我的個人的網頁上找到線代的教學

0:28:27.960,0:28:31.320
就在 machine learning 的網頁下面

0:28:33.820,0:28:38.520
我們就先講結論，假設你不想聽中間的過程的話

0:28:38.520,0:28:42.560
結論就是，這一個 programming 的 problem

0:28:42.560,0:28:44.860
這個 problem 它的 solution 就是

0:28:44.860,0:28:50.220
w^1 是 covariance matrix 的 eigenvector

0:28:50.220,0:28:53.260
它不只是一個  eigenvector

0:28:53.260,0:28:56.740
它是對應到最大的  eigenvalue

0:28:56.740,0:29:00.100
λ1 的那一個  eigenvector

0:29:00.100,0:29:01.600
這個就是結論

0:29:01.600,0:29:03.580
那中間的過程是怎麼樣呢？

0:29:03.580,0:29:06.420
中間的過程是，首先我們要用 Lagrange multiplier

0:29:06.420,0:29:08.120
不過，你對這個東西有碰過的話

0:29:08.120,0:29:09.860
這不是我們這一堂課應該講的

0:29:09.860,0:29:12.980
你就看一下 Bishop 的 Appendix

0:29:12.980,0:29:16.280
在做 Lagrange multiplier 的時候，這邊有個式子

0:29:16.280,0:29:18.540
這個式子長這樣子

0:29:18.540,0:29:20.980
它是先把這一項拿到這邊

0:29:20.980,0:29:25.020
再減掉 α 乘上這個 constraint

0:29:25.020,0:29:26.360
這個 constraint

0:29:26.360,0:29:28.540
然後接下來，你把這個 g

0:29:28.540,0:29:33.900
對所有的 w 做偏微分

0:29:33.900,0:29:38.020
那 w 是一個 vector

0:29:38.020,0:29:39.600
它裡面有很多的 element

0:29:39.600,0:29:40.780
所以你把這個 function

0:29:40.780,0:29:42.840
對 w 的第一個 element 做偏微分

0:29:42.840,0:29:44.580
對第二個 element 做偏微分

0:29:44.580,0:29:48.100
然後，令這些式子通通等於 0

0:29:48.100,0:29:50.980
整理一下以後，你會得到一個式子

0:29:50.980,0:29:52.640
這個式子，是這樣告訴我們的

0:29:52.640,0:29:54.500
這個式子告訴我們說

0:29:54.500,0:30:00.860
這邊的這個 solution

0:30:00.860,0:30:03.400
它會滿足下面這個式子

0:30:03.400,0:30:09.420
S*(w^1) - α*(w^1) 等於 0，再整理一下

0:30:09.420,0:30:12.080
變成這樣

0:30:12.080,0:30:15.360
那這個 w^1 呢

0:30:15.360,0:30:22.260
如果你寫成這樣，就是 S*(w^1) = α*(w^1) 的話

0:30:22.260,0:30:26.240
那 w^1 就是一個 eigenvector

0:30:26.240,0:30:28.040
w^1 就是 S 的 eigenvector

0:30:28.040,0:30:31.660
因為 (w^1)*S 等於自己乘上某一個 scalar

0:30:31.660,0:30:33.740
所以，w^1 是 S 的 eigenvector

0:30:35.780,0:30:39.960
但是現在，S 的 eigenvector 有一大把

0:30:39.960,0:30:45.060
有很多，而且你可以找到一大把 eigenvector

0:30:45.060,0:30:47.860
它的 norm 都是 1

0:30:47.860,0:30:49.800
所以，接下來你要做的事情是

0:30:49.800,0:30:51.820
看哪一個 eigenvector

0:30:51.820,0:30:54.960
代到這個式子裡面，可以 maximize

0:30:54.960,0:30:57.820
(w^1)^T * S * (w^1)

0:30:57.820,0:30:59.640
誰可以 maximize 它呢？

0:30:59.640,0:31:03.120
我們把這個整理一下

0:31:03.120,0:31:07.440
把 (w^1)^T * S * (w^1) 整理一下，它變成

0:31:07.440,0:31:12.000
這個 S*(w^1) 就是 α*(w^1)

0:31:12.000,0:31:16.960
這一項就變成 α * (w^1)^T * (w^1)

0:31:16.960,0:31:20.820
然後，(w^1)^T * (w^1)，這個是 1

0:31:20.820,0:31:25.040
所以，這一邊就得到個 α

0:31:25.460,0:31:28.440
然後，接下來就是找

0:31:28.440,0:31:32.820
誰可以讓這個 α 最大呢？

0:31:32.820,0:31:35.600
既然這個值等於 α 的話，誰可以讓 α 最大呢？

0:31:35.600,0:31:42.880
w^1 是對應到最大的
那個 eigenvalue 的 eigenvector 的時候

0:31:42.880,0:31:47.860
它可以讓 α 最大，這個 α 就是最大的 eigenvalues, λ1

0:31:47.860,0:31:51.620
如果你沒有聽懂的話，你只要記得這個結論就好

0:31:51.620,0:31:54.580
第二個，如果要找 w^2 的話

0:31:54.580,0:31:59.240
我們要解什麼樣的式子呢？我們如果要找 w^2 的話

0:31:59.240,0:32:01.240
我們要解的是

0:32:01.240,0:32:03.420
這樣子的一個 equation

0:32:03.420,0:32:06.320
我們要解說，我們要 maximize

0:32:06.320,0:32:11.220
我們要 maximize 根據 w^2 投影以後的 variance

0:32:11.220,0:32:14.980
這個寫成這樣，(w^2)^T * S * (w^2)

0:32:14.980,0:32:17.520
同時，w^2 要滿足 norm 等於 1

0:32:17.520,0:32:22.900
同時，w^2 跟 w^1 的 inner product

0:32:22.900,0:32:26.460
w^1 跟 w^2，它們要是 orthogonal 的

0:32:26.460,0:32:28.500
那這個結論是什麼呢？

0:32:28.500,0:32:30.440
解完這個問題，你會得到什麼呢？

0:32:30.440,0:32:36.480
你會得到說，w^2 也是 covariance matrix 的一個

0:32:36.480,0:32:40.600
eigenvector，然後，它對應到

0:32:40.600,0:32:45.140
第二大的 eigenvalue, λ2

0:32:46.100,0:32:49.940
現在，我們就要用 Lagrange Multiplier 來解它

0:32:49.940,0:32:53.400
這個解法就是，你先寫一個 function, g

0:32:53.400,0:32:56.740
這個 function, g 裡面包含了你要 maximize 的對象

0:32:56.740,0:33:00.120
還有你的兩個 constraint

0:33:00.120,0:33:04.000
然後，分別要乘上 α 跟 β

0:33:04.000,0:33:07.340
接下來，你對你所有的參數做偏微分

0:33:07.340,0:33:12.460
你對 w^2 裡面的每一個參數都做偏微分

0:33:12.460,0:33:15.340
你對 w^2 的第一個 element，w^2 的第二個 element

0:33:15.340,0:33:17.480
都做偏微分，做完以後

0:33:17.480,0:33:19.780
你得到這個值

0:33:19.780,0:33:24.000
S*(w^2) - α*(^2) - β*(^1) = 0

0:33:24.000,0:33:28.120
接下來，左邊同乘 w^1 的 transpose

0:33:28.120,0:33:32.680
乘 w^1 的 transpose

0:33:32.680,0:33:34.800
乘 w^1 的 transpose 會發生什麼事呢？

0:33:34.800,0:33:39.440
乘 w^1 的 transpose 以後，(w^1)^T * (w^1) 等於 1

0:33:39.440,0:33:43.400
這邊 (w^1)^T * (w^2) 等於 0

0:33:43.400,0:33:48.980
然後，這邊 (w^1)^T * S * (w^2) 等於甚麼呢？

0:33:48.980,0:33:52.220
因為這一項，它是一個 scalar

0:33:52.220,0:33:55.260
這是一個 vector，這是一個 matrix

0:33:55.260,0:33:58.840
這是一個 vector，所以，乘完以後是一個 scalar

0:33:58.840,0:34:00.740
scalar 在做 transpose 以後還是它自己

0:34:00.740,0:34:02.200
所以，你可以直接把它 transpose

0:34:02.200,0:34:03.900
結果是一樣的

0:34:03.900,0:34:05.900
做完 transpose 以後

0:34:05.900,0:34:09.640
你得到 (w^2)^T * (S^T) * (w^1)

0:34:09.640,0:34:11.140
因為 S 是 symmetric 的

0:34:11.140,0:34:12.920
所以，它做 transpose 以後還是它自己

0:34:12.920,0:34:17.600
所以，這一項 S^T 變成它自己，所以變這樣

0:34:17.600,0:34:19.600
接下來，我們已經知道

0:34:19.600,0:34:22.760
w^1是 S 的 eigenvector

0:34:22.760,0:34:26.200
而且它對應到最大的 eigenvalue, λ^1

0:34:26.200,0:34:28.600
所以，S*(w^1) = (λ^1)*(w^1)

0:34:28.600,0:34:31.800
所以，S*(w^1) = (λ^1)*(w^1)

0:34:31.800,0:34:34.940
然後，(w^1)*(w^2)^T 又等於 0

0:34:34.940,0:34:37.520
所以，這一項第一項是 0

0:34:37.520,0:34:39.760
所以，從這邊我們得到什麼結論呢？

0:34:39.760,0:34:41.500
我們得到的結論是

0:34:41.500,0:34:44.900
β = 0，得到的結論是 β = 0

0:34:44.900,0:34:49.020
如果 β 等於 0 的話，這一項就會被拿掉

0:34:49.020,0:34:54.460
所以剩下的就是 S*(w^2) - α*(w^2) = 0

0:34:54.460,0:34:56.300
然後，它會告訴我們說

0:34:56.300,0:35:01.540
S*(w^2) 等於 α*(w^2)

0:35:01.540,0:35:05.420
所以，你知道 w^2 是一個 eigenvector

0:35:05.420,0:35:09.040
但是，它是哪一個 eigenvector 呢

0:35:09.040,0:35:11.500
如果你選它是

0:35:11.500,0:35:13.800
我們知道說如果

0:35:13.800,0:35:17.680
我們都知道說這一項等於 eigenvalue 的值

0:35:17.680,0:35:19.040
等於 eigenvalue 的值

0:35:19.040,0:35:23.700
但是你不能夠選 eigenvalue 最大的那一個 eigenvector

0:35:23.700,0:35:26.540
因為它會跟，它跟 w^1

0:35:26.540,0:35:29.860
不是 orthogonal，但是你可以選第二大

0:35:29.860,0:35:34.240
憑甚麼選第二大的就跟第一大的是 orthogonal 的呢？

0:35:34.240,0:35:37.880
你就去查一下你的線代課本

0:35:37.880,0:35:39.780
因為 S 是 symmetric 的

0:35:39.780,0:35:41.820
所以，你可以這麼做

0:35:44.160,0:35:46.880
那我們今天就把 map 的部分講完

0:35:46.880,0:35:50.720
最後，這個地方要說，在 End of Warning之前

0:35:50.720,0:35:52.260
就只剩下一頁投影片

0:35:52.260,0:35:55.000
這頁投影片要說甚麼呢？這頁投影片是這樣說的

0:35:55.000,0:35:57.860
說 z = W*x

0:35:57.860,0:35:59.940
這邊有一個神奇的地方，就是

0:35:59.940,0:36:04.340
z 的 covariance 會是一個 diagonal matrix

0:36:04.340,0:36:08.180
也就是說，如果我們今天做 PCA

0:36:08.180,0:36:09.960
你原來的 data distribution

0:36:09.960,0:36:12.020
可能是這個樣子

0:36:12.020,0:36:15.860
做完 PCA 以後，你會做 decorrelation

0:36:15.860,0:36:20.520
你會讓你的不同的 dimension 間的 covariance 是 0

0:36:20.520,0:36:22.460
也就是說，如果你算

0:36:22.460,0:36:25.480
z 這個 vector 的 covariance matrix 的話

0:36:25.480,0:36:27.880
會發現它是 diagonal

0:36:27.880,0:36:30.000
這樣做有什麼好處呢

0:36:30.000,0:36:31.700
這樣做，有時候會有幫助的

0:36:31.700,0:36:37.620
假設你現在的 PCA 所得到的新的 feature

0:36:37.620,0:36:40.500
你這個 z，是一種新的 feature

0:36:40.500,0:36:43.960
這個新的 feature 是要給其他的 model 用的

0:36:43.960,0:36:45.980
而你的 model 假設，比如說是

0:36:45.980,0:36:47.780
一個 generative model

0:36:47.780,0:36:51.760
那你用 Gaussian 來描述某一個 class 的 distribution

0:36:51.760,0:36:54.700
而你在做這個 Gaussian 的假設的時候

0:36:54.700,0:37:02.740
你假設說 input data，它的 covariance 就是 diagonal

0:37:02.740,0:37:04.940
你假設不同的 dimension 之間

0:37:04.940,0:37:06.420
沒有 correlation

0:37:06.420,0:37:08.680
這樣可以減少你的參數量

0:37:08.680,0:37:12.280
你做 PCA 的時候，接下來的 model 就可以

0:37:12.280,0:37:15.460
你把你原來的 input data 做 PCA 以後

0:37:15.460,0:37:16.900
再丟給其他的 model

0:37:16.900,0:37:19.460
其他的 model 就可以假設現在的 input data

0:37:19.460,0:37:22.960
它 dimension 間沒有 correlation

0:37:22.960,0:37:25.060
所以，它就可以用比較簡單的 model

0:37:25.060,0:37:26.380
來處理你的 input data

0:37:26.380,0:37:28.840
這樣可以避免 overfitting 的情形

0:37:28.840,0:37:31.480
這件事情，怎麼說明呢？

0:37:31.480,0:37:32.900
這個也是很 trivial 的

0:37:32.900,0:37:36.280
z 的 covariance 就是

0:37:36.280,0:37:40.780
(z-z的平均) * (z-z的平均)^T

0:37:40.780,0:37:44.440
那這一項，你仔細想想看

0:37:44.440,0:37:50.320
把它展開，它是 W*S*(W^T)

0:37:50.320,0:37:53.240
S 是 x 的 covariance

0:37:53.240,0:37:56.500
然後，你就把它展開

0:37:56.500,0:37:59.780
這個 W 的

0:37:59.780,0:38:04.800
這邊有沒有寫錯，沒有寫錯

0:38:04.800,0:38:09.220
這個 W 的 transpose，它的第一個 column 就是 w^1

0:38:09.220,0:38:11.360
一直到第 K 個 column 是 w^K

0:38:11.360,0:38:13.740
把 x 乘進去

0:38:13.740,0:38:15.720
變成這個樣子

0:38:15.720,0:38:17.580
把 x 乘進去以後呢

0:38:17.580,0:38:20.400
S*(w^1)是什麼呢？

0:38:20.400,0:38:22.800
w^1 是 S 的 eigenvector

0:38:22.800,0:38:25.520
所以，(w^1)*S 等於 λ1*(w^1)

0:38:25.520,0:38:29.540
(w^K)*S 等於 λK*(w^K)

0:38:29.540,0:38:34.160
這個 w 是 eigenvector，然後 λ 是 eigenvalue

0:38:34.160,0:38:38.560
然後，我們再把 W 乘進去

0:38:38.560,0:38:46.660
那 W*(w^1) 會是什麼呢？

0:38:46.660,0:38:52.660
想想看 (w^1) 其實是 W 的第一個 row

0:38:52.660,0:38:56.320
而 W 是一個 orthogonal 的 matrix

0:38:56.320,0:39:01.040
所以，W*(w^1) 會等於 e1，e1 就是

0:39:01.040,0:39:05.920
一個 vector，它的第一維是 1，其他都是 0

0:39:05.920,0:39:11.320
W*(w^K) 會等於 eK，eK 就是第 K 維是 1，其他都是 0

0:39:11.320,0:39:17.940
這一個東西就是一個 diagonal 的 matrix，然後

0:39:17.940,0:39:21.120
warning 的部分就講完了

0:39:21.120,0:39:24.700
這個部分，或許你覺得沒有太容易理解

0:39:24.700,0:39:27.160
那我們下次從另外一個角度來看 PCA

0:39:27.160,0:39:30.820
你可能就會更清楚說 PCA 是在做什麼

0:39:30.820,0:39:33.540
今天就先講到這邊，謝謝

0:39:45.600,0:39:49.260
各位同學大家好，我們來上課吧

0:39:50.760,0:39:54.860
我們上次講到 PCA

0:39:55.920,0:40:01.900
然後，PCA 有一個很冗長的證明

0:40:01.900,0:40:05.980
然後我們說，PCA 

0:40:05.980,0:40:10.340
每次找出來的 w

0:40:10.340,0:40:15.220
第一次找出來的 w^1 是 covariance matrix

0:40:15.220,0:40:18.480
對應到最大的 eigenvalue 的 eigenvector

0:40:18.480,0:40:20.660
然後，第二個找出來的 w^2 呢

0:40:20.660,0:40:24.620
就是對應到第二大的 eigenvalue 的 eigenvector

0:40:24.620,0:40:26.280
以此類推，等等

0:40:26.280,0:40:29.240
然後，有一個很長的證明告訴你說

0:40:29.240,0:40:30.360
這麼做的話

0:40:30.360,0:40:33.860
我們每一次投影的時候都可以讓 variance 最大

0:40:33.860,0:40:36.580
假如現在這些東西，你聽不懂的話

0:40:36.580,0:40:38.320
就算了

0:40:38.320,0:40:44.100
我們來看看另外一個，可能是比較直觀的說明

0:40:44.100,0:40:51.200
另外一個比較直觀的 PCA 的想法是這樣子

0:40:51.200,0:40:55.520
假設我們現在考慮的是手寫數字

0:40:55.520,0:40:59.500
那我們知道說，這些數字其實是由一些

0:40:59.500,0:41:02.760
basic 的 component 所組成的

0:41:02.760,0:41:06.140
這些 basic 的 component 可能就代表筆劃

0:41:06.140,0:41:09.100
舉例來說，人所寫的數字

0:41:09.100,0:41:11.860
可能是有這些 basic 的 component 所組成的

0:41:11.860,0:41:16.620
有斜的直線、橫的直線、比較長的直線

0:41:16.620,0:41:19.920
還有小圈、大圈等等，所組成的

0:41:19.920,0:41:23.360
這些 basic 的 component 把它加起來以後

0:41:23.360,0:41:25.100
就可以得到一個數字

0:41:25.100,0:41:27.340
那這些 basic 的 component

0:41:27.340,0:41:30.280
我們這邊寫作 u^1, u^2, u^3 等等

0:41:30.280,0:41:32.400
那這些 basic 的 component，其實就是

0:41:32.400,0:41:35.880
一個一個的 vector，假設我們現在考慮的是 MNIST 的話

0:41:35.880,0:41:39.240
MNIST 的一張 image 是 28*28 pixel

0:41:39.240,0:41:41.680
也就是 28*28 維的一個 vector

0:41:41.680,0:41:45.940
那這些 component，其實也就是 28*28 維的 vector

0:41:45.940,0:41:47.980
把這些 vector 加起來以後

0:41:47.980,0:41:50.000
你所得到的那個 vector

0:41:50.000,0:41:52.880
把這些 vector 所代表的 component 加起來以後

0:41:52.880,0:41:54.380
你所得到的 vector

0:41:54.380,0:41:56.440
就代表了一個 digit

0:41:56.440,0:41:59.500
或者如果 j我們把它寫成 formulation 的話

0:41:59.500,0:42:01.360
寫起來像是這個樣子

0:42:01.360,0:42:07.680
x 代表了某一張 image 裡面的 pixel

0:42:07.680,0:42:10.260
某一個 image 可以用一個 vector 來表示它

0:42:10.260,0:42:12.320
那這個 vector，這邊寫作 x

0:42:12.320,0:42:16.500
那這個 x 會等於 u^1 這個 component

0:42:16.500,0:42:20.680
乘上 c^1 加上 u^2 這個 component 乘上 c2

0:42:20.680,0:42:24.580
一直加到 u^K 這個 component 乘上 cK

0:42:24.580,0:42:28.180
假設我們現在總共有 K 個 component 的話

0:42:28.180,0:42:30.900
然後再加上 x\bar

0:42:30.900,0:42:36.420
x\bar 是所有的 image 的平均，是 x\bar

0:42:36.420,0:42:37.920
所以，每一張 image

0:42:37.920,0:42:40.540
就是有一堆 component 的 linear combination

0:42:40.540,0:42:43.540
然後，再加上它的平均所組成的

0:42:43.540,0:42:46.180
舉例來說，我們說 7

0:42:46.180,0:42:51.160
是這一個 component、這一個 component 
和這一個 component 加起來以後的結果

0:42:51.160,0:42:54.140
所以，這個

0:42:54.140,0:42:58.100
對 7 來說，假設 7 就是 x 的話

0:42:58.100,0:43:00.840
c1 就是 1，c2 就是 0

0:43:00.840,0:43:03.620
c3就是 1，以此類推

0:43:03.620,0:43:06.820
那你可以用 c1, c2 到 cK

0:43:06.820,0:43:09.720
來表示一張 image

0:43:09.720,0:43:13.620
假設你這個 component 的數目是

0:43:13.620,0:43:15.760
遠比 pixel 的數目少的話

0:43:15.760,0:43:18.500
你就可以用這些

0:43:18.500,0:43:21.120
這個 component 的 weight 來描述一張 image

0:43:21.120,0:43:25.200
如果你 component 的數目比 pixel 的數目少的話

0:43:25.200,0:43:27.600
那這個描述，是會比較有效的

0:43:27.600,0:43:30.720
舉例來說，7 是一倍的 u^1，

0:43:30.720,0:43:33.460
一倍的 u^3，一倍的 u^5 所組合而成

0:43:33.460,0:43:36.400
所以，7 你就可以說它是一個 vector

0:43:36.400,0:43:40.560
它第一維、第三維、第五維是 1

0:43:41.640,0:43:44.600
那我們現在知道說

0:43:44.600,0:43:47.780
我們現在知道說 x

0:43:47.780,0:43:51.080
等於一堆 component 的 linear combination

0:43:51.080,0:43:54.400
再加上平均，我們現在把平均移到左邊

0:43:54.400,0:43:58.180
所以，x 減掉所有的 image 的平均

0:43:58.180,0:44:01.440
等於一堆 component 的 linear combination

0:44:01.440,0:44:04.720
那我們說，這一些 linear combination 的結果

0:44:04.720,0:44:07.800
我們寫作 x\head

0:44:07.800,0:44:12.160
現在假設我們不知道這些 component 是什麼

0:44:12.160,0:44:15.400
我們不知道 u^1 到 u^K 的這一些

0:44:15.400,0:44:18.240
這 K 個 vector，它們長什麼樣子

0:44:18.240,0:44:21.880
那我們要怎麼找這 K 個 vector 出來呢？

0:44:21.880,0:44:23.600
我們要做的事情

0:44:23.600,0:44:27.300
就是我們去找這 K 個 vector

0:44:27.300,0:44:32.180
使得 x\head 跟 (x-x\bar)

0:44:32.180,0:44:33.460
越接近越好

0:44:33.460,0:44:35.580
我們要找 K 個 vector

0:44:35.580,0:44:39.580
讓 (x-x\bar) 跟 x\head 越接近越好

0:44:39.580,0:44:42.100
那它們中間的差呢

0:44:42.100,0:44:45.560
沒有辦法用這個 component 來描述的部分

0:44:45.560,0:44:47.780
叫做 Reconstruction error

0:44:47.780,0:44:49.660
那接下來，我們要做的事情就是

0:44:49.660,0:44:53.280
找 K 個 component，其實就是 K 個 vector

0:44:53.280,0:44:55.920
它可以 minimize 這個 Reconstruction error

0:44:55.920,0:44:57.240
這個 Reconstruction error

0:44:57.240,0:44:59.620
如果你要把 formulation 寫出來的話呢

0:44:59.620,0:45:03.400
就是我有一個 Reconstruction error 寫成 L

0:45:03.400,0:45:06.880
我們要找 K 個 vector 去 minimize 它

0:45:06.880,0:45:10.220
要 minimize 的對象就是 (x-x\bar)

0:45:10.220,0:45:15.260
減掉下面這一項的 x\head

0:45:15.260,0:45:17.740
減掉 x\head 的 2-norm

0:45:17.740,0:45:21.440
x\head 是一堆 component 的 linear combination

0:45:22.480,0:45:25.320
那我們先來回憶一下 PCA

0:45:25.320,0:45:27.700
在 PCA 裡面，我們講說

0:45:27.700,0:45:30.840
我們要找一個 matrix, W

0:45:30.840,0:45:34.680
我們原來的 vector, x 乘上 W 以後

0:45:34.680,0:45:39.280
就得到 Dimension Reduction 以後的結果，z

0:45:39.280,0:45:43.340
那我們可以把 W 的每一個 row 都寫出來

0:45:43.340,0:45:45.440
w1, w2 一直到 wK

0:45:51.740,0:45:53.520
那我們說 w1, w2 一直到 wK

0:45:53.520,0:45:56.700
都是 covariance matrix 的 eigenvector

0:45:56.700,0:46:00.380
事實上，如果你要解這個式子

0:46:00.380,0:46:04.040
你要解這個式子，找出 u^1, u^2 到 u^K

0:46:04.620,0:46:09.660
這個 w^1 到 w^K，就是由 PCA 找出來的這個解

0:46:09.660,0:46:14.040
其實，就是可以讓上面這一個式子

0:46:14.040,0:46:17.520
最小化，就可以讓這個 Reconstruction error 最小

0:46:17.520,0:46:20.080
的 u^1 到 u^K

0:46:20.080,0:46:23.260
這個在 Bishop 裡面是有 prove 的

0:46:23.260,0:46:25.400
那我這邊講的跟 Bishop 有點不太一樣而已

0:46:25.400,0:46:28.720
用一個比較簡單的方式來說明給大家聽

0:46:28.720,0:46:34.600
我們現在在 database 裡面有一大堆的 x

0:46:34.600,0:46:35.880
一大堆的 x

0:46:35.880,0:46:37.720
現在假設有一個 x^1

0:46:37.720,0:46:41.020
這個 x^1 減掉平均，x\bar

0:46:41.020,0:46:45.620
等於 u^1 乘上 component 的 weight, c(上標 1, 下標 1)

0:46:45.620,0:46:47.940
這邊 c(上標 1, 下標 1)的意思是說

0:46:47.940,0:46:51.400
c(下標 1) 代表說它是 u^1 的 weight

0:46:51.400,0:46:58.760
上標 1 代表說它是 x^1 的 u1 這個 component 的 weight

0:46:58.760,0:47:01.920
我想這個大家應該知道我的意思

0:47:02.620,0:47:09.320
所以，(x^1 - x\bar) 就等於 c1*(u^1) + c2*(u^2)

0:47:09.320,0:47:12.900
那 (x^1-x\bar)，它是一個 vector

0:47:12.900,0:47:16.220
那我們把這個 vector 拿出來

0:47:16.220,0:47:19.360
這個 u^1, u^2 到 u^K

0:47:19.360,0:47:22.840
它們是一排 vector，就把它們排起來

0:47:22.840,0:47:25.600
其實，它排起來就是一個 matrix

0:47:25.600,0:47:27.640
這個 column 的數目

0:47:27.640,0:47:30.180
是 K 個 column

0:47:30.180,0:47:34.380
那前面 c1, c2 呢？

0:47:34.380,0:47:36.240
它們就是我們把這個

0:47:36.240,0:47:39.620
c1, c2，這邊有個錯誤的動畫

0:47:39.620,0:47:43.420
我們把這個 c1, c2 排成一排

0:47:43.420,0:47:45.100
排成一排這在這邊

0:47:45.100,0:47:48.380
那你現在把這個 c1 乘上它

0:47:48.380,0:47:50.000
把這個 c2 乘上它

0:47:50.000,0:47:51.660
就會得到這個 vector

0:47:51.660,0:47:53.440
也就是說，你把這一個 vector

0:47:53.440,0:47:57.760
你把這些 component 的 weight 排成一排

0:47:57.760,0:47:59.940
這個是一個 vector，這個 vector 乘以這個 matrix

0:47:59.940,0:48:02.320
就會得到這個 vector

0:48:02.320,0:48:05.940
那我們這個 data set 裡面不是只有一筆 data

0:48:05.940,0:48:08.400
我們還有很多，比如說，這邊有一個 x^2

0:48:08.400,0:48:11.360
那 (x^2-x\bar)

0:48:11.360,0:48:14.240
它就是這個第二個黃色的 vector

0:48:14.240,0:48:17.500
u^1, u^2 就在這邊

0:48:17.500,0:48:20.440
第二個 component 的 c1, c2

0:48:20.440,0:48:22.860
第二個 component 的 c1, c2 跟第一個 component 的 c1, c2

0:48:22.860,0:48:25.740
它們是不一樣的，它們的 notation 這邊是不一樣的

0:48:25.740,0:48:29.960
我們把這個值擺在這一邊

0:48:29.960,0:48:33.740
那你把這個 vector 乘上這個 matrix

0:48:33.740,0:48:35.880
就會得到這個 vector

0:48:35.880,0:48:37.600
以此類推

0:48:37.600,0:48:39.660
你把這個 vector

0:48:39.660,0:48:44.820
x3 的 component 的 weight 乘上

0:48:44.820,0:48:46.820
component，就會得到這個 vector

0:48:46.820,0:48:49.880
那如果我們把所有的 data

0:48:49.880,0:48:54.000
都用這個式子來表示，都把它畫在下面的話

0:48:54.000,0:48:57.200
那這一邊我們就得到一個 matrix

0:48:57.200,0:48:59.960
在這個 matrix 的橫軸

0:48:59.960,0:49:03.200
column 的數目就是你的 data 的數目

0:49:03.200,0:49:06.800
你有一萬筆 data，這個橫軸就是一萬

0:49:06.800,0:49:10.780
那我們在要做的事情就是

0:49:10.780,0:49:15.000
用這一個 matrix 去乘上這一個 matrix

0:49:15.000,0:49:17.520
這兩個都是 matrix，把這兩個 matrix

0:49:17.520,0:49:20.260
乘上這個 matrix，那希望它

0:49:20.260,0:49:22.980
越接近這個 matrix 越好

0:49:22.980,0:49:27.520
所以你要 minimize 這兩個相乘以後得到的 matrix

0:49:27.520,0:49:29.940
跟左邊這個 matrix 之間的差距

0:49:29.940,0:49:32.720
你要 minimize 這個 Reconstruction error

0:49:32.720,0:49:35.160
那怎麼解這個問題呢？

0:49:35.160,0:49:37.660
假如你有修過大一線代的話

0:49:37.660,0:49:42.320
你就知道這個問題是怎麼解的

0:49:42.320,0:49:45.680
以下是我教線代的時候的投影片

0:49:45.680,0:49:47.880
放在這邊給大家參考

0:49:47.880,0:49:50.340
這個在線代裡面是怎麼說的呢？

0:49:50.340,0:49:55.120
每一個 matrix, X

0:49:55.120,0:49:59.520
這邊這一個 matrix, X，你可以用 SVD

0:49:59.520,0:50:02.620
把它拆成一個

0:50:02.620,0:50:09.200
matrix, U 乘上一個 matrix, Σ 乘上一個 matrix, V

0:50:09.200,0:50:12.540
這個 U 是 m*k 維

0:50:12.540,0:50:16.280
這個 Σ 是 k*k 維，這個 V 是 k*n 維

0:50:16.280,0:50:19.620
這個 k，它就是 component 的數目

0:50:19.620,0:50:24.060
所以，我們把這個 X 分解成 U, Σ 跟 V

0:50:24.060,0:50:27.900
這一個 U 就是這一個

0:50:27.900,0:50:32.540
那這個 Σ*V 就是這一個

0:50:32.540,0:50:36.440
那我們知道說，如果我們今天的

0:50:36.440,0:50:39.040
我們用 SVD 的方法

0:50:39.040,0:50:42.300
把 X 拆成這三個 matrix 相乘

0:50:42.300,0:50:45.460
那右邊這三個 matrix 相乘的結果

0:50:45.460,0:50:48.660
跟左邊這一個 matrix 它們之間

0:50:48.660,0:50:51.960
的這個 Frobenius 的 norm 呢

0:50:51.960,0:50:54.700
是會被 minimize 的，也就是說

0:50:54.700,0:50:58.980
我們用 SVD 提供給我們的一個 matrix 的拆解方法

0:50:58.980,0:51:01.160
你拆出來的這 3 個 matrix 相乘，它跟

0:51:01.160,0:51:03.320
左邊這一個 matrix，是最接近的

0:51:03.320,0:51:06.400
那解出來結果是怎麼樣

0:51:06.400,0:51:08.980
如果你還記得的話，解出來的結果是這樣子

0:51:08.980,0:51:12.320
U 這個 matrix，它的 k 個 column

0:51:12.320,0:51:17.640
其實就是一組 orthonormal 的 vector

0:51:17.640,0:51:20.180
這一組 orthonormal 的 vector

0:51:20.180,0:51:23.100
它們是 x 乘上 x 的 transpose

0:51:23.100,0:51:27.800
它們是 X*(X^T) 的 eigenvector

0:51:27.800,0:51:31.840
它們這邊總共有 k 個 vector

0:51:31.840,0:51:34.420
這邊總共有 k 個 orthonormal 的 vector

0:51:34.420,0:51:37.580
這 k 個 orthonormal 的 vector 它們就對應到

0:51:37.580,0:51:43.760
X*(X^T) 最大的 k 個  eigenvalue 的 eigenvector

0:51:43.760,0:51:45.960
這個大家聽得懂嗎？

0:51:45.960,0:51:48.040
講到這邊大家有問題嗎？

0:51:48.740,0:51:50.960
那你會發現說

0:51:50.960,0:51:53.740
這個 X*(X^T) 是什麼呢？

0:51:53.740,0:51:55.680
這個 X*(X^T) 是什麼

0:51:55.680,0:52:00.540
這個 X*(X^T) 不就是 covariance matrix 嗎？

0:52:00.540,0:52:04.400
那我們說之前 PCA 找出來的那一些

0:52:04.400,0:52:08.220
w 就是 covariance matrix 的 eigenvector

0:52:08.220,0:52:11.340
而我們在這邊要說，我們做 SVD

0:52:11.340,0:52:13.360
你解出來的 U 的每一個 column

0:52:13.360,0:52:16.760
就是 covariance matrix 的 eigenvector

0:52:16.760,0:52:19.800
所以 U 這個解，其實就是

0:52:19.800,0:52:25.300
就是 PCA 得出來的解

0:52:25.300,0:52:28.960
所以，我們知道說 PCA 現在在做的事情

0:52:28.960,0:52:34.160
你找出來的，你從根據 PCA，你找出來的那些 w

0:52:34.160,0:52:37.380
你找出來的 Dimension Reduction 的 transform

0:52:37.380,0:52:41.260
其實，就是在 minimize 這個 Reconstruction error

0:52:41.260,0:52:43.580
那 Dimension Reduction 的結果

0:52:43.580,0:52:45.520
你得到其實就是這些 vector

0:52:45.520,0:52:47.360
你得到的就是這些 vector

0:52:47.360,0:52:49.360
PCA 裡面你得到的那些 W

0:52:49.360,0:52:51.820
其實就是 component

0:52:54.800,0:52:57.880
如果大家知道這些的話

0:52:57.880,0:53:02.640
我們等一下會看說 PCA 跟 neural network 有什麼樣的關係

0:53:02.640,0:53:05.940
那我們現在已經知道說

0:53:05.940,0:53:09.380
從用 PCA 找出來的 w^1 到 w^K

0:53:09.380,0:53:14.700
就是 K 個 component，u^1, u^2 到 u^K

0:53:14.700,0:53:18.760
那我們說我們有一個

0:53:18.760,0:53:22.740
根據 component linear combination 的結果叫做 x\head

0:53:22.740,0:53:27.860
它是 (w^K)*ck 做 linear combination 的結果

0:53:27.860,0:53:30.080
那我們會希望說，這個 x\head

0:53:30.080,0:53:34.080
跟 (x - x\bar)

0:53:34.080,0:53:37.940
(x - x\bar)，它的平均的是越小越好

0:53:37.940,0:53:40.540
你要 minimize 這個 Reconstruction error

0:53:40.540,0:53:43.500
那我們現在已經根據 SVD

0:53:43.500,0:53:46.640
找出來的 W，W 已經找出來了

0:53:46.640,0:53:50.860
W 已經找出來了，那 ck 的值到底應該是多少呢？

0:53:50.860,0:53:55.560
這個 ck 是每一個 example

0:53:55.560,0:53:58.340
如果是 image recognition 的話，就每一個 image

0:53:58.340,0:54:00.380
都有一組自己的 ck

0:54:00.380,0:54:03.900
所以，你要找這個 ck 就每一個 image 各自找就好

0:54:03.900,0:54:06.000
每一個 image 各自找就好了

0:54:06.000,0:54:07.980
那這個問題，其實就是問說

0:54:07.980,0:54:11.880
我現在有 K 維的 vector

0:54:11.880,0:54:14.360
它們做 span 以後，得到一個 space

0:54:14.360,0:54:17.000
如果我現在

0:54:17.000,0:54:21.020
要用 c1 到 cK 對它做 linear combination

0:54:21.020,0:54:24.780
怎麼樣才能夠最接 (x-x\bar)

0:54:24.780,0:54:26.660
怎麼樣才能夠最接 (x-x\bar) 呢

0:54:26.660,0:54:31.120
因為現在這 K 個 vector 它們是 orthonormal 的

0:54:31.120,0:54:34.420
所以你要得到這個 ck，其實是很簡單的，你只要把

0:54:34.420,0:54:37.880
(x-x\bar) 跟 w^k 做 inner product

0:54:37.880,0:54:39.760
你要找一組 ck 可以

0:54:39.760,0:54:41.400
那這個性質是來自於說

0:54:41.400,0:54:44.020
minimize 左邊這個跟右邊這個的 error

0:54:44.020,0:54:46.740
你只需要把

0:54:46.740,0:54:50.320
這個 (x-x\bar) 跟 w^k 做 inner product 就好了

0:54:50.320,0:54:52.060
那這個性質是來自於說

0:54:52.060,0:54:55.000
這 K 個 vector 是 orthonormal 的

0:54:55.000,0:54:56.860
這個如果你有困惑的話

0:54:56.860,0:55:00.880
就回去 check 一下線性代數的課本

0:55:00.880,0:55:04.720
那我們現在已經知道了這些事情

0:55:04.720,0:55:06.420
我們已經知道說

0:55:06.420,0:55:08.540
c^k 就是長成這個樣子

0:55:08.540,0:55:11.200
那這件事情呢

0:55:11.200,0:55:13.280
這個做 linear combination 的事情

0:55:13.280,0:55:16.620
其實你可以想成用 neural network 來表示它

0:55:16.620,0:55:19.380
什麼意思呢？

0:55:19.380,0:55:23.840
假設我們的 (x-x\bar) 就是一個 vector

0:55:23.840,0:55:28.200
這邊寫作一個三維的 vector

0:55:28.200,0:55:32.540
那我們假設，現在 K 只有兩個 component

0:55:32.540,0:55:34.360
K 等於 2

0:55:34.360,0:55:38.240
那我們先算出 c^1 跟 c^2

0:55:38.240,0:55:39.740
怎麼算 c^1 呢？

0:55:39.740,0:55:43.900
c^1 就是 (x-x\bar) 跟 w^1 的 inner product

0:55:43.900,0:55:46.480
所謂的 inner product 就是 element-wise 的相乘

0:55:46.480,0:55:49.040
也就是把 (x-x\bar) 的每一個 component

0:55:49.040,0:55:51.460
乘上 w^1 的每一個 component

0:55:51.460,0:55:54.360
接下來，你就得到 c^1

0:55:54.360,0:55:56.140
這件事情就好像是說

0:55:56.140,0:55:58.760
這個是 neural network 的 input

0:55:58.760,0:56:02.300
這是一個 neuron，這是 neuron 的 weight

0:56:02.300,0:56:05.620
這個 neuron 它是 linear 的 neuron

0:56:05.620,0:56:09.400
它沒有 activation function，它是 linear 的

0:56:09.400,0:56:12.980
那這個 neuron，你把這個東西 input

0:56:12.980,0:56:15.640
乘上這個 weight，你就得到 c^1

0:56:15.640,0:56:17.520
那 c^2 也是一樣

0:56:17.520,0:56:21.500
c^2，我們這邊

0:56:21.500,0:56:24.880
這邊是這樣子，那我們接下來要把 c^1

0:56:25.320,0:56:27.280
我這邊犯了一個錯

0:56:27.280,0:56:30.140
這個應該是下標

0:56:30.140,0:56:32.540
這個應該是下標

0:56:32.540,0:56:35.940
如果統一起來的話，這個 K 應該是下標

0:56:35.940,0:56:39.640
那我們把這個 c1 乘上 w^1

0:56:39.640,0:56:43.240
所謂的 c1*(w^1)是什麼意思呢？

0:56:43.240,0:56:46.160
你就把 c1*(w^1)

0:56:46.160,0:56:51.180
把 c1 乘上 w^1 的第一維，得到一個 value

0:56:51.180,0:56:55.100
乘上 w^1 的第二維，得到一個 value；
乘上 w^1 的第三維，得到一個 value

0:56:55.100,0:56:58.400
這一項，就是 c1*(w^1)

0:56:58.400,0:57:01.400
接下來，我們再算一下 c^2

0:57:01.400,0:57:04.860
c^2 一樣就是這個 input 一樣乘上

0:57:04.860,0:57:07.040
跟這個 w^2 做 inner product

0:57:07.040,0:57:11.160
得到 c^2，然後再把這個 c^2

0:57:11.160,0:57:15.760
w^2 的三個 element

0:57:15.760,0:57:19.680
再跟原來 w^1 的三個 element 加起來

0:57:19.680,0:57:23.300
得到最後的 output，這一項就是 x\head

0:57:23.300,0:57:25.260
這一項就是 x\head

0:57:25.260,0:57:28.520
接下來，我們 training 的 criteria

0:57:28.520,0:57:30.680
就是 minimize

0:57:30.680,0:57:34.180
我們要讓這個 x\head  跟 (x-x\bar) 越接近越好

0:57:34.180,0:57:36.620
你所以，我們就是希望這個 neural network 的 output

0:57:36.620,0:57:41.080
跟 (x-x\bar) 越接近越好

0:57:41.080,0:57:43.620
這是我們的 input, (x-x\bar)

0:57:43.620,0:57:44.960
它乘上一組 weight

0:57:44.960,0:57:47.160
再 hidden layer 的 output 是 c^1, c^2

0:57:47.160,0:57:49.140
再乘上另外一組 weight，得到 x\head

0:57:49.140,0:57:52.960
那我們希望 (x-x\bar) 越接近越好

0:57:52.960,0:57:55.700
那你就會發現說

0:57:55.700,0:57:59.380
其實 PCA 可以表示成一個 neural network

0:57:59.380,0:58:01.320
它可以表示成一個 neural network，然後

0:58:01.320,0:58:04.300
這個 neural network 它只有一個 hidden layer

0:58:04.300,0:58:07.120
然後，這個 hidden layer 是 linear 的 activation function

0:58:07.120,0:58:11.100
然後，我們現在 train 這個 neural network 的 criterion

0:58:11.100,0:58:15.460
是要讓 input 一個東西，得到 output

0:58:15.460,0:58:18.780
結果這個 output 要跟 input 越接近越好

0:58:18.780,0:58:20.500
這個 output 要跟 input 越接近越好

0:58:20.500,0:58:22.760
這個 output 要跟 input 越接近越好

0:58:22.760,0:58:26.220
這個東西就叫做 Autoencoder

0:58:26.220,0:58:31.640
那這邊我們就有一個問題

0:58:31.640,0:58:34.100
我們這邊就有一個問題

0:58:34.100,0:58:36.540
假設我們現在這個 weight

0:58:36.540,0:58:38.920
不是用 PCA 的方法

0:58:38.920,0:58:42.020
也就不是用找 eigenvector 的方法

0:58:42.020,0:58:46.240
去找出這些 w^1, w^2 ......w^K

0:58:46.240,0:58:47.620
而是，兜一個 neural network

0:58:47.620,0:58:51.100
直接用我們要 minimize 這個 error 的 criterion

0:58:51.100,0:58:55.120
然後用 Gradient Descent 去 train 一發的話

0:58:55.120,0:58:56.960
那你覺得你得到的結果

0:58:56.960,0:59:01.040
會跟用 PCA 解出來的結果一樣嗎？

0:59:01.840,0:59:04.480
給大家一秒鐘的時間想一想

0:59:04.480,0:59:06.860
你覺得會一樣的同學舉手

0:59:06.860,0:59:08.980
有些同學同學會覺得一樣，好

0:59:08.980,0:59:10.900
手放下來 ，你覺得會不一樣的同學舉手

0:59:10.900,0:59:12.700
也有一些同學覺得會不一樣的

0:59:12.700,0:59:14.560
覺得不一樣的稍微多一點

0:59:14.560,0:59:16.320
其實，是會不一樣的

0:59:16.320,0:59:18.940
你仔細想看看 PCA 解出來這些 W

0:59:19.360,0:59:22.280
它們是 orthonormal 的，它們是 orthogonal 的

0:59:22.280,0:59:24.200
它們是垂直的

0:59:24.200,0:59:27.100
你今天如果你用 neural network

0:59:27.100,0:59:29.780
你就兜一個 neural network，硬 learn 一發

0:59:29.780,0:59:32.820
你得到的結果，你沒有辦法保證，會是垂直的阿

0:59:32.820,0:59:34.840
你會得到一個 solution

0:59:34.840,0:59:37.120
但是你沒有辦法保證說

0:59:37.120,0:59:41.300
這組 weight，這個 w^1 跟 w^2 是垂直的

0:59:41.300,0:59:45.180
你得到的是另外一組解

0:59:45.180,0:59:46.900
這樣大家了解我的意思嗎 ?

0:59:46.900,0:59:49.360
我們在前面 SVD 的證明裡面

0:59:49.380,0:59:53.500
已經說 PCA 導出來的這組解  w^1 到 w^K

0:59:53.500,0:59:58.020
它可以讓我們的 Reconstruction error 被 minimize

0:59:58.020,1:00:00.500
那你用這個 neural network 的方法

1:00:00.500,1:00:02.480
去用 Gradient Descent 硬解一發

1:00:02.480,1:00:04.360
你其實也不可能找出來

1:00:04.360,1:00:06.560
你不可能讓你的 Reconstruction error 比

1:00:06.560,1:00:11.340
比 PCA 找出來的 Reconstruction error 還要小

1:00:11.340,1:00:15.100
所以，如果是在 linear 的情況下

1:00:15.100,1:00:16.460
或許你就會想要用

1:00:16.460,1:00:20.040
直接用 PCA 來找這個 w，是比較快的

1:00:20.040,1:00:24.160
你用 neural network，或許是比較麻煩的

1:00:24.160,1:00:27.020
但是，用 neural network 的好處就是

1:00:27.020,1:00:28.760
它可以是 deep 的

1:00:28.760,1:00:31.760
這邊為什麼，只可以有一個 hidden layer 呢

1:00:31.760,1:00:34.280
它的一個 hidden layer，就要改成很多的 hidden layer

1:00:34.280,1:00:37.000
所以這個，就是我們等一下會講的

1:00:37.000,1:00:40.980
下一堂課會講的 Deep Autoencoder

1:00:42.200,1:00:46.920
那 PCA 其實有一些很明顯的弱點

1:00:46.920,1:00:48.760
一個就是，因為它是 unsupervised

1:00:48.760,1:00:50.000
它是 unsupervised

1:00:50.000,1:00:54.320
所以今天假如給它一大堆點，沒有 label

1:00:54.320,1:00:58.700
那對 PCA 來說，假設你把它 project 到一維上

1:00:58.720,1:01:01.760
PCA 會找一個可以讓 data variance 最大的

1:01:01.760,1:01:02.680
那一個 dimension

1:01:02.680,1:01:03.880
比如說，在這個case 裡面

1:01:03.880,1:01:06.240
或許，它就把它 project 到這一維上

1:01:06.240,1:01:09.260
把每一個綠色點，都 project 到這一維上

1:01:09.260,1:01:12.280
但是，有一個可能是，或許其實

1:01:12.280,1:01:15.200
這兩組 data point，它們分別代表了

1:01:15.200,1:01:17.560
兩個 class，代表了兩個 class

1:01:17.560,1:01:20.800
如果你用 PCA 這個方法來做 Dimension Reduction 的話

1:01:20.800,1:01:24.180
你就會使得這兩個藍色跟橙色的 class

1:01:24.180,1:01:25.660
被 merge 在一起

1:01:25.660,1:01:28.640
它們在 PCA 找出來的 single dimension 上

1:01:28.640,1:01:30.920
完全被混在一起，就無法分別

1:01:30.920,1:01:32.960
這個時候怎麼辦呢

1:01:32.960,1:01:36.520
你可能會需要引入 labeled data

1:01:36.520,1:01:42.160
那 LDA，是考慮這個 labeled data 的一個降維的方法

1:01:42.160,1:01:46.780
不過它是 supervised，所以這邊就不是我們要講的對象

1:01:46.780,1:01:51.420
這個 LDA 是 linear discriminant analysis 的縮寫

1:01:51.420,1:01:55.500
另外一個 PCA 的弱點，就是它是 linear 的

1:01:55.500,1:01:58.100
我們剛才在一開始舉例子的時候，我們會說

1:01:58.100,1:02:01.100
這一邊有一個 S 形的這個 manifold

1:02:01.100,1:02:04.820
這邊有一堆的點，它的分佈像是一個 S 形

1:02:04.820,1:02:07.820
那我們期待做 dimension reduction 以後

1:02:07.820,1:02:10.100
可以把這個 S 形的曲面

1:02:10.100,1:02:12.280
把它拉直，但是這一件事情

1:02:12.280,1:02:14.060
對 PCA 來說是做不到的

1:02:14.060,1:02:17.120
你要它做這一件事情，其實是強人所難

1:02:17.120,1:02:19.920
因為你要把這一個 S 形的曲面拉直

1:02:19.920,1:02:21.660
是一個 non-linear 的 transformation

1:02:21.660,1:02:23.400
PCA 做不到這件事情

1:02:23.400,1:02:25.000
如果你做 PCA 的話

1:02:25.000,1:02:26.200
你得到的結果

1:02:26.200,1:02:28.180
就是把這個 S 形的曲面做 PCA

1:02:28.180,1:02:30.620
你得到的結果就是這樣

1:02:30.620,1:02:36.040
你就是把它從藍色這一邊把它打扁這樣

1:02:36.040,1:02:37.980
你會發現說藍色跟紅色的點

1:02:37.980,1:02:41.520
那就被壓在一起，然後綠色的點在這邊

1:02:41.520,1:02:42.960
黃色的點在這一邊

1:02:42.960,1:02:45.480
你會把它打扁而不是把它拉開

1:02:45.480,1:02:47.480
因為把它拉開這件事情

1:02:47.480,1:02:48.960
是 PCA 辦不到的

1:02:48.960,1:02:53.020
那我們等一下會講 non-linear 的 transformation

1:02:53.440,1:02:57.500
再來，我們就把 PCA 用在一些實際的問題上

1:02:57.500,1:03:00.440
比如說，我們用它來分析寶可夢的 data

1:03:00.440,1:03:03.560
我們這一門課的例子都是寶可夢的例子

1:03:03.560,1:03:05.760
寶可夢，我們都知道說

1:03:05.760,1:03:08.380
它總共有 800 種寶可夢

1:03:08.380,1:03:11.840
那其實有人說是 721 種，因為在 800 種裡面

1:03:11.840,1:03:13.820
有一些是重複的

1:03:13.820,1:03:15.620
我也不知道怎麼解釋

1:03:15.620,1:03:18.360
反正就是假設有 800 種好了

1:03:18.360,1:03:21.340
那每一種寶可夢，你可以用 6 個 feature

1:03:21.340,1:03:23.340
來表示它，分別是

1:03:23.340,1:03:25.180
生命值、攻擊力、防禦力

1:03:25.180,1:03:29.860
特殊攻擊力、特殊防禦力還有它的速度等等

1:03:29.860,1:03:31.760
所以，它們就是

1:03:31.760,1:03:34.820
每一個寶可夢就是一個六維度的 data point

1:03:34.820,1:03:36.580
就是一個六維的 vector

1:03:36.580,1:03:40.740
那我們現在，要用 PCA 來分析它

1:03:40.740,1:03:44.200
那在用 PCA 的時候，常常會有人問的問題就是

1:03:44.200,1:03:46.860
我需要有多少個 component

1:03:46.860,1:03:49.460
我到底要把它 project 到一維

1:03:49.460,1:03:52.340
還是二維、還是三維

1:03:52.340,1:03:56.040
這個資訊量才足夠呢，這個 depend on 你想要做甚麼

1:03:56.040,1:03:57.580
假設你想要做 Visualization

1:03:57.580,1:04:01.660
因為現在每一個寶可夢都是 6 維

1:04:01.660,1:04:04.540
你沒有辦法了解這個寶可夢

1:04:04.540,1:04:06.760
它們之間的特性有什麼樣的關係

1:04:06.760,1:04:08.160
因為 6 維你沒有辦法看

1:04:08.160,1:04:10.880
所以，你可能就會想把它 project 到二維

1:04:10.880,1:04:12.780
就比較容易分析

1:04:13.380,1:04:17.200
那其實要用多少的 principle component，就好像是

1:04:17.200,1:04:19.700
neural network 要有幾個 layer，每個 layer 要有多少個

1:04:19.700,1:04:23.400
hidden variable，要有幾個 neuron 一樣

1:04:23.400,1:04:25.780
所以，這個是你要自己決定的

1:04:25.780,1:04:27.560
那一個常見的方法是這樣

1:04:27.560,1:04:29.300
一個常見的方法是說

1:04:29.300,1:04:36.560
我們去計算每一個 principle component 的 λ

1:04:36.560,1:04:38.480
我們知道每一個 principle component 就是一個

1:04:38.480,1:04:41.220
eigenvector，然後這個 eigenvector 又對應到一個

1:04:41.260,1:04:44.280
eigenvalue，就是這邊的 λ

1:04:44.280,1:04:46.380
那這個 eigenvalue 代表什麼意思呢？

1:04:46.380,1:04:48.280
這個 eigenvalue 代表說

1:04:48.280,1:04:51.520
我們用這個 principle component

1:04:51.520,1:04:54.120
去做 dimension reduction 的時候

1:04:54.180,1:04:57.140
在 principle component 的 dimension 上

1:04:57.140,1:05:01.540
它的 variance 有多大，那個 variance 就是 λ

1:05:01.540,1:05:05.700
今天這個寶可夢的 data 總共有 6 維

1:05:05.700,1:05:08.140
所以它 covariance matrix 是六維

1:05:08.140,1:05:12.020
所以，你可以找出 6 個 eigenvector

1:05:12.020,1:05:15.620
你可以找出 6 個 eigenvalue

1:05:15.620,1:05:17.780
我們現在來計算一下

1:05:17.780,1:05:20.700
每一個 eigenvalue 的 ratio

1:05:20.700,1:05:22.200
每一個 eigenvalue 的 ratio

1:05:22.200,1:05:26.040
我們就把每一個 eigenvalue  除掉 6 個 eigenvalue 的總和

1:05:26.040,1:05:28.300
你得到的結果，會是這個樣子

1:05:28.300,1:05:29.840
第一個 eigenvalue

1:05:29.840,1:05:33.060
它佔全部的 eigenvalue 的 0.45

1:05:33.060,1:05:35.440
第二個是 0.18，第三個是 0.13

1:05:35.440,1:05:36.960
以此類推

1:05:36.960,1:05:38.640
現在看到這個結果

1:05:38.640,1:05:41.560
我們可以從這個結果看出來說

1:05:41.560,1:05:47.080
這一邊是 0.45, 0.18, 0.13, 0.12，再來是 0.07, 0.04

1:05:47.080,1:05:49.780
所以，第 5 個 principle component

1:05:49.780,1:05:52.760
和第 6 個 principle component，它們的作用

1:05:52.760,1:05:54.000
其實是比較小的

1:05:54.000,1:05:55.940
你用這兩個 dimension

1:05:55.940,1:05:57.840
來做 projection 的時候

1:05:57.840,1:06:00.320
你 project 出來的 variance 是很小的

1:06:00.320,1:06:03.460
代表說，現在寶可夢的這這些特性

1:06:03.460,1:06:06.740
在第五個和第六個 principle component 上

1:06:06.740,1:06:09.460
是沒有太多的 information

1:06:09.460,1:06:11.780
它 variance 很小，所以它沒有太多的information

1:06:11.780,1:06:14.800
所以，如果我們今天要分析寶可夢的 data 的話

1:06:14.800,1:06:19.220
感覺只需要前面 4 個 principle component 就好了

1:06:19.220,1:06:21.920
所以，我們就實際來分析一下

1:06:21.920,1:06:23.560
實際來分析一下

1:06:23.560,1:06:25.620
如果你做 PCA 以後

1:06:25.620,1:06:28.020
我們得到的四個 principle component

1:06:28.020,1:06:29.920
就是這個樣子

1:06:29.920,1:06:32.360
然後，每一個 principle component 就是一個 vector

1:06:32.360,1:06:36.240
現在每一個寶可夢，它是有用六維的 vector 來描述它

1:06:36.240,1:06:38.540
所以呢，每一個 principle component

1:06:38.540,1:06:40.640
就是一個六維的 vector

1:06:40.640,1:06:43.840
這 4 個 principle component 就是 4 個六維的 vector

1:06:43.840,1:06:46.000
那我們來看一下每一個 principle component

1:06:46.000,1:06:48.440
它在做的事情是什麼

1:06:48.440,1:06:50.960
如果我們看第一個 principle component

1:06:50.960,1:06:55.060
第一個 principle component，它的每一個 dimension

1:06:55.060,1:06:57.720
每一個 dimension 都是正的

1:06:57.720,1:07:01.560
每一個 dimension 都是正的

1:07:01.560,1:07:04.380
那這個東西是什麼意思，這個東西其實就

1:07:04.380,1:07:07.460
代表寶可夢的強度

1:07:07.460,1:07:11.920
知道嗎？就是如果你要產生一隻寶可夢的時候

1:07:11.920,1:07:13.560
每一個寶可夢都是

1:07:13.560,1:07:18.760
由這 4 個 vector 做 linear combination

1:07:18.760,1:07:20.580
每一個寶可夢都可以想成是

1:07:20.580,1:07:22.820
它的數值可以想成是，這 4 個 vector

1:07:22.820,1:07:25.160
做 linear combination 的結果

1:07:25.160,1:07:27.660
combine 的 weight，每一隻寶可夢是不一樣的

1:07:27.660,1:07:31.740
所以，如果你在選第一個

1:07:31.740,1:07:34.460
principle component 的時候，你給它的 weight 比較大

1:07:34.460,1:07:37.440
那這個寶可夢它的六維都是強的

1:07:37.440,1:07:40.920
那如果選擇的值小，它的六維都是弱的

1:07:40.920,1:07:42.540
所以，在 第一個 principle component

1:07:42.540,1:07:45.240
就代表了這一隻寶可夢強度

1:07:45.240,1:07:47.720
那第二個 principle component 是什麼呢？

1:07:47.720,1:07:49.520
第二個 principle component 它在

1:07:49.520,1:07:53.960
它在防禦力的地方是正值

1:07:53.960,1:07:56.700
它在速度的地方是負值

1:07:56.700,1:07:59.620
也就是說，如果你選

1:08:01.460,1:08:04.100
這個防禦力跟速度

1:08:04.100,1:08:05.900
是成反比的

1:08:05.900,1:08:07.080
是成反比的

1:08:07.080,1:08:09.220
你今天選，你今天給這個

1:08:09.220,1:08:11.160
第二個 component 一個 weight 的時候

1:08:11.160,1:08:13.880
你會增加那隻寶可夢的防禦力

1:08:13.880,1:08:16.000
但是，會減低它的速度

1:08:16.000,1:08:19.120
所以，寶可夢防禦力提升的時候

1:08:19.120,1:08:20.860
它的速度會下降

1:08:20.860,1:08:22.640
如果我們把

1:08:22.640,1:08:25.720
第一個和第二個 principle component 畫出來的話

1:08:25.720,1:08:27.240
你發現是這個樣子

1:08:27.240,1:08:30.840
這個圖上，有 800 個點，每一個點

1:08:30.840,1:08:32.300
就對應到一隻寶可夢

1:08:32.300,1:08:34.080
那你把它畫到二維的平面上

1:08:34.080,1:08:36.500
那這樣我們很難知道每一隻寶可夢是甚麼

1:08:36.500,1:08:38.580
所以，我就做了一件瘋狂的事

1:08:38.580,1:08:42.380
如果我們看第三、第四個 component 的話

1:08:42.380,1:08:46.220
會發現說第三個 component

1:08:46.220,1:08:48.980
它是 special defense

1:08:48.980,1:08:50.900
是正的

1:08:50.900,1:08:52.420
special defense 是正的

1:08:52.420,1:08:56.660
然後，攻擊力跟 HP 是負的

1:08:56.660,1:08:58.880
也就是說，這是用

1:08:58.880,1:09:01.780
特殊防禦力來換取

1:09:01.780,1:09:07.220
用攻擊力跟 HP 來換取特殊防禦力的寶可夢

1:09:07.220,1:09:09.460
最後一個呢？最後一個是

1:09:09.460,1:09:11.200
它的 HP 是正的

1:09:11.200,1:09:13.680
然後，攻擊力和防禦率是負的

1:09:13.680,1:09:17.620
也就是說，它是用攻擊力和防禦來換取生命力的寶可夢

1:09:17.620,1:09:19.160
那這些分別是什麼呢

1:09:19.160,1:09:21.400
如果我們把第三個和第四個 principle component

1:09:21.400,1:09:22.580
畫在圖上的話

1:09:22.580,1:09:24.360
其實它們也是橢圓的形狀

1:09:24.360,1:09:25.820
其實也是橢圓的形狀

1:09:25.820,1:09:29.540
不過基本上考慮的是統計的結果，所以會有一些 outlier

1:09:29.540,1:09:31.860
不過，統計的結果最後算出來

1:09:31.860,1:09:34.260
它是 decorrelation 的

1:09:34.260,1:09:37.520
其實，特殊的防禦力用

1:09:37.520,1:09:40.280
攻擊力和生命值換特殊防禦力

1:09:40.280,1:09:41.148
其實也是普普

1:09:41.148,1:09:45.380
它不只是一個防禦力特別高的寶可夢

1:09:45.380,1:09:47.700
它也是一個特殊防禦力特別高的寶可夢

1:09:47.700,1:09:52.380
第二名是冰柱機器人這樣子

1:09:54.720,1:09:57.060
然後，如果我們看生命力的話

1:09:57.060,1:09:59.420
第四個 component 就是生命力特別強的

1:09:59.420,1:10:02.220
那這個其實跟我們預期是一樣的，就是

1:10:02.220,1:10:05.940
這個是吉利蛋跟幸福蛋，對不對

1:10:10.280,1:10:14.560
所以，今天我們至少回答到一個問題

1:10:14.560,1:10:17.580
你知道最強的寶可夢其實是超夢，還有

1:10:17.580,1:10:20.400
那三隻神獸是特別強的

1:10:20.400,1:10:23.180
那如果我們可以拿它來做其他 class

1:10:23.180,1:10:25.060
比如說，我們拿它來做

1:10:25.060,1:10:29.320
影像的手寫數字的辨識的話

1:10:29.320,1:10:33.480
那會怎麼樣呢？我們可以把每一張

1:10:33.480,1:10:36.540
數字都拆成

1:10:36.540,1:10:38.880
component 的 weight 乘上 component

1:10:38.880,1:10:41.380
加上 component 的 weight 乘上另一個 component

1:10:41.380,1:10:46.300
其實，每一個 component 都是一張 image，對不對

1:10:46.300,1:10:49.440
每一個 component 都是一個 28*28 維的 vector

1:10:49.440,1:10:53.920
所以，你可以把它畫在圖上，把它變成一張 image

1:10:53.920,1:10:59.920
我們如果畫前 30 個 component 的話

1:10:59.920,1:11:02.620
我們畫 PCA 得到前 30 個 component 的話

1:11:02.620,1:11:04.820
你得到的結果，其實是這個樣子的

1:11:04.820,1:11:11.140
白色的地方代表是有筆劃，所以這個是 1

1:11:11.140,1:11:13.580
這個看起來有點像 9

1:11:13.580,1:11:17.660
這個看起來是 0，中間接一條線

1:11:17.660,1:11:21.780
這個看起來不知道，像是加了勾勾

1:11:21.780,1:11:25.540
後面這很複雜，看起來像是馬雅文字一樣複雜

1:11:25.540,1:11:31.560
你用這些 component

1:11:31.560,1:11:33.060
做 linear combination

1:11:33.060,1:11:36.600
你就可以得到所有的 digit，就可以得到 0~9

1:11:36.600,1:11:40.440
所以，這些 component 就叫做 Eigen-digit

1:11:40.440,1:11:42.080
之所以叫 Eigen-digit 就是說

1:11:42.080,1:11:44.940
Eigen 就是說，這些 component 其實都是

1:11:44.940,1:11:47.540
covariance matrix 的 eigenvector

1:11:47.540,1:11:49.280
所以，叫它 Eigen-digit

1:11:49.280,1:11:51.140
所以，Eigen-digit 做 linear combination

1:11:51.140,1:11:54.440
就可以得到各種不同的 digit

1:11:54.440,1:11:57.780
如果做人臉辨識的話

1:11:57.780,1:12:00.400
處理人臉的話，得到的結果大概是這樣

1:12:00.400,1:12:04.940
這邊有一大堆的人臉

1:12:04.940,1:12:09.560
然後，你就找它的 principle component

1:12:09.560,1:12:16.160
你就找它的 principle component，你就找前 30 個

1:12:16.160,1:12:17.620
principle component

1:12:17.620,1:12:21.760
你就會發現說找出來是這樣，每一個都是哀怨的臉

1:12:21.760,1:12:24.820
這叫 Eigen-face

1:12:27.400,1:12:31.500
你看每一個都是一個臉，每一張都是一個臉

1:12:31.500,1:12:34.260
那你把這些臉做 linear combination 以後

1:12:34.260,1:12:37.060
就可以得到所有的臉

1:12:37.060,1:12:42.080
但是，這邊你有沒有覺得有些地方

1:12:42.080,1:12:43.880
跟你預期的不太一樣呢？

1:12:43.880,1:12:46.000
我第一次看到的時候，覺得這跟預期的結果不太一樣

1:12:46.000,1:12:47.320
是不是程式有 bug 阿

1:12:47.320,1:12:49.940
因為，我們說P CA 找出來的是

1:12:49.940,1:12:51.740
是 component，對不對

1:12:51.740,1:12:54.040
我們把很多 component linear combine 以後

1:12:54.040,1:12:57.360
它會變成一個 face 或一個 digit

1:12:57.360,1:12:59.640
但我們現在找出來的不是 component 阿

1:12:59.640,1:13:03.600
我們找出來的每一個圖，幾乎都是完整的臉

1:13:03.600,1:13:05.220
幾乎都是完整的臉

1:13:05.220,1:13:07.840
我們剛才前一個數字

1:13:07.840,1:13:11.400
你找出來每一個 Eigen-digit 看起來都是馬雅文字

1:13:11.400,1:13:14.440
它們不是 component，不是

1:13:14.440,1:13:18.420
一筆劃這種東西，不是圈圈、一豎這種東西

1:13:18.420,1:13:20.140
那為甚麼會這樣呢？

1:13:20.140,1:13:22.120
如果你仔細想想看 PCA 的特性

1:13:22.120,1:13:25.220
你就會發現說，會得到這個結果是可能的

1:13:25.220,1:13:28.840
因為在 PCA 裡面，你的這個 weight

1:13:28.840,1:13:30.940
你的這個 linear combination 的 weight

1:13:30.940,1:13:32.500
它可以是任何值

1:13:32.500,1:13:35.400
它可以是正的，也可以是負的

1:13:35.400,1:13:37.580
所以，當我們用這些 principal component

1:13:37.580,1:13:39.480
組成一張 image 的時候

1:13:39.480,1:13:42.080
你可以把這些 component 相加

1:13:42.080,1:13:45.080
也可以把這些 component 相減

1:13:45.080,1:13:48.860
所以，這會導致說你找出來的 component

1:13:48.860,1:13:54.860
不見得是一個圖的 basic 的東西

1:13:54.860,1:13:57.760
舉例來說，假設我想要畫一個 9

1:13:58.120,1:14:00.420
那我可以先畫一個 8

1:14:00.420,1:14:02.720
然後，再把下面的圈圈減掉

1:14:02.720,1:14:05.060
再把一槓加上去

1:14:05.060,1:14:08.120
這樣大家了解我意思嗎？因為現在我們

1:14:08.120,1:14:11.200
不一定是把這些 component 加起來

1:14:11.200,1:14:13.040
我們可以把這些 component 相減

1:14:13.040,1:14:15.920
所以就變成說，你可以先畫一個很複雜的圖

1:14:15.920,1:14:18.520
然後，再把多餘的東西減掉

1:14:18.520,1:14:21.680
這就是為甚麼我們剛才會看到一堆馬雅文字的關係

1:14:21.680,1:14:26.120
這些 component 其實不見得是類似筆劃這種東西

1:14:26.120,1:14:29.560
如果你想要得到類似筆劃的東西的話

1:14:29.560,1:14:32.320
怎麼辦呢？你要用另外一個技術叫做

1:14:32.320,1:14:36.200
Non-negative matrix 的 factorization (NMF)

1:14:36.200,1:14:40.220
那在 NMF 裡面

1:14:40.220,1:14:45.040
我們剛才講說，PCA 它可以看成是對 matrix, X

1:14:45.040,1:14:48.300
做 SVD，SVD 就是一種

1:14:48.300,1:14:51.280
matrix factorization 的技術，就是一種矩陣分解的技術

1:14:51.280,1:14:54.400
那它分解出來的兩個 matrix 的值

1:14:54.400,1:14:56.180
可以是正的，可以是負的

1:14:56.180,1:14:59.080
現在如果你用 NMF 的話

1:14:59.080,1:15:01.560
我們沒有打算要講它的細節

1:15:01.560,1:15:03.360
我等一下列 reference 給大家參考就好

1:15:03.360,1:15:05.560
簡單來說的話，精神就是

1:15:05.560,1:15:07.520
如果我們現在用 NMF 的話

1:15:07.520,1:15:11.760
我們會強迫所有的 component 都是正的

1:15:11.760,1:15:15.140
首先，我會強迫所有 component 的 weight

1:15:15.140,1:15:16.320
都是正的

1:15:16.320,1:15:17.920
是正的好處就是

1:15:17.920,1:15:21.300
現在一張 image 必須由 component 疊加得到

1:15:21.300,1:15:22.840
而你不能說，我先畫一個圖

1:15:22.840,1:15:27.620
很複雜的東西，再把複雜的東西去掉一些部分

1:15:27.620,1:15:28.840
得到一個 digit

1:15:28.840,1:15:33.140
現在因為每一個 weight 都一定要是正的，所以

1:15:33.140,1:15:35.540
你只能相加，再來就是

1:15:35.540,1:15:38.020
所有的 component

1:15:38.020,1:15:40.520
它的每一個 dimension，也都必須要是正的

1:15:40.520,1:15:43.560
如果你用 NMF 的話，會讓你的每一個 dimension

1:15:43.560,1:15:45.220
都是正的

1:15:45.220,1:15:46.640
如果你用 PCA 的話

1:15:46.640,1:15:48.840
你得到的 dimension 不見得每一維都是正的

1:15:48.840,1:15:50.840
你找出來的 principal component 裡面

1:15:50.840,1:15:53.980
它會有一些負值

1:15:53.980,1:15:56.560
那你知道，你今天如果要畫一張 image 的話

1:15:56.560,1:15:59.940
其實，負值你是有點不知道該怎麼處理的

1:15:59.940,1:16:05.220
如果我今天把負值都當作是

1:16:05.220,1:16:07.200
就沒有筆劃，就是 0 的話

1:16:07.200,1:16:08.940
你可能整張圖都看起來黑漆漆

1:16:08.940,1:16:11.060
你大部分的圖都黑漆漆，就很怪

1:16:11.060,1:16:13.740
我前面那幾張圖的作法是

1:16:13.740,1:16:16.360
如果有負值又有正值，會把它 normalize

1:16:16.360,1:16:18.800
把它做一個平移，讓負的也都變成正的

1:16:18.800,1:16:21.440
你看起來的圖才會比較好看

1:16:21.440,1:16:24.640
那這個就比較麻煩，如果你用 NMF 就沒有這個問題了

1:16:24.640,1:16:26.720
你找出來的 component 都是正的

1:16:26.720,1:16:31.060
所以，那些 component 就自然地會形成一張 image

1:16:31.060,1:16:33.900
下面這是 reference 給大家參考

1:16:33.900,1:16:36.340
所以，如果在同樣的 test 上

1:16:36.340,1:16:39.480
比如在手寫數字的 test 上，一樣 apply NMF 的時候

1:16:39.480,1:16:43.120
這個時候，你找出來的那些 principal component

1:16:43.120,1:16:45.900
它就會長這樣

1:16:45.900,1:16:51.020
它就會清楚很多，你就會發現說，這顯然

1:16:51.020,1:16:55.380
都是筆劃，這個是縱的，這個是 0 的兩邊

1:16:55.380,1:16:58.420
這個是斜的，這個是一小點

1:16:58.420,1:17:00.400
一直線、一撇這樣子

1:17:00.400,1:17:02.060
一橫線、一撇這樣子

1:17:02.060,1:17:03.840
你會發現，你找出來的每一個東西

1:17:03.840,1:17:07.260
就都變成是筆劃了，這跟我們

1:17:07.260,1:17:09.840
本來想要找的東西，是比較像的

1:17:09.840,1:17:12.240
如果你看臉的話

1:17:12.240,1:17:16.260
你會發現說，它長的是這個樣子，它比較像是

1:17:16.260,1:17:18.620
臉的一些部分

1:17:18.620,1:17:22.180
比如說，這個是人中的地方

1:17:22.180,1:17:28.660
這個是眉毛，這個是下巴，這個是嘴唇

1:17:28.660,1:17:31.580
這個也是嘴唇等等

1:17:31.580,1:17:33.280
如果，你今天是用 NMF

1:17:33.280,1:17:37.240
對這個人臉圖片做 NMF 的話

1:17:37.240,1:17:40.780
你得到的結果會比較像是

1:17:40.780,1:17:45.140
你期待要找的像是 component 一樣的東西

1:17:46.060,1:17:50.040
那接下來，剩下來的時間

1:17:50.040,1:17:53.640
我們講一下 Matrix Factorization

1:17:53.640,1:17:56.740
Matrix Factorization 是這樣子，有時候

1:17:56.740,1:18:00.320
你會有兩種東西

1:18:00.320,1:18:02.160
你會有兩種 object

1:18:02.160,1:18:09.800
它們之間是受到某種共通的 latent factor 去操控的

1:18:09.800,1:18:13.800
什麼意思呢，假設我們現在做一下調查

1:18:13.800,1:18:17.720
調查每一個人手上有，每一個人有買公仔的數目

1:18:17.720,1:18:21.180
然後，A B C D E 代表 5 個人

1:18:21.180,1:18:24.040
我們調查一下，每個人手上有的公仔的數目

1:18:24.040,1:18:28.160
比如說，調查 A 有 5 個涼宮春日的公仔

1:18:28.180,1:18:30.280
B 有 4個，C 有 1 個， D有 1 個

1:18:30.280,1:18:36.080
然後這個是御坂美琴，A有 3 個，B有 3 個，C有 1 個

1:18:36.080,1:18:41.820
這個是小野寺小咲 (人名)

1:18:41.820,1:18:44.240
我要想看看人名到底是什麼

1:18:44.240,1:18:46.740
小野寺，對不對

1:18:46.740,1:18:49.560
D有4個，E有5個

1:18:49.560,1:18:53.660
然後這個，這個是小唯，是平澤唯

1:18:53.660,1:18:56.660
然後，C有5個，D有4個，E有4個，

1:18:56.660,1:18:58.320
你會發現說

1:18:58.320,1:19:00.240
在這個 matrix 上面

1:19:00.240,1:19:03.620
每一個 table 裡面的 block

1:19:03.620,1:19:05.120
並不是隨機出現

1:19:05.120,1:19:09.540
你會發現，如果有買涼宮春日公仔的人

1:19:09.540,1:19:11.900
就比較有可能會有御坂美琴的公仔

1:19:11.900,1:19:16.180
有這個姐寺公仔的人，就比較有可能有小唯的公仔

1:19:16.180,1:19:17.780
那是因為說

1:19:17.780,1:19:22.480
這每一個人跟每一個人

1:19:22.480,1:19:27.500
跟角色背後是有一些共同的特性

1:19:27.500,1:19:31.140
有一些個共同的 factor 來操控

1:19:31.140,1:19:32.640
這些事情發生

1:19:32.640,1:19:36.580
otaku 這個是御宅族的意思這樣

1:19:36.580,1:19:40.300
每一個御宅族和每一個角色後面，是有一些

1:19:40.300,1:19:44.040
共同的 factor在操控它們的

1:19:44.040,1:19:46.180
什麼意思呢？

1:19:46.180,1:19:51.100
其實這個動漫宅，或許可以分成兩種

1:19:51.100,1:19:53.340
有一種是萌傲嬌的

1:19:53.340,1:19:55.700
有一種是萌天然呆的

1:19:55.700,1:19:59.820
每一個人，其實就是在萌傲嬌和萌天然呆

1:19:59.820,1:20:01.780
這個平面上的一個點

1:20:01.780,1:20:06.780
所以，如果比較偏，A 是比較萌傲嬌的

1:20:06.780,1:20:09.960
那每一個角色，他也是

1:20:09.960,1:20:14.360
他可能是有傲嬌屬性，或者是有天然呆的屬性

1:20:14.360,1:20:17.720
所以每一個角色，也都是平面上的一個點

1:20:17.720,1:20:20.380
每一個角色，你也都可以用一個 vector 來描述它

1:20:20.380,1:20:22.920
如果某一個人萌的屬性

1:20:22.920,1:20:26.660
跟某一個角色，他本身所具有的屬性是 match 的話

1:20:26.660,1:20:28.700
所謂的屬性 match 是說

1:20:28.700,1:20:30.300
他們背後的這個 vector

1:20:30.300,1:20:33.240
他們背後的這個 vector 很像

1:20:33.240,1:20:34.760
他們背後的這個 vector

1:20:34.760,1:20:37.800
比如說，在做 inner product 的時候，值很大

1:20:37.800,1:20:39.460
那這個人

1:20:39.460,1:20:41.600
就會買很多涼宮春日的公仔

1:20:41.600,1:20:44.600
所以，這個他們有沒有 match 呢

1:20:44.600,1:20:48.440
這個 degree，他們這個匹配的程度

1:20:48.440,1:20:52.840
就取決於他們背後的這個的 latent factor 是不是匹配的

1:20:52.840,1:20:55.860
所以，只可能 ABC 他們背後的這個

1:20:55.860,1:20:58.440
他們萌的角色，大概是這個樣子

1:20:58.440,1:21:00.900
A 是萌傲嬌的， B也是萌傲嬌的

1:21:00.900,1:21:03.460
但沒有 A 那麼強 ，C 是萌天然呆的

1:21:03.460,1:21:06.100
然後每一個動漫人物角色的後面

1:21:06.100,1:21:09.020
也都有傲嬌和天然呆，這兩種屬性

1:21:09.020,1:21:12.620
每一角色都有他不同的屬性

1:21:12.620,1:21:14.300
然後，如果他們的屬性

1:21:14.300,1:21:16.100
它們背後的屬性 match 的話

1:21:16.100,1:21:19.520
那這一個人，就會買這個公仔

1:21:19.520,1:21:22.360
這個世界操控的邏輯是這個樣子

1:21:22.360,1:21:24.340
但是這些 factor

1:21:24.340,1:21:27.620
就一個人到底是萌傲嬌還是萌天然呆，這一件事情

1:21:27.620,1:21:30.700
是沒有辦法直接被觀察的

1:21:30.700,1:21:32.140
因為，其實沒有人在意一個

1:21:32.140,1:21:34.240
阿宅他心裡想什麼

1:21:34.960,1:21:38.340
所以這些事情，是沒有人知道的

1:21:38.780,1:21:42.060
你也沒有辦法直接知道說

1:21:42.060,1:21:46.400
每一個動漫人物他背後的屬性是甚麼

1:21:46.400,1:21:48.140
這也是沒有辦法直接觀察到的

1:21:48.140,1:21:50.380
所以我們有的東西是什麼呢？

1:21:50.380,1:21:55.580
我們有的是，這個動漫人物跟阿宅中間的關係

1:21:55.580,1:21:57.500
他們中間的關係，也就是

1:21:57.500,1:22:01.080
也就是他手上有的公仔的數目，他們之間的關係

1:22:01.080,1:22:02.820
那我們要憑著這個關係

1:22:02.820,1:22:08.520
去推論出每一個人，跟每一個動漫人物

1:22:08.520,1:22:11.660
他們背後的 latent factor

1:22:11.660,1:22:14.680
他們每一個人背後都有一個二維的 vector

1:22:14.680,1:22:22.080
分別代表他，就每一個這個阿宅背後都有一個 vector

1:22:22.080,1:22:24.840
代表了他萌傲嬌或是萌天然呆的程度

1:22:24.840,1:22:26.580
他都是用一個 vector來表示

1:22:26.580,1:22:29.420
每一個人物，他背後也都有一個 vector

1:22:29.420,1:22:32.600
代表他傲嬌的屬性和天然呆的屬性

1:22:32.600,1:22:38.880
而我們知道的每一個阿宅，他手上有的這個公仔的數目

1:22:38.880,1:22:43.020
你就可以把他們合起來看作是一個 matrix, X

1:22:43.020,1:22:46.780
我們可以把合起來看做是一個 matrix, X

1:22:46.780,1:22:51.760
那這一個 matrix, X，它的 row 的數目就是

1:22:51.760,1:22:53.700
Otaku 的數目

1:22:53.700,1:22:56.040
它的這個 column 的數目

1:22:56.040,1:22:58.540
就是動漫角色的數目

1:22:58.540,1:23:02.420
那我們現在要做的事情，就是做一個假設

1:23:02.420,1:23:04.140
這一個假設是這個樣子

1:23:04.140,1:23:07.240
每一個，這個 matrix 裡面的 element

1:23:07.240,1:23:11.460
它都來自於兩個 vector 的 inner product

1:23:11.460,1:23:13.900
都來自於兩個 vector 的內積

1:23:13.900,1:23:18.460
這一個 5，為什麼 A 會有 5 個涼宮春日的公仔呢

1:23:18.460,1:23:22.620
是因為 r^A 跟 r^1 的 inner product 很大

1:23:22.620,1:23:24.040
它們的 inner product 是 5

1:23:24.040,1:23:27.880
所以他就會買，他們就會有 5 個涼宮春日的公仔

1:23:27.880,1:23:31.400
如果 r^B 跟 ^r1，它的 inner product 是 4 的話

1:23:31.400,1:23:34.580
那 B 就會有 4個涼宮春日的公仔

1:23:34.580,1:23:35.520
以此類推

1:23:35.520,1:23:38.560
這個世間運作的邏輯就是這個樣子

1:23:38.560,1:23:41.900
那這件事情，如果你用數學式來表示它的話

1:23:41.900,1:23:43.920
你可以寫成這樣子

1:23:43.920,1:23:46.960
我們把 r^A 到 r^E 排成一排

1:23:46.960,1:23:48.660
我們把 r^A 到 r^E 排成一排

1:23:48.660,1:23:53.400
把 r^1 到 r^4

1:23:53.400,1:23:59.360
也當作是另外一個 matrix 的 row 把它排起來

1:23:59.360,1:24:03.360
那這個 K 是 latent factor 的數目

1:24:03.360,1:24:06.360
這個東西，我們是沒有辦法知道的

1:24:06.360,1:24:10.040
我們把人分成只有萌傲嬌和萌天然呆，這樣是一個

1:24:10.040,1:24:12.780
不精確的分析方式

1:24:12.780,1:24:14.440
如果我們有更多的 data 的話

1:24:14.440,1:24:18.180
我們應該可以更準確知道應該要有多少 factor

1:24:18.180,1:24:20.700
但是，實際上要有多少 factor 這件事情

1:24:20.700,1:24:23.960
你必須要試出來，就像 principal component 的數目

1:24:23.960,1:24:27.000
或者是這個 neural network 的陳述一樣

1:24:27.000,1:24:28.900
這個是你要事先決定好的

1:24:28.900,1:24:30.440
我們現在就假設說

1:24:30.440,1:24:32.740
latent factor 的數目

1:24:32.740,1:24:34.940
latent factor的數目就是K

1:24:34.940,1:24:37.560
所以， 這個 r^A 到 r^E

1:24:37.560,1:24:39.100
你把它當作是 matrix row

1:24:39.100,1:24:42.480
這邊就有 N*K，這是一個 N*K 的 matrix

1:24:42.480,1:24:47.140
你把 r^1 到 r^4 當作 column，那就是 K*N 的 matrix

1:24:47.140,1:24:50.320
你把這個 N*K  matrix、 N*K matrix 乘起來

1:24:50.320,1:24:52.640
你就會得到一個

1:24:52.640,1:24:56.200
我寫錯了，這應該是 M，不好意思，這應該是 M

1:24:56.200,1:25:02.260
這個應該是 M 個 Otaku，所以是有 M 個人

1:25:02.260,1:25:05.820
所以，把這個 (M*K) * (K*N)

1:25:05.820,1:25:07.620
得到一個 M*N 的 matrix

1:25:07.620,1:25:10.800
那它的每一個 dimension 分別是甚麼呢？

1:25:10.800,1:25:13.720
我們知道說，最左上角這個 dimension

1:25:13.720,1:25:15.440
最左上角的這個 dimension

1:25:15.440,1:25:17.620
如果你現在熟的話

1:25:17.620,1:25:22.240
它就是 r^A * r^1，對不對

1:25:23.060,1:25:27.800
那這第一個 row，第二個 column 的 element

1:25:27.800,1:25:31.760
n(下標 A2)，就是 r^A * r^2

1:25:31.760,1:25:34.700
n(下標 B1)，就是 r^B * r^1

1:25:34.700,1:25:38.560
n(下標 B2)，就是 r^B * r^2

1:25:38.560,1:25:41.380
所以這一個 matrix 是什麼呢？

1:25:41.380,1:25:44.380
這一個 matrix 其實就是這一個 matrix

1:25:44.380,1:25:45.720
其實就是這一個 matrix

1:25:46.360,1:25:50.200
假設我們說，這一個 matrix 就是這一個 matrix 的話

1:25:50.200,1:25:53.340
那我們要做的事情就是找一組

1:25:53.340,1:25:56.360
r^A 到 r^E，找一組 r^A 到 r^E

1:25:56.360,1:26:00.560
找一組 r^1 到 r^4

1:26:00.560,1:26:02.000
把這兩個 matrix 相乘以後

1:26:02.000,1:26:05.240
跟這個 matrix, X 越接近越好

1:26:05.240,1:26:08.840
minimize 這兩個 matrix 相乘以後

1:26:08.840,1:26:13.420
跟這個 matrix, X 之間的 Reconstruction error

1:26:13.420,1:26:15.760
那這個東西，我們剛才講過

1:26:15.760,1:26:18.180
你就可以用 SVD 來解

1:26:18.180,1:26:21.220
那你可能說 SVD 不是解完以後，有 3 個 matrix 嗎？

1:26:21.220,1:26:22.800
中間還會有 Σ 呢

1:26:22.800,1:26:25.520
你就看你要把 Σ 併到左邊或是併到右邊都可以

1:26:25.520,1:26:30.460
你就把這個 matrix, X 拆成兩個 matrix

1:26:30.460,1:26:34.720
它們可以 minimize Reconstruction error，然後就結束了

1:26:34.720,1:26:36.680
但有時候你可能會遇到這個問題

1:26:36.680,1:26:43.780
就是有一些 information，是 missing，是你不知道的

1:26:43.780,1:26:47.980
這個 information，比如說 ABC 他們

1:26:47.980,1:26:54.240
你並不知道 ABC 手上有沒有，有沒有小野寺的公仔

1:26:54.240,1:26:58.380
有可能就只是在他所在的地區，沒有發行這個公仔而已

1:26:58.380,1:27:01.080
所以，不知道如果發行的話，他到底會不會買

1:27:01.080,1:27:04.020
所以這邊，其實是個問號，這個是問號

1:27:04.020,1:27:07.300
假設這個 table 上有一些問號的話，怎麼辦呢？

1:27:07.300,1:27:09.660
這 table 上有一些問號的話

1:27:09.660,1:27:11.740
你用剛才那個 SVD 的方法

1:27:11.740,1:27:15.040
就會有點卡

1:27:15.040,1:27:16.540
你可能說我把這個值

1:27:16.540,1:27:18.360
都當作是零的話

1:27:18.360,1:27:20.240
做一發，這樣子也可以啦

1:27:20.240,1:27:22.820
不過總覺得有些怪怪的

1:27:22.820,1:27:24.280
可以描述得更精確一點

1:27:24.280,1:27:26.900
所以，如果今天在 matrix 上面

1:27:26.900,1:27:29.300
有一些 missing value 的話，怎麼辦呢？

1:27:29.300,1:27:30.980
有一些 missing value 的話

1:27:30.980,1:27:32.680
我們還是可以做的

1:27:32.680,1:27:35.440
我們就用 Gradient Descent 的方法來做它

1:27:35.440,1:27:38.020
就我們寫一個 loss function

1:27:38.020,1:27:41.020
這一個 loss function 是這樣子

1:27:41.020,1:27:43.360
我們要讓 r^i

1:27:43.360,1:27:48.200
r^i 指的是每一個 Otaku 背後的 latent factor

1:27:48.200,1:27:52.320
r^j 指的是每一個動漫角色背後的 latent factor

1:27:52.320,1:27:54.480
你要讓 i 這個人

1:27:54.480,1:27:57.120
他的 latent factor 乘上 j 這個角色

1:27:57.120,1:27:59.160
他的 latent factor，這個 inner product

1:27:59.160,1:28:02.700
跟他購買的數量，n(下標ij) 越接近越好

1:28:02.700,1:28:04.340
那重點是說

1:28:04.340,1:28:07.640
我今天在 summation over 這些 element 的時候

1:28:07.640,1:28:10.560
我們可以避開這一些 missing 的 data

1:28:10.560,1:28:12.880
我們可以避開這些 missing data

1:28:12.880,1:28:15.340
我們在 summation over 這些 ij 的時候

1:28:15.340,1:28:17.340
如果今天這個值是沒有的

1:28:17.340,1:28:18.520
我就不算它

1:28:18.520,1:28:23.300
我們只算這個值是有定義的部分

1:28:23.300,1:28:25.200
那接下來怎麼辦呢？

1:28:25.200,1:28:27.580
接下來，你都把 loss function 寫出來了

1:28:27.580,1:28:30.320
你要找 r^i 跟 r^j

1:28:30.320,1:28:33.240
就用 Gradient Descent，就好了

1:28:33.240,1:28:35.900
用 Gradient Descent 運算一發

1:28:35.900,1:28:37.680
然後就結束了

1:28:37.680,1:28:39.520
所以，根據剛才那個例子

1:28:39.520,1:28:43.700
我們就可以實際地用 Gradient Descent 來做一下

1:28:43.700,1:28:47.340
那我們假設 latent factor 的數目等於 2

1:28:47.340,1:28:49.480
假設 latent factor 的數目等於 2

1:28:49.480,1:28:52.900
那每一個這個 A 到 E 呢

1:28:52.900,1:28:57.920
它們就都會對應到一個二維的 vector

1:28:57.920,1:28:59.780
A 的 vector是這樣， B 是這樣

1:28:59.780,1:29:01.260
C 是這樣，以此類推

1:29:01.260,1:29:04.080
每一個人都會對應到一個 vector

1:29:04.080,1:29:06.860
那每一個這個角色呢

1:29:06.860,1:29:09.700
他也都可以對應到一個 vector

1:29:09.700,1:29:11.760
我怕大家，如果我只寫這個字的話

1:29:11.760,1:29:13.100
會不知道什麼意思

1:29:13.100,1:29:14.960
所以特別把編號列出來

1:29:14.960,1:29:16.960
這個是春日，這個是炮姐

1:29:16.960,1:29:18.360
這個是姐寺，這個是小唯

1:29:18.360,1:29:20.440
那你就會找出說

1:29:20.440,1:29:23.100
每一個角色，都得到一個 vector

1:29:23.100,1:29:27.200
也都得到一個 latent factor，這代表他的屬性

1:29:27.200,1:29:28.160
代表他的屬性

1:29:28.160,1:29:34.980
這個每一個人的 latent factor 就代表他萌哪一種屬性

1:29:34.980,1:29:37.420
所以，如果我們把他在兩個維度裡面

1:29:37.420,1:29:40.000
比較大的維度挑出來的話

1:29:40.000,1:29:43.520
你就會發現說，A 跟 B 是萌同一組屬性的

1:29:43.520,1:29:45.540
C，D，E 是萌同一組屬性的

1:29:45.540,1:29:48.120
1 跟 2 他們有同樣的屬性

1:29:48.120,1:29:50.740
然後 3 跟 4 他們有同樣的屬性

1:29:50.740,1:29:52.560
你沒有辦法真的知道說，每一個屬性

1:29:52.560,1:29:54.100
分別代表的是甚麼

1:29:54.100,1:29:57.920
你不知道說，這個維度是代表是傲嬌，還是天然呆

1:29:57.920,1:30:00.480
你不知道這個維度是代表傲嬌，還是天然呆

1:30:00.580,1:30:03.460
你會需要先找出這些 latent factor 以後

1:30:03.460,1:30:05.420
再去分析它的結果

1:30:05.420,1:30:06.520
你就可以知道說

1:30:06.520,1:30:08.260
因為我們事先已經知道說

1:30:08.260,1:30:10.780
姐寺跟小唯是有天然呆屬性

1:30:10.780,1:30:14.160
所以第一個維度代表的就是天然呆屬性

1:30:14.160,1:30:17.140
而第二個維度代表的就是傲嬌屬性

1:30:17.140,1:30:19.360
接下來，有這些 data 以後

1:30:19.360,1:30:22.420
你就可以，預測你的 missing value

1:30:22.420,1:30:24.340
你就可以預測問號的值

1:30:24.340,1:30:26.020
怎麼預測問號的值呢

1:30:26.020,1:30:27.820
我們如果已經知道 r^3

1:30:27.820,1:30:29.680
我們已經知道 r^a

1:30:29.680,1:30:32.480
那我們知道說，一個人會購買公仔的數量

1:30:32.480,1:30:34.380
其實是他背後的

1:30:34.380,1:30:38.020
這個動漫角色背後的 latent factor

1:30:38.020,1:30:40.860
那個人背後的 latent factor 做 inner product 的結果

1:30:40.860,1:30:42.740
所以，我們只要把 r^3 跟 r^a

1:30:42.740,1:30:44.080
做 inner product，你就可以預測說

1:30:44.080,1:30:47.060
這個人會買多少的公仔

1:30:47.060,1:30:49.880
所以，做完 inner product 以後的結果

1:30:49.880,1:30:50.960
就是這樣子

1:30:50.960,1:30:52.100
這告訴我們說

1:30:52.100,1:30:56.140
這個 C，如果說

1:30:56.140,1:31:00.360
r^C 跟 r^3 他們的 latent factor，其實是 match 的

1:31:00.360,1:31:02.820
所以 C 這個人呢

1:31:02.820,1:31:04.600
如果他可以買的話

1:31:04.600,1:31:09.240
預期他會買，他會有 2.2 個姐寺的公仔

1:31:09.240,1:31:11.180
所以，你就可以推薦姐寺公仔給他

1:31:11.180,1:31:13.020
你就可以推薦他入坑

1:31:13.020,1:31:15.200
所以，這個方法

1:31:15.200,1:31:17.600
常用在這種推薦系統裡面

1:31:17.600,1:31:20.720
大家可能都有聽過 Netflix 的比賽

1:31:20.720,1:31:25.220
如果你今天把動漫角色換成電影

1:31:25.220,1:31:29.200
你把這個中間的這個數值換成 rating 的話

1:31:29.200,1:31:30.940
你就可以預測某一個人

1:31:30.940,1:31:33.980
會不會喜歡某一部電影，等等

1:31:33.980,1:31:38.700
線上推薦系統，現在很多都會使用這樣的技術

1:31:38.700,1:31:43.480
其實，剛才那個 model 可以做更精緻一點

1:31:43.480,1:31:47.160
我們剛才說，A 背後的 latent factor

1:31:47.160,1:31:49.720
乘上 1 背後的 latent factor

1:31:49.720,1:31:52.660
得到的結果，就是 table 上面的數值

1:31:52.660,1:31:55.040
但是事實上，可能還有別的因素

1:31:55.040,1:31:58.300
會操控他的數值

1:31:58.300,1:32:01.820
所以更精確的寫法，或許是我們可以寫成

1:32:01.820,1:32:04.380
r^A 跟 的 r^1 inner product

1:32:04.380,1:32:07.440
加上某一個跟 A 有關的 scalar

1:32:07.440,1:32:09.920
再加上某一個跟 1 有關的 scalar

1:32:09.920,1:32:11.300
其實才等於 5

1:32:11.300,1:32:14.140
這跟 A 有關的 scalar, b(下標A) 指的是甚麼呢？

1:32:14.140,1:32:20.100
它代表的是說，A 他有多喜歡買公仔

1:32:20.100,1:32:21.860
有的人，他就只是

1:32:21.860,1:32:23.560
其實他也不特別喜歡某一個角色

1:32:23.560,1:32:25.120
他就只是想要買公仔而已

1:32:25.120,1:32:29.860
所以這個 b(下標A)，代表他本身有多想要買公仔

1:32:29.860,1:32:32.020
本身有多想要買公仔

1:32:32.020,1:32:33.880
這個 b1 代表說

1:32:33.880,1:32:36.980
這個角色，他本質上會有

1:32:36.980,1:32:38.580
多想讓人家購買

1:32:38.580,1:32:40.580
這件事情是跟屬性無關的

1:32:40.580,1:32:42.940
就是他本來就會想要購買這個角色

1:32:42.940,1:32:49.240
比如說，最近涼宮春日慶祝動畫十周年

1:32:49.240,1:32:51.160
就出現藍光 DVD，甚麼的

1:32:51.160,1:32:53.660
所以大家就可能會比較想要買

1:32:53.660,1:32:55.400
涼宮春日的公仔，其實也不會啦

1:32:55.400,1:32:58.460
所以就會有這個 b1

1:32:58.460,1:33:02.800
所以，你今天就改一下 minimize 的式子

1:33:02.800,1:33:06.740
就變成說，把 r^i 跟 r^j 的 inner product

1:33:06.740,1:33:10.160
加上 bi 再加上 bj

1:33:10.160,1:33:12.180
然後，你會希望這個值

1:33:12.180,1:33:15.180
跟 n(下標ij) 越接近越好

1:33:15.180,1:33:17.680
你會希望說這兩個 latent factor 的 inner product

1:33:17.680,1:33:23.020
加上這個代表 i 本身的 scalar

1:33:23.020,1:33:24.980
還有代表 j 本身的 scalar

1:33:24.980,1:33:29.360
要跟在 table 上面看到的值，越接近越好

1:33:29.360,1:33:30.800
那這個怎麼解呢

1:33:30.800,1:33:32.460
這沒有什麼好講的

1:33:32.460,1:33:35.540
你就用 Gradient Descent 硬解一發就好了

1:33:35.540,1:33:38.520
你也可以加 Regularization

1:33:38.520,1:33:41.340
如果你覺得應該要 Regularization 的話

1:33:41.340,1:33:45.140
那你也可以在這個 r^i，r^j，bi，bj 上面

1:33:45.140,1:33:47.980
加上 Regularization，比如說你會希望說

1:33:47.980,1:33:51.620
r^i, r^j 是 sparse 的

1:33:51.620,1:33:55.260
每個人要麼就是萌傲嬌，要麼就是萌天然呆

1:33:55.260,1:33:57.000
不會有模糊的人

1:33:57.000,1:34:01.040
你可能就會想要加上 L1 的 Regularization

1:34:01.920,1:34:04.480
如果你想要知道更多的話

1:34:04.480,1:34:07.340
以下是 Matrix Factorization 精簡的 paper

1:34:07.340,1:34:11.160
講的是在這個 Netflix 上面，是怎麼做的

1:34:11.160,1:34:14.080
那 Matrix Factorization 還有很多其他的應用

1:34:14.080,1:34:16.000
比如說，它可以用在 Topic analysis 上面

1:34:16.000,1:34:17.660
這個，應該是大家

1:34:17.660,1:34:22.080
等一下，11 點多的時候，助教會回來講一下作業 4

1:34:22.080,1:34:24.920
作業 4 要做的是跟文字有關的

1:34:24.920,1:34:28.540
裡面你可能會需要用到 Topic analysis 的技術

1:34:28.540,1:34:32.360
如果是把剛才所講的 Matrix Factorization 的方式

1:34:32.360,1:34:34.720
用在 Topic analysis 的上面的話

1:34:34.720,1:34:38.180
就叫做 Latent semantic analysis (LSA)

1:34:38.180,1:34:42.220
它的技術跟我們剛才所講的是一模一樣的

1:34:42.220,1:34:44.180
就只是換一個名詞而已

1:34:44.180,1:34:48.300
就是把剛才的動漫人物，通通換成文章

1:34:48.300,1:34:52.900
把剛才的人都換成詞彙

1:34:52.900,1:34:55.160
那 table 裡面的值呢

1:34:55.160,1:34:56.980
把人物換成文章

1:34:56.980,1:34:58.480
把人都換成詞彙

1:34:58.480,1:34:59.800
那 table 裡面的值呢

1:34:59.800,1:35:02.100
就是 Term frequency

1:35:02.100,1:35:04.140
就是說，投資這個 word

1:35:04.140,1:35:06.580
在 Doc 1 出現 5 次

1:35:06.580,1:35:08.520
股票這個 word 在 Doc1 出現 4 次

1:35:08.520,1:35:09.800
以此類推

1:35:09.800,1:35:12.300
有時候，我們不只會用 Term frequency

1:35:12.300,1:35:13.900
你會把 Term frequency

1:35:13.900,1:35:17.620
再乘上一個 weight，代表說

1:35:17.620,1:35:20.180
這個 term 本身有多重要

1:35:20.180,1:35:23.340
如果你把一個 term 乘上比較大的 weight 的話

1:35:23.340,1:35:26.400
今天你在做 matrix factorization 的時候

1:35:26.400,1:35:30.540
那一個 term 它就比較會被考慮到

1:35:30.540,1:35:34.980
你就會比較想要讓那一個 term 的 Reconstruction error

1:35:34.980,1:35:38.320
你就比較想要讓那個 term 的 Reconstruction error

1:35:38.320,1:35:40.600
比較小

1:35:40.600,1:35:43.840
通常怎麼 evaluate 一個 term 重不重要呢

1:35:43.840,1:35:44.960
有很多方法

1:35:44.960,1:35:48.300
比如說，常用的就是 inverse document frequency

1:35:48.300,1:35:50.560
這個我想等一下，助教會講

1:35:50.560,1:35:52.000
或是說你不知道的話

1:35:52.000,1:35:54.220
也可以自己 google 一下

1:35:54.220,1:35:56.260
所以，inverse document frequency 簡單來講

1:35:56.260,1:35:58.900
就是計算某一個詞彙

1:35:58.900,1:36:03.300
在整個 corpus 裡面，有多少比例的 document

1:36:03.300,1:36:04.780
有涵蓋這個詞彙

1:36:04.780,1:36:07.120
假如，某一個詞彙是每個 document 都有的

1:36:07.120,1:36:08.820
比如說，"的 " 每一個 document 都有

1:36:08.820,1:36:12.080
它的 inverse document frequency 就很小

1:36:12.080,1:36:14.540
代表說這個詞彙的重要性是低的

1:36:14.540,1:36:16.840
假如某一個詞彙，是只有某一篇 document 有

1:36:16.840,1:36:19.140
那它的 inverse document frequency 就很大

1:36:19.140,1:36:21.760
就代表這個詞彙的重要性是高的

1:36:21.760,1:36:24.740
那在這一個 task 裡面

1:36:24.740,1:36:27.740
如果你今天把這個 matrix 做分解的話

1:36:27.740,1:36:31.460
你就會找到每一個 document 背後的 latent factor

1:36:31.460,1:36:35.720
你就會找到每一個詞彙背後的 latent factor

1:36:35.720,1:36:37.800
這邊這個 latent factor 是什麼呢

1:36:37.800,1:36:40.120
今天這個 task 裡面，你的 latent factor 可能指的是

1:36:40.120,1:36:42.360
topic

1:36:42.360,1:36:46.340
可能指的是這個主題

1:36:46.340,1:36:50.860
某一個 document，它背後要談的主題

1:36:50.860,1:36:53.180
有多少部分是跟財經有關的

1:36:53.180,1:36:55.420
有多少部分是跟政治有關的

1:36:55.420,1:36:57.900
那某一個詞彙，它跟

1:36:57.900,1:36:59.480
它有多少部分是跟財經有關的

1:36:59.480,1:37:01.740
它有多少部分是跟政治有關的

1:37:01.740,1:37:04.020
比如說，可能

1:37:04.020,1:37:06.480
可能今天在這個例子裡面

1:37:06.480,1:37:09.780
投資跟股票是跟財經有關的

1:37:09.780,1:37:14.260
那 Doc1 跟 Doc2 又都有

1:37:14.260,1:37:16.700
比較多的投資跟股票這兩個詞彙

1:37:16.700,1:37:18.240
那 Doc1 跟 Doc 2

1:37:18.240,1:37:21.620
它就有比較高可能性

1:37:21.620,1:37:25.240
它背後的 latent factor 是偏向於財經的

1:37:26.260,1:37:30.020
其實 Topic analysis 的方法多如牛毛

1:37:30.020,1:37:32.860
所以，它們基本的精神是差不多的

1:37:32.860,1:37:35.320
但是有很多各種各樣的變化

1:37:35.320,1:37:39.960
常見的是 Probability latent semantic analysis (PLSA)

1:37:39.960,1:37:43.920
另外一個 latent Dirichlet allocation (LDA)

1:37:43.920,1:37:46.780
那這一邊就把 reference 列在這邊，給大家參考

1:37:46.780,1:37:49.100
前面我們有講過另外一個 LDA

1:37:49.100,1:37:52.000
在 machine learning 裡面，有兩個 LDA

1:37:52.000,1:37:54.240
然後是完全不一樣的東西

1:37:54.240,1:37:56.040
就是這麼回事

1:37:56.040,1:38:00.300
這邊就是一些 reference 給大家參考

1:38:00.300,1:38:02.620
其實這種 Dimension Reduction 的方式

1:38:02.620,1:38:03.541
這邊還沒有列

1:38:03.541,1:38:06.920
這邊只列一些跟 PCA 比較像、有比較關係的

1:38:06.920,1:38:11.440
這個 Dimension Reduction 的方法，多如牛毛

1:38:11.440,1:38:12.760
比如說，像 MDS

1:38:12.760,1:38:14.080
MDS，它特別的地方是

1:38:14.080,1:38:18.120
它不需要把每一個 data 都表示成 feature vector

1:38:18.120,1:38:19.160
它要知道的只有

1:38:19.160,1:38:21.380
feature vector 跟 feature vector 之間的 distance

1:38:21.380,1:38:23.920
你只要有這個 distance，你就可以做 Dimension Reduction

1:38:23.920,1:38:26.420
那一般教科書舉的例子，都會舉說

1:38:26.420,1:38:27.740
我現在有一堆程式

1:38:27.740,1:38:29.980
你不知道怎麼把程式描述成一個 vector

1:38:29.980,1:38:32.180
但你知道程式和程式之間的距離

1:38:32.180,1:38:35.520
你就有每一筆 data 之間的距離

1:38:35.520,1:38:39.100
就可以把它畫在一個二維平面上

1:38:39.100,1:38:42.740
那其實 MDS 跟 PCA ，是有一些關係的

1:38:42.740,1:38:43.860
你如果用某些

1:38:43.860,1:38:49.060
如果用某一些特定的這個 distance

1:38:49.060,1:38:51.980
來衡量兩個 data point 之間的距離的話

1:38:51.980,1:38:55.800
你又做 MDS 就等於是做 PCA

1:38:55.800,1:38:57.840
所以，其實 PCA 有一個特性

1:38:57.840,1:38:59.500
是它保留了

1:38:59.500,1:39:01.220
原來在高維空間中的距離

1:39:01.220,1:39:04.380
如果兩個點在高維空間中的距離是遠的

1:39:04.380,1:39:06.620
在低維的空間中，它的距離也是遠的

1:39:06.620,1:39:08.820
在高維空間中是接近的

1:39:08.820,1:39:10.560
在低維空間中也是接近的

1:39:10.560,1:39:13.060
那 PCA 有機率的版本

1:39:13.060,1:39:14.760
叫做 Probabilistic PCA

1:39:14.760,1:39:18.160
PCA 有非線性的版本叫做 Kernel PCA

1:39:18.160,1:39:20.020
另外一個東西叫  CCA

1:39:20.020,1:39:22.600
CCA 是說如果你有兩種不同的 source

1:39:22.600,1:39:24.800
這個時候你會想要用 CCA

1:39:24.800,1:39:27.000
比如說，如果你做語音辨識

1:39:27.000,1:39:29.760
你有兩個 source ，一個是聲音訊號

1:39:29.760,1:39:31.080
另外一個是

1:39:31.080,1:39:32.920
假設你今天其實是有螢幕的

1:39:32.920,1:39:36.460
那你可以看到這個人的唇形

1:39:36.460,1:39:38.400
你可以讀他的唇語

1:39:38.400,1:39:40.220
那你現在有聲音訊號

1:39:40.220,1:39:42.700
有那一個人嘴巴的 image

1:39:42.700,1:39:45.400
你把這兩種不同的 source

1:39:45.400,1:39:47.120
都做 Dimension Reduction

1:39:47.120,1:39:48.540
這個是 CCA

1:39:48.540,1:39:51.520
還有另外一種東西叫做 ICA

1:39:51.520,1:39:53.640
它們的名字聽起來都很像

1:39:53.640,1:39:55.840
ICA 比較常用在 source separation 上面

1:39:55.840,1:39:57.320
它要做的事情是

1:39:57.320,1:39:58.820
原來在 PCA 裡面

1:39:58.820,1:40:00.800
我們找出來的 component 是 orthogonal

1:40:00.800,1:40:03.080
那 ICA 裡面，我們不見得要找

1:40:03.080,1:40:05.780
ICA 說我們不見得要找 orthogonal 的 component

1:40:05.780,1:40:08.260
我們找 independent component 就好

1:40:08.260,1:40:10.360
然後，它用一套很複雜的方法來定義什麼叫做

1:40:10.360,1:40:11.760
independent component

1:40:11.760,1:40:14.360
還有剛才有提到的 LDA

1:40:14.360,1:40:16.120
它是 supervise 的方式

1:40:16.120,1:40:18.200
那以上給大家參考，我們在這邊休息10分鐘，謝謝

1:40:18.200,1:40:20.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
