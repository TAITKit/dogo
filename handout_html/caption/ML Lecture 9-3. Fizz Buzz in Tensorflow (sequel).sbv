0:00:00.000,0:00:04.080
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:04.320,0:00:06.340
所謂的硬 train 的意思就是說

0:00:06.340,0:00:08.280
看起來好像不能 train 的東西

0:00:08.280,0:00:10.620
但是，我們還是用 train 的方法來做

0:00:10.620,0:00:13.960
這個就叫做硬 train，或是叫做硬 train 一發這樣子

0:00:14.180,0:00:16.420
如果你看一下這個

0:00:16.620,0:00:19.540
這個是一個部落格，這個部落格裡面

0:00:19.540,0:00:22.860
有一個 Fizz Buzz in TensorFlow 的故事

0:00:22.860,0:00:24.800
有一天，有一個人他去

0:00:24.800,0:00:26.380
這個人好像還滿知名的

0:00:26.380,0:00:27.920
知名的資料科學家

0:00:27.960,0:00:30.820
他去面試

0:00:30.820,0:00:34.380
然後人家說，那我們來考一下程式能力吧

0:00:34.380,0:00:36.320
在白板上面寫程式

0:00:36.320,0:00:37.400
人家就問說

0:00:37.400,0:00:39.780
你會不會來寫一下這個

0:00:39.780,0:00:41.300
Fizz Buzz 的程式

0:00:41.300,0:00:44.335
Fizz Buzz是甚麼意思呢？Fizz Buzz 的意思是說

0:00:44.335,0:00:46.580
現在讓你 print 一串數字

0:00:46.580,0:00:48.080
比如說，1~100

0:00:48.160,0:00:50.820
但是，如果這串數字裡面

0:00:50.820,0:00:54.580
這個數字可以被 3 整除，我們就 output Fizz

0:00:54.580,0:00:56.220
被 5 整除，你就 output Buzz

0:00:56.280,0:00:59.960
同時可以被 3 和 5 整除，你就 output Fizz Buzz

0:00:59.960,0:01:02.760
所以，你的程式 output 應該是 1, 2

0:01:02.780,0:01:06.520
然後，3 可以被 3 整除 ，你就 output Fizz

0:01:06.520,0:01:09.360
然後 4，然後 5 可以被 5 整除，你就 output Buzz

0:01:09.360,0:01:11.080
然後，output Fizz

0:01:11.080,0:01:14.220
然後，7, 8, Fizz, Buzz, 11, Fizz

0:01:14.220,0:01:15.440
然後，13, 14, Fizz Buzz 這樣

0:01:15.440,0:01:17.740
你應該 output 這樣一個 sequence

0:01:17.740,0:01:19.120
我們今天知道，對大家來說

0:01:19.120,0:01:21.160
這個是一個非常簡單的問題

0:01:21.200,0:01:23.460
那個人就說，怎麼做呢

0:01:23.460,0:01:25.740
怎麼做？我要用 Python 做

0:01:25.740,0:01:29.160
那我要先 import 一些 library，這很正常

0:01:29.160,0:01:31.820
直接 import TensorFlow

0:01:32.620,0:01:36.460
然後，接下來，我要準備一些 training data

0:01:36.460,0:01:38.200
你叫我 output 1~100

0:01:38.200,0:01:40.460
我當然不能用這個當作 training data

0:01:40.460,0:01:45.280
所以，我先要 label 101~1000 的 Fizz Buzz

0:01:45.280,0:01:47.960
怎麼做你就去 Amazon 上找

0:01:47.960,0:01:49.760
你去 amt 上找人 label 就好

0:01:49.760,0:01:53.040
label 好以後，我們就弄一個 network 給它 train 下去

0:01:53.040,0:01:55.320
看看結果會怎麼樣

0:01:55.320,0:01:56.700
然後，它後來 train 下去以後

0:01:56.700,0:01:59.000
哇，正確率才 80 幾這樣子

0:01:59.140,0:02:01.180
然後他就沒有得到那一份工作

0:02:03.820,0:02:07.240
我決定來自己試一下這個 Fizz Buzz 的東西

0:02:07.240,0:02:09.385
到底能不能夠做得起來

0:02:09.385,0:02:11.360
自己來實作一下

0:02:11.360,0:02:16.260
首先，我們先來看我們的 training data 長甚麼樣子

0:02:17.740,0:02:21.100
開一下我的 ipython，run 一下

0:02:25.440,0:02:29.480
那我現在就對數字 101~1000 

0:02:29.480,0:02:32.920
做了 labeling，你就不要管那個 label 是怎麼來的了

0:02:32.920,0:02:35.920
我們先來看一下我們的 training data

0:02:35.920,0:02:38.800
那 training data 的 input 每一筆呢

0:02:38.800,0:02:41.340
每一筆就代表了一個數字

0:02:41.340,0:02:43.700
我們先看一下它的 shape

0:02:44.500,0:02:48.480
總共有 900 筆 data，就是從 101 一直數到 1000

0:02:48.480,0:02:49.480
總共 900 筆 data

0:02:49.480,0:02:52.700
我們把第一筆 data dump 出來看看

0:02:53.620,0:02:59.540
那每一個數字，我們都是用二進位來表示它

0:02:59.540,0:03:01.640
每一個數字，我們都是用

0:03:01.640,0:03:04.235
二進位的數值來表示它

0:03:04.240,0:03:06.400
第一個數字是 101

0:03:06.400,0:03:11.780
101 用二進位來表示就是 1010011000

0:03:11.780,0:03:15.020
第一個數字，代表的是這個

0:03:17.400,0:03:20.440
2 的 0 次方、2 的 1 次方、2 的 2 次方

0:03:20.440,0:03:22.660
然後，2 的 5 次方、2 的 6 次方

0:03:22.660,0:03:25.320
你直接把它加起來，你會發現說，它確實就是

0:03:25.540,0:03:28.940
101 這樣，那如果 102 的話

0:03:28.940,0:03:33.200
就是這個樣子，那 103 的話，就是這個樣子

0:03:33.620,0:03:35.880
然後，我們看一下 label 的 data

0:03:35.880,0:03:37.640
label 的 data 長甚麼樣子呢？

0:03:37.640,0:03:41.120
比如說，101 它可以被 3 整除嗎？不行

0:03:41.120,0:03:44.660
它可以被 5 整除嗎？不行，所以

0:03:44.660,0:03:47.080
它是 output 原來自己的數字

0:03:47.080,0:03:48.705
那我們現在總共有 4 個 class

0:03:48.705,0:03:50.400
這 4 個 class 分別代表了

0:03:50.400,0:03:51.660
output 原來的數字

0:03:51.660,0:03:53.740
output Fizz、output Buzz、output Fizz Buzz

0:03:53.760,0:03:55.320
那如果 output 原來的數字

0:03:55.320,0:03:57.680
就是第一維是 1，其他維是 0

0:03:57.680,0:04:00.180
所以，101 的話，第一維是 1，其他都是 0

0:04:00.460,0:04:03.880
接下來呢，我把前面的燈稍微關一下

0:04:03.980,0:04:07.220
接下來我們考慮說 output 102

0:04:07.560,0:04:08.560
output 102

0:04:08.980,0:04:12.340
那如果 102 的話，它可以被 3 整除嗎？它可以被 3 整除

0:04:12.340,0:04:15.500
但不能被 5 整除，所以應該 output Fizz

0:04:16.100,0:04:20.680
所以，它的第二維是 1，其他都是 0

0:04:20.680,0:04:23.760
就這樣子，硬做一下

0:04:25.520,0:04:27.460
我這邊用的 network 架構

0:04:27.460,0:04:30.800
跟那個人面試時用的 network 架構是一樣的

0:04:31.340,0:04:33.060
input 10 維

0:04:33.060,0:04:36.360
然後，1 個 hidden layer 就是 100 個 neuron

0:04:36.380,0:04:38.700
然後，apply ReLU 的 activation function

0:04:38.700,0:04:42.000
output 4 維，Softmax，然後用 Adam 這樣

0:04:42.700,0:04:43.820
這個拿掉

0:04:44.300,0:04:48.460
那等一下就直接 print 正確率，跑一下

0:04:54.380,0:04:55.380
跑一下

0:05:02.445,0:05:04.675
很快阿

0:05:05.755,0:05:09.055
沒多少才 900 筆 training data，秒 train、秒 train

0:05:09.220,0:05:12.160
哇，這個正確率是 76% 阿

0:05:12.160,0:05:14.160
做出來也不是 100

0:05:14.160,0:05:17.460
但是，我不會就這樣放棄，因為你想想看

0:05:17.460,0:05:18.605
如果

0:05:18.605,0:05:20.900
你想想看你看看你的 training data

0:05:20.900,0:05:23.620
你的 training data 正確率才 80% 而已

0:05:23.620,0:05:26.500
代表說，你還沒有讓你的 network

0:05:26.500,0:05:28.660
真的在 training data 上面學起來

0:05:28.660,0:05:30.980
它還沒有真的去 fit 那個 training data

0:05:30.980,0:05:33.420
我們知道說，做 deep learning 的起手式是

0:05:33.440,0:05:35.600
先想辦法 fit 你的 training data

0:05:35.600,0:05:37.495
怎麼 fit 你的 training data 呢？

0:05:37.495,0:05:40.255
開一個比較大的 network，舉例來說

0:05:40.565,0:05:43.645
我們把 hidden layer 的 size 改成 1000

0:05:52.280,0:05:55.520
那再 train 一下，也是很快，秒 train

0:05:55.660,0:05:57.580
你看現在上升起來了

0:05:57.945,0:06:00.285
上升起來，可以跑到 100 嗎？跑到 100 了

0:06:01.460,0:06:04.640
那你看正確率就是 100%

0:06:04.640,0:06:06.720
這樣我就拿到那個 job 了

0:06:06.720,0:06:09.340
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
