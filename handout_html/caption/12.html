<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.800<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.860,0:00:06.100<br>
如果實作的話，要怎麼做呢？<br>
<br>
0:00:06.720,0:00:11.900<br>
那我今天要教大家的 toolkit 阿，是 Keras<br>
<br>
0:00:11.900,0:00:14.880<br>
那你可能想說，怎麼不教 TensorFlow<br>
<br>
0:00:14.880,0:00:16.420<br>
TensorFlow 的星星數<br>
<br>
0:00:16.420,0:00:20.160<br>
不是 deep learning 的 toolkit 裡面最多的嗎？<br>
<br>
0:00:20.160,0:00:22.440<br>
應該要教 TensorFlow<br>
<br>
0:00:22.440,0:00:24.100<br>
其實是這樣子的<br>
<br>
0:00:24.200,0:00:26.160<br>
在另外一門 deep learning 的課裡面，<br>
<br>
0:00:26.160,0:00:27.400<br>
我們教的是 TensorFlow<br>
<br>
0:00:27.400,0:00:30.980<br>
其實，TensorFlow 沒有那麼好用<br>
<br>
0:00:31.420,0:00:34.080<br>
該怎麼說呢，應該是這樣<br>
<br>
0:00:34.240,0:00:37.980<br>
TensorFlow 跟另外一個跟他比較相近的 toolkit，Theano<br>
<br>
0:00:37.985,0:00:40.795<br>
它們是非常的 flexible<br>
<br>
0:00:40.845,0:00:44.235<br>
也就是說，你甚至可以把它想成是一座微分器<br>
<br>
0:00:44.235,0:00:47.315<br>
它甚至<br>
<br>
0:00:47.320,0:00:49.580<br>
它完全可以做 deep leaning 以外的事情<br>
<br>
0:00:49.580,0:00:53.140<br>
它做的事情就是幫你算微分<br>
<br>
0:00:53.360,0:00:55.540<br>
那它把微分的值算給你以後<br>
<br>
0:00:55.540,0:00:58.240<br>
就可以拿去做 Gradient Descent<br>
<br>
0:00:59.080,0:01:01.360<br>
所以，它非常的 flexible<br>
<br>
0:01:01.360,0:01:05.320<br>
那這麼 flexible 的 toolkit 學起來，是有一些難度的<br>
<br>
0:01:05.320,0:01:07.820<br>
至少我覺得，比如我們在接下來的課裡面<br>
<br>
0:01:07.820,0:01:09.500<br>
大概有半個小時的時間<br>
<br>
0:01:09.500,0:01:12.900<br>
那你沒有辦法在半個小時以內，精通<br>
<br>
0:01:12.900,0:01:14.980<br>
熟悉這個 toolkit<br>
<br>
0:01:15.080,0:01:18.240<br>
但是，另外一個 toolkit，Keras 呢<br>
<br>
0:01:18.240,0:01:19.295<br>
我覺得<br>
<br>
0:01:19.300,0:01:22.040<br>
你是可以在數十分鐘內<br>
<br>
0:01:22.040,0:01:23.980<br>
就精通，就可以非常熟悉它<br>
<br>
0:01:23.980,0:01:26.980<br>
然後，用它來 implement 一個自己的 deep learning<br>
<br>
0:01:27.220,0:01:30.700<br>
那這個 Keras，其實是 TensorFlow 跟 Theano<br>
<br>
0:01:31.015,0:01:32.125<br>
interface<br>
<br>
0:01:32.720,0:01:35.960<br>
所以，就有人覺得說，TensorFlow 其實也沒那麼好用<br>
<br>
0:01:35.960,0:01:39.940<br>
所以，它搭了 TensorFlow 的 interface<br>
<br>
0:01:39.960,0:01:41.920<br>
就叫做 Keras<br>
<br>
0:01:41.920,0:01:45.920<br>
所以，你用 Keras 其實就等於你是在用 TensorFlow<br>
<br>
0:01:45.920,0:01:47.240<br>
只是有人幫你把<br>
<br>
0:01:47.360,0:01:49.940<br>
操縱 TensorFlow 那件事情，先幫你寫好<br>
<br>
0:01:49.940,0:01:52.780<br>
那其實它比較好學<br>
<br>
0:01:52.780,0:01:56.940<br>
而且，其實它也有足夠的彈性<br>
<br>
0:01:56.940,0:01:58.140<br>
如果，你想要做的事情<br>
<br>
0:01:58.140,0:02:01.120<br>
多數你想要做的，除非你想要做 deep learning 的研究<br>
<br>
0:02:01.120,0:02:03.060<br>
你想要自己兜奇奇怪怪的 network，不然<br>
<br>
0:02:03.120,0:02:08.040<br>
多數你可以想到的 network，這裡面都有現成<br>
<br>
0:02:08.120,0:02:11.160<br>
現成的 function 可以 call 了<br>
<br>
0:02:11.160,0:02:14.580<br>
而且它背後就是 TensorFlow 或是 Theano<br>
<br>
0:02:14.580,0:02:16.660<br>
所以，你永遠可以去<br>
<br>
0:02:17.060,0:02:19.860<br>
就是有一天，如果你想更精進自己的<br>
<br>
0:02:20.040,0:02:21.340<br>
能力的話<br>
<br>
0:02:21.340,0:02:23.820<br>
你永遠可以去改 Keras 背後的 TensorFlow 的 code<br>
<br>
0:02:23.820,0:02:27.480<br>
然後，做更厲害的事情<br>
<br>
0:02:27.480,0:02:31.740<br>
其實，這邊有一個，我知道一個小道消息是<br>
<br>
0:02:31.800,0:02:35.575<br>
因為 Keras 的作者，他其實是在 Google 工作<br>
<br>
0:02:35.580,0:02:40.240<br>
所以，據說 Keras 即將要變成，這個<br>
<br>
0:02:40.360,0:02:44.380<br>
TensorFlow 官方的 API<br>
<br>
0:02:44.600,0:02:50.440<br>
所以，當時我選擇教 Keras 真的是站對邊了<br>
<br>
0:02:53.480,0:02:58.000<br>
這邊稍微講一下說 Keras<br>
<br>
0:02:58.000,0:03:00.320<br>
Keras 其實是牛角的意思<br>
<br>
0:03:00.320,0:03:02.520<br>
在希臘文裡面，是牛角的意思<br>
<br>
0:03:02.520,0:03:04.255<br>
這個我有特別查過<br>
<br>
0:03:04.260,0:03:07.040<br>
這個來源就是，因為 Keras 就是一個在<br>
<br>
0:03:07.040,0:03:10.900<br>
叫做夢精靈的 project 裡面被 develop 出來的<br>
<br>
0:03:10.900,0:03:15.540<br>
然後，根據伊利亞德，我忘記還是奧德賽裡面的傳說<br>
<br>
0:03:15.540,0:03:19.080<br>
就是夢精靈在來到人世間的時候<br>
<br>
0:03:19.080,0:03:21.320<br>
他會通過兩種門<br>
<br>
0:03:21.320,0:03:24.580<br>
一種門是象牙做的，另一種是牛角做的<br>
<br>
0:03:24.580,0:03:27.900<br>
那如果是通過象牙做的門的夢精靈呢<br>
<br>
0:03:27.900,0:03:31.300<br>
那個人夢的內容就會被實現<br>
<br>
0:03:31.300,0:03:34.760<br>
那如果是通過牛角門，也就是 Keras 來那個夢精靈呢<br>
<br>
0:03:34.765,0:03:37.295<br>
那你的夢就會被實現<br>
<br>
0:03:37.295,0:03:38.540<br>
那個 project 叫做夢精靈<br>
<br>
0:03:38.540,0:03:42.060<br>
所以，在那個 project 開發出來的 toolkit，就叫做 Keras<br>
<br>
0:03:42.360,0:03:46.660<br>
底下是一些它的 documentation 還有一些 example<br>
<br>
0:03:46.660,0:03:52.520<br>
這個，助教會在助教時間跟大家講一下怎麼安裝 Keras<br>
<br>
0:03:52.520,0:03:53.780<br>
如果你不會安裝的話<br>
<br>
0:03:54.760,0:03:58.020<br>
它的安裝的這個 process<br>
<br>
0:03:58.020,0:04:00.540<br>
它其實是寫得很清楚啦，那如果你不會安裝的話<br>
<br>
0:04:00.575,0:04:02.940<br>
助教會跟你講，所以我們等一下就只是<br>
<br>
0:04:02.940,0:04:03.665<br>
看一下說<br>
<br>
0:04:03.665,0:04:05.300<br>
Keras 如果運作起來<br>
<br>
0:04:05.300,0:04:06.400<br>
如果你真的在操控<br>
<br>
0:04:06.400,0:04:09.440<br>
真的在實作的時候，大概看起來像是甚麼樣子<br>
<br>
0:04:09.740,0:04:13.660<br>
以下是某位同學使用 Keras 的心得<br>
<br>
0:04:13.660,0:04:17.140<br>
那他把他的心得做成 6 格的圖<br>
<br>
0:04:17.140,0:04:18.820<br>
放在 Facebook 上面<br>
<br>
0:04:20.000,0:04:23.460<br>
他說，一個 deep learning 的研究生，都在做甚麼事？<br>
<br>
0:04:23.460,0:04:25.980<br>
朋友覺得他在做 AlphaGo<br>
<br>
0:04:26.020,0:04:27.200<br>
他媽覺得他坐在電腦前面<br>
<br>
0:04:27.200,0:04:29.120<br>
大家都覺得他在做很潮的東西<br>
<br>
0:04:29.120,0:04:31.660<br>
指導教授知道是在解一個 Optimization problem<br>
<br>
0:04:31.660,0:04:34.300<br>
他以為自己很潮這樣子<br>
<br>
0:04:34.300,0:04:36.420<br>
我覺得應該是這個意思啦<br>
<br>
0:04:36.500,0:04:38.540<br>
然後，事實上你在做的事情<br>
<br>
0:04:38.540,0:04:40.020<br>
就是疊積木<br>
<br>
0:04:40.020,0:04:42.445<br>
那這個並不是他玩得很開心的意思<br>
<br>
0:04:42.445,0:04:45.940<br>
疊積木的意思就是說，當你在使用 Keras 的時候<br>
<br>
0:04:45.940,0:04:48.405<br>
它是非常的容易的<br>
<br>
0:04:48.405,0:04:51.635<br>
你在做的事情就是把現成的<br>
<br>
0:04:51.640,0:04:54.020<br>
module 呢，把現成的 function 呢<br>
<br>
0:04:54.020,0:04:56.120<br>
疊來疊去而已，所以<br>
<br>
0:04:56.120,0:04:59.220<br>
用 Keras 就好像是在疊積木一樣<br>
<br>
0:05:00.060,0:05:03.360<br>
那我們等一下就是用，我在前一堂課講的<br>
<br>
0:05:03.500,0:05:06.100<br>
Handwriting 的 Digit Recognition<br>
<br>
0:05:06.100,0:05:08.980<br>
來當作例子，來示範 Keras 怎麼使用<br>
<br>
0:05:08.980,0:05:12.480<br>
如果，你這輩子都還沒有寫過 deep learning 的程式的話<br>
<br>
0:05:12.480,0:05:16.600<br>
那今天這個，就是 deep learning 的 "Hello world"<br>
<br>
0:05:16.600,0:05:19.980<br>
我用的 data，就是 MNIST 的 data<br>
<br>
0:05:19.980,0:05:21.940<br>
MNIST 的 data 就好像是<br>
<br>
0:05:21.940,0:05:24.120<br>
machine learning 的果蠅一樣<br>
<br>
0:05:24.280,0:05:26.480<br>
果蠅知道嗎？<br>
<br>
0:05:27.060,0:05:30.360<br>
果蠅就是，如果你想要很快做一個實驗<br>
<br>
0:05:30.360,0:05:32.400<br>
你就用 MNIST<br>
<br>
0:05:32.400,0:05:35.120<br>
因為它的 data 量少<br>
<br>
0:05:35.120,0:05:38.420<br>
做起來又強<br>
<br>
0:05:39.800,0:05:42.615<br>
然後，這個在 MNIST 裡面阿<br>
<br>
0:05:42.620,0:05:45.380<br>
你要 machine 做的事情就是 input 一個 image<br>
<br>
0:05:45.380,0:05:49.860<br>
然後，output 就是這個 image 是 0~9 的哪一個數值<br>
<br>
0:05:49.860,0:05:53.280<br>
那 input image 的 size 是 28*28<br>
<br>
0:05:53.280,0:05:54.845<br>
它是一個 matrix<br>
<br>
0:05:54.845,0:05:56.380<br>
28*28 的 matrix<br>
<br>
0:05:56.380,0:05:57.575<br>
那其實 Keras<br>
<br>
0:05:57.580,0:06:01.940<br>
有 provide 自動下載 MNIST 的 data 的 function<br>
<br>
0:06:03.740,0:06:06.240<br>
那如果用 Keras 的話<br>
<br>
0:06:06.240,0:06:10.540<br>
你要怎麼做呢？<br>
<br>
0:06:10.660,0:06:12.920<br>
那我們之前有講過說<br>
<br>
0:06:12.920,0:06:14.960<br>
deep learning 就是 3 個步驟<br>
<br>
0:06:14.960,0:06:18.020<br>
第一個步驟就是決定一下你的 function set<br>
<br>
0:06:18.020,0:06:19.815<br>
決定一下你的 neural network<br>
<br>
0:06:19.820,0:06:21.320<br>
要長甚麼樣子<br>
<br>
0:06:21.660,0:06:24.520<br>
那在 Keras 裡面，你就是先宣告<br>
<br>
0:06:24.520,0:06:27.280<br>
model = Sequential( )，你就先宣告一個 model<br>
<br>
0:06:27.625,0:06:30.655<br>
接下來，你就看你的 network 想要長甚麼樣子<br>
<br>
0:06:30.660,0:06:33.420<br>
你就自己決定它要長甚麼樣子<br>
<br>
0:06:33.600,0:06:36.615<br>
舉例來說，我這邊想要疊一個 network<br>
<br>
0:06:36.615,0:06:39.635<br>
它有，我們把滑鼠叫出來<br>
<br>
0:06:39.900,0:06:43.720<br>
我們這邊想要疊一個 network，它有兩個 hidden layer<br>
<br>
0:06:43.720,0:06:47.280<br>
然後，每一個 layer 都有 500 個 neuron<br>
<br>
0:06:47.280,0:06:48.960<br>
我想要做這件事情的話<br>
<br>
0:06:48.960,0:06:50.500<br>
我應該要怎麼做呢？<br>
<br>
0:06:50.500,0:06:51.920<br>
其實很容易<br>
<br>
0:06:51.920,0:06:55.860<br>
你先宣告一個 model，接下來就說 model.add<br>
<br>
0:06:56.060,0:06:58.480<br>
加一個東西，加甚麼？<br>
<br>
0:06:58.480,0:07:01.640<br>
這邊我們要加一個 Fully connected 的 layer<br>
<br>
0:07:01.645,0:07:04.595<br>
Fully connected 的 layer 就是用 Dense 來表示<br>
<br>
0:07:04.595,0:07:07.935<br>
還可以加別的 layer，比如說 convolution 的 layer<br>
<br>
0:07:08.140,0:07:10.260<br>
所以，這邊宣告 Dense<br>
<br>
0:07:10.260,0:07:13.220<br>
( input dimension 是 28*28)<br>
<br>
0:07:13.220,0:07:16.580<br>
output dimension 是 500<br>
<br>
0:07:16.580,0:07:19.560<br>
那你就是 input 一個 28*28 的 vector<br>
<br>
0:07:19.820,0:07:22.700<br>
這個 vector 代表一個 image，然後呢<br>
<br>
0:07:22.780,0:07:26.215<br>
output = 500，就是說你今天要有<br>
<br>
0:07:26.215,0:07:27.715<br>
500 個 neuron<br>
<br>
0:07:28.100,0:07:30.680<br>
接下來呢，你要告訴 network 說你的<br>
<br>
0:07:30.680,0:07:33.675<br>
activation function 要用甚麼，這邊就直接寫說<br>
<br>
0:07:33.680,0:07:37.980<br>
model.add ( Activation ("sigmoid") )<br>
<br>
0:07:37.980,0:07:41.600<br>
那你就是用 sigmoid 當你的 activation function<br>
<br>
0:07:41.600,0:07:43.005<br>
那你可以選別的<br>
<br>
0:07:43.005,0:07:45.880<br>
Keras 裡面你可以選別的，比如說，softplus, softsign 阿<br>
<br>
0:07:45.960,0:07:48.400<br>
relu, tanh 等等一大堆別的<br>
<br>
0:07:48.400,0:07:51.400<br>
而且你要改，加上自己新的 activation function 呢<br>
<br>
0:07:51.400,0:07:53.780<br>
其實也滿容易的<br>
<br>
0:07:53.780,0:07:56.460<br>
你只要找到 Keras 裡面寫 activation function 的地方<br>
<br>
0:07:56.460,0:07:59.940<br>
然後，自己再加一個自己的 function 進去就好了<br>
<br>
0:08:00.560,0:08:01.560<br>
然後<br>
<br>
0:08:01.920,0:08:04.360<br>
如果要再加一個新的 layer 怎麼辦呢？<br>
<br>
0:08:04.360,0:08:07.920<br>
你就一樣下 model.add 一個 dense layer<br>
<br>
0:08:07.920,0:08:09.000<br>
它的 output 是 500<br>
<br>
0:08:09.000,0:08:11.080<br>
這邊你就不需要再給它 input 了<br>
<br>
0:08:11.080,0:08:13.875<br>
因為，下一個 layer 的 input<br>
<br>
0:08:13.875,0:08:15.220<br>
就等於前一個 layer 的 output<br>
<br>
0:08:15.220,0:08:19.380<br>
所以，你不需要再 redefine input 的 dimension 是多少<br>
<br>
0:08:19.380,0:08:23.480<br>
你只要直接告訴它說，output 你要 500 個 neuron 就好<br>
<br>
0:08:23.480,0:08:26.120<br>
不需要告訴它說，input 也是 500 個 neuron<br>
<br>
0:08:26.120,0:08:28.900<br>
這個 Keras 自己知道<br>
<br>
0:08:29.300,0:08:32.280<br>
activation function 這邊一樣是用 sigmoid<br>
<br>
0:08:32.280,0:08:35.060<br>
最後，output 是<br>
<br>
0:08:35.060,0:08:37.880<br>
要做數字分類嘛，要 10 個數字<br>
<br>
0:08:37.880,0:08:40.440<br>
所以你 output 一定要 10 維，不可以設別的數字<br>
<br>
0:08:40.440,0:08:42.440<br>
設 11, 12，那都不 work<br>
<br>
0:08:42.440,0:08:46.100<br>
所以，你這個 output 就是設 10 維<br>
<br>
0:08:46.100,0:08:47.820<br>
output dimension 就是 10 維<br>
<br>
0:08:47.820,0:08:50.020<br>
那 activation function，你這邊呢<br>
<br>
0:08:50.020,0:08:51.500<br>
我們說在 output 的 layer<br>
<br>
0:08:51.500,0:08:53.920<br>
如果把它當成一個 multi-class classifier 來看的話<br>
<br>
0:08:53.920,0:08:56.380<br>
我們就用 softmax，但你也可以用別的<br>
<br>
0:08:56.380,0:08:58.815<br>
我們完全可以用 sigmoid，甚麼都可以<br>
<br>
0:08:58.820,0:09:01.540<br>
那這邊呢，選擇用 softmax<br>
<br>
0:09:02.420,0:09:07.800<br>
那接下來我們要決定一個 function，<br>
要 evaluate 一個 function 的好壞<br>
<br>
0:09:07.820,0:09:10.160<br>
怎麼 evaluate 一個 function 的好壞呢？<br>
<br>
0:09:10.160,0:09:12.360<br>
你要做的事情就是<br>
<br>
0:09:13.140,0:09:14.800<br>
model.compile<br>
<br>
0:09:14.800,0:09:17.340<br>
然後，定義你的 loss 是什麼<br>
<br>
0:09:17.340,0:09:20.640<br>
比如說，如果你要用 cross entropy 的話<br>
<br>
0:09:20.640,0:09:22.700<br>
那你的 loss 就是<br>
<br>
0:09:23.040,0:09:26.220<br>
但在 Keras 裡面，cross entropy 它是寫成<br>
<br>
0:09:26.220,0:09:29.800<br>
categorical cross entropy，就是 cross entropy，OK？<br>
<br>
0:09:29.940,0:09:34.720<br>
寫說你的 loss = cross entropy 就行了<br>
<br>
0:09:34.920,0:09:37.640<br>
其實，它也有支援很多其他的 loss function<br>
<br>
0:09:37.640,0:09:40.920<br>
在不同的場合，你會需要用到不同的 loss function<br>
<br>
0:09:40.920,0:09:45.360<br>
這就留給大家自己去看 Keras 的 documentation 了<br>
<br>
0:09:47.020,0:09:49.580<br>
那再來呢，是 training 的部分<br>
<br>
0:09:49.580,0:09:52.215<br>
在 training 之前，你要先下一些<br>
<br>
0:09:52.220,0:09:55.420<br>
Configuration 告訴它你 training 的時候<br>
<br>
0:09:55.420,0:09:58.580<br>
你打算要怎麼做<br>
<br>
0:09:58.580,0:10:00.180<br>
所以，現在呢<br>
<br>
0:10:00.920,0:10:02.560<br>
所以，現在呢<br>
<br>
0:10:02.560,0:10:06.000<br>
你要下的第一個東西是 optimizer<br>
<br>
0:10:06.000,0:10:08.645<br>
也就是說，你要找最好的 function 的時候<br>
<br>
0:10:08.645,0:10:12.720<br>
你要用甚麼樣的方式，來找最好的 function<br>
<br>
0:10:12.720,0:10:17.120<br>
雖然說，這邊 optimizer 的後面可以接不同的方式<br>
<br>
0:10:17.120,0:10:21.380<br>
但是，這些不同的方式，其實都是 Gradient Descent<br>
<br>
0:10:21.380,0:10:22.580<br>
類似的方法<br>
<br>
0:10:22.800,0:10:25.980<br>
只是他們用的 learning rate<br>
<br>
0:10:25.980,0:10:27.795<br>
learning rate 你可以<br>
<br>
0:10:27.800,0:10:30.120<br>
就是我們在做一般的 Gradient Descent 的時候<br>
<br>
0:10:30.120,0:10:31.840<br>
你要自己設一個 learning rate，對不對？<br>
<br>
0:10:31.840,0:10:35.160<br>
但是，有一些方法是 machine 會<br>
<br>
0:10:35.660,0:10:36.900<br>
自動的<br>
<br>
0:10:37.220,0:10:40.660<br>
跟 empirically 決定 learning rate 的值應該是多少<br>
<br>
0:10:40.660,0:10:43.415<br>
所以，有一些方法是不需要<br>
<br>
0:10:43.420,0:10:44.800<br>
給它 learning rate<br>
<br>
0:10:44.800,0:10:47.740<br>
machine 自己會決定 learning rate 應該要多少<br>
<br>
0:10:47.740,0:10:51.380<br>
那這邊呢，有支援各式各樣的方法<br>
<br>
0:10:52.500,0:10:56.700<br>
那最後就是，決定好<br>
<br>
0:10:56.705,0:10:58.200<br>
要怎麼做 Gradient Descent 以後<br>
<br>
0:10:58.200,0:10:59.915<br>
再來就是真的去跑 Gradient Descent<br>
<br>
0:10:59.980,0:11:02.720<br>
再來就是真的給它做下去<br>
<br>
0:11:02.840,0:11:05.980<br>
那這一步，很簡單<br>
<br>
0:11:05.980,0:11:08.860<br>
你就給它 4 個 input，第一個 input 是<br>
<br>
0:11:08.860,0:11:11.820<br>
training data，在我們的 case 裡面<br>
<br>
0:11:11.820,0:11:15.300<br>
training data 就是一張一張的 image<br>
<br>
0:11:16.180,0:11:18.980<br>
那你要給每一張 training data label，在這邊我們就是<br>
<br>
0:11:18.980,0:11:20.880<br>
要告訴 machine 說，現在 training data 裡面<br>
<br>
0:11:20.880,0:11:23.880<br>
每一張 image 它對應到 0~9 的哪一個數字<br>
<br>
0:11:23.880,0:11:26.680<br>
後面那兩個 flag 我們等一下再解釋<br>
<br>
0:11:27.140,0:11:29.680<br>
我們來看一下，實際上<br>
<br>
0:11:29.680,0:11:32.960<br>
這個 x_train 跟 y_train 應該是長甚麼樣子<br>
<br>
0:11:33.340,0:11:35.840<br>
在這 case 裡面，你 training 的 image 呢<br>
<br>
0:11:35.840,0:11:38.260<br>
就是要存成一個 numpy array 啦<br>
<br>
0:11:38.260,0:11:41.940<br>
那你就不要問我說，怎麼把一個 image<br>
存到 numpy array 裡面去<br>
<br>
0:11:41.940,0:11:45.920<br>
這個，應該作業裡有做過了，對不對？<br>
<br>
0:11:46.720,0:11:50.420<br>
那你要把你的 image 都存到 numpy array 裡面去<br>
<br>
0:11:50.740,0:11:52.800<br>
那它的存法是這樣子<br>
<br>
0:11:52.800,0:11:56.080<br>
這個 numpy array 是 two-dimension 的<br>
<br>
0:11:56.080,0:11:59.180<br>
它是一個 two-dimension 的 matrix<br>
<br>
0:11:59.180,0:12:03.400<br>
它的第一個 dimension 代表你有多少個 example<br>
<br>
0:12:03.400,0:12:06.620<br>
假設你有一萬個 example，<br>
第一個 dimension 就是一萬維<br>
<br>
0:12:07.340,0:12:10.560<br>
第二個 dimension 就是看你的 image 有多大<br>
<br>
0:12:10.560,0:12:12.620<br>
要幾個 pixel，那第二個 dimension 就有多大<br>
<br>
0:12:12.620,0:12:13.560<br>
其實在這個 case 裡面<br>
<br>
0:12:13.560,0:12:16.960<br>
在這個 case 裡面，有 28*28 個 pixel<br>
<br>
0:12:16.960,0:12:20.340<br>
所以，有 784 個 pixel<br>
<br>
0:12:20.340,0:12:23.880<br>
所以，每一個 dimension 就是 784 維<br>
<br>
0:12:24.220,0:12:26.920<br>
那我們來看 y_train<br>
<br>
0:12:26.920,0:12:29.740<br>
y_train 這每一個 image 的 label 怎麼表示呢？<br>
<br>
0:12:29.740,0:12:33.720<br>
一樣第一個 dimension 代表<br>
你有幾個 training 的 example<br>
<br>
0:12:33.720,0:12:37.680<br>
你有幾個 training 的 example，第<br>
一個 dimension 就有多維<br>
<br>
0:12:37.680,0:12:40.520<br>
那第二個 dimension 呢，第二個 dimension 就是 10<br>
<br>
0:12:40.520,0:12:42.920<br>
因為我們現在的 output 就是 10 維嘛<br>
<br>
0:12:42.920,0:12:45.700<br>
我們 label 只有 10 個可能，0~9 而已<br>
<br>
0:12:45.700,0:12:48.720<br>
所以，output dimension 就是 10 維<br>
<br>
0:12:49.000,0:12:53.160<br>
然後，今天這個<br>
<br>
0:12:53.160,0:12:57.100<br>
這個 image 所對應的數字，它就會是 1<br>
<br>
0:12:57.100,0:12:59.540<br>
也就是在那邊是塗黑的，其他是 0<br>
<br>
0:12:59.540,0:13:01.380<br>
舉例來說，第一個 image<br>
<br>
0:13:01.380,0:13:03.360<br>
這個是 5<br>
<br>
0:13:03.360,0:13:05.780<br>
所以，在它的 label 裡面<br>
<br>
0:13:05.780,0:13:09.060<br>
這個數字是 0, 1, 2, 3, 4<br>
<br>
0:13:09.435,0:13:11.580<br>
它從 0 開始算，所以就是<br>
<br>
0:13:11.580,0:13:14.720<br>
對應到 5 的那維是 1，其他是 0<br>
<br>
0:13:14.720,0:13:18.960<br>
比如說，第 4 個數字<br>
<br>
0:13:18.960,0:13:22.260<br>
就是對應到 1 的那一維是 1，其他是 0<br>
<br>
0:13:22.760,0:13:26.015<br>
那前面還有兩個我沒有解釋過的東西，一個是<br>
<br>
0:13:26.020,0:13:29.280<br>
batch_size，另一個是 nb_epoch<br>
<br>
0:13:29.280,0:13:30.700<br>
他們是甚麼意思呢？<br>
<br>
0:13:31.845,0:13:34.865<br>
所謂的 batch_size 是這樣，首先呢<br>
<br>
0:13:34.865,0:13:38.480<br>
這邊有一個秘密，就是<br>
<br>
0:13:38.560,0:13:42.260<br>
我們其實在做 Gradient Descent 的時候，<br>
在做 deep learning 的時候<br>
<br>
0:13:42.340,0:13:45.400<br>
我們並不會真的去<br>
<br>
0:13:45.405,0:13:47.425<br>
minimize total loss<br>
<br>
0:13:47.975,0:13:50.515<br>
我們做的是甚麼呢？我們會把<br>
<br>
0:13:50.520,0:13:55.140<br>
training data 分成一個一個的 batch<br>
<br>
0:13:55.220,0:13:57.200<br>
也就是說你把你的 training data<br>
<br>
0:13:57.200,0:13:58.920<br>
比如說，一萬張 image 拿出來<br>
<br>
0:13:59.060,0:14:01.460<br>
然後，每一次 random 的選<br>
<br>
0:14:01.460,0:14:03.820<br>
100 張進來，作為一個 batch<br>
<br>
0:14:04.500,0:14:08.080<br>
那這個 batch 你要隨機的分<br>
<br>
0:14:08.080,0:14:09.995<br>
如果你 batch 沒有隨機的分<br>
<br>
0:14:10.000,0:14:12.120<br>
比如說，你某一個 batch 裡面通通都是 1<br>
<br>
0:14:12.120,0:14:14.260<br>
那另外一個 batch 裡面通通都是數字 2<br>
<br>
0:14:14.260,0:14:15.680<br>
這個你可以自己試試看<br>
<br>
0:14:15.680,0:14:17.360<br>
你 train 起來，會有問題的<br>
<br>
0:14:17.360,0:14:20.880<br>
對你的 performance 會有不小的影響<br>
<br>
0:14:24.840,0:14:27.000<br>
接下來，怎麼做呢？<br>
<br>
0:14:27.000,0:14:29.820<br>
首先，你先 randomly initialize network 的參數<br>
<br>
0:14:29.820,0:14:31.300<br>
就跟一般的 Gradient Descent 一樣<br>
<br>
0:14:31.300,0:14:33.960<br>
接下來，隨機的選一個 batch 出來<br>
<br>
0:14:33.960,0:14:35.820<br>
比如說，我們選了第一個 batch 出來<br>
<br>
0:14:35.840,0:14:38.380<br>
然後，接下來我們計算<br>
<br>
0:14:38.700,0:14:42.180<br>
對第一個 batch 裡面的 element 的 total loss<br>
<br>
0:14:42.180,0:14:44.160<br>
不是全部 training data 的 total loss 哦<br>
<br>
0:14:44.160,0:14:47.100<br>
是第一個 batch 裡面的 element 的 total loss<br>
<br>
0:14:47.340,0:14:49.360<br>
我們計算說 L'<br>
<br>
0:14:49.520,0:14:51.740<br>
等於 l1 + l31<br>
<br>
0:14:51.740,0:14:54.280<br>
加上別的 batch 的 loss<br>
<br>
0:14:54.280,0:14:58.240<br>
不是別的 batch，<br>
同一個 batch 裡面其他 example 的 loss<br>
<br>
0:14:58.620,0:15:02.540<br>
然後，接下來根據 L' update 參數<br>
<br>
0:15:02.540,0:15:05.800<br>
也就是去計算參數對 L' 的偏微分<br>
<br>
0:15:05.800,0:15:07.660<br>
然後，update 參數<br>
<br>
0:15:07.800,0:15:09.580<br>
接下來，再隨機選一個 batch<br>
<br>
0:15:09.580,0:15:11.960<br>
比如說，這邊選的是第二個 batch<br>
<br>
0:15:12.080,0:15:14.580<br>
那你就計算說，現在你的 total loss<br>
<br>
0:15:14.580,0:15:15.880<br>
變成 L"<br>
<br>
0:15:15.880,0:15:20.740<br>
它不是 total L，它是 l2 + l16<br>
<br>
0:15:20.840,0:15:24.860<br>
再加同一個 batch 裡面其他的 example<br>
<br>
0:15:24.860,0:15:27.960<br>
接下來，計算你的參數對 L" 的偏微分<br>
<br>
0:15:27.960,0:15:30.880<br>
然後，去 update 你的參數<br>
<br>
0:15:30.880,0:15:32.940<br>
你就反覆做這個 process<br>
<br>
0:15:32.940,0:15:36.120<br>
直到把所有的 batch<br>
<br>
0:15:36.120,0:15:38.080<br>
通通選過一次<br>
<br>
0:15:38.680,0:15:43.080<br>
所以，今天假設你有 100 個 batch 的話<br>
<br>
0:15:43.080,0:15:46.780<br>
你就把這個參數 update 100 次<br>
<br>
0:15:46.780,0:15:50.040<br>
那把每一個 batch 都看過以後<br>
<br>
0:15:50.040,0:15:52.360<br>
那你就等於<br>
<br>
0:15:52.360,0:15:56.740<br>
把所有的 batch 都看過一次，叫做一個 epoch<br>
<br>
0:15:56.940,0:15:57.940<br>
叫做一個 epoch<br>
<br>
0:15:58.060,0:15:59.600<br>
那我們要做的事情就是<br>
<br>
0:15:59.600,0:16:01.360<br>
重複以上的 process<br>
<br>
0:16:01.480,0:16:03.160<br>
所以，你在 train  一個 network 的時候<br>
<br>
0:16:03.160,0:16:06.600<br>
你會需要好幾十個 epoch<br>
<br>
0:16:06.600,0:16:08.180<br>
不是只有一個 epoch<br>
<br>
0:16:08.600,0:16:11.895<br>
所以，這邊這兩個 flag，一個是 batch_size<br>
<br>
0:16:11.900,0:16:15.580<br>
就是告訴 Keras 說，我們的一個 batch<br>
<br>
0:16:15.580,0:16:17.600<br>
要有多大<br>
<br>
0:16:17.880,0:16:20.660<br>
舉例來說，這邊 batch_size = 100<br>
<br>
0:16:20.660,0:16:23.700<br>
就是說，我們要把 100 個 example<br>
<br>
0:16:23.945,0:16:26.625<br>
放在一個 batch 裡面<br>
<br>
0:16:26.835,0:16:29.735<br>
那 Keras 會幫你隨機的放<br>
<br>
0:16:29.740,0:16:32.360<br>
所以這個部分你就不需要自己寫 code<br>
<br>
0:16:32.360,0:16:34.360<br>
那 number of epoch 就是說呢<br>
<br>
0:16:34.360,0:16:37.300<br>
每一個 batch 看過一次，叫做一個 epoch<br>
<br>
0:16:37.300,0:16:39.065<br>
那我們到底要用幾個 epoch 呢？<br>
<br>
0:16:39.065,0:16:41.535<br>
這邊 epoch  = 20 就是以上這個 process<br>
<br>
0:16:42.060,0:16:44.680<br>
重複 20 次，也就是每一個 batch<br>
<br>
0:16:44.680,0:16:47.040<br>
被看過 20 次，那你要注意<br>
<br>
0:16:47.040,0:16:50.000<br>
我們並不會在一個<br>
<br>
0:16:50.745,0:16:53.755<br>
在一個 batch 裡面阿<br>
<br>
0:16:53.755,0:16:56.725<br>
我們已經 update 很多次參數了<br>
<br>
0:16:56.725,0:16:58.720<br>
我們每看一個 batch 就 update 一次參數<br>
<br>
0:16:58.720,0:17:00.195<br>
假設我們現在有 100 個 batch<br>
<br>
0:17:00.715,0:17:03.725<br>
那一個 epoch 裡面，我們就已經 update 100 次參數了<br>
<br>
0:17:03.725,0:17:06.900<br>
20 個 epoch 就是 20*100，就是 2000 次參數<br>
<br>
0:17:06.900,0:17:09.685<br>
所以，並不是說這邊設 20 個 epoch<br>
<br>
0:17:09.685,0:17:11.460<br>
我們就只 update 20次參數的意思<br>
<br>
0:17:11.460,0:17:12.895<br>
在一個 epoch 裡面<br>
<br>
0:17:12.895,0:17:14.765<br>
我們會 update 很多次參數<br>
<br>
0:17:15.435,0:17:17.240<br>
那我們記得我們之前<br>
<br>
0:17:17.240,0:17:20.940<br>
林宗男老師應該有講過 Stochastic Gradient Descent<br>
<br>
0:17:20.940,0:17:24.140<br>
今天如果我們的 batch_size 設為 1 的話<br>
<br>
0:17:24.320,0:17:30.320<br>
那就是 equivalent to Stochastic Gradient Descent<br>
<br>
0:17:30.540,0:17:31.540<br>
對不對？<br>
<br>
0:17:31.800,0:17:36.340<br>
那我們之前有講過 Stochastic Gradient Descent 的好處<br>
<br>
0:17:36.340,0:17:39.460<br>
它的好處就是，它的速度比較快<br>
<br>
0:17:39.460,0:17:42.040<br>
對不對，相較於 full 的<br>
<br>
0:17:42.040,0:17:44.240<br>
原來的 Gradient Descent<br>
<br>
0:17:44.260,0:17:45.820<br>
它的速度是比較快的<br>
<br>
0:17:45.820,0:17:48.900<br>
因為原來的 Gradient Descent，你 update 參數的時候<br>
<br>
0:17:48.900,0:17:50.460<br>
你用 Stochastic Gradient Descent<br>
<br>
0:17:50.460,0:17:52.120<br>
假設你有 100 個 training data 的話<br>
<br>
0:17:52.120,0:17:54.420<br>
那你已經 update 100 次參數了<br>
<br>
0:17:54.420,0:17:57.160<br>
雖然說，每一次 update 參數的方向<br>
<br>
0:17:57.160,0:18:01.060<br>
是不穩定的，但是就是天下武功，唯快不破<br>
<br>
0:18:01.160,0:18:04.220<br>
雖然它的出拳可能會落空<br>
<br>
0:18:04.220,0:18:07.660<br>
但是，它可以在別人出一拳的時候，它已經出 100 拳了<br>
<br>
0:18:07.660,0:18:09.080<br>
所以，它是比較強的<br>
<br>
0:18:11.585,0:18:14.705<br>
那你可能會想說，既然是這樣子的話<br>
<br>
0:18:14.705,0:18:18.005<br>
為甚麼我們不用 Stochastic Gradient Descent 就好<br>
<br>
0:18:18.105,0:18:21.160<br>
還要用 Mini-batch 呢？<br>
<br>
0:18:21.160,0:18:24.860<br>
接下來，就是一些實作上的問題<br>
<br>
0:18:24.860,0:18:27.140<br>
讓我們必須要用 Mini-batch<br>
<br>
0:18:27.140,0:18:28.760<br>
Mini-batch 這件事很重要<br>
<br>
0:18:28.760,0:18:32.700<br>
但是，最主要要用 Mini-batch 的理由<br>
<br>
0:18:32.700,0:18:35.380<br>
其實是一個實作上的 issue<br>
<br>
0:18:37.905,0:18:40.245<br>
我們之前有講說<br>
<br>
0:18:40.500,0:18:43.260<br>
你在一個 epoch<br>
<br>
0:18:43.260,0:18:45.120<br>
在一個 epoch<br>
<br>
0:18:45.680,0:18:49.380<br>
舉例來說，我們這邊有 50000 個 example<br>
<br>
0:18:49.660,0:18:52.380<br>
那我們的 batch_size，如果你設 1<br>
<br>
0:18:52.380,0:18:54.620<br>
也就是 Stochastic Gradient Descent 的話<br>
<br>
0:18:54.620,0:18:59.020<br>
那在一個 epoch 裡面，你會 update 50000 次參數<br>
<br>
0:18:59.640,0:19:03.180<br>
如果今天你的 batch_size 設為 10 的話<br>
<br>
0:19:03.340,0:19:06.180<br>
在一個 epoch 裡面<br>
<br>
0:19:06.180,0:19:09.220<br>
你會 update 5000 次參數<br>
<br>
0:19:09.560,0:19:11.600<br>
這樣看起來，好像是<br>
<br>
0:19:12.360,0:19:15.100<br>
Stochastic Gradient Descent 比較快<br>
<br>
0:19:15.100,0:19:16.700<br>
Mini-batch 設為 10<br>
<br>
0:19:16.700,0:19:17.880<br>
你在一個 epoch 裡面<br>
<br>
0:19:17.880,0:19:19.920<br>
才 update 5000 次參數<br>
<br>
0:19:20.140,0:19:21.740<br>
那 Stochastic Gradient Descent<br>
<br>
0:19:21.740,0:19:23.600<br>
它可以 update 50000 次參數<br>
<br>
0:19:24.085,0:19:27.215<br>
它的速度好像應該是它的 10 倍<br>
<br>
0:19:27.335,0:19:28.335<br>
但是<br>
<br>
0:19:28.985,0:19:29.985<br>
實際上阿<br>
<br>
0:19:30.175,0:19:31.455<br>
實際上<br>
<br>
0:19:32.245,0:19:34.755<br>
當你 batch_size 設不一樣的時候<br>
<br>
0:19:35.105,0:19:37.865<br>
一個 epoch 需要的時間<br>
<br>
0:19:38.135,0:19:39.135<br>
是不一樣的<br>
<br>
0:19:39.360,0:19:41.620<br>
這樣大家了解我的意思嗎？<br>
<br>
0:19:41.620,0:19:44.360<br>
就是你想說，這個 training data 不是都是五萬筆嗎？<br>
<br>
0:19:44.360,0:19:46.060<br>
就是這個 training data 不一定都是五萬筆<br>
<br>
0:19:46.060,0:19:48.980<br>
你設 batch_size = 1，你設 batch_size = 10<br>
<br>
0:19:48.980,0:19:51.760<br>
運算量是一樣多的嗎？<br>
<br>
0:19:52.400,0:19:55.535<br>
就是說，雖然運算量不是一樣多<br>
<br>
0:19:55.540,0:19:58.400<br>
你要把五萬筆 example，每一筆 example 都<br>
<br>
0:19:58.400,0:19:59.900<br>
都看過一遍<br>
<br>
0:19:59.980,0:20:04.140<br>
那同一個 epoch 裡面，它要做的運算量不是一樣多的嗎<br>
<br>
0:20:04.240,0:20:06.000<br>
一樣多的運算跟一樣多的時間<br>
<br>
0:20:06.000,0:20:07.500<br>
但是 batch_size 設成 1<br>
<br>
0:20:07.500,0:20:10.220<br>
你可以 update 五萬次，聽起來好像是比較厲害<br>
<br>
0:20:10.220,0:20:12.980<br>
但是，實際上，在實作上<br>
<br>
0:20:13.200,0:20:15.480<br>
當你 batch_size 設不一樣的時候<br>
<br>
0:20:15.480,0:20:17.740<br>
運算的時間是不一樣的<br>
<br>
0:20:17.740,0:20:21.660<br>
就算是同樣多的 example，但它的運算時間是不一樣的<br>
<br>
0:20:21.660,0:20:23.560<br>
那等一下會解釋為甚麼<br>
<br>
0:20:23.560,0:20:25.280<br>
我們先來看實際上的例子<br>
<br>
0:20:25.280,0:20:27.880<br>
這個就是在 GTX 980<br>
<br>
0:20:27.880,0:20:31.100<br>
然後，跑在 MNIST 的五萬個 training example 上面<br>
<br>
0:20:31.100,0:20:35.120<br>
一個 batch 需要的時間，當我<br>
設不同的 batch_size 的時候<br>
<br>
0:20:35.120,0:20:38.800<br>
如果今天 batch_size 設 1，<br>
也就是 Stochastic Gradient Descent<br>
<br>
0:20:38.800,0:20:41.740<br>
一個 epoch 要 166 秒<br>
<br>
0:20:41.740,0:20:45.140<br>
也就是接近 3 分鐘<br>
<br>
0:20:45.660,0:20:48.040<br>
如果，今天 batch_size 設 10 的話<br>
<br>
0:20:48.040,0:20:50.960<br>
那一個 epoch 是 17 秒<br>
<br>
0:20:51.040,0:20:52.720<br>
你會發現說<br>
<br>
0:20:52.720,0:20:55.500<br>
如果你今天設 100, 1000, 10000<br>
<br>
0:20:55.500,0:20:59.580<br>
那它就是越來越快，每一個 epoch 都是越來越快<br>
<br>
0:20:59.580,0:21:02.200<br>
所以，你會發現說今天過了 166 秒<br>
<br>
0:21:02.460,0:21:05.120<br>
它才算一個 epoch<br>
<br>
0:21:05.120,0:21:06.600<br>
166 秒<br>
<br>
0:21:07.275,0:21:10.295<br>
在下面這個 batch_size 設為 10 這個 case<br>
<br>
0:21:10.300,0:21:12.740<br>
它已經算 10 個 epoch 了<br>
<br>
0:21:12.900,0:21:15.860<br>
幾乎已經算 10 個 epoch 了<br>
<br>
0:21:15.860,0:21:18.480<br>
所以，這樣比較起來，因為<br>
<br>
0:21:18.480,0:21:21.660<br>
它算一個 epoch 要 166 秒<br>
<br>
0:21:21.660,0:21:23.560<br>
同樣時間，它已經算 10 個 epoch 了<br>
<br>
0:21:23.600,0:21:25.080<br>
它一個 epoch update 五萬次<br>
<br>
0:21:25.180,0:21:26.780<br>
它一個 epoch update 五千次<br>
<br>
0:21:26.780,0:21:30.180<br>
但是，在同樣時間，它已經跑 10 個 epoch<br>
<br>
0:21:30.180,0:21:32.020<br>
所以，會變成說<br>
<br>
0:21:32.020,0:21:33.840<br>
參數 update 的數目<br>
<br>
0:21:33.965,0:21:36.615<br>
batch_size 設 1 跟 batch_size 設 10<br>
<br>
0:21:36.715,0:21:38.105<br>
幾乎是一樣的<br>
<br>
0:21:39.175,0:21:40.175<br>
然後，再來呢<br>
<br>
0:21:40.220,0:21:45.640<br>
如果今天幾乎，這兩件事情<br>
<br>
0:21:45.640,0:21:46.700<br>
在同樣時間內<br>
<br>
0:21:46.700,0:21:49.635<br>
參數 update 的數目幾乎是一樣的<br>
<br>
0:21:49.635,0:21:52.615<br>
那你其實會想要選 batch_size  = 10<br>
<br>
0:21:52.620,0:21:55.740<br>
為甚麼？因為如果你選 batch_size = 10 的時候<br>
<br>
0:21:55.920,0:21:58.180<br>
是會比較穩定阿<br>
<br>
0:21:58.180,0:22:02.000<br>
我們之前之所以從 Gradient 換成 Stochastic Gradient<br>
<br>
0:22:02.000,0:22:03.840<br>
目標就是因為這樣比較快<br>
<br>
0:22:04.020,0:22:06.020<br>
你 update 次數比較多<br>
<br>
0:22:06.020,0:22:08.500<br>
可是，如果你現在用  Stochastic Gradient<br>
<br>
0:22:08.500,0:22:10.600<br>
其實也不會比較快<br>
<br>
0:22:10.660,0:22:15.420<br>
那你為甚麼不選一個比較穩定，update 次數比較多的呢<br>
<br>
0:22:15.420,0:22:17.140<br>
所以，這邊你就會選擇<br>
<br>
0:22:17.360,0:22:20.440<br>
batch_size = 10 的 case<br>
<br>
0:22:21.980,0:22:27.300<br>
那接下來有人就會想說<br>
<br>
0:22:28.740,0:22:31.640<br>
接下來我們的下一個問題就是，為甚麼<br>
<br>
0:22:31.965,0:22:34.715<br>
為甚麼 batch_size 設比較大的時候<br>
<br>
0:22:35.355,0:22:36.385<br>
速度會比較快<br>
<br>
0:22:37.285,0:22:40.235<br>
這個就是因為我們使用了平行運算<br>
<br>
0:22:40.235,0:22:43.495<br>
用了 GPU，這個我在下一頁會再更仔細的解釋啦<br>
<br>
0:22:43.500,0:22:46.040<br>
你就先知道說，因為我們用了平行運算<br>
<br>
0:22:46.040,0:22:48.795<br>
所以，這 10 個 example 它是同時運算<br>
<br>
0:22:48.795,0:22:50.840<br>
所以，你算 10 個 example 的時間<br>
<br>
0:22:50.840,0:22:53.700<br>
一個 batch 裡面 10 個 example 的時間<br>
跟算一個 example 的時間<br>
<br>
0:22:53.700,0:22:57.340<br>
其實，是可以幾乎一樣的<br>
<br>
0:22:58.740,0:23:02.100<br>
那你會想說，既然 batch_size 越大<br>
<br>
0:23:02.100,0:23:04.700<br>
那既然 batch_size 越大<br>
<br>
0:23:06.925,0:23:09.565<br>
既然 batch_size 越大<br>
<br>
0:23:09.565,0:23:10.920<br>
它會越穩定<br>
<br>
0:23:10.920,0:23:12.740<br>
batch_size 變大的話<br>
<br>
0:23:12.740,0:23:15.420<br>
你還是可以平行運算，那你為甚麼不把 batch_size<br>
<br>
0:23:15.520,0:23:17.840<br>
開超級大就好了呢？<br>
<br>
0:23:18.140,0:23:20.675<br>
這邊有兩個 claim，一個 claim 就是<br>
<br>
0:23:20.680,0:23:23.880<br>
如果你把 batch_size 開到很大<br>
<br>
0:23:23.880,0:23:26.100<br>
最終，GPU 會沒有辦法平行運算<br>
<br>
0:23:26.100,0:23:28.700<br>
它終究是有它的極限，也就是說<br>
<br>
0:23:28.700,0:23:32.700<br>
它同時考慮 10 個 example 跟 1 個 example 的時間<br>
<br>
0:23:32.700,0:23:36.040<br>
是一樣的，但它同時考慮一萬個 example 的時候<br>
<br>
0:23:36.040,0:23:38.740<br>
它的時間就不會跟 1 個 example 一樣<br>
<br>
0:23:38.740,0:23:42.740<br>
所以，batch_size 考慮到硬體真正的限制的話<br>
<br>
0:23:42.860,0:23:46.260<br>
你也沒有辦法無窮盡的增長<br>
<br>
0:23:46.540,0:23:49.280<br>
那撇開硬體的限制不談<br>
<br>
0:23:49.280,0:23:52.940<br>
另外一個，batch_size 不應該設太大的理由是<br>
<br>
0:23:52.940,0:23:54.580<br>
你其實可以自己試試看<br>
<br>
0:23:54.580,0:23:56.320<br>
如果你把 batch_size 設很大<br>
<br>
0:23:56.320,0:24:00.140<br>
在 train Gradient Descent，在做 deep learning 的時候<br>
<br>
0:24:00.140,0:24:02.295<br>
你跑兩下，你的 network 就卡住了<br>
<br>
0:24:02.300,0:24:05.660<br>
你跑兩下，你就陷到 saddle point<br>
<br>
0:24:05.660,0:24:07.120<br>
或是 local minimum 裡面去了<br>
<br>
0:24:07.120,0:24:11.260<br>
如果那個 neural network 的 error surface 上面<br>
<br>
0:24:11.440,0:24:13.420<br>
它不是一個 convex optimization problem<br>
<br>
0:24:13.620,0:24:15.660<br>
它有很多的坑坑洞洞<br>
<br>
0:24:15.660,0:24:16.940<br>
它有很多坑坑洞洞<br>
<br>
0:24:16.960,0:24:20.200<br>
如果，你今天是 full 的 batch<br>
<br>
0:24:20.200,0:24:22.685<br>
原來的 Gradient Descent 裡面有一些 mini batch<br>
<br>
0:24:22.685,0:24:26.680<br>
那你就是完全順著 total loss 的方向走<br>
<br>
0:24:26.680,0:24:29.320<br>
你可以發現說，你每走兩步，就卡住了<br>
<br>
0:24:29.320,0:24:31.080<br>
這件事情，你可以回去自己試試看<br>
<br>
0:24:31.080,0:24:33.060<br>
你把 batch_size 設成 full batch<br>
<br>
0:24:33.060,0:24:34.860<br>
那如果你的 GPU 跑得動的話呢<br>
<br>
0:24:35.060,0:24:37.280<br>
你還是可以得到它的結果<br>
<br>
0:24:37.280,0:24:39.760<br>
但是你的 performance 就會很差，因為<br>
<br>
0:24:39.760,0:24:41.300<br>
你會發現說你的那個<br>
<br>
0:24:41.520,0:24:43.240<br>
在 training set 上的 loss<br>
<br>
0:24:43.240,0:24:45.560<br>
它跑兩下，整個就卡了<br>
<br>
0:24:45.560,0:24:48.720<br>
你就沒有辦法再 train，它就卡到一個 local minimum<br>
<br>
0:24:48.720,0:24:52.700<br>
local minimum 或是 saddle point 的地方，你就無法再 train<br>
<br>
0:24:52.700,0:24:54.980<br>
但是，如果你用 Stochastic 的好處就是<br>
<br>
0:24:54.980,0:24:56.720<br>
如果你有隨機性<br>
<br>
0:24:56.720,0:25:00.120<br>
每一次你走的方向，會是隨機的<br>
<br>
0:25:00.160,0:25:05.020<br>
所以，如果你今天從某一步陷到 local minimum 裡面去<br>
<br>
0:25:05.020,0:25:10.000<br>
如果那個 local minimum 不是一個很深的 local minimum<br>
<br>
0:25:10.000,0:25:11.880<br>
或是那個 saddle point<br>
<br>
0:25:11.880,0:25:14.460<br>
是一個特別麻煩的 saddle point<br>
<br>
0:25:14.460,0:25:17.940<br>
你只要下一步再加一點 random<br>
<br>
0:25:17.940,0:25:21.220<br>
你就可以跳出那個<br>
<br>
0:25:21.220,0:25:22.835<br>
Gradient 是 0 的區域了<br>
<br>
0:25:22.835,0:25:25.345<br>
所以，如果你沒有這個隨機性的話<br>
<br>
0:25:25.345,0:25:27.740<br>
你 train Neural network 其實是會有問題的<br>
<br>
0:25:27.740,0:25:29.900<br>
你如果沒有這個 mini batch 的隨機性的話<br>
<br>
0:25:29.900,0:25:34.460<br>
你每 update 兩次參數<br>
<br>
0:25:34.460,0:25:38.120<br>
你就會卡住，所以，這個 mini batch 是需要的<br>
<br>
0:25:39.040,0:25:40.720<br>
接下來，我們要解釋說<br>
<br>
0:25:40.720,0:25:44.560<br>
為甚麼當有 batch 的時候<br>
<br>
0:25:44.560,0:25:47.580<br>
GPU 是如何平行地加速<br>
<br>
0:25:47.600,0:25:49.560<br>
那我們剛才有講過說<br>
<br>
0:25:49.560,0:25:53.680<br>
整個 network 你就可以把它看成是一連串的<br>
<br>
0:25:53.720,0:25:56.380<br>
矩陣運算的結果<br>
<br>
0:25:56.380,0:25:59.120<br>
不管是 Forward pass 或是 Backward pass<br>
<br>
0:25:59.120,0:26:01.600<br>
都可以看成是一連串矩陣運算的結果<br>
<br>
0:26:01.600,0:26:03.380<br>
Forward pass 就是我們圖上看到這樣<br>
<br>
0:26:03.380,0:26:06.580<br>
Backward pass 我們前一份投影片有解釋過，就是<br>
<br>
0:26:06.580,0:26:09.320<br>
把整個 network 逆轉<br>
<br>
0:26:09.340,0:26:12.800<br>
然後，把 neuron 變成 op-amp<br>
<br>
0:26:14.440,0:26:17.840<br>
那我們今天就可以比較  Stochastic Gradient Descent<br>
<br>
0:26:17.840,0:26:22.080<br>
也就是 batch_size = 1，還有 batch_size = 10 的差別<br>
<br>
0:26:22.460,0:26:24.400<br>
如果，batch_size = 1<br>
<br>
0:26:24.400,0:26:26.940<br>
我們看第一個 layer，你 input 一個 x<br>
<br>
0:26:27.320,0:26:28.820<br>
然後，你乘上 W^1<br>
<br>
0:26:29.140,0:26:30.400<br>
你就得到 z^1<br>
<br>
0:26:30.400,0:26:32.420<br>
在 Forward pass 的時候，你要做這個計算嘛<br>
<br>
0:26:32.420,0:26:35.520<br>
在 Backward pass 你也會做一個類似的計算<br>
<br>
0:26:35.520,0:26:38.760<br>
在 Forward pass，你就會做這樣一個 matrix operation<br>
<br>
0:26:38.760,0:26:42.700<br>
這是第一筆 data，那你做完這些 matrix 的計算以後<br>
<br>
0:26:42.700,0:26:44.260<br>
你會 update 你的參數<br>
<br>
0:26:44.260,0:26:46.380<br>
接下來，你第二筆預測的 x 進來<br>
<br>
0:26:46.520,0:26:47.960<br>
再乘 W^1<br>
<br>
0:26:47.960,0:26:50.600<br>
再得到另外一個 z^1，再 update 參數<br>
<br>
0:26:51.000,0:26:53.200<br>
但是，在 Mini-batch 的時候<br>
<br>
0:26:53.200,0:26:57.635<br>
你會把同一個 batch 裡面的 input 通通集合起來<br>
<br>
0:26:57.640,0:26:59.420<br>
每一個 input 都是一個 vector 嘛<br>
<br>
0:26:59.420,0:27:01.140<br>
假設我們現在 batch_size 就是 2<br>
<br>
0:27:01.160,0:27:04.120<br>
那你裡面有黃色這個 vector，也有綠色這個 vector<br>
<br>
0:27:04.140,0:27:06.900<br>
那你就把黃色的 vector 和綠色的 vector 拼起來<br>
<br>
0:27:06.900,0:27:08.140<br>
變成一個 matrix<br>
<br>
0:27:08.520,0:27:11.780<br>
再把這個 matrix 乘上 W^1<br>
<br>
0:27:11.780,0:27:15.180<br>
你就可以直接得到 z^1 跟 z^2<br>
<br>
0:27:15.180,0:27:17.895<br>
我們可以把 x 乘上 W^1 得到 z^1<br>
<br>
0:27:17.895,0:27:20.005<br>
把 x 乘上 W^1 得到 z^1<br>
<br>
0:27:20.005,0:27:21.800<br>
這兩件事情分開來做<br>
<br>
0:27:21.800,0:27:24.020<br>
也可以把這兩個東西<br>
<br>
0:27:24.500,0:27:25.620<br>
併在一起<br>
<br>
0:27:26.180,0:27:29.620<br>
再乘上 W^1，直接得到 z^1<br>
<br>
0:27:29.795,0:27:30.680<br>
或者 z^2<br>
<br>
0:27:30.680,0:27:32.765<br>
這兩件事情理論上<br>
<br>
0:27:32.875,0:27:35.835<br>
運算量是一模一樣多的<br>
<br>
0:27:35.840,0:27:38.080<br>
對不對？上面這件事<br>
<br>
0:27:38.600,0:27:40.880<br>
跟下面這件事<br>
<br>
0:27:40.880,0:27:44.120<br>
它在理論上的運算量是一樣多的<br>
<br>
0:27:44.120,0:27:45.920<br>
但是，就實作上<br>
<br>
0:27:45.920,0:27:49.640<br>
你覺得哪一件事情是比較快的呢？<br>
<br>
0:27:50.340,0:27:53.900<br>
你覺得上面比較快的同學舉手一下<br>
<br>
0:27:55.080,0:27:59.060<br>
你覺得下面比較快的同學舉手一下<br>
<br>
0:27:59.200,0:28:01.680<br>
所有同學都覺得下面比較好，手放下<br>
<br>
0:28:01.680,0:28:03.820<br>
沒錯，下面就是比較快的<br>
<br>
0:28:03.820,0:28:07.760<br>
因為，如果你今天讓 GPU 做這個運算<br>
<br>
0:28:07.760,0:28:10.100<br>
和讓 GPU 做這個運算<br>
<br>
0:28:10.100,0:28:12.420<br>
它的時間，其實是一樣的<br>
<br>
0:28:12.420,0:28:18.820<br>
對 GPU 來說，你在矩陣運算裡面相乘的每一個 element<br>
<br>
0:28:18.820,0:28:20.725<br>
都是可以平行運算的<br>
<br>
0:28:20.725,0:28:23.300<br>
所以，今天上面這個運算的時間<br>
<br>
0:28:23.300,0:28:26.680<br>
反而會變成下面這個 GPU 運算的時間的兩倍<br>
<br>
0:28:26.680,0:28:31.400<br>
所以，這就是為甚麼我們用 Mini-batch<br>
<br>
0:28:31.460,0:28:34.920<br>
再加上 GPU 的時候，你是可以加速的<br>
<br>
0:28:35.000,0:28:39.080<br>
但是，如果你今天用 GPU<br>
<br>
0:28:39.080,0:28:42.020<br>
但是，你沒用 Mini-batch 的話<br>
<br>
0:28:42.020,0:28:43.900<br>
你其實就加速不了太多<br>
<br>
0:28:44.040,0:28:47.065<br>
所以，這就是有人買了 GPU<br>
<br>
0:28:47.065,0:28:49.640<br>
有人凹了他的老師買了 GPU 來<br>
<br>
0:28:49.640,0:28:51.260<br>
但他不知道要設 Mini-batch<br>
<br>
0:28:51.260,0:28:53.940<br>
所以，裝了 GPU 以後，也沒變快<br>
<br>
0:28:57.680,0:29:00.260<br>
那 Keras 當然可以 save 和 load model<br>
<br>
0:29:00.260,0:29:01.940<br>
你可以把 train 好的 model 存起來<br>
<br>
0:29:01.940,0:29:04.840<br>
然後以後再用另外一個程式讀出來<br>
<br>
0:29:04.840,0:29:07.340<br>
那它也可以幫你做 testing<br>
<br>
0:29:07.340,0:29:10.320<br>
有兩個 testing 的 case，第一個 case 是這個<br>
<br>
0:29:10.320,0:29:11.715<br>
evaluation<br>
<br>
0:29:11.715,0:29:13.700<br>
也就 evaluation case 是<br>
<br>
0:29:13.700,0:29:15.680<br>
我今天有一組 testing set<br>
<br>
0:29:15.700,0:29:17.960<br>
testing set 的答案我也知道<br>
<br>
0:29:17.960,0:29:20.680<br>
那 Keras 就幫你算說你現在的正確率<br>
<br>
0:29:20.680,0:29:24.375<br>
有多少，那這個 evaluation 有兩個 input<br>
<br>
0:29:24.375,0:29:27.395<br>
就是 testing 的 image 和 testing 的 label<br>
<br>
0:29:27.580,0:29:30.460<br>
那在這 case 2 呢，case 2 要做 predict<br>
<br>
0:29:30.460,0:29:33.820<br>
這個時候，你沒有任何的 label data<br>
<br>
0:29:33.820,0:29:35.840<br>
你只有 image，就是你真的 train 好這個 model<br>
<br>
0:29:35.840,0:29:37.440<br>
你要放到網路上讓人家用<br>
<br>
0:29:37.440,0:29:40.600<br>
別人會輸入一個 image，然後你就告訴他你的<br>
<br>
0:29:40.600,0:29:42.400<br>
分類好的數字是多少<br>
<br>
0:29:42.540,0:29:45.695<br>
這個時候，你 input 就只有 x，就只有 image<br>
<br>
0:29:45.700,0:29:48.340<br>
output 就直接是分類的結果<br>
<br>
0:29:48.340,0:29:51.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
