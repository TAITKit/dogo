<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:01.240,0:00:09.980<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:20.820,0:00:22.480<br>
好，各位同學大家早<br>
<br>
0:00:22.480,0:00:24.840<br>
那我們就開始上課吧！<br>
<br>
0:00:26.500,0:00:30.260<br>
今天的第一堂課，我們要講的是 Gradient Descent<br>
<br>
0:00:30.260,0:00:34.580<br>
Gradient Descent 我們上次已經大概講過怎麼做了<br>
<br>
0:00:34.580,0:00:38.220<br>
但是有一些小技巧呢，你可能是不知道的<br>
<br>
0:00:38.220,0:00:43.720<br>
所以我們要再詳細說明一下，Gradient Descent<br>
你要怎麼把它做得更好<br>
<br>
0:00:44.880,0:00:47.120<br>
好，那我們上次是這樣說的<br>
<br>
0:00:48.020,0:00:51.180<br>
在整個 machine learning 的第3個步驟<br>
<br>
0:00:51.180,0:00:54.340<br>
我們要找一個最好的 function<br>
<br>
0:00:54.340,0:00:56.420<br>
那找一個最好的 function 這件事呢<br>
<br>
0:00:56.420,0:00:59.700<br>
是要解一個 optimization 的 problem<br>
<br>
0:00:59.700,0:01:04.180<br>
也就是說，在第二步，<br>
我們先定了一個 loss function： L<br>
<br>
0:01:04.180,0:01:08.280<br>
這個 loss function 呢，它是一個 functional unction<br>
<br>
0:01:08.280,0:01:11.820<br>
你把一個 function 代到這個 loss function 裡面<br>
<br>
0:01:11.820,0:01:16.800<br>
或者是你把一個操控 function 形狀的參數<br>
<br>
0:01:16.800,0:01:21.780<br>
我們現在，在這張投影片裡面，把那些參數寫成 θ<br>
<br>
0:01:21.780,0:01:24.840<br>
L 是 loss function，θ 是那些參數<br>
<br>
0:01:24.840,0:01:28.640<br>
你把一組參數代到一個 loss function 裡面<br>
<br>
0:01:28.640,0:01:32.160<br>
它就會告訴你說，這組參數有多不好<br>
<br>
0:01:32.160,0:01:36.020<br>
那你接下來，要做的事情呢，就是<br>
<br>
0:01:36.880,0:01:41.040<br>
從一開始，我就發現這張投影片，有一個地方寫錯了<br>
<br>
0:01:41.780,0:01:44.580<br>
這裡應該是 minimum 啦<br>
<br>
0:01:44.580,0:01:46.560<br>
因為它是 loss function 嘛！<br>
<br>
0:01:46.560,0:01:49.320<br>
那 loss function 我們是希望它越小越好<br>
<br>
0:01:49.320,0:01:52.940<br>
當然你也可以反過來定義一個 function 是<br>
<br>
0:01:53.640,0:01:57.260<br>
參數越好，它 output 的值越大<br>
<br>
0:01:57.260,0:02:00.320<br>
那這時候就不會叫它 loss function，就會叫他別的名字<br>
<br>
0:02:00.320,0:02:02.880<br>
比如說，叫它 ****** function<br>
<br>
0:02:02.880,0:02:04.440<br>
好，這邊應該是 minimum<br>
<br>
0:02:04.440,0:02:10.480<br>
我們要找一組參數 θ，讓這個  loss function 越小越好<br>
<br>
0:02:10.780,0:02:12.300<br>
那這件事情怎麼做呢？<br>
<br>
0:02:12.300,0:02:14.260<br>
我們可以用  Gradient Descent<br>
<br>
0:02:14.260,0:02:19.240<br>
假設現在這個 θ 是一個參數的 set<br>
<br>
0:02:19.240,0:02:22.360<br>
那裡面有兩個參數：θ1 跟 θ2<br>
<br>
0:02:22.360,0:02:27.320<br>
首先，你先隨機的選取一個起始的點<br>
<br>
0:02:27.320,0:02:29.900<br>
隨機選取一組起始的參數<br>
<br>
0:02:29.900,0:02:34.880<br>
這邊寫成 θ(上標0, 下標1)、θ(上標0, 下標2)<br>
<br>
0:02:34.880,0:02:38.560<br>
用上標 0 來代表說，它是初始的那一組參數<br>
<br>
0:02:38.560,0:02:40.180<br>
那用下標代表說<br>
<br>
0:02:40.180,0:02:44.160<br>
這個是這一組參數裡面的<br>
第 1 個 component 跟第 2 個 component<br>
<br>
0:02:44.160,0:02:50.140<br>
接下來，你計算 θ(上標0, 下標1) 跟 θ(上標0, 下標2)<br>
<br>
0:02:50.140,0:02:53.660<br>
對這個 loss function 的偏微分<br>
<br>
0:02:53.660,0:02:57.880<br>
計算他們的偏微分，然後把這個<br>
<br>
0:02:57.880,0:03:02.860<br>
θ(上標0, 下標1)、θ(上標0, 下標2) 減掉 learning rate<br>
<br>
0:03:02.860,0:03:07.320<br>
乘上這個偏微分的值，得到一組新的參數<br>
<br>
0:03:07.320,0:03:12.640<br>
這個參數，這邊寫做<br>
 θ(上標1, 下標1) 跟 θ(上標1, 下標2)<br>
<br>
0:03:12.640,0:03:15.760<br>
θ 上標1，代表說它是在第二個時間點<br>
<br>
0:03:15.760,0:03:18.840<br>
由 θ 上標0 update 以後得到的參數<br>
<br>
0:03:18.840,0:03:21.700<br>
下標代表說，它有兩個 component<br>
<br>
0:03:21.700,0:03:24.880<br>
那同樣的步驟，你就反覆不斷地進行<br>
<br>
0:03:24.880,0:03:30.260<br>
接下來你有 θ(上標1, 下標1) 跟 θ(上標1, 下標2) 以後呢<br>
<br>
0:03:30.260,0:03:32.360<br>
你就一樣去計算它們的偏微分<br>
<br>
0:03:32.360,0:03:33.940<br>
再乘上 learning rate，<br>
<br>
0:03:33.940,0:03:40.660<br>
再去減 θ(上標1, 下標1) 跟 θ(上標1, 下標2)<br>
<br>
0:03:40.660,0:03:43.400<br>
得到下一組參數，每個反覆進行這個 process<br>
<br>
0:03:43.400,0:03:45.200<br>
這個就是 Gradient Descent<br>
<br>
0:03:45.200,0:03:47.900<br>
那如果你想要也得更簡潔一點的話呢<br>
<br>
0:03:47.900,0:03:49.040<br>
其實你可以這樣寫<br>
<br>
0:03:49.040,0:03:54.220<br>
假設你現在有兩個參數：θ1 跟 θ2<br>
<br>
0:03:54.220,0:03:59.940<br>
這個應該是 L，不好意思，<br>
這個 C 呢，這邊應該是寫成 L<br>
<br>
0:03:59.940,0:04:04.840<br>
你把這個 θ1 跟 θ2 對這個 loss function 做偏微分<br>
<br>
0:04:04.840,0:04:09.320<br>
你把這兩個偏微分得到的值串在一起，<br>
變成一個 vector 以後<br>
<br>
0:04:09.320,0:04:12.520<br>
那這個 vector 呢，就叫做 Gradient<br>
<br>
0:04:12.520,0:04:15.940<br>
你把 L 前面，加一個倒三角形<br>
<br>
0:04:15.940,0:04:18.000<br>
這個東西呢，就叫做 Gradient<br>
<br>
0:04:18.000,0:04:19.880<br>
那它其實是一個 vector<br>
<br>
0:04:19.880,0:04:22.580<br>
所以你可以把這個式子寫成<br>
<br>
0:04:22.580,0:04:26.620<br>
你把這個東西，寫成一個 vector，θ 上標 1<br>
<br>
0:04:26.620,0:04:29.540<br>
這個東西，寫成一個 vector，θ 上標 0<br>
<br>
0:04:29.540,0:04:32.660<br>
這一項就是這一項<br>
<br>
0:04:32.660,0:04:37.600<br>
那你就寫成呢，L 在 (θ上標0) 這個地方的 Gradient<br>
<br>
0:04:37.600,0:04:40.440<br>
所以你可以把 update 參數呢<br>
<br>
0:04:40.440,0:04:45.260<br>
簡單寫成 (θ上標0) 減掉 learning rate<br>
<br>
0:04:45.260,0:04:47.420<br>
乘上 Gradient，就等於(θ上標1)<br>
<br>
0:04:47.420,0:04:51.780<br>
同理，(θ上標1) 減掉 learning rate 乘上 Gradient，<br>
就得到 (θ上標2)<br>
<br>
0:04:51.780,0:04:54.260<br>
如果把它 visualize 的話呢<br>
<br>
0:04:54.260,0:04:56.500<br>
它看起來像是這個樣子<br>
<br>
0:04:56.500,0:04:59.060<br>
假設我們現在有兩個參數，θ1、θ2<br>
<br>
0:04:59.060,0:05:02.720<br>
那你隨機的選一個初始的位置，θ^0<br>
<br>
0:05:03.180,0:05:07.500<br>
然後接下來呢，你計算在 θ^0 這個點<br>
<br>
0:05:07.500,0:05:13.780<br>
這個參數對 loss function 的 Gradient<br>
<br>
0:05:13.780,0:05:17.240<br>
假設參數對  loss function 的 Gradient<br>
是這個紅色的箭頭<br>
<br>
0:05:17.240,0:05:20.780<br>
Gradient 就是一個 vector，它是一個紅色的箭頭<br>
<br>
0:05:21.140,0:05:24.600<br>
那，如果你不知道 Gradient 是甚麼的話呢<br>
<br>
0:05:24.600,0:05:29.180<br>
你就想成，它是等高線的法線方向<br>
<br>
0:05:29.180,0:05:32.420<br>
這個箭頭，它指的方向就是<br>
<br>
0:05:32.420,0:05:34.940<br>
如果你把 loss 的等高線畫出來的話呢<br>
<br>
0:05:34.940,0:05:39.020<br>
這個箭頭指的方向，就是等高線的法線方向<br>
<br>
0:05:39.660,0:05:42.020<br>
那怎麼 update 參數呢？<br>
<br>
0:05:42.020,0:05:46.880<br>
你就把這個 Gradient 乘上 learning rate<br>
<br>
0:05:46.880,0:05:48.720<br>
然後再取一個負號<br>
<br>
0:05:48.720,0:05:51.060<br>
Gradient 乘上 learning rate，然後再取一個負號<br>
<br>
0:05:51.060,0:05:52.580<br>
就是這個藍色的箭頭<br>
<br>
0:05:52.580,0:05:56.000<br>
再把它加上 θ^0 ，就得到 θ ^1<br>
<br>
0:05:56.000,0:05:58.540<br>
那這個步驟就反覆地持續進行下去<br>
<br>
0:05:58.540,0:06:02.100<br>
再計算一遍  Gradient<br>
<br>
0:06:02.100,0:06:04.240<br>
你得到另外一個紅色的箭頭<br>
<br>
0:06:04.240,0:06:06.680<br>
紅色箭頭指向這個地方<br>
<br>
0:06:06.680,0:06:11.840<br>
那你現在走的方向呢，就變成是紅色的箭頭的相反<br>
<br>
0:06:11.840,0:06:16.160<br>
紅色箭頭乘上一個負號，再乘上 learning rate<br>
<br>
0:06:16.160,0:06:19.720<br>
就是現在這個藍色的箭頭 ，得到 θ^2<br>
<br>
0:06:19.720,0:06:21.880<br>
這個步驟就反覆一直進行下去<br>
<br>
0:06:21.880,0:06:25.220<br>
再算一下在 θ^2 這個地方的 Gradient<br>
<br>
0:06:25.220,0:06:27.140<br>
然後再決定要走的方向<br>
<br>
0:06:27.140,0:06:29.380<br>
再算一次 Gradient，再決定要走的方向<br>
<br>
0:06:29.380,0:06:31.320<br>
這個就是 Gradient Descent<br>
<br>
0:06:31.320,0:06:33.920<br>
這些，我們上次其實都講過了<br>
<br>
0:06:33.920,0:06:36.900<br>
接下來呢，要講一些 Gradient Descent 的 tip<br>
<br>
0:06:36.960,0:06:38.440<br>
首先，第一件事情就是<br>
<br>
0:06:38.440,0:06:42.500<br>
你要小心地調你的 learning rate<br>
<br>
0:06:43.020,0:06:46.220<br>
如果你已經開始做作業一的話呢 ，你會知道說<br>
<br>
0:06:46.220,0:06:50.520<br>
有時候，learning rate 是可以給你造成一些問題的<br>
<br>
0:06:51.320,0:06:56.460<br>
舉例來說，假設這個是我們的 loss function 的 surface，<br>
假設長這樣子<br>
<br>
0:06:56.460,0:06:59.300<br>
如果你今天 learning rate 調剛剛好的話<br>
<br>
0:06:59.300,0:07:03.380<br>
你從左邊這邊開始，那你可能就是順著紅色的箭頭<br>
<br>
0:07:03.380,0:07:05.720<br>
很順利地走到了最低點<br>
<br>
0:07:06.660,0:07:09.400<br>
可是，如果你今天 learning rate 調的太小的話<br>
<br>
0:07:09.400,0:07:12.620<br>
那它走的速度呢，會變得非常慢<br>
<br>
0:07:12.620,0:07:18.200<br>
雖然只要給它夠多的時間，它終究會<br>
走到這個 local minimum 的地方<br>
<br>
0:07:18.200,0:07:19.980<br>
但是如果它走得太慢的話呢<br>
<br>
0:07:19.980,0:07:22.080<br>
你會沒有辦法接受這件事情<br>
<br>
0:07:22.700,0:07:24.400<br>
你可能會來不及交作業<br>
<br>
0:07:24.400,0:07:27.400<br>
如果你今天這個 learning rate 調得稍微大一點<br>
<br>
0:07:27.400,0:07:29.160<br>
比如說，像綠色這個箭頭的話<br>
<br>
0:07:29.160,0:07:32.940<br>
那就變成說，它的步伐太大了<br>
<br>
0:07:32.940,0:07:34.980<br>
它變得像一個巨人一樣，步伐太大了<br>
<br>
0:07:34.980,0:07:38.020<br>
它永遠沒有辦法走到這個特別低的地方<br>
<br>
0:07:38.020,0:07:41.480<br>
它都在這個山谷的口上面震盪<br>
<br>
0:07:41.480,0:07:43.620<br>
它永遠走不下去，那甚至如果<br>
<br>
0:07:43.620,0:07:45.800<br>
你今天真的把 learning rate 調太大的話<br>
<br>
0:07:45.800,0:07:48.640<br>
它可能就，一瞬間就飛出去了<br>
<br>
0:07:48.640,0:07:50.700<br>
結果你 update 參數以後<br>
<br>
0:07:50.700,0:07:53.060<br>
loss 反而越 update，越大<br>
<br>
0:07:53.440,0:07:58.660<br>
那其實只有在你的參數是一維或二維的時候<br>
<br>
0:07:58.660,0:08:01.980<br>
你才能夠把這樣子的圖 visualize 出來<br>
<br>
0:08:01.980,0:08:04.800<br>
如果你今天是有很多維參數<br>
<br>
0:08:04.800,0:08:07.740<br>
這個 error 的 surface，在這個高維的空間裡面<br>
<br>
0:08:07.740,0:08:09.540<br>
你是沒有辦法 visualize 它的<br>
<br>
0:08:09.540,0:08:12.520<br>
但是，有另外一個東西，你是可以 visualize 的<br>
<br>
0:08:12.520,0:08:13.840<br>
什麼東西呢？<br>
<br>
0:08:13.840,0:08:21.800<br>
你可以 visualize 參數的變化對這個 loss 的變化<br>
<br>
0:08:21.800,0:08:25.060<br>
你可以 visualize  每次參數 update 的時候<br>
<br>
0:08:25.060,0:08:27.540<br>
loss 的改變的情形<br>
<br>
0:08:27.540,0:08:31.540<br>
所以，今天如果你 learning rate 設得太小的話<br>
<br>
0:08:31.540,0:08:36.220<br>
你就會發現說，這個 loss 它下降地非常非常慢<br>
<br>
0:08:36.220,0:08:40.000<br>
今天如果你 learning rate 調得太大的話<br>
<br>
0:08:40.000,0:08:43.660<br>
你在左邊這個圖會看到說 loss 先快速地下降<br>
<br>
0:08:43.660,0:08:45.920<br>
接下來呢，它就卡住了<br>
<br>
0:08:45.920,0:08:47.860<br>
所以，如果你 learning rate 調得太大的話<br>
<br>
0:08:47.860,0:08:53.580<br>
你把參數的 update 對 loss 的變化，<br>
做出來會看到的是綠色這條線<br>
<br>
0:08:53.580,0:08:58.140<br>
你的 loss 很快地下降，但很快地卡住了，<br>
很快地不再下降<br>
<br>
0:08:58.140,0:09:01.780<br>
那如果你今天 learning rate 是調得太大，你就會發現<br>
<br>
0:09:01.780,0:09:07.180<br>
你的 loss 就飛出去了，你需要調整它，讓它調到剛剛好<br>
<br>
0:09:07.180,0:09:09.460<br>
那你才能夠得到一個好的結果<br>
<br>
0:09:09.460,0:09:11.380<br>
所以你在做 Gradient Descent 的時候<br>
<br>
0:09:12.120,0:09:15.960<br>
你應該要把這個圖畫出來<br>
<br>
0:09:15.960,0:09:18.580<br>
沒有把這個圖畫出來，你就會非常非常地卡<br>
<br>
0:09:18.580,0:09:23.880<br>
有人就說，它就把 Gradient 的程式寫好<br>
<br>
0:09:23.880,0:09:26.820<br>
那寫好放下去之後開始跑，他去打一場 LOL<br>
<br>
0:09:26.820,0:09:30.460<br>
然後，打完回來就發現說：結果爛掉啦<br>
<br>
0:09:30.460,0:09:32.520<br>
然後，他也不知道爛在哪裡這樣子<br>
<br>
0:09:32.520,0:09:34.260<br>
所以如果你在做 Gradient Descent 的時候<br>
<br>
0:09:34.260,0:09:35.680<br>
你應該把這個圖畫出來<br>
<br>
0:09:35.680,0:09:39.960<br>
然後你要先看一下，它前幾次 update 參數的時候<br>
<br>
0:09:39.960,0:09:42.620<br>
它 update 的走法是怎麼樣<br>
<br>
0:09:42.620,0:09:47.060<br>
搞不好它，你 learning rate 調太大，它一下子就爆炸了<br>
<br>
0:09:47.060,0:09:49.420<br>
所以這個時候，你就知道你要趕快調 learning rate<br>
<br>
0:09:49.420,0:09:51.920<br>
你要確定它呢，是穩定地下降<br>
<br>
0:09:51.920,0:09:53.660<br>
才能去打 LOL 這樣子<br>
<br>
0:09:54.580,0:09:58.680<br>
好，但是要調 learning rate 很麻煩<br>
<br>
0:09:58.680,0:10:01.560<br>
有沒有辦法自動地調 learning rate 呢？<br>
<br>
0:10:01.560,0:10:04.640<br>
有一些自動的方法可以幫我們調 learning rate<br>
<br>
0:10:04.820,0:10:07.940<br>
最基本而簡單的大原則是<br>
<br>
0:10:07.940,0:10:15.640<br>
通常 learning rate 是隨著參數的 update 會越來越小的<br>
<br>
0:10:15.640,0:10:18.140<br>
為甚麼會這樣呢？<br>
<br>
0:10:18.140,0:10:21.180<br>
因為當你在剛開始的起始點的時候<br>
<br>
0:10:21.180,0:10:25.780<br>
它通常離最低點，是比較遠的<br>
<br>
0:10:25.780,0:10:28.480<br>
所以你步伐呢，要踏大一點<br>
<br>
0:10:28.480,0:10:31.400<br>
就是走得快一點，才能夠趕快走到最低點<br>
<br>
0:10:31.640,0:10:36.500<br>
但是，經過好幾次的參數 update 以後呢？<br>
<br>
0:10:36.500,0:10:39.480<br>
你已經比較靠近你的目標了<br>
<br>
0:10:39.480,0:10:42.860<br>
所以這個時候呢，你就應該減少你的 learning rate<br>
<br>
0:10:42.860,0:10:47.020<br>
這樣它能夠收斂在你最低點的地方<br>
<br>
0:10:47.020,0:10:49.480<br>
舉例來說，你 learning rate 的設法可能是這樣<br>
<br>
0:10:49.480,0:10:52.020<br>
好，你可以設成說<br>
<br>
0:10:53.020,0:10:58.400<br>
這個 learning rate  是一個 t dependent 的函數<br>
<br>
0:10:58.400,0:11:02.720<br>
它是 depend on 你現在參數 update 的次數<br>
<br>
0:11:02.720,0:11:04.980<br>
在第 t 次 update 參數的時候<br>
<br>
0:11:04.980,0:11:10.340<br>
你就把你的 learning rate <br>
設成一個 constant η，除以 sqrt (t+1)<br>
<br>
0:11:10.340,0:11:13.020<br>
這樣當你參數 update 的次數越多的時候呢<br>
<br>
0:11:13.020,0:11:16.060<br>
這個 learning rate 就會越來越小<br>
<br>
0:11:16.380,0:11:18.760<br>
但是光這樣呢，是不夠的<br>
<br>
0:11:19.240,0:11:22.280<br>
你到這邊，我們需要因材施教<br>
<br>
0:11:22.280,0:11:25.360<br>
所以這每一個不同的參數<br>
<br>
0:11:25.360,0:11:28.380<br>
最好的狀況應該是，每一個不同的參數<br>
<br>
0:11:28.380,0:11:31.840<br>
都給它不同的 learning rate<br>
<br>
0:11:31.840,0:11:35.960<br>
這件事情呢，是有很多的小技巧的<br>
<br>
0:11:35.960,0:11:40.540<br>
其中，我覺得最簡單，最容易實作的，叫做 Adagrad<br>
<br>
0:11:41.160,0:11:43.540<br>
那 Adagrad 是這樣子的<br>
<br>
0:11:43.760,0:11:47.300<br>
他說，每一個參數的 learning rate 呢<br>
<br>
0:11:47.300,0:11:55.440<br>
都把它除上，之前算出來的微分值的 root mean square<br>
<br>
0:11:55.440,0:11:58.620<br>
什麼意思呢，我們原來的 Gradient Descent 是這樣<br>
<br>
0:11:58.620,0:12:01.660<br>
假設 w 是某一個參數<br>
<br>
0:12:01.660,0:12:05.480<br>
這個時候 w 不是一組參數，我們現在只考慮一個參數<br>
<br>
0:12:05.480,0:12:08.180<br>
因為我們現在，在做 Adagrad 這個做法的時候<br>
<br>
0:12:08.180,0:12:09.980<br>
它是 adaptive 的 learning rate<br>
<br>
0:12:09.980,0:12:17.160<br>
所以，每一個參數，它都有不同的 learning rate<br>
<br>
0:12:17.160,0:12:22.260<br>
所以呢，我們現在要把每一個參數都分開來考慮<br>
<br>
0:12:22.260,0:12:24.080<br>
每一個參數都分開來考慮<br>
<br>
0:12:24.080,0:12:27.260<br>
那 w 呢，是某一個參數<br>
<br>
0:12:27.260,0:12:32.580<br>
那 w 的 learning rate，在一般的 Gradient Descent 呢<br>
<br>
0:12:33.940,0:12:37.340<br>
depend on 時間的值，比如說 η^t<br>
<br>
0:12:37.340,0:12:40.840<br>
但是你可以把這件事情呢，做的更好<br>
<br>
0:12:40.840,0:12:46.620<br>
在 Adagrad 裡面呢， 你把這個 η^t / σ^t<br>
<br>
0:12:47.340,0:12:52.900<br>
這個 σ^t 是甚麼呢？ 這個 σ^t 是過去<br>
<br>
0:12:53.900,0:12:58.940<br>
這邊這個 g 呢，是這個偏微分的值，g 是偏微分的值<br>
<br>
0:12:58.940,0:13:04.860<br>
這個 σ^t 呢，是過去所有微分的值的 root mean square<br>
<br>
0:13:04.880,0:13:06.860<br>
是過去所有微分的值的 root mean square<br>
<br>
0:13:06.860,0:13:11.640<br>
這個值，對每一個參數而言，都是不一樣的<br>
<br>
0:13:11.640,0:13:14.280<br>
所以現在就會變成說，不同的參數<br>
<br>
0:13:14.280,0:13:16.860<br>
它的 learning rate 都是不一樣的<br>
<br>
0:13:17.120,0:13:22.300<br>
那我們實際舉個例子，來看看這件事情是怎麼實作的<br>
<br>
0:13:22.440,0:13:25.780<br>
假設你現在初始的值，是 w^0<br>
<br>
0:13:26.080,0:13:28.420<br>
假設你初始的值，是 w^0<br>
<br>
0:13:28.420,0:13:31.520<br>
那接下來呢，你就計算在 w^0 那點的微分<br>
<br>
0:13:31.520,0:13:33.760<br>
這邊呢，寫作 g^0<br>
<br>
0:13:34.420,0:13:37.160<br>
然後，它的 learning rate 是多少呢？<br>
<br>
0:13:37.160,0:13:42.380<br>
它的 learning rate 是  η^0 / σ^0<br>
<br>
0:13:42.380,0:13:46.160<br>
η^0 是一個時間 dependent 的參數 ，那 σ^0 是甚麼呢？<br>
<br>
0:13:46.160,0:13:48.960<br>
σ^0 是一個參數 dependent 的參數<br>
<br>
0:13:48.960,0:13:54.640<br>
σ^0 它是過去算過所有微分值的 root mean square<br>
<br>
0:13:54.640,0:13:57.980<br>
那在這個 case 裡面，我們過去只算過<br>
一個微分值，就是 g^0<br>
<br>
0:13:58.280,0:14:01.440<br>
所以這個 σ^0，就是 g^0 的平方再開根號<br>
<br>
0:14:02.360,0:14:04.760<br>
那接下來呢，你再 update 參數<br>
<br>
0:14:04.760,0:14:09.560<br>
你把這個 w^0 更新變成 w^1<br>
<br>
0:14:10.020,0:14:12.620<br>
在 w^1 這個地方，你再算一次 Gradient，就是 g^1<br>
<br>
0:14:13.040,0:14:15.400<br>
那 g^1 的 learning rate 應該乘上多少呢？<br>
<br>
0:14:15.400,0:14:19.640<br>
它要乘 η^1 / σ^1<br>
<br>
0:14:19.640,0:14:24.940<br>
那 σ^1，它是過去所有微分值的 root mean square<br>
<br>
0:14:24.940,0:14:27.180<br>
過去我們已經算過兩次微分值<br>
<br>
0:14:27.180,0:14:29.740<br>
一次是 g^0，一次是 g^1<br>
<br>
0:14:29.740,0:14:34.580<br>
所以 σ^1，就變成 g^0 跟 g^1 的 root mean square<br>
<br>
0:14:34.580,0:14:40.580<br>
也就是把 g^0 平方再加 g^1 平方，<br>
再取平均值 ，再開根號<br>
<br>
0:14:41.440,0:14:46.740<br>
那 w^3 一樣，你就是 update 參數就得到 w^2<br>
<br>
0:14:46.740,0:14:49.640<br>
你有了 w^2 以後，你可以算 g^2，<br>
<br>
0:14:49.640,0:14:51.960<br>
在 w^2 地方的微分值就是 g^2<br>
<br>
0:14:51.960,0:14:55.300<br>
它的 learning rate 就是 η^2 / σ^2<br>
<br>
0:14:55.300,0:14:58.980<br>
那這個 σ^2呢 ，就是過去<br>
算出來所有微分值的 root mean square<br>
<br>
0:14:58.980,0:15:02.800<br>
過去算出 g^0, g^1, g^2，你就把 g^0, g^1, g^2 都平方<br>
<br>
0:15:02.800,0:15:05.320<br>
再平均，然後再開根號<br>
<br>
0:15:05.320,0:15:09.100<br>
得到σ^2，然後把它放在這邊，搭配參數得到 w^3<br>
<br>
0:15:09.100,0:15:11.620<br>
這個步驟呢，就反覆地一直繼續下去<br>
<br>
0:15:12.300,0:15:15.960<br>
到第 t 次 update 參數的時候<br>
<br>
0:15:15.960,0:15:18.460<br>
你有一個微分直 g^t<br>
<br>
0:15:18.880,0:15:23.960<br>
那這個 g^t 的 learning rate 就是 η^t / σ^t<br>
<br>
0:15:23.960,0:15:27.280<br>
這個 σ^t，是過去所有微分的值的 root mean square<br>
<br>
0:15:27.280,0:15:31.520<br>
過去已經算出 g^0, g^1, g^2 .... 一直到 g^t<br>
<br>
0:15:31.520,0:15:34.280<br>
你就把 g^0, g^1, g^2 .... 一直到 g^t<br>
<br>
0:15:34.280,0:15:37.620<br>
都取平方，再加起來，再平均，再開根號<br>
<br>
0:15:37.620,0:15:40.720<br>
得到 σ^t，就把它放在這邊<br>
<br>
0:15:41.580,0:15:46.660<br>
所以，現在如果我們用 Adagrad 的時候呢<br>
<br>
0:15:46.660,0:15:50.580<br>
它 update 參數的式子，寫成這樣子<br>
<br>
0:15:50.580,0:15:54.540<br>
這個 σ^t，我們就寫成這樣子<br>
<br>
0:15:54.540,0:15:57.800<br>
這個 root mean square，我們在前頁投影片已經看到的<br>
<br>
0:15:58.440,0:16:02.040<br>
那這個 time dependent 的 learning rate 呢？<br>
<br>
0:16:02.040,0:16:06.900<br>
這個 time dependent 的 learning rate，<br>
你可以寫成 η / sqrt(t+1)<br>
<br>
0:16:07.500,0:16:11.240<br>
那你可以發現說，當你把這一項除以這一項的時候<br>
<br>
0:16:11.240,0:16:13.580<br>
因為他們都有：根號 (t+1)<br>
<br>
0:16:13.580,0:16:17.120<br>
所以根號 (t+1) 是可以刪掉的<br>
<br>
0:16:17.120,0:16:21.260<br>
所以整個 Adagrad 的式子，你就可以寫成<br>
<br>
0:16:21.260,0:16:25.100<br>
它的 learning rate，你就可以寫成，一個 constant η<br>
<br>
0:16:25.660,0:16:33.080<br>
除掉 根號(過去所有 gradient 的平方和)<br>
<br>
0:16:33.080,0:16:35.280<br>
但不用算平均這樣<br>
<br>
0:16:35.420,0:16:42.600<br>
因為平均這件事情，會跟上面 <br>
time dependent 的 learning rate 抵銷掉<br>
<br>
0:16:42.600,0:16:46.300<br>
所以你在寫 Adagrad 的式子的時候，你可以簡化成<br>
<br>
0:16:46.300,0:16:50.660<br>
不需要把 time dependent 的這件事情 explicitly 的寫出來<br>
<br>
0:16:50.660,0:16:52.960<br>
你就直接把你的 learning rate 寫成<br>
<br>
0:16:52.960,0:16:58.380<br>
η 除以 根號(過去算出來的 gradient 的平方)<br>
<br>
0:16:58.380,0:17:00.700<br>
再開根號就好<br>
<br>
0:17:02.120,0:17:05.520<br>
這個方法，你可以接受嗎？<br>
<br>
0:17:05.920,0:17:09.160<br>
講到這邊，大家有問題嗎？來，請說<br>
<br>
0:17:09.160,0:17:14.240<br>
在做 Adagrad 的時候，它在後面下降的速度，<br>
慢到令人髮指<br>
<br>
0:17:14.240,0:17:16.700<br>
慢到令人髮指是嗎？<br>
<br>
0:17:16.700,0:17:20.800<br>
就是看的過1000筆資料，才降不到0.1<br>
<br>
0:17:20.800,0:17:23.540<br>
這是正常的嗎？<br>
<br>
0:17:23.540,0:17:25.540<br>
這其實是正常的<br>
<br>
0:17:25.540,0:17:32.120<br>
就是說 Adagrad 它的參數 update，其實<br>
整體而言是會是越來越慢的<br>
<br>
0:17:32.140,0:17:34.660<br>
因為它有加上 time depend<br>
<br>
0:17:34.660,0:17:37.820<br>
如果你不喜歡這個結果的話<br>
<br>
0:17:37.820,0:17:41.020<br>
有很多比這個更強的方法<br>
<br>
0:17:41.020,0:17:44.480<br>
這個 adaptive 的 learning rate 其實是一系列的方法<br>
<br>
0:17:44.480,0:17:47.180<br>
今天講 Adagrad 其實是裡面最簡單的<br>
<br>
0:17:47.180,0:17:50.380<br>
還有很多其他的，他們都是用 Ada- 開頭這樣子<br>
<br>
0:17:50.380,0:17:53.240<br>
比如說 Adadelta, Adam 阿，甚麼之類的<br>
<br>
0:17:53.240,0:17:55.360<br>
所以，如果你用別的方法<br>
<br>
0:17:55.360,0:18:01.620<br>
比如說，Adam 的話，它就比較不會有這個情形這樣子<br>
<br>
0:18:03.280,0:18:07.660<br>
其實，如果你沒有甚麼特別偏好的話<br>
<br>
0:18:07.660,0:18:11.900<br>
你現在可以用 Adam 啦，它應該是現在我覺得最穩定的<br>
<br>
0:18:11.900,0:18:15.200<br>
但是它 implement 比較複雜就是了<br>
<br>
0:18:15.200,0:18:17.800<br>
但其實也沒有什麼啦，好，講到這邊大家有甚麼問題嗎？<br>
<br>
0:18:21.060,0:18:24.300<br>
好，那我其實有一個問題啦<br>
<br>
0:18:24.300,0:18:28.020<br>
我其實有一個問題<br>
<br>
0:18:28.500,0:18:30.880<br>
我們在做一般的 Gradient Descent 的時候<br>
<br>
0:18:30.880,0:18:34.460<br>
我們這個參數的 update 取決於兩件事情<br>
<br>
0:18:34.460,0:18:37.940<br>
一件事情是 learning rate，另外一件事情是 Gradient<br>
<br>
0:18:38.040,0:18:43.340<br>
我們的意思就是說 Gradient 越大，你參數 update 就越快<br>
<br>
0:18:43.340,0:18:45.600<br>
斜率算出來越大，參數 update 就越快<br>
<br>
0:18:45.600,0:18:48.160<br>
我相信你可以接受這件事情<br>
<br>
0:18:48.580,0:18:52.140<br>
但是在 Adagrad 裡面，你不覺得相當矛盾嗎？<br>
<br>
0:18:52.140,0:18:54.340<br>
你不覺得有某些怪怪的地方<br>
<br>
0:18:54.780,0:19:01.060<br>
這一項告訴我們，微分的值越大，你參數 update 越快<br>
<br>
0:19:01.060,0:19:04.620<br>
但是這一項它是相反的，對不對？<br>
<br>
0:19:04.620,0:19:16.700<br>
當這一項跟這一項，它們卻是不一樣<br>
<br>
0:19:16.700,0:19:23.040<br>
對不對，你有沒有覺得說，這邊有一些奇怪的地方<br>
<br>
0:19:24.860,0:19:29.280<br>
也就是說，今天當你的 Gradient 越大的時候<br>
<br>
0:19:29.280,0:19:34.500<br>
當 Gradient 越大的時候，你底下算出來的這項就越大<br>
<br>
0:19:34.500,0:19:40.600<br>
你底下算出來的這項越大，<br>
你的參數 update 的步伐就越小<br>
<br>
0:19:40.600,0:19:45.080<br>
這不就跟我們原來要做的事情是有所衝突的嗎？<br>
<br>
0:19:45.080,0:19:46.800<br>
在分母的地方告訴我們說<br>
<br>
0:19:46.800,0:19:51.940<br>
Gradient 越大，踏的步伐越大 ，參數就 update 的越大<br>
<br>
0:19:51.940,0:19:54.480<br>
但是分母的地方卻說<br>
<br>
0:19:54.480,0:19:58.780<br>
如果 Gradient 越大，參數 update 的越小這樣<br>
<br>
0:19:59.320,0:20:03.080<br>
好，怎麼解釋這件事情呢？<br>
<br>
0:20:03.080,0:20:07.340<br>
有一些 paper 這樣解釋的<br>
<br>
0:20:07.340,0:20:14.800<br>
這個 Adagrad 它想要考慮的是：<br>
今天這個 Gradient 有多 surprise<br>
<br>
0:20:14.800,0:20:17.840<br>
也就是所謂的"反差"這樣<br>
<br>
0:20:17.840,0:20:19.820<br>
反差，大家知道嗎？<br>
<br>
0:20:19.820,0:20:22.160<br>
就是比如說，反差萌的意思就是說<br>
<br>
0:20:22.160,0:20:25.440<br>
如果本來一個很兇惡的角色，突然對你很溫柔<br>
<br>
0:20:25.440,0:20:29.620<br>
你就會覺得它特別溫柔這樣，所以呢，對 Gradient 來說<br>
<br>
0:20:29.620,0:20:31.480<br>
也是一樣的道理<br>
<br>
0:20:31.480,0:20:35.560<br>
假設有某一個參數 ，它在第一次 update 參數的時候<br>
<br>
0:20:35.560,0:20:37.300<br>
它算出來的 Gradient 是 0.001<br>
<br>
0:20:37.300,0:20:40.500<br>
再來又算 0.001, 0.003, 等等...等等<br>
<br>
0:20:40.500,0:20:43.280<br>
到某一次呢，它 Gradient 算出來是 0.1<br>
<br>
0:20:43.280,0:20:49.240<br>
你就會覺得特別大，因為它比之前算出來的<br>
Gradient 都大了 100 倍，特別大<br>
<br>
0:20:49.640,0:20:52.400<br>
但是，如果是有另外一個參數<br>
<br>
0:20:52.400,0:20:56.100<br>
它一開始算出來是 10.8, 再來算 20.9, 再來算 31.7<br>
<br>
0:20:56.100,0:20:57.880<br>
它的 Gradient 平常都很大<br>
<br>
0:20:57.880,0:21:02.180<br>
但是它在某一次算出來的 Gradient 是 0.1<br>
<br>
0:21:02.180,0:21:04.540<br>
這時候，你就會覺得它特別小這樣子<br>
<br>
0:21:04.540,0:21:08.580<br>
所以為了強調這種反差的效果<br>
<br>
0:21:08.580,0:21:13.740<br>
所以在 Adagrad 裡面呢，我們就把它除以這項<br>
<br>
0:21:13.740,0:21:19.200<br>
這項就是把過去這些 Gradient 的平方<br>
<br>
0:21:19.200,0:21:24.140<br>
把它算出來，我們就想要知道說過去 Gradient 有多大<br>
<br>
0:21:24.140,0:21:27.600<br>
然後再把它們相除，看這個反差有多大這樣<br>
<br>
0:21:27.600,0:21:30.740<br>
這個是直觀的解釋<br>
<br>
0:21:30.740,0:21:34.540<br>
那更正式的解釋呢，我有這樣的解釋<br>
<br>
0:21:34.540,0:21:40.120<br>
我們來考慮一個二次函數，來考慮一個二次函數<br>
<br>
0:21:40.120,0:21:43.120<br>
這個二次函數呢，我們就寫成這樣子<br>
<br>
0:21:43.120,0:21:45.980<br>
他只有一個參數，就是 x<br>
<br>
0:21:46.000,0:21:51.320<br>
如果我們把這個二次函數，對 x 做微分的話<br>
<br>
0:21:51.320,0:21:56.820<br>
把 y 對 x 做微分，這個國中生就知道，這是 2ax + b<br>
<br>
0:21:56.820,0:21:59.680<br>
如果它絕對值算出來的話，長這樣子<br>
<br>
0:22:00.160,0:22:03.040<br>
好，那這個二次函數的最低點在哪裡呢？<br>
<br>
0:22:03.040,0:22:06.760<br>
是 -(b/2a)，我國中就被過這個式子了<br>
<br>
0:22:07.640,0:22:10.840<br>
如果你今天呢，在這個二次函數上，<br>
<br>
0:22:10.840,0:22:16.660<br>
你隨機的選一個點開始 ，你要做 Gradient Descent<br>
<br>
0:22:16.820,0:22:21.360<br>
那你的步伐多大，踏出去是最好的？<br>
<br>
0:22:21.660,0:22:24.320<br>
假設這個起始的點是 x0<br>
<br>
0:22:24.320,0:22:26.820<br>
最低點是 -(b/2a)<br>
<br>
0:22:26.820,0:22:30.120<br>
那踏出去一步，最好的步伐，<br>
<br>
0:22:30.120,0:22:33.180<br>
其實就是這兩個點之間的距離<br>
<br>
0:22:33.180,0:22:37.040<br>
因為如果你踏出去的步伐，是這兩個點之間的距離的話<br>
<br>
0:22:37.040,0:22:39.780<br>
你就一步到位了<br>
<br>
0:22:39.780,0:22:41.820<br>
這兩個點之間的距離是甚麼呢？<br>
<br>
0:22:41.820,0:22:43.820<br>
這兩個點之間的距離，你整理一下，<br>
<br>
0:22:43.820,0:22:47.480<br>
它是 |2a x0 + b| / 2a<br>
<br>
0:22:47.480,0:22:53.520<br>
|2a x0 + b| 這一項，就是這一項<br>
<br>
0:22:53.520,0:22:58.880<br>
2a x0 + b 就是 x0 這一點的微分<br>
<br>
0:22:58.880,0:23:01.580<br>
x0 這一點的一次微分<br>
<br>
0:23:01.580,0:23:04.540<br>
所以 Gradient Descent 你不覺得說聽起來很有道理<br>
<br>
0:23:04.540,0:23:08.120<br>
就是說，如果我今天算出來的微分越大<br>
<br>
0:23:08.460,0:23:10.640<br>
我就離原點越遠<br>
<br>
0:23:11.080,0:23:17.140<br>
如果踏出去的(我最好的)步伐，是跟微分的大小成正比<br>
<br>
0:23:17.140,0:23:21.060<br>
如果踏出去的步伐跟微分的大小成正比<br>
<br>
0:23:21.060,0:23:24.420<br>
它可能是最好的步伐<br>
<br>
0:23:25.140,0:23:31.460<br>
但是，這件事情只有在，只考慮一個參數的時候才成立<br>
<br>
0:23:31.460,0:23:35.220<br>
如果我們今天呢，同時有好幾個參數<br>
<br>
0:23:35.240,0:23:38.100<br>
我們要同時考慮好幾個參數的時候<br>
<br>
0:23:38.100,0:23:41.420<br>
這個時候呢，剛才的論述就不見得成立了<br>
<br>
0:23:41.420,0:23:46.760<br>
也就是說，Gradient 的值越大就跟最低點的距離越遠<br>
<br>
0:23:46.760,0:23:50.740<br>
這件事情，在有好多個參數的時候 ，是不一定成立的<br>
<br>
0:23:50.740,0:23:55.540<br>
比如說，你想看看，我們現在考慮 w1 跟 w2 兩個參數<br>
<br>
0:23:55.540,0:24:00.320<br>
這個圖上面的顏色，是它的 loss<br>
<br>
0:24:01.160,0:24:05.240<br>
那如果我們考慮 w1 的變化<br>
<br>
0:24:05.240,0:24:09.620<br>
我們就在藍色這條線這邊切一刀<br>
<br>
0:24:09.620,0:24:12.800<br>
我們把藍色這條線切一刀，<br>
<br>
0:24:12.800,0:24:17.300<br>
我們看到的 error surface 長得是這個樣子<br>
<br>
0:24:17.640,0:24:21.200<br>
如果你比較圖上的兩個點，a 點跟 b 點<br>
<br>
0:24:21.200,0:24:26.440<br>
那確實 a 點的微分值比較大，那它就距離最低點比較遠<br>
<br>
0:24:26.440,0:24:29.240<br>
但是，如果我們同時考慮幾個參數<br>
<br>
0:24:29.240,0:24:33.040<br>
我們同時考慮 w2 這個參數<br>
<br>
0:24:33.040,0:24:36.840<br>
我們在綠色的這條線上切一刀<br>
<br>
0:24:36.840,0:24:39.840<br>
如果我們在綠色這條線上切一刀的話<br>
<br>
0:24:39.840,0:24:42.260<br>
我們得到的值是這樣子<br>
<br>
0:24:42.260,0:24:43.860<br>
我們得到的 error surface 是這樣子<br>
<br>
0:24:43.860,0:24:47.820<br>
它是比較尖的，這個谷呢，是比較深的<br>
<br>
0:24:47.820,0:24:52.340<br>
因為你會發現說，w2 在這個方向的變化是比較猛烈的<br>
<br>
0:24:52.340,0:24:59.380<br>
如果我們只比較在 w2 這條線上的兩個點 , c 跟 d 的話<br>
<br>
0:24:59.380,0:25:01.320<br>
確實 c 的微分比較大<br>
<br>
0:25:01.320,0:25:05.460<br>
所以，它距離最低點是比較遠的<br>
<br>
0:25:05.580,0:25:09.540<br>
但是，如果我們今天的比較是跨參數的話<br>
<br>
0:25:09.540,0:25:14.480<br>
如果我們比較 a 這的點對 w1 的微分<br>
<br>
0:25:14.480,0:25:17.000<br>
c 這個點對 w2 的微分<br>
<br>
0:25:17.000,0:25:19.360<br>
這個結論呢，就不成立了<br>
<br>
0:25:19.360,0:25:24.660<br>
雖然說，c 這個點對 w2 的微分值是比較大的<br>
<br>
0:25:24.660,0:25:26.480<br>
這個微分值是比較小的<br>
<br>
0:25:26.480,0:25:31.140<br>
但 c 呢，是離最低點比較近的，而 a 是比較遠的<br>
<br>
0:25:31.140,0:25:34.700<br>
所以，當我們 update 參數<br>
<br>
0:25:34.700,0:25:39.440<br>
當我們 update 參數選擇跟微分值成正比<br>
<br>
0:25:39.440,0:25:44.840<br>
這樣的論述是在，沒有考慮跨參數的條件下<br>
<br>
0:25:44.840,0:25:46.680<br>
這件事情才成立的<br>
<br>
0:25:46.680,0:25:49.860<br>
當我們要同時考慮好幾個參數的時候呢<br>
<br>
0:25:49.860,0:25:52.760<br>
我們這樣想呢，就不足夠了<br>
<br>
0:25:52.760,0:25:56.560<br>
所以，如果我們今天要同時考慮好幾個參數的話<br>
<br>
0:25:56.560,0:25:58.440<br>
我們應該要怎麼想呢？<br>
<br>
0:25:58.440,0:26:01.840<br>
如果你看看，我們說的最好的 step 的話<br>
<br>
0:26:01.840,0:26:03.460<br>
我們看最好的這個 step<br>
<br>
0:26:03.480,0:26:07.560<br>
它其實還有分母這一項 ，它的分母這一項呢，是 2a<br>
<br>
0:26:07.560,0:26:12.180<br>
這個 2a 哪來的呢？這個 2a 是甚麼呢？<br>
<br>
0:26:12.180,0:26:20.460<br>
這個 2a 呢，如果我們今天把這個 y 做2次微分<br>
<br>
0:26:20.460,0:26:23.260<br>
我們做一次微分得到這個式子<br>
<br>
0:26:24.300,0:26:28.380<br>
那如果我們做二次微分的話，就得到 2a<br>
<br>
0:26:28.380,0:26:30.980<br>
那它是一個 constant<br>
<br>
0:26:30.980,0:26:35.320<br>
這個 2a 呢，就出現在最好的 step 的分母的地方<br>
<br>
0:26:35.320,0:26:41.720<br>
所以，今天最好的 step，它不只是要正比於一次微分<br>
<br>
0:26:41.720,0:26:45.620<br>
它同時要和二次微分的大小成反比<br>
<br>
0:26:45.620,0:26:48.060<br>
如果你二次微分比較大<br>
<br>
0:26:48.060,0:26:51.080<br>
這個時候你參數 update 量應該要小<br>
<br>
0:26:51.080,0:26:55.600<br>
如果二次微分小的話，你參數 update 量應該要比較大<br>
<br>
0:26:55.600,0:26:59.460<br>
所以，最好的 step 應該要把二次微分考慮進來<br>
<br>
0:26:59.700,0:27:03.560<br>
所以，如果我們今天把二次微分考慮進來的話<br>
<br>
0:27:03.560,0:27:08.360<br>
你會發現說，在 w1 這個方向上<br>
<br>
0:27:09.300,0:27:11.760<br>
你的二次微分是比較小的<br>
<br>
0:27:11.760,0:27:15.700<br>
因為這個是一個比較平滑的弧<br>
<br>
0:27:15.700,0:27:18.120<br>
所以這個二次微分是比較小的<br>
<br>
0:27:18.120,0:27:21.860<br>
在 w2 的方向上<br>
<br>
0:27:22.480,0:27:25.260<br>
這個是一個比較尖的弧、比較深的弧<br>
<br>
0:27:25.500,0:27:30.580<br>
它是一個比較尖的弧，所以它的二次微分是比較大的<br>
<br>
0:27:30.580,0:27:36.400<br>
所以你光比較 a 跟 c 的微分值呢 ，是不夠的<br>
<br>
0:27:36.400,0:27:41.400<br>
你要比較 a 的微分值除掉它的二次<br>
<br>
0:27:41.400,0:27:46.080<br>
跟 c 的微分值除掉它的二次，再去比<br>
<br>
0:27:46.080,0:27:49.760<br>
如果你做這件事，你才能夠真正顯示<br>
<br>
0:27:49.760,0:27:53.040<br>
這些點跟最低點的距離這樣<br>
<br>
0:27:53.040,0:27:55.540<br>
雖然 a 這個點，它的微分是比較小的<br>
<br>
0:27:55.540,0:27:59.100<br>
但它的二次也同時是比較小的<br>
<br>
0:27:59.100,0:28:01.900<br>
c 比較大、二次是比較大的<br>
<br>
0:28:01.900,0:28:06.040<br>
所以，如果你把二次微分的值呢，考慮進去<br>
<br>
0:28:06.040,0:28:10.740<br>
做這個評檢、做調整的話<br>
<br>
0:28:10.740,0:28:17.940<br>
那你這個時候，才能真正反映，<br>
你現在所在位置跟最低點的距離<br>
<br>
0:28:18.760,0:28:21.920<br>
好，那這件事情跟 Adagrad 的關係是甚麼呢？<br>
<br>
0:28:22.380,0:28:25.760<br>
如果你把 Adagrad 的式子列出來的話<br>
<br>
0:28:25.760,0:28:27.500<br>
你把 Adagrad 的式子列出來的話<br>
<br>
0:28:27.500,0:28:31.380<br>
它參數的 update 量是這個樣子的<br>
<br>
0:28:31.380,0:28:33.680<br>
η 是一個 constant，所以我們就不理它<br>
<br>
0:28:33.680,0:28:38.560<br>
這個 g^t 阿，它就是一次微分，對不對<br>
<br>
0:28:38.560,0:28:44.040<br>
下面這個，過去所有微分值的平方和開根號<br>
<br>
0:28:44.040,0:28:48.780<br>
神奇的是，它想要代表的是二次微分<br>
<br>
0:28:48.780,0:28:52.640<br>
那你可能會問說，怎麼不直接算二次微分呢？<br>
<br>
0:28:52.640,0:28:55.460<br>
你可以直接算二次微分<br>
<br>
0:28:55.460,0:29:00.440<br>
確實可以這麼做，也有這樣的方法，<br>
而且你確實可以這麼做<br>
<br>
0:29:00.440,0:29:03.480<br>
但是，有時候你會遇到的狀況是<br>
<br>
0:29:03.840,0:29:07.340<br>
你在作業一裡面是比較簡單的 case<br>
<br>
0:29:07.340,0:29:11.220<br>
相信你都秒算，秒給你結果<br>
<br>
0:29:11.220,0:29:16.620<br>
但是，有時候你參數量大、data 多的時候<br>
<br>
0:29:16.620,0:29:19.620<br>
你可能算一次微分就花一天這樣子<br>
<br>
0:29:19.620,0:29:22.980<br>
然後你再算二次微分，你要再多花一天<br>
<br>
0:29:22.980,0:29:26.020<br>
有時候，這樣子的結果是你不能承受的<br>
<br>
0:29:26.020,0:29:29.540<br>
而且你多花一天 performance 還不見得會比較好<br>
<br>
0:29:29.580,0:29:31.500<br>
其實這個結果，是你不能承受的<br>
<br>
0:29:31.500,0:29:34.320<br>
所以，Adagrad 它提供的做法就是<br>
<br>
0:29:34.320,0:29:37.840<br>
我們在沒有增加任何額外運算的前提之下<br>
<br>
0:29:38.260,0:29:40.920<br>
想辦法能不能夠做一件事情<br>
<br>
0:29:40.920,0:29:43.920<br>
去估一下，二次的微分應該是多少<br>
<br>
0:29:43.920,0:29:47.220<br>
在 Adagrad 裡面，你只需要一次微分的值<br>
<br>
0:29:47.220,0:29:49.380<br>
那這個東西我們本來就要算它了<br>
<br>
0:29:49.380,0:29:52.800<br>
所以並沒有，多做任何多餘的運算<br>
<br>
0:29:53.760,0:29:56.360<br>
好，怎麼做呢？<br>
<br>
0:29:56.360,0:30:01.520<br>
如果我們考慮一個二次微分比較小的峽谷<br>
<br>
0:30:01.520,0:30:06.060<br>
跟一個二次微分比較大的峽谷<br>
<br>
0:30:06.260,0:30:11.140<br>
然後我們把它的一次微分的值，考慮進來的話<br>
<br>
0:30:11.140,0:30:13.580<br>
這個是長這樣<br>
<br>
0:30:13.580,0:30:15.700<br>
這個是長這樣<br>
<br>
0:30:15.700,0:30:20.140<br>
如果你只是在，這個區間和這個區間裡面<br>
<br>
0:30:20.140,0:30:23.200<br>
隨機 sample 一個點，算它的一次微分的話<br>
<br>
0:30:23.200,0:30:26.920<br>
你看不出來它的二次微分值是多少<br>
<br>
0:30:26.920,0:30:30.100<br>
但是如果你 sample 夠多點<br>
<br>
0:30:30.100,0:30:33.840<br>
你在某一個 range 之內，sample 夠多點的話<br>
<br>
0:30:33.840,0:30:37.940<br>
那你就會發現說，在這個比較平滑的峽谷裡面<br>
<br>
0:30:37.940,0:30:40.280<br>
它的一次微分通常就是比較小的<br>
<br>
0:30:40.280,0:30:44.180<br>
在比較尖的峽谷裡面，它的一次微分通常是比較大的<br>
<br>
0:30:44.180,0:30:47.460<br>
而 Adagrad 這邊，這一件事情<br>
<br>
0:30:47.460,0:30:51.500<br>
summation over 過去所有的微分的平方，這件事情<br>
<br>
0:30:51.500,0:30:56.280<br>
你就可以想成，在這個地方呢，做 sampling<br>
<br>
0:30:56.280,0:30:58.360<br>
就在這個地方呢，做 sampling<br>
<br>
0:30:58.360,0:31:01.840<br>
那你再把它的平方和呢，再開根號算出來<br>
<br>
0:31:01.840,0:31:06.260<br>
那這個東西，就反映了二次微分的大小<br>
<br>
0:31:07.460,0:31:11.520<br>
這個 Adagrad 怎麼做，其實我們上次已經有示範過了<br>
<br>
0:31:11.520,0:31:13.860<br>
那所以我們就不再示範<br>
<br>
0:31:13.860,0:31:18.620<br>
接下來我們要獎的另外一件事情呢，<br>
是 Stochastic 的 Gradient Descent<br>
<br>
0:31:18.620,0:31:22.140<br>
那它可以讓你的 training 呢，更快一點<br>
<br>
0:31:22.640,0:31:23.800<br>
好，這個怎麼說呢？<br>
<br>
0:31:23.800,0:31:27.880<br>
我們之前講說，我們的 loss function<br>
<br>
0:31:27.880,0:31:31.500<br>
它的 loss function，它的樣子呢<br>
<br>
0:31:31.500,0:31:35.440<br>
如果我們今天做的是這個 Regression 的話<br>
<br>
0:31:35.440,0:31:37.980<br>
這個是 Regression 的式子<br>
<br>
0:31:37.980,0:31:40.900<br>
Regression 得到的 estimation 的結果<br>
<br>
0:31:40.900,0:31:43.480<br>
那你把 Regression 得到 estimation 的結果<br>
<br>
0:31:43.480,0:31:47.780<br>
減掉 y\head，再去平方<br>
<br>
0:31:47.780,0:31:50.540<br>
再 summation over 所有的 training data<br>
<br>
0:31:50.540,0:31:52.200<br>
這是我們的 loss function<br>
<br>
0:31:52.200,0:31:55.100<br>
所以，這個式子非常合理<br>
<br>
0:31:55.100,0:31:58.720<br>
我們的 loss 本來就應該考慮所有的 example<br>
<br>
0:31:58.720,0:32:02.300<br>
它本來就應該 summation over 所有的 example<br>
<br>
0:32:02.300,0:32:05.000<br>
有這些以後，你就可以去算 Gradient<br>
<br>
0:32:05.000,0:32:07.420<br>
然後你就可以做 Gradient Descent<br>
<br>
0:32:07.420,0:32:10.580<br>
但 Stochastic Gradient Descent，他的想法不一樣<br>
<br>
0:32:10.580,0:32:13.040<br>
Stochastic Gradient Descent 它做的事情是<br>
<br>
0:32:13.040,0:32:17.280<br>
每次就拿一個 x^n 出來<br>
<br>
0:32:17.280,0:32:21.980<br>
這邊你可以隨機取，也可以按照順序取<br>
<br>
0:32:21.980,0:32:23.420<br>
那其實隨機取的時候<br>
<br>
0:32:23.420,0:32:26.100<br>
如果你今天是在做 deep learning 的 case<br>
<br>
0:32:26.100,0:32:30.000<br>
也就是說你的 error surface 不是 convex<br>
<br>
0:32:30.000,0:32:33.020<br>
是非常崎嶇的，隨機取呢，是有幫助的<br>
<br>
0:32:33.020,0:32:36.180<br>
總之，你就取一個 example 出來<br>
<br>
0:32:36.180,0:32:38.120<br>
假設取出來的 example 是 x^n<br>
<br>
0:32:38.120,0:32:41.300<br>
這個時候呢，你要計算你的 loss<br>
<br>
0:32:41.300,0:32:45.140<br>
你的 loss 呢，只考慮一個 example<br>
<br>
0:32:45.140,0:32:51.580<br>
你只考慮你現在的參數，對這個 example 的 y 的估測值<br>
<br>
0:32:51.580,0:32:54.480<br>
再減掉它的正確答案，再做平方<br>
<br>
0:32:54.480,0:32:56.680<br>
然後就不 summation over 所有的 example<br>
<br>
0:32:56.680,0:32:59.360<br>
因為你現在只取一個 example 出來<br>
<br>
0:32:59.800,0:33:03.800<br>
你只算某一個 example 的 loss<br>
<br>
0:33:04.400,0:33:07.060<br>
那接下來呢，你在 update 參數的時候<br>
<br>
0:33:07.060,0:33:11.180<br>
你只考慮那一個 example<br>
<br>
0:33:11.180,0:33:14.700<br>
我們只考慮一個 example 的 loss function，我們就寫成<br>
<br>
0:33:14.700,0:33:18.580<br>
L^n，代表它是考慮第 n 個 example 的 loss function<br>
<br>
0:33:18.580,0:33:20.380<br>
那你在算 Gradient 的時候呢？<br>
<br>
0:33:20.380,0:33:26.700<br>
你不是算對 total 所有的 data，它的 Gradient 的和<br>
<br>
0:33:26.700,0:33:32.280<br>
你只算對某一個 example，它的 loss 的 Gradient<br>
<br>
0:33:32.280,0:33:35.920<br>
然後呢，你就很急躁的 update 參數了<br>
<br>
0:33:35.920,0:33:40.720<br>
所以在原來的 Gradient Descent 裡面，<br>
你計算所有 data 的 loss<br>
<br>
0:33:40.720,0:33:42.060<br>
然後才 update 參數<br>
<br>
0:33:42.060,0:33:44.500<br>
但是在 Stochastic Gradient Descent  裡面<br>
<br>
0:33:44.500,0:33:48.240<br>
你看一個 example，就 update 一個參數這樣<br>
<br>
0:33:48.240,0:33:51.280<br>
你可能想說，這有啥好呢？<br>
<br>
0:33:51.280,0:33:53.380<br>
聽起來好像沒有甚麼好的<br>
<br>
0:33:53.380,0:33:55.680<br>
那我們實際來操作一下好了<br>
<br>
0:33:55.680,0:34:01.920<br>
剛才看到圖呢，它可能是這個樣子的<br>
<br>
0:34:04.800,0:34:08.380<br>
我們剛才看到的圖呢，它可能是這個樣子<br>
<br>
0:34:08.380,0:34:12.100<br>
原來的 Gradient Descent，你看完所有的 example 以後<br>
<br>
0:34:12.100,0:34:14.820<br>
你就 update 一次參數<br>
<br>
0:34:14.820,0:34:17.220<br>
那它其實是比較穩定<br>
<br>
0:34:17.220,0:34:19.680<br>
你會發現說，它走的方向<br>
<br>
0:34:19.680,0:34:24.460<br>
就是按照 Gradient 建議我們的方向呢，來走<br>
<br>
0:34:24.460,0:34:28.120<br>
但是如果你是用 Stochastic Gradient Descent 的話<br>
<br>
0:34:28.120,0:34:33.840<br>
你每看到一個 example ，你就 update 一次參數<br>
<br>
0:34:34.400,0:34:37.140<br>
如果你有 20 個 example 的時候<br>
<br>
0:34:37.140,0:34:39.880<br>
那你就 update 20 次參數<br>
<br>
0:34:39.880,0:34:43.260<br>
那這邊他是看完 20 個 example 才 update 一次參數<br>
<br>
0:34:43.260,0:34:46.340<br>
這邊是，每一個 example 都 update 一次參數<br>
<br>
0:34:46.340,0:34:48.780<br>
所以在它看 20 個 example 的時候<br>
<br>
0:34:48.780,0:34:53.360<br>
你這邊也已經看了 20 個 example，<br>
而且 update 20 次參數了<br>
<br>
0:34:53.360,0:34:57.000<br>
所以 update 20 次參數的結果呢，看起來就像是這樣<br>
<br>
0:34:57.000,0:35:01.780<br>
從一樣的起始點開始，但它已經 update 了 20 次參數<br>
<br>
0:35:01.780,0:35:06.060<br>
所以，這個如果只看一個 example 的話<br>
<br>
0:35:06.060,0:35:08.140<br>
它的步伐是小的<br>
<br>
0:35:08.140,0:35:11.880<br>
而且可能是散亂的<br>
<br>
0:35:11.880,0:35:15.800<br>
因為你每次只考慮一個 example<br>
<br>
0:35:15.800,0:35:19.060<br>
所以它參數 update 的方向，跟這個 Gradient Descent<br>
<br>
0:35:19.060,0:35:23.100<br>
total loss 的 error surface 界定我們走的方向<br>
<br>
0:35:23.100,0:35:30.680<br>
不見得是一致的，但是因為我們可以看很多個 example<br>
<br>
0:35:30.680,0:35:34.120<br>
所以天下武功，為快不破。在它走一步的時候<br>
<br>
0:35:34.120,0:35:37.960<br>
你已經出 20 拳了，所以它走的反而是比較快的<br>
<br>
0:35:40.420,0:35:44.860<br>
然後呢，接下來我們要講的是第三個 tip<br>
<br>
0:35:44.860,0:35:47.640<br>
就是你可以做 Feature 的 Scaling<br>
<br>
0:35:47.640,0:35:51.400<br>
所謂的 Feature Scaling 意思呢是這樣子<br>
<br>
0:35:51.400,0:35:55.600<br>
假設我們現在要做 Regression<br>
<br>
0:35:55.600,0:35:58.040<br>
那我們這個 Regression 的 function 裡面<br>
<br>
0:35:58.040,0:36:02.500<br>
input 的 feature 有兩個，x1 跟 x2<br>
<br>
0:36:02.500,0:36:06.360<br>
比如說，如果是要 predict 寶可夢進化以後 CP 值的話<br>
<br>
0:36:06.360,0:36:12.640<br>
那 x1 是進化前的 CP值，x2 是它的生命值...等等這樣<br>
<br>
0:36:12.640,0:36:16.000<br>
你有兩個 input feature, x1 跟 x2<br>
<br>
0:36:16.400,0:36:21.820<br>
那如果你看你今天的 x1 跟 x2<br>
<br>
0:36:21.820,0:36:25.800<br>
它們分佈的 range 是很不一樣的話<br>
<br>
0:36:25.800,0:36:29.400<br>
那就建議你呢，把它們做 scaling<br>
<br>
0:36:29.400,0:36:31.940<br>
把它們的 range 分佈變成是一樣<br>
<br>
0:36:31.940,0:36:37.840<br>
比如，這邊的 x2 它的分佈是遠比 x1 大<br>
<br>
0:36:37.840,0:36:42.220<br>
那就建議你把 x2 這個值呢，做一下 rescaling<br>
<br>
0:36:42.220,0:36:48.300<br>
把它的值縮小，讓 x2 的分佈跟 x1 的分佈是比較像的<br>
<br>
0:36:48.300,0:36:52.880<br>
你希望不同的 feature，他們的 scale 是一樣的<br>
<br>
0:36:53.280,0:36:55.420<br>
為甚麼要這麼做呢？<br>
<br>
0:36:55.820,0:36:57.440<br>
我們舉個例子<br>
<br>
0:36:57.440,0:37:02.220<br>
假設這個是我們的 Regression 的 function<br>
<br>
0:37:02.220,0:37:05.140<br>
那我們寫成這樣，這邊這個意思跟這個是一樣的啦<br>
<br>
0:37:05.140,0:37:08.120<br>
y = b + w1*x1 + w2*x2<br>
<br>
0:37:08.120,0:37:14.420<br>
y = b + w1*x1 再加 w2*x2<br>
<br>
0:37:14.420,0:37:20.300<br>
那假設 x1 平常的值，都是比較小的，假設說 1, 2 之類的<br>
<br>
0:37:20.300,0:37:22.600<br>
假設 x2 它平常的值都很大<br>
<br>
0:37:22.600,0:37:25.920<br>
它 input 的值都很大，100, 200 之類的<br>
<br>
0:37:26.800,0:37:30.640<br>
那這個時候，如果你把 loss 的 surface 畫出來<br>
<br>
0:37:30.640,0:37:33.040<br>
會遇到甚麼樣的狀況呢？<br>
<br>
0:37:33.340,0:37:39.080<br>
你會發現說，如果你更動 w1 跟 w2 的值<br>
<br>
0:37:39.080,0:37:46.140<br>
假設你把 w1 跟 w2 的值都做一樣的更動<br>
<br>
0:37:46.140,0:37:49.760<br>
都加個 △w ，你會發現說<br>
<br>
0:37:50.180,0:37:54.440<br>
w1 的變化，對 y 的變化而言是比較小的<br>
<br>
0:37:54.440,0:37:58.760<br>
w2 的變化，對 y 的變化而言是比較大的<br>
<br>
0:37:58.760,0:38:00.340<br>
對不對，這件事情是很合理的<br>
<br>
0:38:00.340,0:38:04.840<br>
因為你要把 w2 乘上它 input 的這些值<br>
<br>
0:38:04.840,0:38:07.200<br>
你要把 w1 乘上它 input 的這些值<br>
<br>
0:38:07.200,0:38:11.720<br>
如果 w2 它乘的這些 input 的值是比較大的<br>
<br>
0:38:11.720,0:38:16.120<br>
那只要把 w2 小小的變化，那 y 就會有很大的變化<br>
<br>
0:38:16.120,0:38:20.040<br>
那同樣的變化，w1 它 input 的值是比較小的<br>
<br>
0:38:20.040,0:38:23.360<br>
它對 y 的影響呢，就變成是比較小的<br>
<br>
0:38:23.360,0:38:26.360<br>
所以如果你把他們的 error surface 畫出來的話呢<br>
<br>
0:38:26.360,0:38:30.520<br>
你看到的可能像是這個樣子<br>
<br>
0:38:30.520,0:38:35.420<br>
所以如果你把他們的 error surface 畫出來的話呢，<br>
你看到的可能像是這樣<br>
<br>
0:38:35.420,0:38:38.260<br>
這個圖，是甚麼意思呢？<br>
<br>
0:38:38.260,0:38:44.240<br>
因為 w1 對 y 的影響比較小<br>
<br>
0:38:44.240,0:38:47.500<br>
所以 w1 就對 loss 的影響比較小<br>
<br>
0:38:47.500,0:38:52.560<br>
所以 w1 對 loss 是有比較小的微分的<br>
<br>
0:38:52.560,0:38:55.380<br>
所以 w1 這個方向上，它是比較平滑<br>
<br>
0:38:55.880,0:39:00.620<br>
w2 對 y 的影響比較大，所以它對 loss 的影響比較大<br>
<br>
0:39:00.620,0:39:03.380<br>
所以改變 w2 的時候，它對 loss 的影響比較大<br>
<br>
0:39:03.380,0:39:06.900<br>
所以，它在這個方向上，是比較 shock 的<br>
<br>
0:39:06.900,0:39:09.760<br>
所以這個方向上，有一個比較尖的峽谷<br>
<br>
0:39:10.220,0:39:15.280<br>
那如果今天，x1 跟 x2 的值，它們的 scale 是接近的<br>
<br>
0:39:15.740,0:39:17.780<br>
那如果你把 loss 畫出來的話呢<br>
<br>
0:39:17.780,0:39:19.640<br>
它就會比較接近圓形<br>
<br>
0:39:19.640,0:39:24.240<br>
w1 跟 w2 呢，對你的 loss 是有差不多的影響力<br>
<br>
0:39:24.620,0:39:27.740<br>
這個對做 Gradient Descent 會有甚麼樣的影響呢？<br>
<br>
0:39:27.740,0:39:28.920<br>
是會有影響的<br>
<br>
0:39:28.920,0:39:31.860<br>
比如說，如果你從這個地方開始<br>
<br>
0:39:31.860,0:39:34.460<br>
其實我們上次已經有看到了，就是這樣<br>
<br>
0:39:34.460,0:39:37.900<br>
這種長橢圓的 error surface 阿<br>
<br>
0:39:37.900,0:39:42.160<br>
如果你不出些 Adagrad 甚麼的，你是很難搞定它的<br>
<br>
0:39:42.160,0:39:45.920<br>
因為就在這個方向上，和這個方向上<br>
<br>
0:39:45.920,0:39:48.080<br>
你會需要非常不同的 learning rate<br>
<br>
0:39:48.080,0:39:49.540<br>
你同一組 learning rate 會搞不定它<br>
<br>
0:39:49.540,0:39:53.060<br>
你要 adaptive learning 才能夠搞定它<br>
<br>
0:39:53.060,0:39:58.520<br>
所以這樣子的狀況，沒有 scaling 的時候，<br>
它 update 參數是比較難的<br>
<br>
0:39:58.520,0:40:02.960<br>
但是，如果你有 scale 的話，它就變成一個正圓形<br>
<br>
0:40:02.960,0:40:06.540<br>
如果是在正圓形的時候 ，update 參數就會變得比較容易<br>
<br>
0:40:06.540,0:40:12.000<br>
而且，你知道說 Gradient Descent<br>
它並不是向著最低點走<br>
<br>
0:40:12.000,0:40:14.800<br>
在這個藍色圈圈，它的最低點是在這邊<br>
<br>
0:40:14.800,0:40:16.620<br>
綠色圈圈最低點是在這邊<br>
<br>
0:40:16.620,0:40:20.380<br>
但是你今天在 update 參數的時候，走的方向是順著<br>
<br>
0:40:20.380,0:40:23.640<br>
等高線的方向，是順著 Gradient 箭頭的方向<br>
<br>
0:40:23.640,0:40:25.260<br>
所以，雖然最低點在這邊<br>
<br>
0:40:25.260,0:40:28.840<br>
你從邊開始走，你還是會走這個方向，再走進去<br>
<br>
0:40:28.840,0:40:32.400<br>
你不會只向那個最低點去走<br>
<br>
0:40:32.480,0:40:35.640<br>
那如果是綠色的呢，綠色的又不一樣<br>
<br>
0:40:35.640,0:40:37.880<br>
因為，它如果真的是一個正圓的話<br>
<br>
0:40:37.880,0:40:42.020<br>
你不管在這個區域的哪一個點<br>
<br>
0:40:42.020,0:40:45.580<br>
它都會向著圓心走<br>
<br>
0:40:45.580,0:40:47.740<br>
所以，如果你有做 feature scaling 的時候<br>
<br>
0:40:47.740,0:40:50.380<br>
你在做參數的 update 的時候呢<br>
<br>
0:40:50.380,0:40:52.840<br>
它是會比較有效率的<br>
<br>
0:40:53.140,0:40:55.520<br>
那你可能會問說，怎麼做 scaling<br>
<br>
0:40:55.520,0:40:58.540<br>
這個方法有千百種啦<br>
<br>
0:40:58.540,0:41:00.520<br>
你就選一個你喜歡的就是了<br>
<br>
0:41:00.520,0:41:02.400<br>
那常見的作法是這樣<br>
<br>
0:41:02.400,0:41:07.380<br>
假設我有 r 個 example, x^1, x^2 到 x^R<br>
<br>
0:41:08.120,0:41:13.320<br>
每一筆 example，裡面都有一組 feature<br>
<br>
0:41:13.880,0:41:16.920<br>
x^1  它第一個 component 就是 x(1,1)<br>
<br>
0:41:16.920,0:41:19.960<br>
x^2  它第一個 component 就是 x(2,1)<br>
<br>
0:41:19.960,0:41:23.240<br>
x^1  它第二個 component 就是 x(1,2)<br>
x^2  它第二個 component 就是 x(2,2)<br>
<br>
0:41:23.600,0:41:25.640<br>
那怎麼做 feature scaling？<br>
<br>
0:41:25.640,0:41:28.460<br>
你就對每一個 dimension i<br>
<br>
0:41:28.460,0:41:31.940<br>
都去算它的 mean，這邊寫成 m_i<br>
<br>
0:41:32.680,0:41:37.200<br>
都去算它的 deviation，這邊寫成 σ_i<br>
<br>
0:41:38.260,0:41:44.680<br>
然後呢，對第 r 個 example 的第 i 個 component<br>
<br>
0:41:44.680,0:41:49.460<br>
對第 r 個 example 的第 i 個 component<br>
<br>
0:41:49.460,0:41:55.440<br>
你就把它減掉，所有的 data 的<br>
第 i 個 component 的 mean，也就是 m_i<br>
<br>
0:41:55.440,0:41:59.020<br>
你就把它減掉所有的 data 的<br>
第 i 個 component 的 mean<br>
<br>
0:41:59.020,0:42:08.780<br>
再除掉所有的 data 的第 i 個 component 的 standard deviation<br>
<br>
0:42:08.780,0:42:11.200<br>
然後呢，你就會得到說<br>
<br>
0:42:11.200,0:42:15.180<br>
你做完這件事以後<br>
<br>
0:42:15.180,0:42:18.340<br>
你所有 dimension 的 mean 就會是 0<br>
<br>
0:42:18.340,0:42:20.840<br>
你的 variance 就會是 1<br>
<br>
0:42:20.840,0:42:23.960<br>
這是其中一個常見地做 localization 的方法<br>
<br>
0:42:24.740,0:42:29.700<br>
最後，在下課前呢，我們來講一下<br>
<br>
0:42:29.700,0:42:32.740<br>
為甚麼 Gradient Descent 它會 work<br>
<br>
0:42:32.740,0:42:35.580<br>
Gradient Descent 背後的理論基礎是什麼<br>
<br>
0:42:35.580,0:42:39.020<br>
那在真正深入數學部分的基礎之前呢<br>
<br>
0:42:39.020,0:42:40.680<br>
我們來問大家一個問題<br>
<br>
0:42:40.680,0:42:44.480<br>
大家都已經知道 Gradient Descent 是怎麼做的<br>
<br>
0:42:44.480,0:42:48.000<br>
假設，我問你一個這樣的是非題<br>
<br>
0:42:49.340,0:42:53.600<br>
每一次，我們在 update 參數的時候<br>
<br>
0:42:53.600,0:42:56.440<br>
我們都得到一個新的 θ<br>
<br>
0:42:57.100,0:43:02.140<br>
這個新的 θ，總是會讓我們的 loss 比較小<br>
<br>
0:43:02.140,0:43:08.220<br>
這個陳述，是對的嗎？<br>
<br>
0:43:09.580,0:43:14.420<br>
好，也就是意思就是說 θ_0 你把它代到 L 裡面<br>
<br>
0:43:14.420,0:43:19.060<br>
它會大於 θ_1 代到 L 裡面，它會大於 θ_2 代到 L 裡面<br>
<br>
0:43:19.060,0:43:24.520<br>
每次 update 參數的時候，<br>
這個 loss 的值，它都是越來越小的<br>
<br>
0:43:24.520,0:43:26.960<br>
這陳述，是正確的嗎？<br>
<br>
0:43:26.960,0:43:30.240<br>
你覺得它是正確的同學舉手<br>
<br>
0:43:30.240,0:43:34.520<br>
那你覺得這個陳述，它是不對的同學舉手<br>
<br>
0:43:34.520,0:43:36.200<br>
好，手放下<br>
<br>
0:43:36.200,0:43:38.220<br>
大家的觀念都很正確，沒錯<br>
<br>
0:43:38.220,0:43:41.840<br>
就是 update 參數以後，loss 不見得會下降的<br>
<br>
0:43:41.840,0:43:45.820<br>
所以如果你今天自己 implement Gradient Descent<br>
<br>
0:43:45.820,0:43:48.440<br>
做出來，update 參數以後的 loss 沒有下降<br>
<br>
0:43:48.440,0:43:50.260<br>
那不見得是你的程式有 bug<br>
<br>
0:43:50.260,0:43:52.940<br>
因為，本來就有可能發生這種事情<br>
<br>
0:43:52.940,0:43:55.900<br>
我們剛已經看過說，如果你 learning rate 調太大的話<br>
<br>
0:43:55.900,0:43:57.400<br>
會發生這種事情<br>
<br>
0:43:57.400,0:44:00.840<br>
或許，我們可以在下課前，做一個 demo<br>
<br>
0:44:02.540,0:44:06.180<br>
好，那在解釋 Gradient Descent 的 Theory 之前<br>
<br>
0:44:06.180,0:44:08.920<br>
這邊有一個 Warning of Math ，意思就是說<br>
<br>
0:44:08.920,0:44:11.740<br>
這個部分，就算是你沒有聽懂，也沒有關係<br>
<br>
0:44:11.740,0:44:14.260<br>
太陽明天依舊會升起<br>
<br>
0:44:15.500,0:44:20.180<br>
好，那我們先不要管 Gradient Descent<br>
<br>
0:44:20.180,0:44:25.100<br>
我們先來想想看，<br>
假如你要解一個 Optimization 的 problem<br>
<br>
0:44:25.100,0:44:30.220<br>
你要在這一個 figure 上面，找他的最低點<br>
<br>
0:44:30.220,0:44:32.560<br>
你到底應該要怎麼做？<br>
<br>
0:44:32.560,0:44:34.520<br>
那有一個這樣子的作法<br>
<br>
0:44:35.340,0:44:39.800<br>
如果今天給我一個起始的點 ，也就是 θ_0<br>
<br>
0:44:40.360,0:44:44.280<br>
我們有方法，在這個起始點的附近<br>
<br>
0:44:44.280,0:44:48.740<br>
畫一個圓圏、畫一個範圍、畫一個紅色圈圈<br>
<br>
0:44:48.740,0:44:51.840<br>
然後，在這個紅色圈圈裡面<br>
<br>
0:44:51.840,0:44:53.380<br>
找出它的最低點<br>
<br>
0:44:53.380,0:44:57.380<br>
比如說，紅色圈圈裡面的最低點，就是在這個邊上<br>
<br>
0:44:58.020,0:45:02.260<br>
這個意思就是說，如果你給我一整個 error function<br>
<br>
0:45:02.260,0:45:05.360<br>
我沒有辦法，馬上一秒鐘就告訴你說<br>
<br>
0:45:05.360,0:45:08.440<br>
我沒有辦法馬上告訴你說，它的最低點在哪裡<br>
<br>
0:45:08.660,0:45:12.700<br>
但是如果你給我一個 error function，加上一個初始的點<br>
<br>
0:45:12.700,0:45:16.680<br>
我可以告訴你說，在這個初始點附近，畫一個範圍之內<br>
<br>
0:45:16.680,0:45:19.140<br>
哦，有問題是嗎？<br>
<br>
0:45:23.100,0:45:24.860<br>
謝謝，謝謝，沒有問題<br>
<br>
0:45:24.860,0:45:28.720<br>
我們可以在那個附近，找出一個最小的值<br>
<br>
0:45:40.180,0:45:42.700<br>
那你假設找到最小的值以後<br>
<br>
0:45:42.700,0:45:47.940<br>
我們就更新我們中間的位置<br>
<br>
0:45:47.940,0:45:50.080<br>
中間的位置挪到 θ_1<br>
<br>
0:45:50.080,0:45:52.280<br>
接下來呢，再畫一個圓圈<br>
<br>
0:45:52.280,0:45:54.560<br>
我們可以在這個圓圈範圍之內<br>
<br>
0:45:54.560,0:45:56.300<br>
再找一個最小的點<br>
<br>
0:45:56.300,0:45:59.180<br>
假設呢，它是落在這個地方<br>
<br>
0:45:59.580,0:46:01.980<br>
然後，你就再更新中心點的參數<br>
<br>
0:46:01.980,0:46:04.020<br>
到 θ_2 這個地方<br>
<br>
0:46:04.020,0:46:07.500<br>
然後，你就可以再找小小範圍內的最小值<br>
<br>
0:46:07.500,0:46:11.300<br>
然後，再更新你的參數，就一直這樣下去<br>
<br>
0:46:11.420,0:46:13.140<br>
好，那現在的問題就是<br>
<br>
0:46:13.140,0:46:16.360<br>
怎麼很快的在紅色圈圈裡面<br>
<br>
0:46:16.360,0:46:21.360<br>
找一個可以讓 loss 最小的參數呢？<br>
<br>
0:46:21.360,0:46:23.240<br>
怎麼做這件事呢？<br>
<br>
0:46:23.240,0:46:26.680<br>
這個地方要從 Taylor series 說起<br>
<br>
0:46:26.720,0:46:29.980<br>
假設你是知道 Taylor series 的，那個微積分有教過<br>
<br>
0:46:29.980,0:46:32.040<br>
Taylor series 告訴我們什麼呢？<br>
<br>
0:46:32.460,0:46:34.460<br>
它告訴我們說，任何一個 function h(x)<br>
<br>
0:46:34.460,0:46:41.060<br>
如果它在 x = x_0 這點呢<br>
<br>
0:46:41.060,0:46:43.820<br>
是 infinitely differentiable<br>
<br>
0:46:43.820,0:46:49.380<br>
那你可以把這個 h(x) 寫成以下這個樣子<br>
<br>
0:46:49.380,0:46:52.500<br>
你可以把 h(x) 寫成<br>
<br>
0:46:52.500,0:46:57.760<br>
Σ(k=0, ∞)，這裡 k 代表微分的次數<br>
<br>
0:46:57.760,0:47:06.200<br>
(h 在 x_0 微分 k 次以後的值) / k!<br>
<br>
0:47:06.200,0:47:08.740<br>
然後 (x-x_0)^k<br>
<br>
0:47:09.540,0:47:14.220<br>
不過，把它展開的話，你可以把 h(x) 寫成 h(x_0)<br>
<br>
0:47:14.220,0:47:17.600<br>
+ h'(x_0) * (x - x_0)<br>
<br>
0:47:17.600,0:47:23.060<br>
+ h''(x_0) * (x - x_0)^2<br>
<br>
0:47:23.800,0:47:27.660<br>
那當 x 很接近 x_0 的時候<br>
<br>
0:47:27.660,0:47:29.960<br>
當 x 很接近 x_0 的時候<br>
<br>
0:47:29.960,0:47:36.560<br>
(x - x_0) 就會遠大於 (x - x_0)^2，就會遠大於<br>
後面的 3次,、4次，到無窮多次<br>
<br>
0:47:36.560,0:47:41.520<br>
所以，這個時候，你可以把後面的高次項刪掉<br>
<br>
0:47:41.520,0:47:44.600<br>
所以，當 x 很接近 x_0 的時候<br>
<br>
0:47:45.240,0:47:48.840<br>
這個只有在 x 很接近 x_0 的時候才成立<br>
<br>
0:47:48.840,0:47:50.520<br>
h(x)  就可以寫成<br>
<br>
0:47:50.520,0:47:54.840<br>
h(x_0) + h'(x_0) * (x - x_0)<br>
<br>
0:47:55.240,0:47:59.800<br>
那這個是只有考慮一個 variable 的 case<br>
<br>
0:47:59.800,0:48:03.400<br>
那其實，我這邊有個例子<br>
<br>
0:48:03.400,0:48:06.380<br>
假設 h(x) = sin(x)<br>
<br>
0:48:07.140,0:48:10.480<br>
那在 x_0 約等於 (π/4) 的地方<br>
<br>
0:48:10.480,0:48:12.620<br>
sin(x) 你可以寫成什麼樣子呢？<br>
<br>
0:48:12.620,0:48:17.600<br>
你用計算機算一下，它算出來是這樣子<br>
<br>
0:48:17.600,0:48:24.740<br>
這個 sin(x)，可以寫成這麼多這麼多這麼多項的相加<br>
<br>
0:48:25.620,0:48:30.620<br>
那如果我們把這些項，畫出來的話<br>
<br>
0:48:30.620,0:48:34.020<br>
你得到這樣子，一個結果<br>
<br>
0:48:34.020,0:48:37.880<br>
如果是 1/sqrt(2)，只有考慮 0 次的話<br>
<br>
0:48:37.880,0:48:40.060<br>
是這條水平線<br>
<br>
0:48:40.400,0:48:45.380<br>
考慮 1/sqrt(2) + (x-π/4)/sqrt(2)<br>
<br>
0:48:45.380,0:48:49.240<br>
考慮一次的話，是這條斜線<br>
<br>
0:48:49.520,0:48:53.280<br>
如果你有再把 2 次考慮進去，考慮 0 次, 1 次, 2 次的話<br>
<br>
0:48:53.280,0:48:57.120<br>
我猜你得到的，可能是這條線<br>
<br>
0:48:57.120,0:49:00.720<br>
如果你再把 3 次考慮進去的話，你得到這條線<br>
<br>
0:49:01.220,0:49:05.780<br>
如果你再把 4 次考慮進去的話，你可能得到橙色這條線<br>
<br>
0:49:06.200,0:49:08.540<br>
但是，雖然說，比如說如果你看<br>
<br>
0:49:08.680,0:49:14.620<br>
成色這條線，應該是 sin(x)，不好意思<br>
<br>
0:49:15.200,0:49:20.260<br>
好，你發現說，如果你只有考慮一次的時候<br>
<br>
0:49:20.260,0:49:26.920<br>
它其實跟這個 sin(x)，橙色這條線差很多啊，根本不像<br>
<br>
0:49:26.920,0:49:29.980<br>
但是，它在 (π/4) 2的附近<br>
<br>
0:49:29.980,0:49:32.740<br>
在這個地方附近，它是像的<br>
<br>
0:49:32.740,0:49:35.880<br>
因為，如果 x 很接近 (π/4) 的話<br>
<br>
0:49:35.880,0:49:39.700<br>
那後面這些項，平方項、三次方項這些都很小<br>
<br>
0:49:39.700,0:49:43.180<br>
所以就可以忽略它們，只考慮一次的部分<br>
<br>
0:49:44.120,0:49:47.760<br>
那這個 Taylor series 也可以是有好幾個參數的<br>
<br>
0:49:47.760,0:49:49.600<br>
如果今天有好幾個參數的話<br>
<br>
0:49:49.600,0:49:51.120<br>
那你就可以這樣子做<br>
<br>
0:49:51.120,0:49:55.780<br>
這個 h(x, y)，假設這個 function 有兩個參數<br>
<br>
0:49:55.780,0:49:59.480<br>
它在 x_0 和 y_0 附近<br>
<br>
0:49:59.480,0:50:03.820<br>
你可以把它寫成呢<br>
<br>
0:50:04.380,0:50:09.020<br>
這個 h(x, y)，你可以用 Taylor series 把它展開成這樣<br>
<br>
0:50:09.020,0:50:13.320<br>
就有 0 次的，有考慮 (x - x_0) 的<br>
<br>
0:50:13.320,0:50:15.420<br>
有考慮 (y - y_0) 的<br>
<br>
0:50:15.420,0:50:18.740<br>
還有考慮 (x - x_0)^2 跟 (y - y_0)^2 的<br>
<br>
0:50:18.740,0:50:21.920<br>
如果今天 x, y 很接近 x_0, y_0 的話<br>
<br>
0:50:21.920,0:50:24.320<br>
那平方項呢，就可以被消掉<br>
<br>
0:50:24.320,0:50:26.360<br>
就只剩這個部份而已<br>
<br>
0:50:26.960,0:50:29.980<br>
所以，今天 x, y 如果很接近 x_0, y_0 的話<br>
<br>
0:50:29.980,0:50:32.800<br>
那 h(x, y) 就可以寫成呢<br>
<br>
0:50:32.800,0:50:36.280<br>
約等於 h(x_0, y_0) 加上<br>
<br>
0:50:36.860,0:50:42.840<br>
(x - x_0) * (x_0, y_0) 對 x 做偏微分<br>
<br>
0:50:42.840,0:50:47.080<br>
(y - y_0) * (x_0, y_0) 對 y 做偏微分<br>
<br>
0:50:47.080,0:50:49.060<br>
這個偏微分的值，你不要看他這麼複雜<br>
<br>
0:50:49.060,0:50:53.000<br>
微分的值，它其實就是一個 constant 而已<br>
<br>
0:50:53.000,0:50:55.100<br>
就是一個常數項而已<br>
<br>
0:50:55.100,0:50:58.240<br>
這個是一個常數項，這個也是一個常數項<br>
<br>
0:50:58.760,0:51:02.680<br>
好，那如果我們今天考慮 Gradient Descent 的話<br>
<br>
0:51:02.680,0:51:04.880<br>
如果我們今天考慮我們剛才講的問題<br>
<br>
0:51:04.880,0:51:09.840<br>
如果，今天給我一個中心點，這是 a 跟 b<br>
<br>
0:51:09.840,0:51:13.480<br>
那我畫了一個很小很小的圓圈<br>
<br>
0:51:13.480,0:51:16.240<br>
紅色的圓圈，假設它是很小的<br>
<br>
0:51:16.240,0:51:18.800<br>
再這個紅色圓圈的範圍之內<br>
<br>
0:51:19.200,0:51:23.660<br>
我其實可以把 loss function 用 Taylor series 做簡化<br>
<br>
0:51:23.660,0:51:27.160<br>
我可以把 loss function, L(θ) 寫成<br>
<br>
0:51:27.160,0:51:36.640<br>
L(a, b) + θ_1 對 loss function 的偏微分，<br>
在 (a, b) 這個位置的偏微分 ，乘上 (θ_1 - a)<br>
<br>
0:51:36.640,0:51:43.160<br>
加上 θ_2 對 loss function 在 (a, b) 這個位置 的偏微分，再乘上 (θ_2 - b)<br>
<br>
0:51:43.160,0:51:48.140<br>
所以在紅色的圈圈內，loss function 可以寫成這樣子<br>
<br>
0:51:49.020,0:51:58.780<br>
那我們把 L(a,b)，L 用 (a, b) 代進去，<br>
它就是一個 constant，用一個 s 來表示<br>
<br>
0:51:59.400,0:52:02.800<br>
那 θ_1 對 L 的偏微分<br>
<br>
0:52:02.800,0:52:06.900<br>
在 (a, b) 這個位置，這也是一個 constant，<br>
所以我們用 u 來表示<br>
<br>
0:52:06.900,0:52:09.580<br>
這也是一個 constant，所以我們用 v 來表示<br>
<br>
0:52:09.580,0:52:11.860<br>
這樣這個式子呢，看起來就非常簡單了<br>
<br>
0:52:11.860,0:52:13.360<br>
所以在這個範圍之內<br>
<br>
0:52:13.920,0:52:17.260<br>
L 對 θ 跟 θ_1<br>
<br>
0:52:28.980,0:52:31.820<br>
所以呢，在紅色圈圈範圍內呢<br>
<br>
0:52:31.820,0:52:34.240<br>
這個式子是非常簡單的<br>
<br>
0:52:34.240,0:52:37.540<br>
就寫成左下角這個樣子<br>
<br>
0:52:38.440,0:52:44.180<br>
再來，如果告訴你說，紅色圈圈內的式子都長這個樣子<br>
<br>
0:52:44.180,0:52:49.980<br>
你能不能秒算，<br>
哪一個 θ_1 跟 θ_2 可以讓它的 loss 最小呢？<br>
<br>
0:52:49.980,0:52:52.300<br>
我相信你可以秒算這個結果<br>
<br>
0:52:52.300,0:52:54.780<br>
不過，我們還是很快地稍微看一下<br>
<br>
0:52:54.780,0:52:56.800<br>
好，L 寫成這樣<br>
<br>
0:52:56.800,0:53:01.760<br>
s, u, v 都是常數，我們就把它放在藍色的框框那裡面<br>
<br>
0:53:01.760,0:53:04.100<br>
不用管它值是多少<br>
<br>
0:53:04.100,0:53:06.300<br>
我們現在的問題，就是找<br>
<br>
0:53:07.060,0:53:12.000<br>
在紅色的圈圈內呢，找 θ_1 跟 θ_2 讓 loss 最小<br>
<br>
0:53:12.000,0:53:15.020<br>
那所謂的在紅色圈圈內的意思就是說<br>
<br>
0:53:15.020,0:53:18.000<br>
紅色圈圈的中心就是 a 跟 b<br>
<br>
0:53:18.000,0:53:23.200<br>
所以你這個 (θ_1 - a)^2 + (θ_2 - b)^2 ≦ d^2<br>
<br>
0:53:23.200,0:53:26.840<br>
他們要在這個紅色圈圈的範圍內<br>
<br>
0:53:26.840,0:53:29.700<br>
這件事情，其實就是秒算對不對<br>
<br>
0:53:29.700,0:53:32.880<br>
太簡單了，你一眼就可以看出來<br>
<br>
0:53:33.340,0:53:38.340<br>
如果你今天把 (θ_1 - a) 都用 △θ_1 表示<br>
<br>
0:53:38.340,0:53:42.780<br>
(θ_2 - b) 都用 △θ_2 來表示<br>
<br>
0:53:42.780,0:53:47.500<br>
s 你可以不用理它，因為它跟 θ 沒關係啊<br>
<br>
0:53:47.500,0:53:51.680<br>
所以你要找不同 θ 讓它值最小，不用管 s<br>
<br>
0:53:52.260,0:53:54.400<br>
好，如果我們看一下 L<br>
<br>
0:53:54.400,0:53:59.320<br>
你會發現說它是  u * △θ_1 + v * △θ_2<br>
<br>
0:53:59.320,0:54:01.460<br>
也就是說，它就好像是<br>
<br>
0:54:01.460,0:54:06.920<br>
它的值就是，有一個 vector ，叫做 (△θ_1, △θ_2)<br>
<br>
0:54:07.320,0:54:10.540<br>
有另外一個 vector，叫做 (u, v)<br>
<br>
0:54:10.940,0:54:14.640<br>
你把這個 vector 跟這個 vector 做 inner product<br>
<br>
0:54:14.640,0:54:18.000<br>
你就把 △θ_1 * u + θ_2 * v<br>
<br>
0:54:18.000,0:54:22.560<br>
你就得到這個值，如果我們忽略 s 的話，<br>
你就得到這個值<br>
<br>
0:54:22.560,0:54:26.700<br>
接下來的問題就是，如果我們要讓 L(θ) 最小<br>
<br>
0:54:27.000,0:54:33.720<br>
我們應該選擇什麼樣的 (△θ_1, △θ_2) 呢？<br>
<br>
0:54:33.720,0:54:36.660<br>
我們要選擇什麼樣的 (△θ_1, △θ_2)<br>
<br>
0:54:36.660,0:54:39.460<br>
我們才能夠讓 L(θ) 最小呢？<br>
<br>
0:54:39.460,0:54:47.660<br>
這個，太容易了，就是選正對面的，對不對？<br>
<br>
0:54:47.660,0:54:56.580<br>
如果我們今天把 (△θ_1, △θ_2) 轉成跟 (u, v) 這條反方向<br>
<br>
0:54:57.280,0:55:02.100<br>
然後，再把 (△θ_1, △θ_2) 的長度增長<br>
<br>
0:55:02.100,0:55:06.520<br>
我們把它轉到反方向，再把它伸長<br>
<br>
0:55:06.520,0:55:10.580<br>
長到極限，也就是長到這個紅色圈圈的邊緣<br>
<br>
0:55:11.080,0:55:14.420<br>
那這個 (△θ_1, △θ_2) 跟 (u, v)<br>
<br>
0:55:14.420,0:55:18.000<br>
它們做 inner product 的時候，它的值是最大的<br>
<br>
0:55:18.420,0:55:20.720<br>
所以，這告訴我們說<br>
<br>
0:55:20.720,0:55:24.840<br>
什麼樣的 (△θ_1, △θ_2) 可以讓 loss 的值最小呢？<br>
<br>
0:55:24.840,0:55:28.340<br>
就是它是 (u, v) 乘上負號<br>
<br>
0:55:28.340,0:55:31.300<br>
再乘上一個 scale<br>
<br>
0:55:31.300,0:55:34.560<br>
再乘上一個 constant，也就是說你要把 (△θ_1, △θ_2)<br>
<br>
0:55:34.560,0:55:40.880<br>
調整它的長度，長到正好頂到這個紅色圈圈的邊邊<br>
<br>
0:55:40.900,0:55:43.520<br>
這個時候呢，它算出來的 loss 是最小的<br>
<br>
0:55:43.520,0:55:49.100<br>
這一項應該跟長度是成正比的<br>
<br>
0:55:50.380,0:55:52.640<br>
所以呢，我們再整理一下式子<br>
<br>
0:55:52.640,0:55:57.700<br>
△θ_1 就是 (θ_1 - a)，△θ_2 就是 (θ_2 - b)<br>
<br>
0:55:57.700,0:56:00.860<br>
所以，如果我們今天要再紅色圈圈裡面<br>
<br>
0:56:00.860,0:56:03.540<br>
找一個 θ_1 跟 θ_2 讓 loss 最小的話<br>
<br>
0:56:03.840,0:56:08.800<br>
那怎麼做呢？那個最小的值，就是中心點 (a, b)<br>
<br>
0:56:08.800,0:56:13.500<br>
減掉某一個 constant 乘上 (u, v)<br>
<br>
0:56:13.500,0:56:17.260<br>
中心點 (a, b) 減掉某一個 constant 乘上 (u, v)<br>
<br>
0:56:18.800,0:56:22.000<br>
所以，我們就知道了這件事<br>
<br>
0:56:22.000,0:56:27.820<br>
那你接下來要做的事，就是把 (u, v) 帶進去<br>
<br>
0:56:27.820,0:56:30.000<br>
把它帶進去，就得到這樣子的式子<br>
<br>
0:56:30.000,0:56:32.380<br>
那這個式子，你就發現它其實<br>
<br>
0:56:32.380,0:56:36.280<br>
exactly 就是 Gradient Descent<br>
<br>
0:56:36.280,0:56:40.180<br>
對不對？我們做 Gradient Descent 的時候，就是找一個初始值<br>
<br>
0:56:40.180,0:56:43.340<br>
算每一個參數在初始值的地方的偏微分<br>
<br>
0:56:43.340,0:56:45.080<br>
把它排成一個 vector，就是 Gradient<br>
<br>
0:56:45.080,0:56:48.040<br>
然後再乘上某一個東西，叫做 learning rate，再把它減掉<br>
<br>
0:56:48.040,0:56:52.260<br>
所以這個式子，exactly 就是 Gradient Descent 的式子<br>
<br>
0:56:52.940,0:56:56.160<br>
但你要想想看，我們今天可以做這件事情<br>
<br>
0:56:56.160,0:56:58.160<br>
我們可以用這個方法，找一個最小值<br>
<br>
0:56:58.160,0:57:02.560<br>
它的前提是什麼？ 它的前提是<br>
<br>
0:57:02.560,0:57:06.540<br>
你的上面這個式子，要成立<br>
<br>
0:57:06.540,0:57:10.540<br>
Maclaurin series給你的這個 approximation 是夠精確的<br>
<br>
0:57:10.540,0:57:13.820<br>
什麼樣 Taylor series 給我們的 <br>
approximation 才夠精確呢？<br>
<br>
0:57:13.820,0:57:17.120<br>
當你今天畫出來的紅色圈圈夠小的時候<br>
<br>
0:57:17.120,0:57:21.980<br>
Taylor series 給我們的 approximation 才會夠精確<br>
<br>
0:57:21.980,0:57:24.820<br>
好，才會夠精確<br>
<br>
0:57:24.820,0:57:27.400<br>
所以，這個就告訴我們什麼？<br>
<br>
0:57:27.400,0:57:30.120<br>
這個告訴我們說，你這個紅色圈圈的半徑是小的<br>
<br>
0:57:30.120,0:57:31.780<br>
那這個 η，這個 learning rate<br>
<br>
0:57:31.780,0:57:34.760<br>
它跟紅色圈圈的半徑是成正比的<br>
<br>
0:57:34.760,0:57:36.500<br>
所以這個 learning rate 不能太大<br>
<br>
0:57:36.500,0:57:40.640<br>
你 learning rate 要很小，<br>
你這個 learning rate 無窮小的時候呢<br>
<br>
0:57:40.640,0:57:41.920<br>
這個式子才會成立<br>
<br>
0:57:41.920,0:57:45.160<br>
所以 Gradient Descent，如果你要讓你每次 update 參數的時候<br>
<br>
0:57:45.160,0:57:47.020<br>
你的 loss 都越來越小的話<br>
<br>
0:57:47.020,0:57:52.120<br>
其實，理論上你的 learning rate 要無窮小，<br>
你才能夠保證這件事情<br>
<br>
0:57:52.120,0:57:56.200<br>
雖然實作上，只要夠小就行了<br>
<br>
0:57:56.200,0:58:00.040<br>
所以，你會發現說，如果你的 learning rate 沒有設好<br>
<br>
0:58:00.040,0:58:02.940<br>
是有可能說，你每次 update 參數的時候<br>
<br>
0:58:02.940,0:58:04.360<br>
這個式子是不成立的<br>
<br>
0:58:04.360,0:58:08.960<br>
所以導致你做 Gradient Descent 的時候，<br>
你沒有辦法讓 loss 越來越小<br>
<br>
0:58:09.440,0:58:10.820<br>
那你會發現說<br>
<br>
0:58:10.820,0:58:13.720<br>
這個 L，它只考慮了<br>
<br>
0:58:14.360,0:58:16.720<br>
Taylor series 裡面的一次式<br>
<br>
0:58:16.720,0:58:18.480<br>
可不可以考慮二次式呢？<br>
<br>
0:58:18.480,0:58:22.860<br>
Taylor series 不是有二次、三次，還有很多嗎？<br>
<br>
0:58:22.860,0:58:25.300<br>
如果你把二次式考慮進來<br>
<br>
0:58:25.300,0:58:26.460<br>
你把二次式考慮進來<br>
<br>
0:58:26.460,0:58:31.280<br>
理論上，你的 learning rate 就可以設大一點<br>
<br>
0:58:31.280,0:58:34.560<br>
對不對，如果我們把二次式考慮進來<br>
<br>
0:58:34.560,0:58:36.580<br>
可不可以呢？是可以的<br>
<br>
0:58:36.580,0:58:40.820<br>
那有一些方法，我們今天沒有要講，<br>
是有考慮到二次式的<br>
<br>
0:58:40.820,0:58:42.460<br>
比如說，牛頓法這樣子<br>
<br>
0:58:42.460,0:58:45.840<br>
那在實作上，尤其是假設你在做 deep learning 的時候<br>
<br>
0:58:45.840,0:58:49.900<br>
這樣的方法，不見得太普及，不見得太 practical<br>
<br>
0:58:49.900,0:58:53.360<br>
為甚麼呢？因為你現在要算二次微分<br>
<br>
0:58:53.360,0:58:58.400<br>
甚至它還會包含一個 Hessian 的 matrix<br>
<br>
0:58:58.400,0:59:02.360<br>
和 Hessian matrix 的 inverse，總之，你會多很多運算<br>
<br>
0:59:02.360,0:59:04.980<br>
而這些運算，在做 deep learning 的時候呢<br>
<br>
0:59:04.980,0:59:07.420<br>
你是無法承受的<br>
<br>
0:59:07.420,0:59:11.340<br>
你用這個運算，來換你 update 的時候比較有效率<br>
<br>
0:59:11.340,0:59:13.100<br>
會覺得是不划算的<br>
<br>
0:59:13.100,0:59:16.320<br>
所以，今天如果在做，比如說，deep learning 的時候<br>
<br>
0:59:16.320,0:59:21.700<br>
通常，還是 Gradient Descent 是比較普及、主流的作法<br>
<br>
0:59:22.440,0:59:24.620<br>
上面如果你沒有聽懂的話，也沒關係<br>
<br>
0:59:24.620,0:59:29.220<br>
在最後一頁，我們要講的是 Gradient Descent 的限制<br>
<br>
0:59:29.480,0:59:32.220<br>
 Gradient Descent 有什麼樣的限制呢？<br>
<br>
0:59:32.220,0:59:37.200<br>
有一個大家都知道的是，<br>
它會卡在這個 local minimum 的地方<br>
<br>
0:59:37.200,0:59:40.380<br>
它會卡在 local minimum 的地方<br>
<br>
0:59:41.500,0:59:44.700<br>
所以，如果這是你的 error surface<br>
<br>
0:59:44.880,0:59:49.320<br>
那你從這個地方，當作你的初始值，去更新你的參數<br>
<br>
0:59:49.320,0:59:53.460<br>
最後走到一個微分值是 0，也就是 local minimum 的地方<br>
<br>
0:59:53.460,0:59:56.260<br>
你參數的更新，就停止了<br>
<br>
0:59:56.980,1:00:00.420<br>
但是，一般人就只知道這個問題而已<br>
<br>
1:00:00.420,1:00:02.240<br>
那其實還有別的問題<br>
<br>
1:00:02.240,1:00:06.860<br>
事實上，這個微分值是 0 的地方，<br>
並不是只有 local minimum 阿<br>
<br>
1:00:06.860,1:00:12.440<br>
對不對，settle point 也是微分值是 0<br>
<br>
1:00:12.440,1:00:14.940<br>
所以，你今天在參數 update 的時候<br>
<br>
1:00:14.940,1:00:20.380<br>
你也是有可能卡在一個不是 local minimum，<br>
但是微分值是 0 的地方<br>
<br>
1:00:20.380,1:00:23.140<br>
這件事情，也是有可能發生的<br>
<br>
1:00:23.820,1:00:28.140<br>
但是，這什麼卡在 local minimum 或微分值是 0 的地方啊<br>
<br>
1:00:28.140,1:00:31.120<br>
這都只是幻想啦<br>
<br>
1:00:31.240,1:00:33.520<br>
其實，真正的問題是這樣<br>
<br>
1:00:33.520,1:00:35.560<br>
你今天其實只要<br>
<br>
1:00:35.560,1:00:38.980<br>
你想想看，你 implement 作業一了<br>
<br>
1:00:38.980,1:00:44.120<br>
你幾時是真的算出來，那個微分值 exactly 等於 0 的時候<br>
<br>
1:00:44.120,1:00:45.440<br>
就把它停下來了<br>
<br>
1:00:45.440,1:00:49.700<br>
也就是，你最多就做微分值小於 10^(-6)<br>
<br>
1:00:49.700,1:00:52.160<br>
小於一個很小的值，你就把它停下來了，對不對？<br>
<br>
1:00:52.260,1:00:56.960<br>
但是，你怎麼知道，那個微分值算出來很小的時候<br>
<br>
1:00:56.960,1:00:59.440<br>
它就很接近 local minimum 呢？<br>
<br>
1:00:59.440,1:01:01.240<br>
不見得很接近 local minimum 阿<br>
<br>
1:01:01.240,1:01:06.340<br>
有可能，微分值算出來很小，<br>
但它其實是在一個高原的地方<br>
<br>
1:01:06.740,1:01:09.960<br>
而那高原的地方，微分值算出來很小，你就覺得說<br>
<br>
1:01:09.960,1:01:13.440<br>
哦，那這個一定就是很接近 local minimum<br>
<br>
1:01:13.440,1:01:15.600<br>
在 local minimum 附近，所以你就停下來了<br>
<br>
1:01:15.600,1:01:19.020<br>
因為你們真的很少有機會 exactly 微分值算出來是 0 嘛<br>
<br>
1:01:19.020,1:01:23.200<br>
對不對，你可能覺得說，<br>
微分算出來很小，就很接近 local minimum<br>
<br>
1:01:23.200,1:01:26.980<br>
你就把它停下來，那其實搞不好，它是一個高原的地方<br>
<br>
1:01:26.980,1:01:29.460<br>
它離那個 local minimum 還很遠啊<br>
<br>
1:01:29.460,1:01:31.640<br>
這也是有可能的<br>
<br>
1:01:32.380,1:01:35.300<br>
講到這邊，有人都會問我一個問題<br>
<br>
1:01:35.300,1:01:37.460<br>
這個問題，我很難回答，他說<br>
<br>
1:01:37.460,1:01:41.100<br>
你怎麼會不知道你是不是接近 local minimum 了呢？<br>
<br>
1:01:41.100,1:01:44.060<br>
我一眼就知道說 local minimum 在這邊阿<br>
<br>
1:01:45.180,1:01:47.220<br>
呵呵，你怎麼會不知道呢？<br>
<br>
1:01:47.220,1:01:49.800<br>
所以我覺得這些圖都沒有辦法<br>
表示 Gradient Descent 的精神<br>
<br>
1:01:49.800,1:01:51.800<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
