<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.960<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.960,0:00:05.200<br>
deep learning 現在非常的熱門<br>
<br>
0:00:05.200,0:00:06.980<br>
所以，它可以用在甚麼地方<br>
<br>
0:00:06.980,0:00:09.700<br>
我覺得真的還不需要多講<br>
<br>
0:00:09.700,0:00:11.940<br>
我覺得大家搞不好都知道得比我更多<br>
<br>
0:00:11.940,0:00:14.660<br>
我相信如果你隨便用 deep learning 當作關鍵字<br>
<br>
0:00:14.660,0:00:15.960<br>
胡亂 google 一下<br>
<br>
0:00:15.960,0:00:21.420<br>
你就可以找到一大堆的、exciting 的 result<br>
<br>
0:00:21.760,0:00:23.920<br>
所以，我們就直接用這個圖呢<br>
<br>
0:00:23.920,0:00:26.180<br>
來簡單地 summarize 一下這個趨勢<br>
<br>
0:00:26.180,0:00:29.300<br>
這個圖呢，是 google 的 Jeff Dean<br>
<br>
0:00:29.300,0:00:33.640<br>
它在 sigmoid 的一個 keynote speech 的一張投影片<br>
<br>
0:00:33.640,0:00:35.980<br>
那這個圖想要表達的事情是這樣<br>
<br>
0:00:35.980,0:00:37.860<br>
橫軸代表時間<br>
<br>
0:00:37.860,0:00:42.020<br>
從 2012 的 Q1 到 2016<br>
<br>
0:00:42.020,0:00:45.180<br>
縱軸，代表說在 google 內部<br>
<br>
0:00:45.180,0:00:47.620<br>
有用到 deep learning 的 project 的數目<br>
<br>
0:00:47.620,0:00:49.740<br>
那可以發現說，這個趨勢<br>
<br>
0:00:49.740,0:00:53.220<br>
是從幾乎 0 到超過 2000<br>
<br>
0:00:53.220,0:00:56.540<br>
所以，這個使用 deep learning 的 project 數目呢<br>
<br>
0:00:56.540,0:00:59.440<br>
是指數成長的<br>
<br>
0:00:59.440,0:01:01.840<br>
那如果你看它的應用的話<br>
<br>
0:01:01.840,0:01:05.560<br>
它有各種不同的應用，涵蓋幾乎你可以想像的領域<br>
<br>
0:01:05.560,0:01:08.160<br>
比如說，Android, Apps, drug discovery<br>
<br>
0:01:08.160,0:01:11.580<br>
Gmail, Image understanding, <br>
Natural language understanding<br>
<br>
0:01:11.580,0:01:14.160<br>
Speech, 各種不同的應用<br>
<br>
0:01:14.160,0:01:17.280<br>
通通有用到 deep learning<br>
<br>
0:01:17.280,0:01:21.220<br>
那 deep learning 可以做的應用實在是太多了<br>
<br>
0:01:21.220,0:01:23.960<br>
我們這邊就不要花時間來講這些東西<br>
<br>
0:01:23.960,0:01:25.620<br>
如果要講這些東西的話呢<br>
<br>
0:01:25.620,0:01:27.980<br>
再用兩、三堂課，其實也是講不完的<br>
<br>
0:01:27.980,0:01:31.600<br>
那這個隨便 google 就有的東西呢，我們就不要再講了<br>
<br>
0:01:31.600,0:01:33.760<br>
我們來稍微回顧一下<br>
<br>
0:01:33.760,0:01:35.600<br>
這個 deep learning 的歷史<br>
<br>
0:01:35.600,0:01:40.340<br>
在歷史上呢，它是有經過好幾次的沉沉浮浮<br>
<br>
0:01:40.640,0:01:42.540<br>
首先，在 1958 年<br>
<br>
0:01:42.540,0:01:45.840<br>
有一個技術叫做 Perceptron 被提出來<br>
<br>
0:01:45.840,0:01:50.000<br>
Perceptron 這個技術，它也是一個 linear 的 model<br>
<br>
0:01:50.000,0:01:51.680<br>
它非常非常像<br>
<br>
0:01:51.680,0:01:56.560<br>
我們在前一堂課講的 Logistic Regression<br>
<br>
0:01:56.560,0:01:59.800<br>
它只是沒有 sigmoid 的部分<br>
<br>
0:01:59.800,0:02:01.640<br>
但它還是 linear 的 model<br>
<br>
0:02:01.640,0:02:04.620<br>
那一開始，有人提出這個想法的時候<br>
<br>
0:02:04.620,0:02:06.780<br>
提出 Perceptron 這個想法的時候呢<br>
<br>
0:02:06.780,0:02:09.340<br>
大家非常非常的興奮<br>
<br>
0:02:09.340,0:02:12.680<br>
這個是 Frank Rosenblatt<br>
<br>
0:02:12.680,0:02:17.340<br>
在海軍的 project 裡面提出來的<br>
<br>
0:02:17.340,0:02:19.660<br>
他一開始提出來的時候，大家覺得非常非常興奮<br>
<br>
0:02:19.660,0:02:22.280<br>
那個時候要做 Perceptron 的運算呢<br>
<br>
0:02:22.280,0:02:25.820<br>
如果你看 Bishop 的教科書，裡面有一張的機器呢<br>
<br>
0:02:25.820,0:02:28.420<br>
看起來是房間那麼大的機器<br>
<br>
0:02:28.420,0:02:33.660<br>
那個時候，New York Times 據說還寫了一個報導說<br>
<br>
0:02:33.660,0:02:36.220<br>
從此以後，人工智慧就要產生了<br>
<br>
0:02:36.220,0:02:39.320<br>
電腦可以自己學習，就像現在這個樣子<br>
<br>
0:02:39.320,0:02:42.320<br>
可是後來呢<br>
<br>
0:02:42.320,0:02:44.760<br>
MIT 有一個<br>
<br>
0:02:44.760,0:02:48.540<br>
有人呢，就寫了一本教科書<br>
<br>
0:02:48.540,0:02:51.840<br>
這本教科書的名字就叫做 Perceptron<br>
<br>
0:02:51.840,0:02:55.080<br>
這本教科書裡面，他就指出了 linear 的 model<br>
<br>
0:02:55.080,0:02:58.100<br>
是有極限的，就像我們在上一張投影片講的<br>
<br>
0:02:58.100,0:03:01.360<br>
linear model 有很多事都辦不到<br>
<br>
0:03:01.360,0:03:03.740<br>
就像是我們剛才舉的那麼簡單的 example<br>
<br>
0:03:03.740,0:03:05.600<br>
它都辦不到<br>
<br>
0:03:05.600,0:03:07.800<br>
然後大家希望就破滅了<br>
<br>
0:03:07.800,0:03:11.900<br>
然後，當時在 1958 年，剛提出 Perceptron 的時候呢<br>
<br>
0:03:11.900,0:03:14.680<br>
有各種驚人的 application<br>
<br>
0:03:14.680,0:03:17.060<br>
就像現在 deep learning 一樣，有人就說<br>
<br>
0:03:17.060,0:03:18.800<br>
我用了 Perceptron<br>
<br>
0:03:18.800,0:03:21.420<br>
結果我可以分辨說，給我一張照片<br>
<br>
0:03:21.420,0:03:25.380<br>
那照片裡面有坦克，或是一般的卡車<br>
<br>
0:03:25.380,0:03:28.820<br>
那它可以正確地分辨說，那些照片裡面是坦克<br>
<br>
0:03:28.820,0:03:30.160<br>
那些照片裡面是卡車<br>
<br>
0:03:30.160,0:03:33.200<br>
就算那些坦克呢，被藏在叢林裡面<br>
<br>
0:03:33.200,0:03:36.260<br>
就算被樹木蓋住呢，也偵測得出來<br>
<br>
0:03:36.260,0:03:39.180<br>
那其他人就覺得說，太厲害了，這就是人工智慧<br>
<br>
0:03:39.180,0:03:42.720<br>
可是既然 Perceptron 有這樣的<br>
<br>
0:03:42.720,0:03:45.780<br>
後來大家就發現說 Perceptron 其實很多 limitation<br>
<br>
0:03:45.780,0:03:51.600<br>
那怎麼可能分辨卡車跟坦克，這麼複雜的 object 呢？<br>
<br>
0:03:51.820,0:03:54.980<br>
所以，有人就去把那個 data，再拿出來看了一下<br>
<br>
0:03:54.980,0:03:58.220<br>
就發現說，原來卡車跟坦克的照片<br>
<br>
0:03:58.220,0:03:59.840<br>
是在不同的日子所拍攝的<br>
<br>
0:03:59.840,0:04:02.380<br>
所以，一天是雨天，一天是晴天<br>
<br>
0:04:02.380,0:04:04.700<br>
所以，這個照片本身的亮度就不一樣<br>
<br>
0:04:04.700,0:04:07.120<br>
所以，Perceptron 它唯一抓到的東西<br>
<br>
0:04:07.120,0:04:08.660<br>
就只有亮度而已<br>
<br>
0:04:10.400,0:04:14.220<br>
所以大家就崩潰了，這個方法的名字就臭掉了<br>
<br>
0:04:17.200,0:04:20.100<br>
後來就有人想說，既然<br>
<br>
0:04:20.100,0:04:25.140<br>
一個 Perceptron 不行，我能不能連接很多個 Perceptron<br>
<br>
0:04:25.140,0:04:27.160<br>
就像我們剛才講的，Logistic Regression<br>
<br>
0:04:27.160,0:04:30.400<br>
都接在一起，它應該就很 powerful<br>
<br>
0:04:30.400,0:04:33.920<br>
這個就叫做 Multi-layer 的 Perceptron<br>
<br>
0:04:33.920,0:04:37.140<br>
事實上，1980 年代<br>
<br>
0:04:37.140,0:04:40.720<br>
Multi-layer 的 Perceptron 它的技術<br>
<br>
0:04:40.720,0:04:43.040<br>
基本上在 1980 年代的時候呢<br>
<br>
0:04:43.040,0:04:44.860<br>
就都開發得差不多了<br>
<br>
0:04:44.860,0:04:47.120<br>
那個時候已經開發完的技術<br>
<br>
0:04:47.120,0:04:49.100<br>
其實就跟今天的 deep learning<br>
<br>
0:04:49.100,0:04:52.740<br>
是沒有太 significant 的差別的<br>
<br>
0:04:53.340,0:04:57.100<br>
然後，有一個關鍵的技術是 1986 年的時候<br>
<br>
0:04:57.100,0:05:00.140<br>
Hinton propose 的 Backpropagation<br>
<br>
0:05:00.140,0:05:02.440<br>
那其實同時也有很多人 propose Backpropagation<br>
<br>
0:05:02.440,0:05:05.600<br>
只是大部分人把這個 credit 歸給 Hinton 的 paper<br>
<br>
0:05:05.600,0:05:07.720<br>
他那篇 paper 是比較著名的<br>
<br>
0:05:07.720,0:05:10.740<br>
但是，在這個時候遇到的問題就是<br>
<br>
0:05:10.740,0:05:14.840<br>
通常超過 3 個 layer 的 neural network<br>
<br>
0:05:14.840,0:05:17.520<br>
你就 train 不出好的結果<br>
<br>
0:05:17.520,0:05:21.060<br>
通常一個還可以，再多你就 train 不出好的結果了<br>
<br>
0:05:21.060,0:05:23.200<br>
那後來在 1989 年<br>
<br>
0:05:23.200,0:05:26.060<br>
有人就發現了一個理論<br>
<br>
0:05:26.060,0:05:28.300<br>
就是說，一個 hidden layer<br>
<br>
0:05:28.300,0:05:29.760<br>
其實就可以<br>
<br>
0:05:29.760,0:05:32.860<br>
model 任何可能的 function<br>
<br>
0:05:32.860,0:05:35.940<br>
了解嗎？只要一個 neural network 有一個 hidden layer<br>
<br>
0:05:35.940,0:05:38.440<br>
它就可以是任何的 function<br>
<br>
0:05:38.440,0:05:42.100<br>
它就已經夠強了，所以根本沒有必要<br>
<br>
0:05:42.100,0:05:43.940<br>
疊很多個 hidden layer<br>
<br>
0:05:43.940,0:05:47.680<br>
所以，這個 Multi-layer 的 Perceptron 的方法，又臭掉了<br>
<br>
0:05:47.680,0:05:51.500<br>
然後，大家就比較喜歡做 SVM<br>
<br>
0:05:51.500,0:05:56.600<br>
那這個方法就臭掉了，據說那一陣子呢<br>
<br>
0:05:56.600,0:05:59.200<br>
Multi-layer 的 Perceptron 這個方法<br>
<br>
0:05:59.200,0:06:02.140<br>
也有人叫它 neural network 這個方法<br>
<br>
0:06:02.140,0:06:03.980<br>
它就像是一個髒話一樣<br>
<br>
0:06:03.980,0:06:07.020<br>
寫在 paper 裡面，那個 paper 一定被 reject<br>
<br>
0:06:10.760,0:06:12.980<br>
後來呢<br>
<br>
0:06:12.980,0:06:14.660<br>
有人就想到一個突破的點<br>
<br>
0:06:14.660,0:06:16.600<br>
這個突破的點，關鍵的地方就是<br>
<br>
0:06:16.600,0:06:21.080<br>
把它改個名字，這個方法已經臭掉了<br>
<br>
0:06:21.080,0:06:24.000<br>
只好改一個名字，把它改成 deep learning<br>
<br>
0:06:24.000,0:06:25.940<br>
整個就潮起來了<br>
<br>
0:06:25.940,0:06:33.480<br>
所以我講說，改名字其實是有很大的力量的<br>
<br>
0:06:33.480,0:06:35.140<br>
大家現在都不想念博士<br>
<br>
0:06:35.140,0:06:38.360<br>
我們把博士改個名字，大家都想唸了<br>
<br>
0:06:38.360,0:06:41.140<br>
下次跟系主任建議一下<br>
<br>
0:06:41.140,0:06:45.820<br>
很多人覺得說，有一個關鍵的技術是<br>
<br>
0:06:45.820,0:06:48.100<br>
應該在 2006 年提的<br>
<br>
0:06:48.100,0:06:51.380<br>
用 Restricted Boltzmann Machine 做 initialization<br>
<br>
0:06:51.380,0:06:54.720<br>
很多人覺得說，這是個突破這樣<br>
<br>
0:06:54.720,0:06:57.300<br>
甚至有一陣子呢<br>
<br>
0:06:57.300,0:07:00.640<br>
大家的認知是，到底 deep learning 跟<br>
<br>
0:07:00.640,0:07:03.420<br>
1980 年代的 Multi-layer Perceptron<br>
<br>
0:07:03.420,0:07:06.420<br>
有何不同呢？它的不同之處在於<br>
<br>
0:07:06.420,0:07:09.720<br>
如果你有用 RBM 做 initialization<br>
<br>
0:07:09.720,0:07:12.180<br>
你在做 Gradient Descent 的時候，不是要找一個<br>
<br>
0:07:12.180,0:07:16.020<br>
初始的值嗎？如果你是用 RBM 找的，叫 deep learning<br>
<br>
0:07:16.020,0:07:21.500<br>
你沒有用 RBM 找的，是傳統的、<br>
1980 年代的  Multi-layer Perceptron<br>
<br>
0:07:21.500,0:07:24.640<br>
後來呢，大家逐漸意識到說<br>
<br>
0:07:24.640,0:07:27.320<br>
因為 RBM 這個方法呢<br>
<br>
0:07:27.320,0:07:30.020<br>
非常的複雜，假設沒有<br>
<br>
0:07:30.020,0:07:32.000<br>
假設你是 machine learning 的初學者的話<br>
<br>
0:07:32.000,0:07:34.160<br>
你看這個 paper，我相信你是看不懂的<br>
<br>
0:07:34.160,0:07:36.840<br>
如果我們今天要講 RBM<br>
<br>
0:07:36.840,0:07:39.600<br>
要從現在開始<br>
<br>
0:07:39.600,0:07:42.660<br>
假設用我們上課那些知識，講到讓你聽懂<br>
<br>
0:07:42.660,0:07:46.520<br>
要再另外多講 3 周這樣子<br>
<br>
0:07:46.520,0:07:49.920<br>
它是有用到一些比較深的理論<br>
<br>
0:07:49.920,0:07:52.420<br>
大家覺得說，哇！這個這麼複雜<br>
<br>
0:07:52.420,0:07:55.040<br>
一定就是非常的 powerful 阿<br>
<br>
0:07:55.040,0:07:57.280<br>
而且它不是 Neural network base 的方法<br>
<br>
0:07:57.280,0:07:58.600<br>
它是 graphical model<br>
<br>
0:07:58.600,0:08:01.360<br>
它這個大家覺得說，這個這麼複雜，我們看都看不懂<br>
<br>
0:08:01.360,0:08:02.800<br>
這個一定是有用的<br>
<br>
0:08:02.800,0:08:05.500<br>
後來大家逐漸試來試去以後，就發現說<br>
<br>
0:08:05.500,0:08:07.500<br>
這招其實沒什麼用這樣子<br>
<br>
0:08:07.500,0:08:11.340<br>
你可以發現說，如果你讀 deep learning 的文獻<br>
<br>
0:08:11.340,0:08:17.220<br>
現在已經不太有人用 RBM 做 initialization 了<br>
<br>
0:08:17.220,0:08:19.660<br>
因為這一招帶給我們的幫助呢<br>
<br>
0:08:19.660,0:08:21.740<br>
並沒有說特別大<br>
<br>
0:08:21.740,0:08:25.720<br>
連 Hinton 自己都知道這點，<br>
他在某篇 paper 裡面提過這件事情<br>
<br>
0:08:25.720,0:08:28.960<br>
只是大家可能都沒有注意到那篇 paper 就是了<br>
<br>
0:08:29.740,0:08:34.680<br>
所以說，但是他有個最強的地方<br>
<br>
0:08:34.680,0:08:36.640<br>
它最強的地方就是它讓大家<br>
<br>
0:08:36.640,0:08:40.560<br>
重新、再一次對這個 model 有了興趣<br>
<br>
0:08:40.560,0:08:43.180<br>
因為它很複雜，所以大家就會開始想要研究<br>
<br>
0:08:43.180,0:08:46.300<br>
deep learning 是什麼樣的東西，花很多力氣去研究<br>
<br>
0:08:46.300,0:08:48.740<br>
所以，其實我聽過有一個<br>
<br>
0:08:48.740,0:08:51.680<br>
google 的人對 RBM 的評論<br>
<br>
0:08:51.680,0:08:54.780<br>
他說這個方法就是石頭湯裡面的石頭<br>
<br>
0:08:54.780,0:08:56.800<br>
石頭湯的故事大家聽過嗎？<br>
<br>
0:08:56.800,0:08:59.140<br>
就有一個人說，我要煮一碗石頭湯<br>
<br>
0:08:59.140,0:09:02.300<br>
有一個士兵，他到一個村莊裡面借宿<br>
<br>
0:09:02.300,0:09:06.140<br>
然後，他說：我要煮一碗石頭湯<br>
<br>
0:09:06.140,0:09:08.920<br>
然後，大家都說：你要怎麼煮一碗石頭湯？<br>
<br>
0:09:08.920,0:09:10.340<br>
我用石頭就可以了<br>
<br>
0:09:10.340,0:09:13.460<br>
就用石頭煮一碗湯，如果再加點鹽就更好了<br>
<br>
0:09:13.460,0:09:14.220<br>
就加點鹽<br>
<br>
0:09:14.220,0:09:16.240<br>
再加點米就更好了，再加點米<br>
<br>
0:09:16.240,0:09:18.320<br>
再加點菜就更好了，再加點菜<br>
<br>
0:09:18.320,0:09:19.240<br>
然後，就煮了一鍋湯<br>
<br>
0:09:19.240,0:09:22.000<br>
大家就覺得好好喝，用石頭就可以煮湯這樣<br>
<br>
0:09:22.000,0:09:23.760<br>
但是，那個石頭其實並沒有什麼作用<br>
<br>
0:09:23.760,0:09:27.820<br>
所以，其實 RBM 就類似這種東西<br>
<br>
0:09:34.020,0:09:38.240<br>
然後，我覺得有一個關鍵的突破是在 09 年的時候<br>
<br>
0:09:38.240,0:09:42.780<br>
我們，知道要用 GPU 來加速<br>
<br>
0:09:42.780,0:09:46.200<br>
這件事情還頗關鍵的，過去如果你做 deep learning<br>
<br>
0:09:46.200,0:09:48.480<br>
train 一次，一周就過去了<br>
<br>
0:09:48.480,0:09:50.800<br>
然後，結果實驗又失敗，你就不會想做第 2 次了<br>
<br>
0:09:52.240,0:09:55.680<br>
其實，我小時候，其實也不是小時候啦<br>
<br>
0:09:55.680,0:09:59.840<br>
我多年前有跟一個學弟試著想要做 deep learning<br>
<br>
0:09:59.840,0:10:01.580<br>
可是，那個時候還不知道疊很多層<br>
<br>
0:10:01.580,0:10:02.440<br>
那個時候大家都疊一層<br>
<br>
0:10:02.440,0:10:05.080<br>
直覺想法就知道說，一定要疊很多層阿<br>
<br>
0:10:05.080,0:10:06.200<br>
怎麼會只有疊一層？<br>
<br>
0:10:06.200,0:10:08.420<br>
所以想說，我們來把它疊很多層吧<br>
<br>
0:10:08.420,0:10:11.440<br>
然後，我們就開始做，train 一次要一周<br>
<br>
0:10:11.440,0:10:12.860<br>
train 完以後，結果沒有比較好<br>
<br>
0:10:12.860,0:10:14.960<br>
就沒有人想要再繼續做下去了<br>
<br>
0:10:14.960,0:10:17.160<br>
本來想找專題生做，專題生也都不想做<br>
<br>
0:10:17.160,0:10:20.920<br>
就沒有人要做，這個題目就沒有人要做了<br>
<br>
0:10:20.920,0:10:23.980<br>
那個時候我們把他好好做出來的話，我們現在就發了<br>
<br>
0:10:27.660,0:10:29.680<br>
不過那時候，我們不知道要用 GPU<br>
<br>
0:10:29.680,0:10:32.180<br>
所以，train 一次要一周，想要做出來也是很難<br>
<br>
0:10:32.180,0:10:34.600<br>
現在有 GPU 以後，本來要一周的東西<br>
<br>
0:10:34.600,0:10:37.940<br>
你可能幾個小時，就可以馬上看到結果了<br>
<br>
0:10:37.940,0:10:41.240<br>
那在 11 年的時候<br>
<br>
0:10:41.240,0:10:44.500<br>
這個方法被引入到語音辨識裡面<br>
<br>
0:10:44.500,0:10:47.780<br>
語音辨識發現說，這招果然很有用<br>
<br>
0:10:47.780,0:10:50.460<br>
然後，開始瘋狂地用 deep learning 的技術<br>
<br>
0:10:50.460,0:10:51.980<br>
到 12 年的時候呢<br>
<br>
0:10:51.980,0:10:53.260<br>
這個 deep learning 的技術<br>
<br>
0:10:53.260,0:10:55.760<br>
贏了一個很重要的 image 的比賽<br>
<br>
0:10:55.760,0:11:00.700<br>
所以，在 image 那邊的人<br>
也瘋狂地用 deep learning 的技術<br>
<br>
0:11:00.700,0:11:03.500<br>
其實，deep learning 的技術並沒有<br>
<br>
0:11:03.500,0:11:06.500<br>
很複雜，它其實非常簡單<br>
<br>
0:11:06.500,0:11:10.340<br>
我們之前講說，machine learning 就是 3 個 step<br>
<br>
0:11:10.340,0:11:14.400<br>
那其實 deep learning 也一樣就是，這 3 個 step<br>
<br>
0:11:14.400,0:11:18.040<br>
講說 deep learning 就這 3 個 step<br>
<br>
0:11:18.040,0:11:20.760<br>
就好像是在說，把那個大象放進冰箱一樣<br>
<br>
0:11:20.760,0:11:22.280<br>
大象放進冰箱，大家知道嗎？<br>
<br>
0:11:22.280,0:11:24.500<br>
把門打開，把大象趕進去<br>
<br>
0:11:24.500,0:11:27.820<br>
把冰箱門關起來，就把大象放進冰箱了<br>
<br>
0:11:27.820,0:11:30.260<br>
3 個 step 聽起來就像這樣子<br>
<br>
0:11:31.300,0:11:34.200<br>
deep learning 是很簡單的<br>
<br>
0:11:34.200,0:11:39.480<br>
你其實可以非常快的了解，其實你可以秒懂它<br>
<br>
0:11:40.660,0:11:42.240<br>
其實在 deep learning 裡面<br>
<br>
0:11:42.240,0:11:44.200<br>
我們說，在 machine learning 裡面第一個 step<br>
<br>
0:11:44.200,0:11:46.180<br>
就是要 define 一個 function<br>
<br>
0:11:46.180,0:11:50.680<br>
這個 function，其實就是一個 Neural network<br>
<br>
0:11:51.100,0:11:54.420<br>
那這個 Neural network 是<br>
<br>
0:11:54.880,0:11:57.420<br>
是甚麼呢？我們剛才已經講說呢<br>
<br>
0:11:57.420,0:12:02.540<br>
我們把這個 Logistic Regression 呢<br>
<br>
0:12:02.540,0:12:05.100<br>
前後 content 在一起<br>
<br>
0:12:05.100,0:12:07.620<br>
然後把一個 Logistic Regression 稱之為 Neuron<br>
<br>
0:12:07.620,0:12:08.980<br>
整個稱之為 Neural Network<br>
<br>
0:12:08.980,0:12:11.820<br>
我們其實就得到了一個 Neural Network<br>
<br>
0:12:11.820,0:12:14.420<br>
那我們可以用不同的方法<br>
<br>
0:12:14.420,0:12:17.320<br>
來連接這些 Neural Network<br>
<br>
0:12:17.320,0:12:19.840<br>
我們用不同的方法來連接這些 Neural Network<br>
<br>
0:12:19.840,0:12:23.500<br>
我們就得到了不同的 structure<br>
<br>
0:12:23.500,0:12:24.980<br>
在這個 Neural Network 裡面<br>
<br>
0:12:24.980,0:12:27.440<br>
我們有一大堆的 Logistic Regression<br>
<br>
0:12:27.440,0:12:30.500<br>
每個 Logistic Regression，它都有自己的 weight<br>
<br>
0:12:30.500,0:12:31.820<br>
跟自己的 bias<br>
<br>
0:12:31.820,0:12:34.080<br>
這些 weight 跟 bias 集合起來<br>
<br>
0:12:34.080,0:12:36.380<br>
就是這個 network 的 parameter<br>
<br>
0:12:36.380,0:12:39.300<br>
我們這邊用 θ 來描述它<br>
<br>
0:12:39.300,0:12:42.680<br>
那這些 Logistic Regression<br>
<br>
0:12:42.680,0:12:45.320<br>
或這些 Neuron<br>
<br>
0:12:45.320,0:12:48.860<br>
我們應該怎麼把它接起來呢<br>
<br>
0:12:48.860,0:12:50.600<br>
我們應該怎麼把它接起來呢<br>
<br>
0:12:50.600,0:12:52.800<br>
有各種不同的方式<br>
<br>
0:12:52.800,0:12:55.980<br>
怎麼連接，其實是你手動去設計的<br>
<br>
0:12:55.980,0:12:58.040<br>
是你手動去連接的<br>
<br>
0:12:58.040,0:12:59.660<br>
最常見的連接方式呢？<br>
<br>
0:12:59.660,0:13:03.220<br>
叫做 Fully Connected Feedforward Network<br>
<br>
0:13:03.220,0:13:06.200<br>
在 Fully Connected Feedforward Network 裡面呢<br>
<br>
0:13:06.200,0:13:10.900<br>
你就把你的 Neuron 排成一排一排<br>
<br>
0:13:10.900,0:13:14.480<br>
這邊有 6 個 Neuron，就兩個兩個一排<br>
<br>
0:13:14.480,0:13:17.680<br>
然後，每一組 Neuron，它都有一組 weight<br>
<br>
0:13:17.680,0:13:19.200<br>
都有一組 bias<br>
<br>
0:13:19.200,0:13:23.400<br>
那這個 weight 跟 bias 是根據 training data 去找出來的<br>
<br>
0:13:23.400,0:13:27.880<br>
假設上面這個藍色 Neuron <br>
它的 weight 是 1, -2，它的 bias 是 1<br>
<br>
0:13:27.880,0:13:31.940<br>
下面呢，它的 weight 是 -1, 1，它的 bias 是 0<br>
<br>
0:13:32.200,0:13:34.480<br>
假設我們現在的輸入<br>
<br>
0:13:34.480,0:13:36.000<br>
是 1 跟 -1<br>
<br>
0:13:36.000,0:13:39.100<br>
那這兩個藍色的 Neuron 它的 output 是甚麼呢？<br>
<br>
0:13:39.100,0:13:43.600<br>
這個做一下小學生會做的運算，你就可以得到答案<br>
<br>
0:13:43.600,0:13:47.560<br>
1*1 + (-1)*(-2)，再加上 bias 1<br>
<br>
0:13:47.560,0:13:49.560<br>
通過 sigmoid function 以後<br>
<br>
0:13:49.560,0:13:53.000<br>
你得到的結果呢，就是 0.98<br>
<br>
0:13:53.000,0:13:57.360<br>
那下面呢，你把 1*(-1)、(-1)*1 + 0<br>
<br>
0:13:57.360,0:14:01.040<br>
再通過 sigmoid function 以後，你就得到 0.12<br>
<br>
0:14:01.040,0:14:05.540<br>
接下來，假設這一個 structure 裡面的每一個 neuron<br>
<br>
0:14:05.540,0:14:09.560<br>
它的 weight 跟 bias，我們都是知道的<br>
<br>
0:14:09.560,0:14:12.020<br>
那我們就可以反覆進行剛才的運算<br>
<br>
0:14:12.020,0:14:18.360<br>
1 跟 -1 通過這兩個 neuron，變成 0.98 跟 0.12<br>
<br>
0:14:18.360,0:14:25.260<br>
再通過這兩個 neuron，變成 0.86 跟 0.11<br>
<br>
0:14:25.260,0:14:31.020<br>
再通過這兩個 neuron，得到 0.62 跟 0.83<br>
<br>
0:14:31.020,0:14:32.780<br>
所以，輸入 1 跟 -1<br>
<br>
0:14:32.780,0:14:34.980<br>
經過一串很複雜的轉換以後呢<br>
<br>
0:14:34.980,0:14:38.400<br>
就得到 0.62 跟 0.83<br>
<br>
0:14:38.400,0:14:42.480<br>
那如果你輸入是 0 跟 0 的話<br>
<br>
0:14:42.480,0:14:45.380<br>
你輸入 0 跟 0 的話，你得到的 output 就是<br>
<br>
0:14:45.380,0:14:50.380<br>
經過一番一模一樣的運算，你得到的是 0.51, 0.85<br>
<br>
0:14:50.380,0:14:53.320<br>
所以，一個 neural network 你就可以把它看作是<br>
<br>
0:14:53.320,0:14:55.160<br>
一個 function<br>
<br>
0:14:55.160,0:14:57.240<br>
如果一個 neural network 裡面的參數<br>
<br>
0:14:57.240,0:14:59.160<br>
weight 跟 bias 我們都知道的話<br>
<br>
0:14:59.160,0:15:01.180<br>
它就是一個 function<br>
<br>
0:15:01.180,0:15:05.700<br>
它的 input 是一個 vector，它的 output 是另一個 vector<br>
<br>
0:15:05.700,0:15:08.360<br>
舉例來說，我們剛才看到說 input 是 [1, -1]<br>
<br>
0:15:08.360,0:15:10.320<br>
output 是 [0.62, 0.83]<br>
<br>
0:15:10.320,0:15:15.720<br>
input 是 [0, 0]，output 是 [0.51, 0.85]<br>
<br>
0:15:15.720,0:15:19.880<br>
所以，一個 network，如果我們已經把參數設上去的話<br>
<br>
0:15:19.880,0:15:21.940<br>
它就是一個 function<br>
<br>
0:15:21.940,0:15:24.620<br>
如果我們今天還不知道參數<br>
<br>
0:15:24.620,0:15:27.580<br>
我只是定出了這個 network 的 structure<br>
<br>
0:15:27.580,0:15:30.760<br>
我只是決定好說，這些 neuron 間<br>
<br>
0:15:30.760,0:15:33.400<br>
我們要怎麼連接在一起<br>
<br>
0:15:33.400,0:15:35.920<br>
這樣子的一個 network structure<br>
<br>
0:15:35.920,0:15:39.440<br>
它其實就是 define 了一個 function set，對不對？<br>
<br>
0:15:39.440,0:15:43.820<br>
我們可以給這個 network 設不同的參數<br>
<br>
0:15:43.820,0:15:45.300<br>
它就變成不同的 function<br>
<br>
0:15:45.300,0:15:47.800<br>
把這些可能的 function 通通集合起來<br>
<br>
0:15:47.800,0:15:51.260<br>
我們就得到了一個 function set<br>
<br>
0:15:51.260,0:15:53.600<br>
所以，一個 neural network 你還沒有認參數<br>
<br>
0:15:53.600,0:15:55.740<br>
你只是把它架構架起來<br>
<br>
0:15:55.740,0:15:57.880<br>
你決定這些 neuron 要怎麼連接<br>
<br>
0:15:57.880,0:16:01.080<br>
你把連接的圖，畫出來的時候<br>
<br>
0:16:01.080,0:16:04.080<br>
你其實就決定了一個 function set<br>
<br>
0:16:04.080,0:16:06.040<br>
這跟我們之前做的東西都是一樣的<br>
<br>
0:16:06.040,0:16:10.040<br>
我們之前也是做 Logistic Regression, Linear Regression<br>
<br>
0:16:10.040,0:16:12.020<br>
我們都是也決定了一個 function set<br>
<br>
0:16:12.020,0:16:16.180<br>
那這邊呢，我們也只是換一個方式，來決定 function set<br>
<br>
0:16:16.180,0:16:18.760<br>
只是如果我們用 neural network 決定 function set 的時候<br>
<br>
0:16:18.760,0:16:21.160<br>
你的 function set 是比較大的<br>
<br>
0:16:21.160,0:16:24.220<br>
它包含了很多原來你做 Logistic Regression<br>
<br>
0:16:24.220,0:16:27.560<br>
做 Linear Regression 所沒有辦法包含的 function<br>
<br>
0:16:29.400,0:16:34.420<br>
那剛才講的是一個比較簡單的例子<br>
<br>
0:16:34.420,0:16:36.200<br>
在這個例子裡面呢<br>
<br>
0:16:36.200,0:16:38.320<br>
我們把 neuron 分成一排一排的<br>
<br>
0:16:38.320,0:16:41.480<br>
然後說每一排的 neuron 都兩兩互相連接<br>
<br>
0:16:41.480,0:16:44.800<br>
藍色 neuron 的 output 都接給紅色<br>
<br>
0:16:45.280,0:16:48.880<br>
紅色的都接給綠色<br>
<br>
0:16:48.880,0:16:52.340<br>
綠色後面沒有別人可以接了，<br>
所以它是整個 network 的輸出<br>
<br>
0:16:52.340,0:16:56.700<br>
藍色前面沒有其他人了，<br>
所以它就是整個 network 的輸入<br>
<br>
0:16:57.080,0:16:58.740<br>
那 in general 而言呢<br>
<br>
0:16:58.740,0:17:00.580<br>
我們可以把 network 畫成這樣<br>
<br>
0:17:00.580,0:17:03.120<br>
你有好多好多排的 neuron<br>
<br>
0:17:03.120,0:17:06.220<br>
你有第 1 排、第 2 排.......到第 L 排<br>
<br>
0:17:06.220,0:17:10.840<br>
很多排 neuron，每一排 neuron 它裡面的 neuron 的數目<br>
<br>
0:17:10.840,0:17:16.260<br>
可能很多，比如 1000 個、2000 個阿，這個 scale<br>
<br>
0:17:16.260,0:17:19.460<br>
那這邊每一顆球，代表一個 neuron<br>
<br>
0:17:19.460,0:17:22.680<br>
在 layer 和 layer 之間的 neuron，是兩兩互相連接的<br>
<br>
0:17:22.680,0:17:27.620<br>
layer 1 它的 neuron 的 output 會接給<br>
<br>
0:17:27.620,0:17:32.280<br>
每一個 layer 2 的 neuron<br>
<br>
0:17:32.280,0:17:36.140<br>
那 layer 2 的 neuron 的 input 就是所有 layer 1 的 output<br>
<br>
0:17:36.140,0:17:38.140<br>
就是所有 layer 1 的 output<br>
<br>
0:17:38.140,0:17:41.340<br>
因為 layer 和 layer 間，所有的 neuron 都有兩兩連接<br>
<br>
0:17:41.340,0:17:44.380<br>
所以它叫 Fully Connected 的 Network<br>
<br>
0:17:44.380,0:17:47.120<br>
那因為現在傳遞的方向是從 1 到 2<br>
<br>
0:17:47.120,0:17:49.220<br>
從 2 到 3，由後往前傳<br>
<br>
0:17:49.220,0:17:52.400<br>
所以它叫做 Feedforward Network<br>
<br>
0:17:52.820,0:17:55.040<br>
那整個 network 呢，需要一組<br>
<br>
0:17:55.040,0:17:58.840<br>
需要一個 input，這個 input 就是一個 vector<br>
<br>
0:17:58.840,0:18:01.760<br>
那對 layer 1 的每一個 neuron 來說<br>
<br>
0:18:01.760,0:18:03.360<br>
每一個 neuron，它的 input<br>
<br>
0:18:03.360,0:18:06.580<br>
就是 input layer 的每一個 dimension<br>
<br>
0:18:06.580,0:18:09.380<br>
就是 input layer 的每一個 dimension<br>
<br>
0:18:09.380,0:18:12.140<br>
那最後第 L 的那些 neuron<br>
<br>
0:18:12.140,0:18:14.140<br>
它後面沒有接其他東西了<br>
<br>
0:18:14.140,0:18:17.940<br>
所以，它的 output 就是整個 network 的 output<br>
<br>
0:18:17.940,0:18:21.280<br>
假設第 L 排有 M 個 neuron 的話<br>
<br>
0:18:21.280,0:18:24.820<br>
它的 output 就是 y1. y2 到 y(下標 M)<br>
<br>
0:18:25.500,0:18:28.700<br>
這邊每一個 layer，它是有一些名字的<br>
<br>
0:18:28.700,0:18:31.320<br>
input 的地方，我們叫做 input layer<br>
<br>
0:18:31.320,0:18:33.820<br>
嚴格說起來，input 其實不是一個 layer<br>
<br>
0:18:33.820,0:18:38.160<br>
它跟其他 layer 不一樣，它不是由其他 neuron 所組成的<br>
<br>
0:18:38.160,0:18:40.780<br>
但是，我們也把它當作一個 layer 來看<br>
<br>
0:18:40.780,0:18:42.100<br>
所以，叫它 input layer<br>
<br>
0:18:42.100,0:18:45.060<br>
output 的地方，我們叫它 output layer<br>
<br>
0:18:45.060,0:18:48.820<br>
其餘的部分，就叫做 hidden layer<br>
<br>
0:18:48.820,0:18:51.140<br>
那所謂的 deep 是什麼意思呢？<br>
<br>
0:18:51.140,0:18:54.220<br>
所謂的 deep 就是有很多 hidden layer<br>
<br>
0:18:54.220,0:18:55.540<br>
就叫做 deep<br>
<br>
0:18:55.540,0:18:58.660<br>
那有人就會問一個問題，說要幾個 hidden layer<br>
<br>
0:18:58.660,0:18:59.660<br>
才叫做 deep learning<br>
<br>
0:18:59.660,0:19:02.240<br>
這個就很難說了<br>
<br>
0:19:02.240,0:19:05.680<br>
有人會告訴你說，要 3 層以上才叫做 deep<br>
<br>
0:19:05.680,0:19:09.040<br>
有人會告訴你說，要 8 層以上才叫做 deep<br>
<br>
0:19:09.040,0:19:11.460<br>
所以就看每一個人的定義，都不一樣<br>
<br>
0:19:11.460,0:19:13.360<br>
本來沒有 deep learning 這個詞的時候<br>
<br>
0:19:13.360,0:19:15.460<br>
大家都說：我在做 neural network<br>
<br>
0:19:15.460,0:19:18.700<br>
通常都只有一層，自從有 deep learning 這個詞以後<br>
<br>
0:19:18.700,0:19:21.700<br>
有一層的人，都說他在做 deep learning 這樣<br>
<br>
0:19:23.780,0:19:26.760<br>
所以，現在基本上只要是 neural network base 的方法呢<br>
<br>
0:19:26.760,0:19:29.620<br>
大家都會說是 deep learning 的方法<br>
<br>
0:19:30.240,0:19:33.300<br>
那到底可以有幾層呢？<br>
<br>
0:19:33.300,0:19:35.100<br>
在 2012 年的時候<br>
<br>
0:19:35.100,0:19:38.140<br>
參加 ImageNet 比賽得到冠軍的 AlexNet<br>
<br>
0:19:38.140,0:19:41.460<br>
它有 8 層，它的錯誤率是 16.4 %<br>
<br>
0:19:41.480,0:19:45.660<br>
大家可能都知道說，它比第二名的好非常多<br>
<br>
0:19:45.660,0:19:48.560<br>
第二名的 error rate 是 30%<br>
<br>
0:19:48.560,0:19:52.560<br>
到 14 年的時候，VGG 有 19層<br>
<br>
0:19:52.560,0:19:54.760<br>
它的 error rate 降到 7%<br>
<br>
0:19:54.760,0:19:59.160<br>
GoogleNet 有 22 層，它的 error rate 降到 6.7%<br>
<br>
0:19:59.160,0:20:01.300<br>
但是，這個都還不算甚麼<br>
<br>
0:20:01.300,0:20:03.580<br>
Residual Network 152 層這樣<br>
<br>
0:20:03.580,0:20:07.100<br>
如果它跟 GoogleNet, VGG, AlexNet 比起來<br>
<br>
0:20:07.100,0:20:08.980<br>
它大概是長這個樣子<br>
<br>
0:20:08.980,0:20:11.740<br>
它的 error rate 是 3.57%<br>
<br>
0:20:11.740,0:20:14.920<br>
如果你看 Benchmark Corpus 的話，其實<br>
<br>
0:20:14.920,0:20:19.320<br>
它這個 performance 是比人在同一個 test 上做的還要好<br>
<br>
0:20:19.320,0:20:20.560<br>
你可能會很懷疑說<br>
<br>
0:20:20.560,0:20:23.360<br>
人怎麼可能會在影像辨識上輸給機器呢？<br>
<br>
0:20:23.360,0:20:25.240<br>
因為那個 test 其實還滿難的<br>
<br>
0:20:25.240,0:20:28.880<br>
它給你一張圖，一張狗的圖<br>
<br>
0:20:28.880,0:20:32.240<br>
你光回答狗並不是正確答案，你要回答哈士奇這樣子<br>
<br>
0:20:35.080,0:20:38.120<br>
那個 test 其實還蠻難的，但其實為了要公平的比較<br>
<br>
0:20:38.120,0:20:42.200<br>
當時，在人跟機器比較的時候<br>
<br>
0:20:42.200,0:20:46.260<br>
那些人是有事先看過 training data 的<br>
<br>
0:20:46.260,0:20:49.640<br>
也就是說，你有先讓它訓練辨識不同狗的種類<br>
<br>
0:20:49.640,0:20:51.460<br>
不同花的種類、不同豬的種類<br>
<br>
0:20:51.460,0:20:56.420<br>
只是經過訓練以後，它還是沒有機器那麼強就是了<br>
<br>
0:20:58.800,0:21:01.200<br>
那這個是跟 101 做一下比較<br>
<br>
0:21:01.200,0:21:02.980<br>
101 在這邊<br>
<br>
0:21:04.720,0:21:09.060<br>
這個我們之後再講，那 Residual Network 呢<br>
<br>
0:21:09.060,0:21:12.440<br>
它不是一般的 Fully Connected Feedforward Network<br>
<br>
0:21:12.440,0:21:14.760<br>
如果你用一般 Fully Connected Feedforward Network<br>
<br>
0:21:14.760,0:21:18.140<br>
搞在這個地方，其實結果是會有問題的<br>
<br>
0:21:18.140,0:21:21.260<br>
它不是 overfitting，而是 train 都 train 不起來<br>
<br>
0:21:21.260,0:21:23.360<br>
所以，你其實要特別的 structure<br>
<br>
0:21:23.360,0:21:25.520<br>
才能搞定這麼深的 network<br>
<br>
0:21:25.520,0:21:26.920<br>
這個我們之後再講<br>
<br>
0:21:27.560,0:21:29.060<br>
那 network 的運作呢<br>
<br>
0:21:29.060,0:21:34.420<br>
我們常常會把它用 Matrix Operation 來表示<br>
<br>
0:21:34.420,0:21:37.020<br>
怎麼說呢，我們舉剛才的例子<br>
<br>
0:21:37.020,0:21:40.420<br>
假設第一個 layer 的兩個 neuron<br>
<br>
0:21:40.420,0:21:43.200<br>
他們的 weight 分別是 1, -2, -1, 1<br>
<br>
0:21:43.460,0:21:49.360<br>
那你可以把 1, -2, -1, 1 排成一個 matrix<br>
<br>
0:21:49.360,0:21:53.240<br>
把這個 1, -2, -1, 1 排成一個 matrix<br>
<br>
0:21:54.000,0:21:57.040<br>
那當我們 input 1, -1 要做運算的時候<br>
<br>
0:21:57.040,0:22:03.080<br>
我們就是把 1*1、(-1)*(-2)、1*(-1)、(-1)*1<br>
<br>
0:22:03.080,0:22:08.040<br>
所以我們就可以把 1 跟 -1 當做一個 vector 排在這邊<br>
<br>
0:22:08.040,0:22:11.900<br>
當我們把這個 matrix 跟這個 vector<br>
<br>
0:22:11.900,0:22:15.700<br>
做運算的時候，我們算 1*1 + (-2)*(-1)<br>
<br>
0:22:15.700,0:22:19.000<br>
就等於是做 1*1、(-1)*(-2)<br>
<br>
0:22:19.000,0:22:22.260<br>
當我們做 1*(-1)、(-1)*1 的時候<br>
<br>
0:22:22.260,0:22:24.800<br>
就等於是做 1*(-1)、(-1)*1 這樣<br>
<br>
0:22:25.380,0:22:28.480<br>
接下來呢，有 bias 1 跟 0<br>
<br>
0:22:28.480,0:22:31.720<br>
所以，我們要在後面把 bias 排成一個 vector<br>
<br>
0:22:31.720,0:22:33.960<br>
再把這個 vector 加上去<br>
<br>
0:22:33.960,0:22:37.200<br>
這個結果，算出來就是 [4, -2]<br>
<br>
0:22:37.200,0:22:41.640<br>
就是通過 activation function 之前的值，就是 4, -2<br>
<br>
0:22:41.640,0:22:43.560<br>
然後，通過這個 sigmoid function<br>
<br>
0:22:43.560,0:22:46.060<br>
然後在這個 neural network 的文件裡面<br>
<br>
0:22:46.060,0:22:50.580<br>
我們把這個 function 稱之為 activation function<br>
<br>
0:22:50.580,0:22:52.780<br>
事實上，它不見得是 sigmoid function<br>
<br>
0:22:52.780,0:22:55.240<br>
因為到現在，大家都換成別的 function<br>
<br>
0:22:55.240,0:22:58.900<br>
如果你是從 Logistic Regression 想過來的話<br>
<br>
0:22:58.900,0:23:02.800<br>
你會覺得它是一個 sigmoid function，<br>
但現在已經較少用 sigmoid function<br>
<br>
0:23:02.800,0:23:05.440<br>
假設我們這邊用的是 sigmoid function 的話呢<br>
<br>
0:23:05.440,0:23:08.960<br>
我們就是把 4, -2 丟到 sigmoid function 裡面<br>
<br>
0:23:08.960,0:23:12.900<br>
接下來算出來，就是 0.98, 0.12<br>
<br>
0:23:12.900,0:23:15.800<br>
所以，一個 neural network，一個 feedforward network<br>
<br>
0:23:15.800,0:23:19.980<br>
它的一個 layer 的運算，你從 1, -1 到 0.98, 0.12<br>
<br>
0:23:19.980,0:23:22.740<br>
你做的運算就是把 input 1 跟 -1<br>
<br>
0:23:22.740,0:23:25.960<br>
1 跟 -1 乘一個 matrix<br>
<br>
0:23:25.960,0:23:28.100<br>
加上一個 bias 所成的 vector<br>
<br>
0:23:28.100,0:23:31.040<br>
再通過一個 sigmoid function<br>
<br>
0:23:31.040,0:23:33.380<br>
得到最後的結果<br>
<br>
0:23:33.720,0:23:35.540<br>
所以，以 general 來說<br>
<br>
0:23:35.540,0:23:37.420<br>
一個 neural network<br>
<br>
0:23:37.420,0:23:40.940<br>
假設我們說第一個 layer 的 weight<br>
<br>
0:23:40.940,0:23:43.900<br>
全部集合起來，當作一個 matrix, W^1<br>
<br>
0:23:43.900,0:23:45.660<br>
這個 W^1 是一個 matrix<br>
<br>
0:23:45.660,0:23:48.220<br>
把它的 bias 全部集合起來<br>
<br>
0:23:48.220,0:23:49.800<br>
當作一個 vector, b^1<br>
<br>
0:23:50.080,0:23:53.620<br>
把第二個 matrix 的 weight 全部集合起來<br>
<br>
0:23:53.620,0:23:55.360<br>
當作 W^2<br>
<br>
0:23:55.360,0:23:58.700<br>
把它的每一個 neural 的 bias 集合起來，當作 b^2<br>
<br>
0:23:58.700,0:24:01.720<br>
到第 L 個 layer，所有的 weight 集合起來，變成 W^L<br>
<br>
0:24:01.720,0:24:04.780<br>
bias 集合起來，變成 b^L<br>
<br>
0:24:04.780,0:24:10.400<br>
那你今天給它一個 input x 的時候<br>
<br>
0:24:10.400,0:24:13.840<br>
output 的這個 y 要怎麼算出來呢？<br>
<br>
0:24:13.840,0:24:19.140<br>
我們假設我們把 x1, x2, 到 xN，接起來，變成一個 x<br>
<br>
0:24:19.140,0:24:23.940<br>
那這個 output 的 y 要怎麼把它算出來呢？<br>
<br>
0:24:23.940,0:24:25.200<br>
你就這樣算<br>
<br>
0:24:26.160,0:24:29.960<br>
x * W^1 + b^1<br>
<br>
0:24:29.960,0:24:31.860<br>
再通過 activation function<br>
<br>
0:24:31.860,0:24:36.060<br>
然後，你就算出第二排這些 neuron 的 output<br>
<br>
0:24:36.060,0:24:40.360<br>
我們稱之為 a^1，你把 x * W^1 + b^1，就得到 a^1<br>
<br>
0:24:40.360,0:24:42.420<br>
x * W^1 + b^1<br>
<br>
0:24:42.420,0:24:45.780<br>
再通過 activation function 以後，就得到 a^1<br>
<br>
0:24:46.180,0:24:48.580<br>
接下來呢，你做一樣的運算<br>
<br>
0:24:48.580,0:24:51.700<br>
把 a1 * W^2 + b^2<br>
<br>
0:24:51.700,0:24:55.380<br>
再通過 activation function 以後，就得到 a^2<br>
<br>
0:24:55.380,0:25:00.140<br>
然後，就這樣一層一層的做下去到最後一層<br>
<br>
0:25:00.140,0:25:05.340<br>
你把 a^(L-1) * W^L + b^L 通過 activation function 以後<br>
<br>
0:25:05.340,0:25:08.000<br>
得到這個 network 的最終 output y<br>
<br>
0:25:08.000,0:25:11.080<br>
所以，整個 neural network 的運算其實就是<br>
<br>
0:25:11.080,0:25:15.240<br>
一連串 matrix 的 operation<br>
<br>
0:25:15.240,0:25:16.820<br>
就是這個 function<br>
<br>
0:25:16.820,0:25:19.520<br>
x 它的 input, output，x 跟 y 的關係<br>
<br>
0:25:19.520,0:25:21.880<br>
x 跟 y 的關係，他們是怎麼樣的關係呢？<br>
<br>
0:25:21.880,0:25:26.240<br>
你把 x * W^1 + b^1，再通過 activation function<br>
<br>
0:25:26.240,0:25:30.560<br>
再把這個 output 再乘上 W^2，加 b^2<br>
<br>
0:25:30.560,0:25:33.180<br>
再通過 activation function，再通過 sigmoid function<br>
<br>
0:25:33.180,0:25:35.460<br>
最後，到乘上 W^L，加上 b^L<br>
<br>
0:25:35.460,0:25:37.720<br>
再通過這個 sigmoid function<br>
<br>
0:25:37.720,0:25:39.740<br>
就得到最後的 y<br>
<br>
0:25:39.740,0:25:41.860<br>
所以，一個 neural network<br>
<br>
0:25:41.860,0:25:45.420<br>
實際上做的事情就是一連串的<br>
<br>
0:25:45.420,0:25:48.460<br>
vector 乘上 matrix ，再加上 vector<br>
<br>
0:25:48.460,0:25:52.920<br>
就是一連串我們在線性代數就有學過的：矩陣運算<br>
<br>
0:25:52.920,0:25:56.000<br>
那把這件事情寫成矩陣運算的好處就是<br>
<br>
0:25:56.000,0:25:58.460<br>
你可以用 GPU 加速<br>
<br>
0:25:58.460,0:26:01.720<br>
實際上呢，現在一般在用 GPU 加速的時候<br>
<br>
0:26:01.720,0:26:04.500<br>
這個 GPU 的加速並不是<br>
<br>
0:26:04.500,0:26:08.760<br>
真的有去對 neural network 做甚麼特化<br>
<br>
0:26:08.760,0:26:11.740<br>
是說，現在有一些特別的技術有做特化<br>
<br>
0:26:11.740,0:26:15.360<br>
但是，如果你 general 是買那種玩遊戲的顯卡的話<br>
<br>
0:26:15.360,0:26:19.320<br>
那它是沒有對這個 neural network 做甚麼特化<br>
<br>
0:26:19.320,0:26:21.620<br>
那你實際上拿來加速的方式是<br>
<br>
0:26:21.620,0:26:23.880<br>
當你需要算矩陣運算的時候<br>
<br>
0:26:23.880,0:26:27.220<br>
你就 call 一下 GPU，叫它幫你做矩陣運算<br>
<br>
0:26:27.220,0:26:31.520<br>
這會比你用 CPU 來算還要快<br>
<br>
0:26:31.520,0:26:35.360<br>
所以，我們在寫 neural network 式子的時候<br>
<br>
0:26:35.360,0:26:38.920<br>
我們習慣把它寫成 matrix operation 的樣子<br>
<br>
0:26:38.920,0:26:41.320<br>
那裡面如果有需要用到矩陣運算的時候<br>
<br>
0:26:41.320,0:26:43.240<br>
就 call GPU 來做它<br>
<br>
0:26:44.620,0:26:48.180<br>
那這整個 network，我們怎麼看待呢？<br>
<br>
0:26:48.360,0:26:53.960<br>
我們可以把它到 output layer 之前的部分阿<br>
<br>
0:26:53.960,0:26:55.880<br>
到 output layer 之前的部分<br>
<br>
0:26:55.880,0:27:00.560<br>
看作是一個 feature 的 extractor<br>
<br>
0:27:00.560,0:27:03.300<br>
這個 feature extractor 就 replace 我們之前<br>
<br>
0:27:03.300,0:27:06.180<br>
要手動做 feature engineering<br>
<br>
0:27:06.180,0:27:09.780<br>
做 feature transformation 這件事情<br>
<br>
0:27:09.780,0:27:12.860<br>
所以，你把一個 x input，通過很多很多 hidden layer<br>
<br>
0:27:12.860,0:27:15.000<br>
在最後一個 hidden layer 的 output<br>
<br>
0:27:15.000,0:27:17.840<br>
每一個 neuron 的 output，x1, x2, 到 xK<br>
<br>
0:27:17.840,0:27:21.200<br>
你就可以把它想成是一組新的 feature<br>
<br>
0:27:21.200,0:27:24.860<br>
那 output layer 做的事情呢？output layer 就是<br>
<br>
0:27:24.860,0:27:28.060<br>
一個 Multi-class 的 Classifier<br>
<br>
0:27:28.060,0:27:32.580<br>
這個 Multi-class Classifier，它是拿前一個 layer 的 output<br>
<br>
0:27:32.580,0:27:34.100<br>
當作 feature<br>
<br>
0:27:34.100,0:27:38.540<br>
這個 Multi-class Classifier，<br>
它用的 feature 不是直接從 x 抽出來的<br>
<br>
0:27:38.540,0:27:42.460<br>
它是經過很多個 hidden layer，做很複雜的轉換以後<br>
<br>
0:27:42.460,0:27:44.620<br>
抽出一組特別好的 feature<br>
<br>
0:27:44.620,0:27:48.360<br>
這組好的 feature 可能是能夠被 separable<br>
<br>
0:27:48.360,0:27:50.280<br>
經過這一連串的轉換以後呢<br>
<br>
0:27:50.280,0:27:53.860<br>
它們可以被用一個簡單的 layer 的<br>
<br>
0:27:53.860,0:27:58.740<br>
Multi-class Classifier 就把它分類好<br>
<br>
0:27:58.740,0:28:00.780<br>
那我們剛才其實有講過說<br>
<br>
0:28:00.780,0:28:05.080<br>
Multi-class Classifier 它要通過一個 sigmoid function<br>
<br>
0:28:05.080,0:28:07.880<br>
而不要通過一個 Softmax function，對不對？<br>
<br>
0:28:07.880,0:28:09.980<br>
要通過一個 Softmax function<br>
<br>
0:28:09.980,0:28:13.040<br>
因為我們把 output layer<br>
<br>
0:28:13.040,0:28:17.020<br>
也看作是一個 Multi-class Classifier<br>
<br>
0:28:17.020,0:28:19.960<br>
因為我們把 output layer 也看作是<br>
一個 Multi-class Classifier<br>
<br>
0:28:19.960,0:28:24.380<br>
所以，我們最後一個 layer 也會加上 Softmax<br>
<br>
0:28:24.400,0:28:26.520<br>
一般你在做 neural network 的時候呢<br>
<br>
0:28:26.520,0:28:28.780<br>
是會這樣做<br>
<br>
0:28:29.340,0:28:33.040<br>
那舉一個不是寶可夢的例子<br>
<br>
0:28:33.040,0:28:38.580<br>
不是寶可夢的例子，我之後會示範一下做這個例子<br>
<br>
0:28:38.580,0:28:43.220<br>
就 input 一張 image，它是一張手寫的數字<br>
<br>
0:28:43.220,0:28:46.620<br>
然後 output 說這個 input 的 image，它對應的是甚麼？<br>
<br>
0:28:46.620,0:28:49.680<br>
那在這個問題裡面，你的 input 是一張 image<br>
<br>
0:28:49.680,0:28:54.200<br>
但對機器來說，一張 image 就是一個 vector<br>
<br>
0:28:54.200,0:28:57.760<br>
假設這是一個解析度 16*16 的 image<br>
<br>
0:28:57.760,0:29:00.100<br>
那它有 256 個 pixel<br>
<br>
0:29:00.100,0:29:05.900<br>
對 machine 來說，它就是一個 256 維的 vector<br>
<br>
0:29:05.900,0:29:08.240<br>
那在這個 image 裡面呢<br>
<br>
0:29:08.240,0:29:10.500<br>
每一個 pixel 就對應到其中的 dimension<br>
<br>
0:29:10.500,0:29:13.000<br>
所以，右上角這個 pixel 就對應到 x1<br>
<br>
0:29:13.000,0:29:16.760<br>
這個就對應到 x2，右下角的就對應到 x256<br>
<br>
0:29:16.760,0:29:19.520<br>
如果你可以說，有塗黑的地方就是 1<br>
<br>
0:29:19.520,0:29:23.440<br>
那沒有塗黑的地方，它對應的數字就是 0<br>
<br>
0:29:23.440,0:29:25.080<br>
那 output 呢？<br>
<br>
0:29:25.080,0:29:29.820<br>
neural network 的 output，如果你用 Softmax 的話<br>
<br>
0:29:29.820,0:29:34.040<br>
那它的 output 代表了一個 <br>
probability distribution，對不對？<br>
<br>
0:29:34.040,0:29:37.600<br>
所以，今天假設 output 是 10 維的話<br>
<br>
0:29:37.600,0:29:39.800<br>
就可以把這個 output 看成是<br>
<br>
0:29:39.800,0:29:44.360<br>
對應到每一個，你可以把這個 output 看成是<br>
<br>
0:29:44.360,0:29:49.100<br>
對應到每一個數字的機率<br>
<br>
0:29:49.100,0:29:52.360<br>
y1，代表了 input 這張 image<br>
<br>
0:29:52.360,0:29:55.780<br>
根據這個 neural network 判斷，它是屬於 1 的機率<br>
<br>
0:29:55.780,0:29:59.980<br>
代表它是屬於 2 的機率，代表它是屬於 0 的機率<br>
<br>
0:29:59.980,0:30:02.620<br>
那你就實際上讓 network 幫你算一下說<br>
<br>
0:30:02.620,0:30:06.680<br>
每一個 input 加上數字屬於 image 的機率是多少？<br>
<br>
0:30:06.680,0:30:09.360<br>
假設屬於數字 2 的機率最大是 0.7<br>
<br>
0:30:09.360,0:30:13.020<br>
那你的 machine 就會 output 說<br>
<br>
0:30:13.020,0:30:16.200<br>
這張 image 它是屬於數字 2<br>
<br>
0:30:17.020,0:30:21.100<br>
那在這個 application 裡面，<br>
假設你要解這個手寫數字辨識的問題<br>
<br>
0:30:21.100,0:30:24.760<br>
那你唯一需要的，就是一個 function<br>
<br>
0:30:24.760,0:30:27.240<br>
這個 function input 是一個二維的 vector<br>
<br>
0:30:27.240,0:30:30.260<br>
output 是一個 10 維的 vector<br>
<br>
0:30:30.260,0:30:35.000<br>
而這個 function 就是 neural network<br>
<br>
0:30:35.000,0:30:38.640<br>
所以，你只要兜一個 neural network<br>
<br>
0:30:38.640,0:30:40.660<br>
你可以用簡單的 feedforward network 就好了<br>
<br>
0:30:40.660,0:30:46.280<br>
兜一個 neural network，<br>
它的 input 有 256維、是一張 image<br>
<br>
0:30:46.280,0:30:49.220<br>
它的 output 你特別設成 10 維<br>
<br>
0:30:49.220,0:30:52.180<br>
這 10 維裡面，每一個 dimension<br>
<br>
0:30:52.180,0:30:55.140<br>
都對應到一個數字<br>
<br>
0:30:55.140,0:30:59.040<br>
如果你做這樣的設計，input 是 256 維<br>
<br>
0:30:59.040,0:31:03.240<br>
output 固定是 10 維的話<br>
<br>
0:31:03.240,0:31:06.200<br>
那這一個 network，它其實就<br>
<br>
0:31:06.200,0:31:10.740<br>
代表了一個可以拿來做手寫數字的 function<br>
<br>
0:31:10.740,0:31:14.040<br>
這個 function set 裡面呢<br>
<br>
0:31:14.040,0:31:16.500<br>
這個 network 的 structure 就 define 了一個 function set<br>
<br>
0:31:16.500,0:31:17.280<br>
這個 function set 裡面<br>
<br>
0:31:17.280,0:31:19.940<br>
每一個 function 都可以拿來做手寫數字辨識<br>
<br>
0:31:19.940,0:31:22.860<br>
只是有些做出來結果比較好<br>
<br>
0:31:22.860,0:31:24.640<br>
有些做出來結果比較差<br>
<br>
0:31:24.640,0:31:26.660<br>
那接下來你要做的事情就是<br>
<br>
0:31:26.660,0:31:29.980<br>
用 Gradient Descent 去找一組參數<br>
<br>
0:31:29.980,0:31:33.600<br>
去挑一個最適合拿來做手寫數字辨識的 function<br>
<br>
0:31:33.600,0:31:35.620<br>
那在這個 process 裡面呢<br>
<br>
0:31:35.620,0:31:38.320<br>
我們需要做一些 design<br>
<br>
0:31:38.320,0:31:40.240<br>
之前在做 Logistic Regression<br>
<br>
0:31:40.240,0:31:42.320<br>
或是 Linear Regression 的時候<br>
<br>
0:31:42.340,0:31:46.520<br>
我們對 model 的 structure 是沒有甚麼好設計的<br>
<br>
0:31:46.520,0:31:48.640<br>
但是，對 neural network 來說<br>
<br>
0:31:48.640,0:31:52.200<br>
我們現在唯一的 constraint 只有 input 要是 256 維<br>
<br>
0:31:52.200,0:31:53.880<br>
output 要是 10 維<br>
<br>
0:31:54.080,0:31:56.480<br>
但是中間要有幾個 hidden layer<br>
<br>
0:31:56.480,0:31:58.700<br>
每一個 hidden layer 要有多少的 neuron<br>
<br>
0:31:58.700,0:32:00.660<br>
是沒有限制的<br>
<br>
0:32:00.660,0:32:04.340<br>
你必須要自己去設計它<br>
<br>
0:32:04.340,0:32:07.100<br>
你必須自己去決定說，我要甚麼 layer<br>
<br>
0:32:07.100,0:32:09.780<br>
每個 layer 要有多少的 neuron<br>
<br>
0:32:09.780,0:32:12.560<br>
那決定 layer 的數目<br>
<br>
0:32:12.560,0:32:15.040<br>
和每一個 layer 的 neuron 數這件事情<br>
<br>
0:32:15.040,0:32:18.520<br>
就等於是決定了你的 function set 長甚麼樣子<br>
<br>
0:32:18.520,0:32:22.100<br>
你可以想像說，如果我今天決定了一個差的 function set<br>
<br>
0:32:22.100,0:32:25.320<br>
那裡面沒有包含任何好的 function<br>
<br>
0:32:25.320,0:32:28.160<br>
那你之後在找最好的 function 的時候，就好像是<br>
<br>
0:32:28.160,0:32:30.700<br>
大海撈針，結果針不在海裡這樣<br>
<br>
0:32:30.700,0:32:32.660<br>
怎麼找都找不到一個好的 function<br>
<br>
0:32:32.660,0:32:34.960<br>
所以，決定一個好的 function set<br>
<br>
0:32:34.960,0:32:37.800<br>
其實很關鍵，你就決定這個 network 的 structure<br>
<br>
0:32:37.800,0:32:39.300<br>
是很關鍵的<br>
<br>
0:32:39.300,0:32:43.120<br>
講到這邊，總是會有人問我一個問題<br>
<br>
0:32:43.120,0:32:46.620<br>
假設我們今天讓 machine 來聽我的 talk<br>
<br>
0:32:46.620,0:32:49.920<br>
然後，叫它 predict 之後會有甚麼問題的話<br>
<br>
0:32:49.920,0:32:51.860<br>
它一定可以預測接下來的問題<br>
<br>
0:32:51.860,0:32:54.640<br>
那我們到底應該要怎麼決定 layer 的數目<br>
<br>
0:32:54.640,0:32:56.940<br>
還有每一個 layer 的 neuron 的數目呢<br>
<br>
0:32:56.940,0:32:59.880<br>
這個答案，就是我不知道這樣子<br>
<br>
0:32:59.880,0:33:04.120<br>
這個問題很難，就好像問說怎麼成為寶可夢大師一樣<br>
<br>
0:33:04.580,0:33:09.540<br>
這只能夠憑著經驗和直覺這樣<br>
<br>
0:33:09.540,0:33:11.760<br>
這個 network structure 要長甚麼樣子<br>
<br>
0:33:11.760,0:33:15.780<br>
就是憑著直覺還有多方的常識阿<br>
<br>
0:33:15.780,0:33:19.960<br>
然後，後續想辦法找一個最後的 network structure<br>
<br>
0:33:19.960,0:33:23.960<br>
找 network structure 這件事情，並沒有那麼容易<br>
<br>
0:33:23.960,0:33:25.860<br>
它有時候是滿困難的<br>
<br>
0:33:25.860,0:33:28.180<br>
有時候甚至需要一些 domain knowledge<br>
<br>
0:33:28.180,0:33:31.500<br>
所以，我覺得從非 deep learning 的方法<br>
<br>
0:33:31.500,0:33:33.860<br>
到 deep learning 的方法<br>
<br>
0:33:33.860,0:33:37.900<br>
我並不認為machine learning 真的變得比較簡單<br>
<br>
0:33:37.900,0:33:41.180<br>
而是我們把一個問題轉化成另一個問題<br>
<br>
0:33:41.180,0:33:43.780<br>
就本來不是 deep 的 model<br>
<br>
0:33:43.780,0:33:44.960<br>
我們要得到好的結果<br>
<br>
0:33:44.960,0:33:47.420<br>
你往往需要做 Feature Engineering<br>
<br>
0:33:47.420,0:33:51.100<br>
也就是做 Feature Transform，然後找一組好的 feature<br>
<br>
0:33:51.100,0:33:53.400<br>
但是，如果今天是做 deep learning 的時候<br>
<br>
0:33:53.400,0:33:55.360<br>
你往往不需要找一個好的 feature<br>
<br>
0:33:55.360,0:33:57.140<br>
比如說，做影像辨識的時候<br>
<br>
0:33:57.140,0:33:59.580<br>
你可以直接把 pixel 丟進去<br>
<br>
0:33:59.580,0:34:02.560<br>
過去做影像辨識的時候，你需要對影像抽一些 feature<br>
<br>
0:34:02.560,0:34:04.000<br>
抽一些人定的 feature<br>
<br>
0:34:04.000,0:34:06.140<br>
這件事情就是 Feature Transform<br>
<br>
0:34:06.140,0:34:09.960<br>
但是有 deep learning 之後，你可以直接丟 pixel 硬做<br>
<br>
0:34:09.960,0:34:13.620<br>
但是，今天 deep learning 製造了一個新的問題<br>
<br>
0:34:13.620,0:34:17.600<br>
它所製造的新的問題就是，<br>
你需要去 design network 的 structure<br>
<br>
0:34:17.600,0:34:19.980<br>
就你的問題變成本來抽 feature<br>
<br>
0:34:19.980,0:34:23.440<br>
轉化成怎麼抽、怎麼 design network structure<br>
<br>
0:34:23.440,0:34:26.760<br>
那我覺得 deep learning 是不是真的好用就 depend on<br>
<br>
0:34:26.760,0:34:29.600<br>
你覺得哪一個問題比較容易<br>
<br>
0:34:29.600,0:34:32.520<br>
我個人覺得如果是<br>
<br>
0:34:32.520,0:34:34.680<br>
語音辨識或是影像辨識的話<br>
<br>
0:34:34.680,0:34:37.860<br>
design network structure 可能比 <br>
Feature Engineering 容易<br>
<br>
0:34:37.860,0:34:42.680<br>
因為，雖然說我們人工會看、會聽<br>
<br>
0:34:42.680,0:34:44.560<br>
我們自己都做得嚇嚇叫<br>
<br>
0:34:44.560,0:34:46.960<br>
但是，這件事情它太過潛意識了<br>
<br>
0:34:46.960,0:34:48.960<br>
它離我們意識的層次太遠<br>
<br>
0:34:48.960,0:34:50.200<br>
我們其實不知道<br>
<br>
0:34:50.200,0:34:53.620<br>
我們無法意識到，我們到底是怎麼做語音辨識這件事情<br>
<br>
0:34:54.060,0:34:57.500<br>
所以，對人來說你要抽一組好的 feature<br>
<br>
0:34:57.500,0:35:01.780<br>
讓機器可以用很方便的用 linear 的方法做語音辨識<br>
<br>
0:35:01.780,0:35:04.900<br>
這件事對人來說很難，因為<br>
根本不知道好的 feature 長甚麼樣子<br>
<br>
0:35:04.900,0:35:07.980<br>
所以，還不如 design 一個 network structure<br>
<br>
0:35:07.980,0:35:09.800<br>
或是嘗試各種 network structure<br>
<br>
0:35:09.800,0:35:13.260<br>
讓 machine 自己去找出好的 feature<br>
<br>
0:35:13.260,0:35:15.300<br>
這件事情反而變得比較容易<br>
<br>
0:35:15.300,0:35:17.460<br>
我覺得對影像來說，也是一樣<br>
<br>
0:35:17.460,0:35:20.260<br>
那對其他 case 來說，我覺得就是 case by case<br>
<br>
0:35:20.260,0:35:22.760<br>
比如說，你有聽過一個說法是<br>
<br>
0:35:22.760,0:35:26.080<br>
deep learning 在 NLP 上面<br>
<br>
0:35:26.080,0:35:28.560<br>
覺得 performance 沒有那麼好<br>
<br>
0:35:28.560,0:35:31.900<br>
有聽過這個說法的同學舉手一下<br>
<br>
0:35:31.900,0:35:34.620<br>
好，沒關係，手放下<br>
<br>
0:35:34.620,0:35:38.120<br>
好像沒有太多人聽過這個說法<br>
<br>
0:35:38.120,0:35:40.020<br>
那這件事情是這樣<br>
<br>
0:35:40.020,0:35:44.440<br>
如果你開語音辨識跟影像辨識的文件<br>
<br>
0:35:44.440,0:35:47.060<br>
語音辨識跟影像辨識這兩個 community<br>
<br>
0:35:47.060,0:35:49.760<br>
是最早開始用 deep learning 的<br>
<br>
0:35:49.760,0:35:52.720<br>
一用下去，進步量就非常驚人<br>
<br>
0:35:52.720,0:35:57.920<br>
比如說，電視的錯誤率相對下降了 20% 以上<br>
<br>
0:35:58.480,0:36:01.720<br>
那如果是 NLP 的話<br>
<br>
0:36:01.720,0:36:04.620<br>
你就會覺得說，它的進步量<br>
<br>
0:36:04.620,0:36:06.060<br>
似乎沒有那麼驚人<br>
<br>
0:36:06.060,0:36:09.740<br>
甚至很多 NLP 的人，現在仍然認為說 <br>
deep learning 不見得那麼 work<br>
<br>
0:36:09.740,0:36:12.560<br>
那我覺得這個，我自己的猜想是<br>
<br>
0:36:12.560,0:36:18.020<br>
這個原因就是，人在做 NLP 這件事情<br>
<br>
0:36:18.020,0:36:20.900<br>
文字處理來說，人是比較強的<br>
<br>
0:36:20.900,0:36:23.360<br>
比如說，叫你說<br>
<br>
0:36:23.360,0:36:25.620<br>
叫你設計一個 rule detect 說<br>
<br>
0:36:25.620,0:36:28.920<br>
一篇 document 它是正面的情緒，還是負面的情緒<br>
<br>
0:36:28.920,0:36:30.960<br>
你可以說我就列表<br>
<br>
0:36:30.960,0:36:33.300<br>
列一些正面情緒的詞彙跟負面情緒的詞彙<br>
<br>
0:36:33.300,0:36:36.480<br>
然後看這個 document 裡面，<br>
正面情緒的詞彙出現百分之多少<br>
<br>
0:36:36.480,0:36:38.860<br>
你可能就可以得到一個不錯的結果<br>
<br>
0:36:38.860,0:36:40.180<br>
所以，NLP 這個 task<br>
<br>
0:36:40.180,0:36:42.620<br>
對人來說，你比較容易設計 rule<br>
<br>
0:36:42.620,0:36:44.960<br>
所以，你設計的那些 ad-hoc 的 rule<br>
<br>
0:36:44.960,0:36:47.720<br>
我好像可以給你一個還不錯的結果<br>
<br>
0:36:47.720,0:36:51.260<br>
這就是為甚麼 deep learning 相較於 NLP 傳統的方法<br>
<br>
0:36:51.260,0:36:53.800<br>
覺得進步沒有那麼地顯著<br>
<br>
0:36:53.800,0:36:56.140<br>
但它其實還是有一些進步的<br>
<br>
0:36:56.140,0:36:59.800<br>
只是(覺得沒有)其他的領域<br>
<br>
0:36:59.800,0:37:04.220<br>
沒有語音和影像處理看起來那麼地顯著<br>
<br>
0:37:04.220,0:37:05.340<br>
但還是有進步的<br>
<br>
0:37:05.340,0:37:07.540<br>
那我覺得就長久而言呢<br>
<br>
0:37:07.540,0:37:09.860<br>
因為文字處理其實也是很困難的問題<br>
<br>
0:37:09.860,0:37:14.140<br>
你沒有很多幽微的資訊，可能是人自己也不知道的<br>
<br>
0:37:14.140,0:37:15.800<br>
所以，就長久而言呢<br>
<br>
0:37:15.800,0:37:18.760<br>
deep learning，讓 machine 自己去學這件事情<br>
<br>
0:37:18.760,0:37:21.040<br>
還是可以佔到一些優勢<br>
<br>
0:37:21.040,0:37:25.100<br>
只是一下子，眼下看起來進步沒有那麼顯著<br>
<br>
0:37:25.100,0:37:28.500<br>
它跟傳統的方法比起來的差異就沒有那麼驚人<br>
<br>
0:37:28.500,0:37:30.260<br>
但是還是有進步的<br>
<br>
0:37:30.260,0:37:31.540<br>
這是我一些想法<br>
<br>
0:37:31.760,0:37:33.460<br>
那再來就是有人會問說<br>
<br>
0:37:33.460,0:37:35.620<br>
能不能夠自動學 network 的 structure<br>
<br>
0:37:35.620,0:37:38.640<br>
其實是可以的，你可以去問余天立老師<br>
<br>
0:37:39.020,0:37:43.340<br>
就是在基因演算法那邊，有很多的 technique<br>
<br>
0:37:43.340,0:37:48.360<br>
是可以讓 machine 自動的去找出 network structure<br>
<br>
0:37:48.360,0:37:51.740<br>
不過這些方法，目前還沒有非常的普及<br>
<br>
0:37:51.740,0:37:54.100<br>
你看到那些非常驚人的應用<br>
<br>
0:37:54.100,0:37:55.480<br>
比如說，AlphaGo 什麼的阿<br>
<br>
0:37:55.480,0:37:57.540<br>
都不是用這些方法所做出來的<br>
<br>
0:37:58.620,0:38:00.320<br>
那還有一個常問的問題就是<br>
<br>
0:38:00.320,0:38:04.200<br>
我們能不能自己去設計 network 的 structure<br>
<br>
0:38:04.200,0:38:06.060<br>
我可不可以不要 fully connected<br>
<br>
0:38:06.060,0:38:09.740<br>
我可不可以說，第一個連到第三個，第二個連到第四個<br>
<br>
0:38:09.740,0:38:11.400<br>
就自己亂接，可以<br>
<br>
0:38:11.400,0:38:14.480<br>
一個特殊的接法就是 Convolutional Neural Network<br>
<br>
0:38:14.480,0:38:17.960<br>
這個我們下一堂課再講<br>
<br>
0:38:18.140,0:38:21.060<br>
那接下來，第二步<br>
<br>
0:38:21.060,0:38:24.180<br>
第二步跟第三步真的很快，我們等一下就秒講這樣子<br>
<br>
0:38:24.180,0:38:25.960<br>
第二步是甚麼呢？<br>
<br>
0:38:25.960,0:38:28.960<br>
要定義一個 function 的好壞<br>
<br>
0:38:28.960,0:38:30.660<br>
在 Neural Network 裡面<br>
<br>
0:38:30.660,0:38:33.820<br>
怎麼決定一組參數它的好壞呢？<br>
<br>
0:38:33.820,0:38:36.120<br>
就假設給定一組參數<br>
<br>
0:38:36.120,0:38:38.080<br>
我要做手寫數字辨識<br>
<br>
0:38:38.080,0:38:42.340<br>
所以，我有一張 image，跟它的 label<br>
<br>
0:38:42.340,0:38:45.660<br>
然後，把這張 image 呢，這個 label 告訴我們說<br>
<br>
0:38:45.660,0:38:49.600<br>
因為你現在是一個 Multiclass classification 的問題<br>
<br>
0:38:49.600,0:38:53.280<br>
所以，今天這個 label 1，告訴我們說<br>
<br>
0:38:53.280,0:38:56.240<br>
你現在的 target 是一個 vector<br>
<br>
0:38:56.240,0:38:58.520<br>
你現在的 target 是一個 10 維的 vector<br>
<br>
0:38:58.520,0:39:02.340<br>
只有在第一維，對應到數字 1 的地方<br>
<br>
0:39:02.340,0:39:05.500<br>
它值是 1，其他呢，都是 0<br>
<br>
0:39:05.960,0:39:08.860<br>
那你就 input 這張 image 的 pixel<br>
<br>
0:39:08.860,0:39:11.680<br>
然後，通過這個 neural network 以後呢<br>
<br>
0:39:11.680,0:39:13.240<br>
你會得到一個 output<br>
<br>
0:39:13.240,0:39:15.760<br>
那這個 output，我們就稱之為 y<br>
<br>
0:39:15.760,0:39:18.960<br>
我把我們的 target，稱之為 y\head<br>
<br>
0:39:18.960,0:39:20.820<br>
接下來你要做的事情就是<br>
<br>
0:39:20.820,0:39:25.760<br>
計算這個 y 跟 y\head 之間的 cross entropy<br>
<br>
0:39:25.760,0:39:29.280<br>
就跟我們在做  Multiclass classification 的時候<br>
<br>
0:39:29.280,0:39:30.960<br>
是一模一樣的<br>
<br>
0:39:30.960,0:39:34.940<br>
我們就計算 y 跟 y\head 之間的 cross entropy<br>
<br>
0:39:34.940,0:39:37.800<br>
然後，接下來我們就是要調整 network 的參數<br>
<br>
0:39:37.800,0:39:40.980<br>
去讓這個 cross entropy 越小越好<br>
<br>
0:39:41.120,0:39:44.860<br>
當然你整個 training data 裡面，<br>
不會只有一筆 data，你有一大堆的 data<br>
<br>
0:39:44.860,0:39:45.840<br>
你有第一筆 data<br>
<br>
0:39:45.840,0:39:48.740<br>
那它所算出來的 cross entropy 是 C^1<br>
<br>
0:39:48.740,0:39:50.340<br>
第二筆 data 算出來是 C^2<br>
<br>
0:39:50.340,0:39:52.280<br>
到第 N 筆 data 算出來是 C^N<br>
<br>
0:39:52.280,0:39:55.760<br>
那你會把所有 data 的 cross entropy 全部 sum 起來<br>
<br>
0:39:55.760,0:39:58.700<br>
得到一個 total loss，L<br>
<br>
0:39:59.780,0:40:03.040<br>
然後，你接下來要做的事情就是<br>
<br>
0:40:03.040,0:40:06.040<br>
在 function set 裡面，找一個 function<br>
<br>
0:40:06.040,0:40:07.920<br>
它可以 minimize 這個 total loss<br>
<br>
0:40:07.920,0:40:11.040<br>
或者是找一組 network 的 parameter<br>
<br>
0:40:11.040,0:40:13.200<br>
現在寫成 θ*<br>
<br>
0:40:13.200,0:40:16.000<br>
它可以 minimize 這個 total loss<br>
<br>
0:40:16.000,0:40:18.620<br>
怎麼解這個問題？<br>
<br>
0:40:18.620,0:40:22.540<br>
怎麼找一個 θ* minimize 這個 total loss 呢？<br>
<br>
0:40:22.540,0:40:25.680<br>
你用的方法就是 Gradient Descent<br>
<br>
0:40:25.680,0:40:29.300<br>
Gradient Descent 大家已經太熟了，沒有甚麼好講的<br>
<br>
0:40:29.300,0:40:31.660<br>
實際上，在 deep learning 裡面用 Gradient Descent<br>
<br>
0:40:31.660,0:40:35.240<br>
跟 Linear Regression 那邊沒有甚麼差別，就一模一樣<br>
<br>
0:40:35.240,0:40:37.560<br>
所以，你要做的事情，只是 function 變複雜而已<br>
<br>
0:40:37.560,0:40:40.520<br>
其他東西都是一樣的<br>
<br>
0:40:40.520,0:40:41.820<br>
也就是說<br>
<br>
0:40:41.820,0:40:43.980<br>
你的 θ 裡面是一大堆的參數<br>
<br>
0:40:43.980,0:40:46.060<br>
一大堆的 weight 跟一大堆的 bias<br>
<br>
0:40:46.060,0:40:48.560<br>
你先 random 找一個初始值<br>
<br>
0:40:48.560,0:40:50.800<br>
random 給每一個數字一個初始值<br>
<br>
0:40:50.800,0:40:54.460<br>
接下來呢，去計算一下它的 Gradient<br>
<br>
0:40:54.460,0:40:58.660<br>
計算每一個參數，對你 total loss 的偏微分<br>
<br>
0:40:58.660,0:41:01.360<br>
那把這些偏微分全部集合起來呢<br>
<br>
0:41:01.360,0:41:03.960<br>
叫做 Gradient<br>
<br>
0:41:03.960,0:41:07.960<br>
有了這些偏微分以後，你就可以更新你的參數<br>
<br>
0:41:07.960,0:41:12.180<br>
你就把所有的參數，都減掉一個 learning rate<br>
<br>
0:41:12.180,0:41:15.620<br>
這邊寫成 μ，乘上偏微分的值<br>
<br>
0:41:15.620,0:41:17.500<br>
你就得到一組新的參數<br>
<br>
0:41:17.500,0:41:19.300<br>
這個 process 就反覆進行下去<br>
<br>
0:41:19.300,0:41:23.500<br>
你有了新的參數，再計算一下它的 Gradient<br>
<br>
0:41:23.500,0:41:26.340<br>
然後，再根據你的 Gradient<br>
<br>
0:41:26.340,0:41:29.480<br>
再更新你的參數，你就得到一組新的參數<br>
<br>
0:41:29.480,0:41:32.120<br>
按照這個 process，繼續下去<br>
<br>
0:41:32.120,0:41:33.880<br>
這個都是我們講過的東西<br>
<br>
0:41:33.880,0:41:35.580<br>
你就可以找到一組好的參數<br>
<br>
0:41:35.580,0:41:37.500<br>
你就做完 neural network 的 training 了<br>
<br>
0:41:37.500,0:41:41.000<br>
所以，其實就這樣子啦<br>
<br>
0:41:41.000,0:41:43.140<br>
deep learning 的 training 就這樣子<br>
<br>
0:41:43.140,0:41:45.360<br>
就算是最潮的 AlphaGo<br>
<br>
0:41:45.360,0:41:47.100<br>
也是用 Gradient Descent train 的啦！<br>
<br>
0:41:47.100,0:41:49.900<br>
所以，大家可能會想像說<br>
<br>
0:41:49.900,0:41:51.320<br>
如果是 deep learning 的話<br>
<br>
0:41:51.320,0:41:52.900<br>
machine 的 learning 應該是這樣子<br>
<br>
0:41:52.900,0:41:54.080<br>
其實上，我們知道說<br>
<br>
0:41:54.080,0:41:56.740<br>
Gradient Descent 就是玩世紀帝國，所以其實是這個樣子<br>
<br>
0:41:56.740,0:41:59.020<br>
然後，希望你不要覺得太失望<br>
<br>
0:41:59.800,0:42:02.280<br>
然後，你可能會問說，那這個 Gradient Descent<br>
<br>
0:42:02.280,0:42:03.860<br>
的 function 式子長甚麼樣子呢？<br>
<br>
0:42:03.860,0:42:07.820<br>
之前我們都手把手的把那個算式算出來給大家看<br>
<br>
0:42:07.820,0:42:09.960<br>
但是，在 neural network 裡面<br>
<br>
0:42:09.960,0:42:11.500<br>
因為 function 比較複雜<br>
<br>
0:42:11.500,0:42:14.680<br>
所以，如果我們要手把手的算出來給大家看<br>
<br>
0:42:14.680,0:42:17.380<br>
是比較難，需要花一些時間<br>
<br>
0:42:17.380,0:42:20.500<br>
其實，在現代這個時代<br>
<br>
0:42:20.500,0:42:24.300<br>
我還記得幾年前，在做 deep learning 很痛苦<br>
<br>
0:42:24.300,0:42:26.360<br>
因為你要自己 implement Backpropagation<br>
<br>
0:42:26.360,0:42:31.000<br>
那現在呢，你應該已經沒有人<br>
自己 implement Backpropagation 了<br>
<br>
0:42:31.000,0:42:33.420<br>
因為有太多太多太多的 toolkit<br>
<br>
0:42:33.420,0:42:35.920<br>
可以幫你算 Backpropagation<br>
<br>
0:42:35.920,0:42:38.980<br>
那在作業三，我們要做 deep learning，為了容許你用<br>
<br>
0:42:38.980,0:42:40.060<br>
這些 toolkit<br>
<br>
0:42:40.060,0:42:43.360<br>
所以，你就算不會算這個微分<br>
<br>
0:42:43.360,0:42:47.380<br>
Backpropagation 就是算這個微分的一個比較有效的方式<br>
<br>
0:42:47.380,0:42:48.960<br>
因為參數非常多嘛<br>
<br>
0:42:48.960,0:42:50.420<br>
你有 million的參數<br>
<br>
0:42:50.420,0:42:54.300<br>
你沒有辦法為每個 million 的參數<br>
都算微分，這太花時間了<br>
<br>
0:42:54.300,0:42:58.980<br>
Backpropagation 是一個比較有效率的算這個微分的方式<br>
<br>
0:42:58.980,0:43:01.320<br>
如果你想要更知道詳情的話<br>
<br>
0:43:01.320,0:43:03.420<br>
我之前的 deep learning 課呢<br>
<br>
0:43:03.420,0:43:07.040<br>
內容是有錄影的，你可以聽一下這個上課的內容<br>
<br>
0:43:07.040,0:43:11.360<br>
如果你不想知道的話呢，其實也沒什麼關係<br>
<br>
0:43:11.360,0:43:13.460<br>
我聽過一個傳聞是說<br>
<br>
0:43:13.460,0:43:17.640<br>
就是說有某個公司，它在應徵 deep learning 的人<br>
<br>
0:43:17.640,0:43:20.760<br>
我就問他說，你們要應徵 deep learning 的人，<br>
會問他什麼問題<br>
<br>
0:43:20.760,0:43:22.700<br>
他說，我問她說如何算微分<br>
<br>
0:43:22.700,0:43:26.440<br>
75%、號稱是 deep learning 專家的人，其實不會算微分<br>
<br>
0:43:26.440,0:43:29.080<br>
所以，大家已經太習慣用 toolkit 了<br>
<br>
0:43:29.080,0:43:30.540<br>
也不太會算微分了<br>
<br>
0:43:30.540,0:43:35.660<br>
那 toolkit 很多啦，這邊就列一些出來，給大家參考<br>
<br>
0:43:37.560,0:43:43.940<br>
那最後，在請助教講作業二之前<br>
<br>
0:43:43.940,0:43:46.100<br>
我們有一個最後的問題<br>
<br>
0:43:47.740,0:43:50.320<br>
為甚麼我們要 deep learning？<br>
<br>
0:43:50.320,0:43:52.760<br>
你可能直覺地說這個答案很簡單啊<br>
<br>
0:43:52.760,0:43:55.740<br>
因為越 deep，performance 就越好<br>
<br>
0:43:55.740,0:43:58.080<br>
這個是一個很早年的實驗<br>
<br>
0:43:58.080,0:44:00.920<br>
這個是 2011 年，Interspeech 裡面的某一篇 paper<br>
<br>
0:44:00.920,0:44:03.500<br>
他做的是 word error rate<br>
<br>
0:44:03.500,0:44:06.520<br>
word error rate 是越小越好，你會發現<br>
<br>
0:44:06.520,0:44:07.480<br>
1 個 hidden layer<br>
<br>
0:44:07.480,0:44:10.940<br>
每個 hidden layer，2k 個 neuron，word error rate 是24.2<br>
<br>
0:44:10.940,0:44:12.960<br>
那越來越 deep 以後<br>
<br>
0:44:12.960,0:44:15.980<br>
它的 performance，error rate 就越來越低<br>
<br>
0:44:15.980,0:44:19.940<br>
但是，如果你稍微有一點 machine learning 的常識的話<br>
<br>
0:44:19.940,0:44:23.120<br>
這個結果並沒有讓你太 surprise 阿，因為<br>
<br>
0:44:23.120,0:44:26.420<br>
本來，model 越多的 parameter<br>
<br>
0:44:26.420,0:44:29.680<br>
它 cover 的 function set 就越大<br>
<br>
0:44:29.680,0:44:32.560<br>
它的 bias 就越小，對不對？<br>
<br>
0:44:32.560,0:44:34.760<br>
你今天如果有越多的 training data<br>
<br>
0:44:34.760,0:44:37.360<br>
你有夠多的 training data 去控制它的 variance<br>
<br>
0:44:37.360,0:44:41.540<br>
一個複雜的 model，一個參數比較多的 model<br>
<br>
0:44:41.540,0:44:44.700<br>
它 performance 比較好，是很正常的阿<br>
<br>
0:44:44.700,0:44:48.000<br>
那變 deep 有甚麼特別了不起的地方？<br>
<br>
0:44:48.000,0:44:51.700<br>
有一個理論是這樣告訴我們<br>
<br>
0:44:51.700,0:44:53.740<br>
這個理論是這樣說的<br>
<br>
0:44:53.740,0:44:56.000<br>
任何連續的 function<br>
<br>
0:44:56.000,0:45:00.280<br>
假設任何連續的 function，<br>
它的 input 是一個 N 維的 vector<br>
<br>
0:45:00.280,0:45:02.680<br>
它 output 是一個 M 維的 vector<br>
<br>
0:45:02.680,0:45:07.560<br>
它都可以用一個 hidden layer 的 neural network 來表示<br>
<br>
0:45:07.560,0:45:11.220<br>
只要你這個 hidden layer 的 neuron 夠多<br>
<br>
0:45:11.220,0:45:15.760<br>
它可以表示成任何的 function<br>
<br>
0:45:15.760,0:45:18.040<br>
那既然一個 hidden layer 的 neural network<br>
<br>
0:45:18.040,0:45:20.260<br>
它可以表示成任何 function<br>
<br>
0:45:20.260,0:45:23.940<br>
而我們在做 machine learning 的時候，<br>
我們需要的東西就只是一個 function 而已<br>
<br>
0:45:23.940,0:45:25.680<br>
既然 hidden layer 就可以表示任何 function<br>
<br>
0:45:25.680,0:45:27.720<br>
那做 deep 的意義何在呢？<br>
<br>
0:45:27.720,0:45:29.500<br>
沒有甚麼特別的意義阿<br>
<br>
0:45:29.500,0:45:32.900<br>
所以有人說，deep learning 就只是一個噱頭而已<br>
<br>
0:45:32.900,0:45:34.780<br>
就是做 deep 感覺比較潮<br>
<br>
0:45:34.780,0:45:37.140<br>
如果你只是把它變寬，變成 fat neural network<br>
<br>
0:45:37.140,0:45:38.660<br>
那感覺就太虛弱了<br>
<br>
0:45:38.660,0:45:41.560<br>
所以它沒有辦法引起大家的注意<br>
<br>
0:45:41.560,0:45:43.240<br>
所以，我們要做 deep learning<br>
<br>
0:45:43.240,0:45:47.200<br>
真的是這樣嗎？這個我們在往後的 lecture<br>
<br>
0:45:47.200,0:45:48.580<br>
再來告訴大家<br>
<br>
0:45:48.580,0:45:50.080<br>
最後就是列一些 reference 阿<br>
<br>
0:45:50.080,0:45:53.200<br>
如果你對 deep learning 很有興趣的話，你可以先看一下<br>
<br>
0:45:53.200,0:45:55.960<br>
那還有我上學期講 deep learning 的錄影呢<br>
<br>
0:45:55.960,0:45:57.000<br>
在這邊也找的到<br>
<br>
0:45:57.000,0:46:01.380<br>
那另外呢，我有一個 6 小時的 tutorial 的 slide<br>
<br>
0:46:01.380,0:46:04.200<br>
那你在這個連結上，也可以找到<br>
<br>
0:46:04.200,0:46:09.940<br>
這個 tutorial ，我 11 月還會在新竹再講一次<br>
<br>
0:46:09.940,0:46:12.820<br>
這或許是我人生中最後一次講這個 tutorial<br>
<br>
0:46:12.820,0:46:14.900<br>
千萬不要來聽，因為<br>
<br>
0:46:14.900,0:46:17.280<br>
為甚麼呢？因為在 tutorial 講的東西<br>
<br>
0:46:17.280,0:46:20.680<br>
在未來的課程裡面，都會涵蓋，它就只是一個 subset 而已<br>
<br>
0:46:20.680,0:46:25.160<br>
那接下來來我們就先休息 5 分鐘，<br>
然後趕快就助教來換場一下<br>
<br>
0:46:25.160,0:46:29.320<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
