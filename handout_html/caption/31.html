<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.320,0:00:02.960<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:03.220,0:00:06.720<br>
接下來要講 ensemble<br>
<br>
0:00:16.000,0:00:24.220<br>
那 ensemble 這種方法其實就是團隊合作<br>
<br>
0:00:24.220,0:00:27.500<br>
好幾個模型一起上的方法<br>
<br>
0:00:28.020,0:00:31.280<br>
在做 ensemble 的時候通常狀況是這樣<br>
<br>
0:00:31.720,0:00:37.275<br>
有一打的 classifier，假設現在要做分類的問題<br>
<br>
0:00:39.160,0:00:41.120<br>
f1(x), f2(x), f3(x)<br>
<br>
0:00:41.340,0:00:44.020<br>
你想把這一把的 classifier<br>
<br>
0:00:44.155,0:00:47.225<br>
集合起來讓他們發揮原來<br>
<br>
0:00:47.225,0:00:51.460<br>
一個 classifier 所沒有辦法發揮的更強大的力量<br>
<br>
0:00:51.460,0:00:55.040<br>
這些 classifier 通常會希望他們是 diverse 的<br>
<br>
0:00:55.040,0:00:59.820<br>
每一個 classifier 有不同的屬性<br>
<br>
0:00:59.860,0:01:02.400<br>
有不同的作用<br>
<br>
0:01:04.580,0:01:08.040<br>
如果今天大家一起出團去打王的時候<br>
<br>
0:01:08.140,0:01:13.155<br>
每一個人都會有自己需要做的工作<br>
<br>
0:01:13.160,0:01:16.620<br>
需要一個團隊裡面有各種不同的角色<br>
<br>
0:01:16.620,0:01:19.880<br>
有人扮演坦、有人扮演補、有人扮演DD<br>
<br>
0:01:19.980,0:01:23.600<br>
DD就是輸出類似<br>
<br>
0:01:24.420,0:01:31.320<br>
你要把不同的 classifier 把它集合在一起<br>
<br>
0:01:31.440,0:01:37.600<br>
集合的時候必須要用比較好的方法來把他們集合在一起<br>
<br>
0:01:37.600,0:01:39.960<br>
就好像是在打王的時候<br>
<br>
0:01:39.960,0:01:44.240<br>
坦、補、DD他們有不同要站的位子<br>
<br>
0:01:44.600,0:01:48.335<br>
ensemble 最適合在期末的時候講<br>
<br>
0:01:48.340,0:01:52.700<br>
為甚麼呢？因為假設現在已經開始做 final<br>
<br>
0:01:52.720,0:01:54.640<br>
我相信其實你很累了<br>
<br>
0:01:54.860,0:02:01.980<br>
你可能沒什麼時間為 final 寫甚麼 fancy、新的程式<br>
<br>
0:02:02.080,0:02:04.940<br>
也許你想要摳一下手上現有的程式<br>
<br>
0:02:04.940,0:02:07.400<br>
然後調調參數看可以做到多好<br>
<br>
0:02:07.600,0:02:12.360<br>
而且有一個大招可以迅速 improve 你的 performance<br>
<br>
0:02:12.360,0:02:14.160<br>
就是 ensemble<br>
<br>
0:02:14.160,0:02:18.660<br>
如果你今天已經想不到新招術了<br>
<br>
0:02:18.660,0:02:22.340<br>
你現在做一做已經卡住了不知道怎麼進步的話<br>
<br>
0:02:22.340,0:02:27.960<br>
通常用 ensemble 可以讓你的 performance 再提升一個 level<br>
<br>
0:02:27.960,0:02:31.380<br>
你會發現在 machine learning 比賽的時候<br>
<br>
0:02:31.380,0:02:34.080<br>
在 Kaggle 上的比賽的時候<br>
<br>
0:02:34.080,0:02:37.560<br>
你有一個好的模型，你可以拿到前幾名<br>
<br>
0:02:37.700,0:02:43.360<br>
但你要奪得冠軍你通常會需要 ensemble<br>
<br>
0:02:43.360,0:02:47.720<br>
都會需要群毆的方式才能得到冠軍<br>
<br>
0:02:47.720,0:02:51.620<br>
今天就是要講怎麼來做群毆<br>
<br>
0:02:52.340,0:02:57.800<br>
群毆的方式有幾種不同的方法<br>
<br>
0:02:57.840,0:03:00.915<br>
我們先講 Bagging 這個方法<br>
<br>
0:03:00.920,0:03:07.000<br>
注意一下等一下除了會講 Bagging 之外還會講 Boosting<br>
<br>
0:03:07.040,0:03:12.700<br>
Bagging 和 Boosting 使用的場合是不太一樣的<br>
<br>
0:03:13.360,0:03:15.860<br>
這個大家要特別注意一下<br>
<br>
0:03:15.860,0:03:19.700<br>
複習一下我們在開學講過的東西<br>
<br>
0:03:19.700,0:03:26.040<br>
我們開學講過做 Machine Learning 的時候有 Bias 和 Variance 的 trade-off<br>
<br>
0:03:26.140,0:03:29.120<br>
如果有一個很簡單的 Model<br>
<br>
0:03:29.120,0:03:32.580<br>
我們會有很大的 Bias、比較小的 Variance<br>
<br>
0:03:32.580,0:03:37.680<br>
如果我們有複雜的 Model 可能是小的 Bias、大的 Variance<br>
<br>
0:03:37.780,0:03:48.480<br>
在這兩者的組合下，我們會看到我們的 Error rate 隨著 Model 的複雜度增加逐漸下降再逐漸上升<br>
<br>
0:03:51.140,0:03:56.280<br>
之前也有舉過說假設現在在不同的世界裡面<br>
<br>
0:03:56.280,0:03:58.640<br>
我們都在抓寶可夢<br>
<br>
0:03:58.640,0:04:03.080<br>
在不同的世界裡面我們都會得到一個模型<br>
<br>
0:04:03.120,0:04:07.740<br>
假設我們用的是很複雜的模型<br>
<br>
0:04:07.740,0:04:10.160<br>
我們會有很大的 Variance<br>
<br>
0:04:10.160,0:04:17.280<br>
在不同的世界，我們所預測出來可以預測寶可夢 CP 值的模型會非常的不一樣<br>
<br>
0:04:17.720,0:04:22.920<br>
但是這些結果雖然 Variance 不大<br>
<br>
0:04:22.940,0:04:27.200<br>
但是他們的 Bias 是小的<br>
<br>
0:04:27.520,0:04:31.240<br>
所以我們可以把不同的模型<br>
<br>
0:04:31.240,0:04:39.440<br>
我們可以把不同的模型通通集合起來<br>
<br>
0:04:39.440,0:04:44.940<br>
我們可以把不同模型的輸出做一個平均，得到一個新的模型 f hat<br>
<br>
0:04:44.940,0:04:50.980<br>
這個新的模型 f hat 可能就會跟正確答案是接近的<br>
<br>
0:04:51.120,0:04:55.740<br>
那 Bagging 其實就是要體現這件事情<br>
<br>
0:04:56.420,0:04:59.160<br>
Bagging 要做的事情就是<br>
<br>
0:04:59.200,0:05:04.460<br>
雖然我們不可能到不同的宇宙蒐集 data<br>
<br>
0:05:04.460,0:05:08.380<br>
但是我們可以創造出不同的 data set<br>
<br>
0:05:08.380,0:05:11.900<br>
再用不同的 data set 各自訓練一個複雜的 Model<br>
<br>
0:05:12.000,0:05:16.200<br>
雖然每一個 Model 獨自拿出來看可能 Variance 很大<br>
<br>
0:05:16.200,0:05:19.740<br>
把不同的 Variance 很大的 Model 集合起來以後<br>
<br>
0:05:19.740,0:05:22.080<br>
他的 Variance 就不會那麼大<br>
<br>
0:05:22.080,0:05:24.400<br>
他的 Bias 會是小的<br>
<br>
0:05:24.800,0:05:27.200<br>
怎麼自己製造不同的 data 呢<br>
<br>
0:05:27.200,0:05:30.340<br>
假設現在有 N 筆 Training Data<br>
<br>
0:05:31.140,0:05:35.380<br>
對這 N 筆 Training Data 做 Sampling<br>
<br>
0:05:35.380,0:05:41.340<br>
從這 N 筆 Training Data 裡面每次取 N' 筆 data<br>
<br>
0:05:41.480,0:05:43.900<br>
組成一個新的 Data Set<br>
<br>
0:05:44.000,0:05:48.180<br>
通常在做 Sampling 的時候會做 replacement<br>
<br>
0:05:48.180,0:05:51.480<br>
抽出一筆 data 以後會再把它放到 pool 裡面去<br>
<br>
0:05:52.300,0:05:56.980<br>
那所以通常 N' 可以設成 N<br>
<br>
0:05:57.360,0:05:59.380<br>
所以把 N' 設成 N<br>
<br>
0:05:59.380,0:06:01.200<br>
從 N 這個 Data Set 裡面<br>
<br>
0:06:01.620,0:06:04.700<br>
做 N 次的 Sample with replacement<br>
<br>
0:06:04.800,0:06:10.400<br>
得到的 Data Set 跟原來的這 N 筆 data 並不會一樣<br>
<br>
0:06:10.440,0:06:14.200<br>
因為你可能會反覆抽到同一個 example<br>
<br>
0:06:14.200,0:06:19.360<br>
總之我們就用 sample 的方法建出好幾個 Data Set<br>
<br>
0:06:19.360,0:06:22.840<br>
每一個 Data Set 都有 N' 筆 Data<br>
<br>
0:06:23.020,0:06:27.040<br>
每一個 Data Set 裡面的 Data 都是不一樣的<br>
<br>
0:06:27.040,0:06:28.240<br>
接下來<br>
<br>
0:06:28.520,0:06:31.260<br>
你再用一個複雜的模型<br>
<br>
0:06:31.260,0:06:34.820<br>
去對這四個 Data Set 做 Learning<br>
<br>
0:06:34.820,0:06:38.080<br>
就找出了四個 function<br>
<br>
0:06:38.200,0:06:40.900<br>
接下來在 testing 的時候<br>
<br>
0:06:41.000,0:06:44.600<br>
就把一筆 testing data 丟到這四個 function 裡面<br>
<br>
0:06:44.600,0:06:48.220<br>
再把得出來的結果作平均<br>
<br>
0:06:48.220,0:06:50.300<br>
或者是作 Voting<br>
<br>
0:06:50.300,0:06:54.080<br>
通常就會比只有一個 function 的時候<br>
<br>
0:06:54.080,0:06:55.860<br>
performance 還要好<br>
<br>
0:06:55.860,0:06:58.320<br>
performance 還要好是指說你的<br>
<br>
0:06:58.440,0:07:00.360<br>
Variance 會比較小<br>
<br>
0:07:00.360,0:07:04.340<br>
所以你得到的結果會是比較 robust 的<br>
<br>
0:07:04.340,0:07:07.175<br>
比較不容易 Overfitting<br>
<br>
0:07:07.180,0:07:10.100<br>
如果做的是 regression 方法的時候<br>
<br>
0:07:10.100,0:07:15.140<br>
你可能會用 average 的方法來把四個不同 function 的結果組合起來<br>
<br>
0:07:15.140,0:07:21.240<br>
如果是分類問題的話可能會用 Voting 的方法把四個結果組合起來<br>
<br>
0:07:21.360,0:07:25.180<br>
就看這四個 function 裡面哪一個類別<br>
<br>
0:07:25.220,0:07:31.420<br>
最多 classifier 投票給他，就選那個 class 當作 Model 的 output<br>
<br>
0:07:31.860,0:07:34.740<br>
注意一下，甚麼時候做 Bagging<br>
<br>
0:07:34.740,0:07:39.980<br>
當你的 model 很複雜的時候、擔心它 Overfitting 的時候<br>
<br>
0:07:40.040,0:07:42.120<br>
才做 Bagging<br>
<br>
0:07:42.600,0:07:47.560<br>
做 Bagging 的目的是為了要減低 Variance<br>
<br>
0:07:48.180,0:07:51.980<br>
你的 model Bias 已經很小但 Variance 很大<br>
<br>
0:07:51.980,0:07:54.320<br>
想要減低 Variance 的時候<br>
<br>
0:07:54.320,0:07:55.360<br>
你才做 Bagging<br>
<br>
0:07:55.360,0:07:57.920<br>
所以適用做 Bagging 的情況是<br>
<br>
0:07:57.920,0:08:02.400<br>
你的 Model 本身已經很複雜在 Training Data 上很容易就 Overfit<br>
<br>
0:08:02.400,0:08:04.720<br>
這個時候你會想要用 Bagging<br>
<br>
0:08:04.720,0:08:06.700<br>
甚麼樣的 Model 會很容易 Overfit 呢<br>
<br>
0:08:06.700,0:08:09.760<br>
有人會說 NN 很容易 Overfit<br>
<br>
0:08:09.760,0:08:11.920<br>
沒有，其實 NN 沒有那麼容易 Overfit<br>
<br>
0:08:11.920,0:08:13.860<br>
相較於看你跟誰比<br>
<br>
0:08:13.860,0:08:20.960<br>
很多人就憑著直覺說：一個 Neural Network 參數那麼多應該很容易 Overfit 吧<br>
<br>
0:08:21.060,0:08:24.075<br>
如果你有實作過 Neural Network 的話<br>
<br>
0:08:24.080,0:08:27.780<br>
我想你其實是不會這麼想<br>
<br>
0:08:27.780,0:08:30.620<br>
做 Neural Network 的時候常常遇到的問題是<br>
<br>
0:08:30.620,0:08:34.980<br>
你沒辦法在 Training Set 上 Overfit<br>
而不是你非常容易 Overfit<br>
<br>
0:08:35.080,0:08:36.080<br>
對不對<br>
<br>
0:08:36.900,0:08:39.480<br>
甚麼樣的 Model 非常容易 Overfit 呢<br>
<br>
0:08:39.480,0:08:42.060<br>
舉例來說 Decision Tree<br>
<br>
0:08:42.340,0:08:44.980<br>
就是一個非常容易 Overfit 的方法<br>
<br>
0:08:44.980,0:08:48.420<br>
Decision Tree 你只要想的話，把樹長得很深<br>
<br>
0:08:48.420,0:08:54.080<br>
他在 Training data 上只要夠深都能可以拿到 100% 的正確率<br>
<br>
0:08:54.300,0:08:56.540<br>
NN 很難拿到 100% 的正確率<br>
<br>
0:08:56.540,0:09:01.220<br>
要拿到 100% 的正確率就要在 MNIST 上好好調參數<br>
才能拿到 100% 正確率<br>
<br>
0:09:01.220,0:09:06.000<br>
但像 Decision Tree 這種方法只要他想他可以拿到 100% 正確率<br>
<br>
0:09:06.000,0:09:10.760<br>
但在 Training data 上拿到 100% 正確率不見得有甚麼特別厲害的地方<br>
<br>
0:09:10.780,0:09:13.540<br>
其實就只是 Overfitting 而已<br>
<br>
0:09:13.860,0:09:17.360<br>
所以甚麼時候要做 Bagging<br>
<br>
0:09:17.360,0:09:20.600<br>
就是 Model 很容易 Overfitting 的時候要做 Bagging<br>
<br>
0:09:20.600,0:09:23.140<br>
所以 Decision Tree 很需要做 Bagging<br>
<br>
0:09:23.280,0:09:29.120<br>
Random Forest 就是 Decision Tree 做 Bagging 的版本<br>
就是 Random Forest<br>
<br>
0:09:29.120,0:09:31.340<br>
我們沒有講過 Decision Tree<br>
<br>
0:09:31.340,0:09:34.440<br>
那其實我覺得也不見得需要講<br>
<br>
0:09:34.500,0:09:36.920<br>
你們每個人都知道 Decision Tree 對不對<br>
<br>
0:09:36.920,0:09:40.800<br>
我看大家在作業裡面都能夠用到 Decision Tree<br>
<br>
0:09:40.800,0:09:44.840<br>
都用得很爽，所以就好像是不太需要講<br>
<br>
0:09:44.840,0:09:48.420<br>
我們就秒講過去<br>
<br>
0:09:48.740,0:09:53.980<br>
現在假設每個 object，有兩個 feature x1 跟 x2<br>
<br>
0:09:53.980,0:10:03.520<br>
Decision Tree 就是根據 Training data 建出一棵樹<br>
<br>
0:10:03.660,0:10:08.900<br>
這棵樹告訴我們<br>
<br>
0:10:08.900,0:10:15.580<br>
如果輸入的 object x1 < 0.5 就是 yes<br>
x1 > 0.5 就是 no<br>
<br>
0:10:15.580,0:10:19.860<br>
所以就是在 x1 = 0.5 的地方切一刀<br>
<br>
0:10:19.860,0:10:26.640<br>
以左就走到左邊這條路上去<br>
往右就走到右邊這條路上去<br>
<br>
0:10:26.640,0:10:30.980<br>
接下來再看 x2 < 0.3 的時候<br>
<br>
0:10:30.980,0:10:34.800<br>
那就說是 Class 1，藍色<br>
<br>
0:10:34.860,0:10:39.240<br>
x2 > 0.3 的時候就說是 Class 2，就紅色<br>
<br>
0:10:39.340,0:10:42.280<br>
那如果在右邊呢<br>
<br>
0:10:42.280,0:10:45.820<br>
右邊如果 x2 < 0.7 的時候就塗紅色<br>
<br>
0:10:45.820,0:10:49.360<br>
x2 > 0.7 的時候就塗藍色<br>
<br>
0:10:49.780,0:10:54.820<br>
那這邊這個 Decision Tree 的問題是比較簡單的<br>
<br>
0:10:54.820,0:10:58.820<br>
它只看一個 dimension<br>
其實可以同時看兩個 dimension<br>
<br>
0:10:58.880,0:11:00.600<br>
其實可以問更複雜的問題<br>
<br>
0:11:00.600,0:11:04.740<br>
要問甚麼問題是人自己決定的<br>
<br>
0:11:05.620,0:11:10.120<br>
所以做 Decision Tree 的時候會有很多需要注意的地方<br>
<br>
0:11:10.120,0:11:17.040<br>
舉例來說，在每個節點做多少分支<br>
<br>
0:11:17.040,0:11:21.240<br>
要用甚麼樣的 criterion 來做分支<br>
<br>
0:11:21.240,0:11:24.040<br>
要甚麼時候停止分支<br>
<br>
0:11:24.080,0:11:30.040<br>
有可以問的問題在集合裡面，有那些問題等等<br>
<br>
0:11:32.880,0:11:39.160<br>
也是有很多參數要調，跟 NN 一樣有一些東西是需要調整的<br>
<br>
0:11:39.820,0:11:42.400<br>
我們就來舉一個 Decision Tree 的例子吧<br>
<br>
0:11:42.400,0:11:45.880<br>
我們把 Decision Tree 實作在以下這個 task 上面<br>
<br>
0:11:45.880,0:11:48.740<br>
這個 task 叫做初音 task<br>
<br>
0:11:48.860,0:11:52.720<br>
這個 task 是這樣的，有一個分類的問題<br>
<br>
0:11:52.720,0:12:01.920<br>
這個分類的問題是說輸入的 feature 就是二維<br>
<br>
0:12:01.920,0:12:06.260<br>
這個紅色的部分是屬於 Class 1<br>
<br>
0:12:06.260,0:12:11.100<br>
在藍色的部分是屬於另外一個 Class，Class 2<br>
<br>
0:12:11.100,0:12:17.200<br>
Class 1 分佈的樣子正好就跟初音是一樣的<br>
<br>
0:12:17.200,0:12:22.040<br>
如果你要用這個 data 我放在這邊<br>
你可以載這個 data 來用<br>
<br>
0:12:22.040,0:12:24.280<br>
這個是初音的 task<br>
<br>
0:12:24.280,0:12:28.880<br>
一般教科書都會用方型、圈圈那個都太弱了<br>
<br>
0:12:28.880,0:12:31.280<br>
這個要用初音的 task<br>
<br>
0:12:32.020,0:12:35.860<br>
Class 1 的分佈就跟初音一樣<br>
<br>
0:12:35.860,0:12:44.020<br>
現在 Decision Tree 能不能夠在這個 task 裡面把 Class 1 和 Class 2 進行正確的分類呢<br>
<br>
0:12:44.040,0:12:45.680<br>
我們來看一下結果<br>
<br>
0:12:45.680,0:12:48.055<br>
現在用一棵 Decision Tree<br>
<br>
0:12:48.060,0:12:51.900<br>
那這個 Decision Tree 的深度是 5<br>
<br>
0:12:51.960,0:12:54.860<br>
他沒有辦法把 Class 1 和 Class 2 分開<br>
<br>
0:12:54.880,0:12:59.880<br>
它只能框說這個方塊的地方就是 Class 1<br>
<br>
0:12:59.880,0:13:05.080<br>
如果更深的 Decision Tree 呢<br>
如果 Decision Tree 的深度深達 10 的話<br>
<br>
0:13:05.080,0:13:08.020<br>
看起來就有點初音的樣子<br>
<br>
0:13:08.020,0:13:14.300<br>
不過它有很明顯的鋸齒狀<br>
看起來像是在 Minecraft 世界裡面看到的初音<br>
<br>
0:13:14.780,0:13:18.700<br>
如果 Depth = 15 的話就看起來更好<br>
<br>
0:13:18.700,0:13:21.115<br>
這個樣子看起來就滿對的<br>
<br>
0:13:21.120,0:13:26.000<br>
有一些地方還是有點怪怪的比如說這裡<br>
這邊凸起來一塊<br>
<br>
0:13:26.520,0:13:29.460<br>
如果 Decision Tree 的深度是 20 的話<br>
<br>
0:13:29.460,0:13:36.320<br>
那你就可以完美的把 Class 1 的位置跟 Class 2 的位置區別開來<br>
<br>
0:13:36.320,0:13:40.660<br>
就可以完美的把初音的樣子勾勒出來<br>
<br>
0:13:40.880,0:13:45.280<br>
這個其實沒有甚麼，Decision Tree 只要你想的話<br>
<br>
0:13:45.280,0:13:47.785<br>
永遠可以做到 Error Rate 是 0<br>
<br>
0:13:47.785,0:13:50.060<br>
永遠可以做到正確率是 100<br>
<br>
0:13:50.060,0:13:54.180<br>
因為你想想看最極端的 case 就是這個 tree 一直長下去<br>
<br>
0:13:54.180,0:13:59.980<br>
每一筆 data point 就是一個很深的樹的其中一個節點<br>
<br>
0:13:59.980,0:14:04.760<br>
的其中一片葉子<br>
<br>
0:14:04.800,0:14:07.380<br>
這樣正確率就一定是 100%<br>
<br>
0:14:07.380,0:14:12.720<br>
所以這個沒有甚麼<br>
樹夠深，Decision Tree 可以做出任何 function<br>
<br>
0:14:14.200,0:14:19.340<br>
但是因為 Decision Tree 太容易 Overfitting<br>
<br>
0:14:19.340,0:14:23.080<br>
所以單用一棵 Decision Tree 你往往不見得可以得到好的結果<br>
<br>
0:14:23.120,0:14:28.580<br>
所以要對 Decision Tree 做 Bagging 這個方法就是 Random Forest<br>
<br>
0:14:28.700,0:14:31.820<br>
我們可以用傳統 Bagging 的方法<br>
<br>
0:14:31.820,0:14:36.780<br>
來做 Random Forest<br>
可以用傳統的剛才講的 sample 那個方法來做 Bagging<br>
<br>
0:14:36.780,0:14:42.520<br>
但是如果用那種方法你得到的 tree 通常每棵都沒有差太多<br>
<br>
0:14:42.520,0:14:46.440<br>
所以光用 sample 的方法看起來是不太夠的<br>
<br>
0:14:46.940,0:14:50.120<br>
在做 Random Forest 比較 typical 的方法是<br>
<br>
0:14:50.120,0:14:54.400<br>
在每一次要產生 Decision Tree 的 branch 的時候<br>
<br>
0:14:54.400,0:15:03.800<br>
都 random 的決定哪一些 feature 或哪一些問題是不能用<br>
<br>
0:15:03.940,0:15:07.920<br>
你 random 的決定現在要做 split 的時候<br>
<br>
0:15:07.920,0:15:11.420<br>
那些 question 或那些 feature 不能用就可以促使<br>
<br>
0:15:11.420,0:15:13.900<br>
就算你用的是一模一樣的 dataset<br>
<br>
0:15:13.900,0:15:17.540<br>
每一次你產生的 Decision Tree 也會是不一樣的<br>
<br>
0:15:17.620,0:15:20.840<br>
最後再把所有 Decision Tree 的結果通通集合起來<br>
<br>
0:15:20.840,0:15:23.640<br>
就得到 Random Forest<br>
<br>
0:15:24.240,0:15:28.100<br>
如果是用 Bagging 的方法有一個<br>
<br>
0:15:28.160,0:15:32.040<br>
有一個叫 Out-of-bag 的方法可以幫你做 Validation<br>
<br>
0:15:32.040,0:15:37.000<br>
一般我們在做 validation 都是把手上的 training set 切成兩塊<br>
<br>
0:15:37.000,0:15:41.320<br>
手上原來有 label 的 data 切成兩塊<br>
training set 跟 validation set<br>
<br>
0:15:41.380,0:15:44.140<br>
如果是用 Bagging 的方法的話<br>
<br>
0:15:44.140,0:15:48.260<br>
你可以不要把你的 labeled data 切成 training set 跟 validation set<br>
<br>
0:15:48.260,0:15:54.420<br>
但一樣有 validation 的效果，怎麼做呢<br>
<br>
0:15:54.420,0:15:57.420<br>
因為我們知道在做 Bagging 的時候<br>
<br>
0:15:57.420,0:16:03.280<br>
每一個 function，train 出來的每一個 model 他都只用到部分的 data<br>
<br>
0:16:03.280,0:16:08.540<br>
假設現在 training data 裡面有 x1 到 x4 總共有四筆 data<br>
<br>
0:16:08.660,0:16:12.720<br>
f1 只用第一筆和第二筆 data train<br>
<br>
0:16:12.720,0:16:15.200<br>
f2 只用第三筆、第四筆 data train<br>
<br>
0:16:15.200,0:16:19.800<br>
f3 只用 1、3筆 data train<br>
f4 只用 2、4筆 data train<br>
<br>
0:16:21.160,0:16:30.180<br>
那我們就會知道實際上在 train f2 跟 f4 的時候<br>
<br>
0:16:30.180,0:16:32.355<br>
其實沒有用到 x1<br>
<br>
0:16:32.360,0:16:41.060<br>
所以可以用 f2 加 f4 Bagging 的結果去在 x1 上面 testing 他的 performance<br>
<br>
0:16:41.060,0:16:42.520<br>
同理我們可以用<br>
<br>
0:16:42.520,0:16:45.960<br>
x2 跟 x3 做 Bagging 的結果去 test x2<br>
<br>
0:16:45.960,0:16:48.940<br>
用 f1 跟 f4 Bagging 的結果 test x3<br>
<br>
0:16:48.940,0:16:54.260<br>
用 f1 跟 f3 Bagging 的結果 test x4<br>
<br>
0:16:54.860,0:16:58.780<br>
接下來再把 x1 跟 x4 的結果<br>
<br>
0:16:58.780,0:17:00.740<br>
把它做平均<br>
<br>
0:17:00.740,0:17:05.500<br>
算一下 error rate 就得到 Out-of-bag 的 error<br>
<br>
0:17:05.500,0:17:09.200<br>
雖然這邊沒有明確的切出一個 Validation Set<br>
<br>
0:17:09.220,0:17:11.940<br>
但是在做 testing 的時候所用的 model<br>
<br>
0:17:11.940,0:17:14.680<br>
並沒有看過那些 testing 的 data<br>
<br>
0:17:14.680,0:17:17.600<br>
在 test x1 到 x4 的時候<br>
<br>
0:17:17.600,0:17:27.560<br>
這些 model 並沒有看過 x1 到 x4<br>
<br>
0:17:27.560,0:17:37.460<br>
所以這個 Out-of-bag error 其實也是一個可以在 Testing set 上可以反應 testing set 結果的 estimation<br>
<br>
0:17:37.620,0:17:44.780<br>
那我們看一下 Random Forest 在初音的 task 上的結果<br>
<br>
0:17:47.900,0:17:51.320<br>
這邊是做一百棵樹<br>
<br>
0:17:51.320,0:17:54.820<br>
你會發現如果是一百棵 Depth = 5 的樹<br>
<br>
0:17:54.820,0:17:57.080<br>
做出來的結果是這個樣子<br>
<br>
0:17:57.220,0:18:02.740<br>
這邊要強調一下做 Bagging 並不會使你的 model 更能夠 fit data<br>
<br>
0:18:03.080,0:18:06.460<br>
所以 Depth 是 5 的樹沒有辦法 fit 出那個 function<br>
<br>
0:18:06.460,0:18:09.760<br>
用 Random Forest 還是沒有辦法 fit 出那個 function<br>
<br>
0:18:09.760,0:18:14.400<br>
可以得到的結果只是現在因為是把五棵樹平均起來<br>
<br>
0:18:14.400,0:18:20.560<br>
所以得到整體的 function 他是比較平滑的而已<br>
<br>
0:18:20.620,0:18:25.560<br>
所以比如說 Depth = 10<br>
看起來就比較不像 Minecraft 的世界就是了<br>
<br>
0:18:25.880,0:18:28.460<br>
如果 Depth = 15 得到的結果是這樣<br>
<br>
0:18:28.460,0:18:31.980<br>
看起來很好但其實他是有一個瑕疵的<br>
<br>
0:18:31.980,0:18:36.180<br>
他有一些地方沒有做好<br>
我記得這邊有一條頭髮垂下來<br>
<br>
0:18:36.180,0:18:39.760<br>
他沒有把那條頭髮框出來的樣子<br>
<br>
0:18:39.760,0:18:42.680<br>
我看一下，喔對沒錯<br>
如果是 Depth = 20<br>
<br>
0:18:42.680,0:18:47.360<br>
把一棵 Depth = 20 的樹<br>
就可以完美的把初音框出來<br>
<br>
0:18:47.360,0:18:49.880<br>
這邊其實是有一條頭髮<br>
<br>
0:18:49.880,0:18:52.380<br>
要把這個做出來才是真的正確<br>
<br>
0:18:54.240,0:18:58.420<br>
接下來要講 Boosting<br>
<br>
0:18:58.420,0:19:00.780<br>
Boosting 跟剛才的 Bagging 是不一樣的<br>
<br>
0:19:00.780,0:19:02.960<br>
Bagging 是用在很強的 model<br>
<br>
0:19:02.960,0:19:08.040<br>
Boosting 是用在弱的 model 上面<br>
<br>
0:19:08.040,0:19:11.100<br>
當你有一些弱的 model<br>
<br>
0:19:11.140,0:19:17.060<br>
但問題是你沒有辦法讓他們去 fit data 的時候<br>
<br>
0:19:17.120,0:19:20.420<br>
這個時候你就會想要用 Boosting<br>
<br>
0:19:20.520,0:19:24.380<br>
Boosting 是這樣<br>
Boosting 他<br>
<br>
0:19:24.760,0:19:29.540<br>
它有一個很 powerful 的 guarantee<br>
<br>
0:19:29.540,0:19:31.860<br>
這個很 powerful 的 guarantee 是這樣說的<br>
<br>
0:19:31.860,0:19:34.320<br>
假設有一個 ML 的 algorithm<br>
<br>
0:19:34.560,0:19:40.580<br>
它可以給你一個錯誤率高過 50% 的 classifier<br>
<br>
0:19:40.580,0:19:42.700<br>
假設要做分類的問題<br>
<br>
0:19:42.740,0:19:45.560<br>
那錯誤率高過 50% 的 classifier<br>
<br>
0:19:46.100,0:19:47.700<br>
假設這二元分類的問題<br>
<br>
0:19:47.700,0:19:49.820<br>
用 random 猜都高過 50%<br>
<br>
0:19:49.820,0:19:54.420<br>
很爛的模型都可以辦到<br>
<br>
0:19:54.420,0:19:58.540<br>
只要能夠做到這件事<br>
<br>
0:19:58.680,0:20:08.880<br>
Boosting 這個方法可以保證最後把這些錯誤率僅略高於 50% 的 classifier 組合起來以後<br>
<br>
0:20:08.880,0:20:12.160<br>
它可以讓錯誤率達到 0%<br>
<br>
0:20:12.160,0:20:16.160<br>
有沒有聽起來非常神奇<br>
<br>
0:20:16.260,0:20:18.780<br>
聽起來就是非常的強<br>
<br>
0:20:18.840,0:20:21.460<br>
Boosting 的 framework<br>
<br>
0:20:21.640,0:20:25.360<br>
整個大架構大概是這樣<br>
<br>
0:20:25.740,0:20:29.760<br>
首先找一個 classifier f1<br>
<br>
0:20:29.760,0:20:33.280<br>
這個 classifier f1 很弱沒有關係<br>
<br>
0:20:33.280,0:20:38.960<br>
接下來再找一個 classifier f2 他去輔助 f1<br>
<br>
0:20:38.960,0:20:45.440<br>
但要注意 f2 跟 f1 不可以很像<br>
他們要是互補<br>
<br>
0:20:45.540,0:20:47.800<br>
f2 跟 f1 的特性是互補<br>
<br>
0:20:47.900,0:20:50.815<br>
f2 要去彌補 f1 的缺失<br>
<br>
0:20:50.815,0:20:53.995<br>
f2 要去做 f1 沒有辦法做到的事情<br>
<br>
0:20:54.020,0:20:57.580<br>
這樣進步量才大<br>
那 Boosting<br>
<br>
0:20:57.640,0:21:00.500<br>
等一下就會講怎麼樣找到一個 f2<br>
<br>
0:21:00.500,0:21:03.300<br>
它跟 f1 是最互補的<br>
<br>
0:21:03.300,0:21:05.680<br>
然後就得到第二個 classifier f2<br>
<br>
0:21:05.860,0:21:08.880<br>
接下來呢 你再找說 我先找 classifier f2<br>
<br>
0:21:08.880,0:21:11.480<br>
再找一個 f3 跟 f2 是互補的<br>
<br>
0:21:11.480,0:21:14.220<br>
接下來再找一個 f4 跟 f3 是互補的<br>
<br>
0:21:14.220,0:21:17.940<br>
這個 process 就繼續下去，找到一把的 classifier<br>
<br>
0:21:17.940,0:21:22.040<br>
再把這把 classifier 及合起來就可以得到很低的 error rate<br>
<br>
0:21:22.040,0:21:24.735<br>
就算是每個 classifier 他們都很弱<br>
<br>
0:21:24.740,0:21:26.960<br>
也沒有關係<br>
<br>
0:21:27.240,0:21:30.940<br>
要注意的是在做 Boosting 的時候<br>
<br>
0:21:31.260,0:21:35.560<br>
classifier 的訓練是有順序的<br>
<br>
0:21:35.560,0:21:37.940<br>
要先找出 f1<br>
<br>
0:21:37.940,0:21:40.800<br>
才找得出 f2 才找得出 f3<br>
<br>
0:21:40.800,0:21:42.780<br>
它是 sequential 的<br>
<br>
0:21:42.780,0:21:46.740<br>
要先找 f1 才知道怎麼找 f2 跟 f1 是互補的<br>
<br>
0:21:46.800,0:21:49.160<br>
所以它是有順序的找<br>
<br>
0:21:49.160,0:21:51.440<br>
那前面在 Bagging 的時候<br>
<br>
0:21:51.440,0:21:54.140<br>
每一個 classifier 是沒有順序的<br>
<br>
0:21:56.380,0:21:59.120<br>
在做 Random Forest 要 train 一百棵 Decision Tree<br>
<br>
0:21:59.120,0:22:01.620<br>
這一百棵 Decision Tree 可以一起做<br>
<br>
0:22:01.620,0:22:07.620<br>
但這邊如果是要把一百個 Decision Tree 用 Boosting 的方法把它變得很強的話<br>
<br>
0:22:07.660,0:22:12.340<br>
要按順序做，它不是平行做的方法<br>
<br>
0:22:12.420,0:22:17.000<br>
這邊假設我們考慮的一個 task 是一個 Binary Classification 的 task<br>
<br>
0:22:17.000,0:22:19.600<br>
就是有一堆 training data，x 跟 y hat<br>
<br>
0:22:19.640,0:22:22.620<br>
y hat 就等於 +-1<br>
<br>
0:22:33.540,0:22:41.680<br>
接下來要講的是怎麼得到不同的 classifier<br>
<br>
0:22:41.680,0:22:45.140<br>
剛剛在 Bagging 的時候講過<br>
<br>
0:22:45.360,0:22:47.340<br>
要得到不同的 classifier<br>
<br>
0:22:47.340,0:22:50.700<br>
可以用製造不同的 training set 的方式<br>
<br>
0:22:50.700,0:22:54.140<br>
來得到不同的 classifier<br>
<br>
0:22:54.240,0:22:59.020<br>
在 Boosting 的時候也可以這麼做<br>
<br>
0:22:59.020,0:23:07.720<br>
可以用 resample 的方式來製造不同的 training data<br>
然後得到不同的 classifier<br>
<br>
0:23:07.980,0:23:12.820<br>
但是有另外一種方法可以幫你製造出不同的 dataset<br>
<br>
0:23:12.820,0:23:17.840<br>
你可以給你 training data 裡面的每一筆 data 一個 weight<br>
<br>
0:23:17.960,0:23:19.340<br>
舉例來說<br>
<br>
0:23:19.340,0:23:25.020<br>
這邊用 u 來代表每一筆 data 的 weight<br>
<br>
0:23:25.420,0:23:27.100<br>
一開始<br>
<br>
0:23:27.100,0:23:34.040<br>
可以藉由改變這個 weight 來製造不同的 dataset<br>
<br>
0:23:34.040,0:23:36.760<br>
舉例來說現在本來有三筆 data<br>
<br>
0:23:36.760,0:23:39.120<br>
每一筆 data 的 weight 都是 1<br>
<br>
0:23:39.140,0:23:43.420<br>
你可以把它改成第一筆 data weight 是 0.4<br>
<br>
0:23:43.420,0:23:47.040<br>
第二筆 data weight 是 2.1<br>
第三筆 data weight 是 0.7<br>
<br>
0:23:47.100,0:23:53.100<br>
這樣就等於製造出了一個新的 dataset<br>
<br>
0:23:53.220,0:23:57.100<br>
其實 sampling 也可以視同是改了 weight<br>
<br>
0:23:57.100,0:23:59.760<br>
只是 sampling 比如說某一筆 data 被 sample 兩次<br>
<br>
0:23:59.760,0:24:02.275<br>
就代表他的 weight 變成 2<br>
<br>
0:24:02.275,0:24:05.665<br>
如果用 sampling 的方法 weight 只能是整數<br>
<br>
0:24:06.315,0:24:09.395<br>
直接調一個 weight u 的話可以給小數就是了<br>
<br>
0:24:11.620,0:24:14.460<br>
就算是改變了 weight<br>
<br>
0:24:14.460,0:24:16.920<br>
對 training 也不會有太大的影響<br>
<br>
0:24:16.920,0:24:19.200<br>
我們知道在 training 的時候<br>
<br>
0:24:19.200,0:24:24.060<br>
原來 Objective Function 是寫成這個樣子<br>
<br>
0:24:24.180,0:24:26.805<br>
有一個 Loss Function 要去 minimize 他<br>
<br>
0:24:26.805,0:24:32.120<br>
這個 Loss Function 是 summation over 所有的 training data<br>
<br>
0:24:32.120,0:24:35.580<br>
對每一筆 training data x n<br>
<br>
0:24:35.600,0:24:38.940<br>
都把他帶到 function f 裡面去得到 f(x n)<br>
<br>
0:24:38.940,0:24:42.320<br>
計算 f(x n) 跟 y hat n 的差距<br>
<br>
0:24:42.320,0:24:45.520<br>
這個差距就用 Loss Function 來表示<br>
<br>
0:24:45.520,0:24:48.700<br>
這個小 L 可以是各種不同的 function<br>
<br>
0:24:48.700,0:24:54.040<br>
反正能夠量 f(x n) 跟 y hat n 之間的差異就行了<br>
<br>
0:24:54.040,0:24:56.160<br>
然後就用 Gradient Descent 的方法<br>
<br>
0:24:56.160,0:25:01.580<br>
去找一個 function f 來 minimize 大 L 這個 Total Loss Function<br>
<br>
0:25:01.580,0:25:04.300<br>
如果加上 weight 的話有甚麼不同呢<br>
<br>
0:25:04.300,0:25:10.220<br>
沒有甚麼不同，唯一的不同只有會在每一個小 L 的 function 前面<br>
<br>
0:25:10.220,0:25:12.680<br>
乘上 u<br>
<br>
0:25:12.680,0:25:18.060<br>
會在每一個小 L 的 function 前面乘上那筆 data 的 weight<br>
<br>
0:25:18.060,0:25:20.240<br>
代表那筆 data 的權重<br>
<br>
0:25:20.240,0:25:23.080<br>
如果有一筆 data 它的權重比較重<br>
<br>
0:25:23.080,0:25:26.200<br>
他的 u 比較大，那在 training 的時候<br>
<br>
0:25:26.200,0:25:29.380<br>
他就會被多考慮一點<br>
<br>
0:25:29.960,0:25:32.980<br>
那有了這個概念以後<br>
<br>
0:25:32.980,0:25:35.880<br>
Adaboost 的精神是甚麼<br>
<br>
0:25:37.500,0:25:44.340<br>
Boosting 有很多的方法等一下我們要介紹其中最經典的 Adaboost<br>
<br>
0:25:46.800,0:25:52.180<br>
Adaboost 他的想法是<br>
<br>
0:25:54.040,0:25:58.940<br>
先訓練好一個 classifier f1<br>
<br>
0:25:59.040,0:26:05.560<br>
要去找一組新的 training data<br>
<br>
0:26:05.560,0:26:10.860<br>
所謂找一組新的 training data 其實就是 reweight training 的 example<br>
<br>
0:26:10.860,0:26:13.040<br>
要去找一組新的 training data<br>
<br>
0:26:17.400,0:26:22.140<br>
讓 f1 在這組新的 training data 上結果是會爛掉的<br>
<br>
0:26:22.140,0:26:25.480<br>
會 fail 掉，正確率會變成只有 50%<br>
<br>
0:26:25.480,0:26:29.880<br>
要找一組新的 training data<br>
f1 在這組新的 training data 是做不好的<br>
<br>
0:26:29.880,0:26:35.140<br>
然後再用 f2 在這組新的 training data 上面訓練<br>
<br>
0:26:35.280,0:26:39.580<br>
接下來怎麼找 f1<br>
<br>
0:26:39.680,0:26:48.820<br>
怎麼找一個新的 training data 可以讓 f1 壞掉<br>
假設給你一個 f1<br>
<br>
0:26:49.100,0:26:55.960<br>
先來看一下 f1 在 training data 上的 Error Rate 怎麼計算<br>
<br>
0:26:56.040,0:27:01.500<br>
f1 在 training data 上的 Error Rate 這邊寫成 epsilon 1<br>
<br>
0:27:01.500,0:27:03.880<br>
epsilon 1 的計算方法就是<br>
<br>
0:27:03.880,0:27:11.280<br>
summation over 所有的 training example n<br>
<br>
0:27:11.280,0:27:20.720<br>
然後去計算每一筆 training example 的結果是不是對的<br>
<br>
0:27:20.720,0:27:23.120<br>
如果是對的話就是 0<br>
<br>
0:27:23.120,0:27:25.040<br>
如果是錯的話就是 1<br>
<br>
0:27:25.040,0:27:32.280<br>
每一筆 training example 還要乘上他的 weight u n<br>
<br>
0:27:32.340,0:27:36.860<br>
然後再做一下 normalization<br>
因為這個 u n 的值<br>
<br>
0:27:37.040,0:27:40.620<br>
合起來不見得是 1<br>
所以要做一個 normalization<br>
<br>
0:27:40.620,0:27:43.640<br>
這個 normalization 就是 summation over<br>
<br>
0:27:43.640,0:27:45.480<br>
所有的 u1<br>
<br>
0:27:45.480,0:27:47.540<br>
summation over 所有的 weight<br>
<br>
0:27:47.540,0:27:50.320<br>
就是這個 normalization 的 term<br>
<br>
0:27:50.320,0:27:53.660<br>
那 epsilon 1 一定會小於 0.5<br>
<br>
0:27:53.660,0:27:57.580<br>
因為我們假設 classifier 是還可以的<br>
<br>
0:27:58.820,0:28:01.620<br>
不是一個完全 random 的 classifier<br>
<br>
0:28:01.620,0:28:04.620<br>
所以 Error Rate 總是可以小於 0.5<br>
<br>
0:28:06.620,0:28:10.160<br>
其實沒有辦法製造一個 classifier 他的 Error Rate > 0.5<br>
<br>
0:28:10.160,0:28:14.720<br>
你知道嗎，因為 classifier 他的 Error Rate > 0.5只要把它的 output 反過來<br>
<br>
0:28:14.720,0:28:17.620<br>
它的 Error Rate 就小於 0.5 了<br>
<br>
0:28:26.520,0:28:29.160<br>
原來 training data 的 weight 是 u1<br>
<br>
0:28:29.160,0:28:32.880<br>
要給一組新的 training data 的 weight u2<br>
<br>
0:28:32.880,0:28:37.600<br>
這組新的 training data 的 weight 會使得<br>
<br>
0:28:37.600,0:28:41.580<br>
如果把上面這個算 epsilon 1 的式子的 u1<br>
<br>
0:28:41.700,0:28:44.640<br>
換成 u2 得到的結果<br>
<br>
0:28:44.640,0:28:46.480<br>
會變成 0.5<br>
<br>
0:28:46.480,0:28:49.480<br>
本來 epsilon 1 是小於 0.5<br>
<br>
0:28:49.520,0:28:53.920<br>
在 u1 作為 weight 做計算的時候小於 0.5<br>
<br>
0:28:53.920,0:29:00.140<br>
把 u1 換成 u2 weight 就變成 0.5<br>
<br>
0:29:00.480,0:29:05.480<br>
這個就好像是假如重新 weight 我們的 training data<br>
<br>
0:29:05.480,0:29:07.800<br>
本來用 u1 作為 training data 的 weight<br>
<br>
0:29:07.800,0:29:09.380<br>
現在用 u2 作為 training data 的 weight<br>
<br>
0:29:09.380,0:29:11.560<br>
在這組新的 weight 上面<br>
<br>
0:29:11.560,0:29:16.340<br>
f1 的 performance 就像是隨機的一樣<br>
<br>
0:29:16.340,0:29:22.080<br>
接下來再拿這組新的 training data<br>
<br>
0:29:22.080,0:29:26.660<br>
用 u2 當作 weight 的 training data 再去訓練 f2<br>
<br>
0:29:26.660,0:29:30.140<br>
f2 就會跟 f1 是互補的<br>
<br>
0:29:30.900,0:29:35.240<br>
這樣講也許有點抽象舉一個實際的例子<br>
<br>
0:29:35.240,0:29:37.380<br>
現在有四筆 training data<br>
<br>
0:29:37.800,0:29:41.780<br>
這四筆 training data 的 weight 就是 u1 到 u4<br>
<br>
0:29:41.780,0:29:47.940<br>
假設 u1, u2, u3, u4 通通等於 1<br>
這四筆 training data 的 weight 是一樣的<br>
<br>
0:29:47.940,0:29:52.020<br>
用這四筆 training data 去訓練一個模型<br>
<br>
0:29:52.020,0:29:54.920<br>
去訓練一個 classifier f1<br>
<br>
0:29:54.920,0:29:58.700<br>
假設 f1 它不是一個特別 powerful 的 algorithm<br>
<br>
0:29:58.700,0:30:03.300<br>
所以就算是 training data 他也沒有辦法每一筆 training data 都分類正確<br>
<br>
0:30:03.300,0:30:07.760<br>
假設他指正確分類三筆 training data<br>
一筆 training data 是分類是錯的<br>
<br>
0:30:08.560,0:30:11.680<br>
所以它的 Error Rate 是 0.25<br>
<br>
0:30:11.680,0:30:16.620<br>
四筆 training data 它分錯一筆所以它的 Error Rate 是 0.25<br>
<br>
0:30:16.720,0:30:19.520<br>
接下來要改變 data 的 weight<br>
<br>
0:30:19.520,0:30:22.860<br>
要把 u 值變一下<br>
<br>
0:30:22.860,0:30:26.980<br>
讓 f1 在新的 training dataset 上<br>
<br>
0:30:26.980,0:30:29.080<br>
它的 error 變成 0.5<br>
<br>
0:30:29.080,0:30:30.900<br>
怎麼改<br>
其實有不同的改法<br>
<br>
0:30:33.200,0:30:36.340<br>
假設 u1 的 weight 是 1/√3<br>
<br>
0:30:36.340,0:30:41.840<br>
現在要讓 f1 的 error 變大<br>
<br>
0:30:41.840,0:30:46.480<br>
怎麼讓 f1 的 error 變大<br>
就是看它答對哪幾題<br>
<br>
0:30:46.480,0:30:48.460<br>
那幾題的配分就變小<br>
<br>
0:30:48.500,0:30:51.840<br>
答錯哪幾題，那幾題的配分就變大<br>
<br>
0:30:51.840,0:30:56.780<br>
考試的時候先把考卷寫完<br>
<br>
0:30:56.780,0:31:00.220<br>
老師也改完以後再重新去計算配分<br>
<br>
0:31:00.220,0:31:03.940<br>
答錯的配分就比較高<br>
答對的配分就比較低<br>
<br>
0:31:03.940,0:31:05.960<br>
你就會發狂生氣<br>
<br>
0:31:06.280,0:31:09.280<br>
今天要做的事情就是要讓 f1 生氣<br>
<br>
0:31:11.040,0:31:13.560<br>
我們先看它答對那些答錯那些<br>
<br>
0:31:13.560,0:31:18.880<br>
它答對的<br>
本來跟他說好每一題的配分都是一樣的<br>
<br>
0:31:18.880,0:31:21.600<br>
但是你騙它，它答完以後<br>
<br>
0:31:21.600,0:31:23.640<br>
再改一下題目的配分<br>
<br>
0:31:23.640,0:31:27.560<br>
第一題它答對了<br>
所以配分就變成 1/√3<br>
<br>
0:31:27.620,0:31:31.220<br>
第二題它答錯了<br>
所以配分就增加變成 √3<br>
<br>
0:31:31.220,0:31:35.340<br>
第三題跟第四題都答對了<br>
所以配分都減少變成 1/√3<br>
<br>
0:31:35.340,0:31:39.760<br>
如果在這筆新的 training data 情況下就會變成<br>
<br>
0:31:40.160,0:31:43.540<br>
f1 就會變得很糟<br>
因為你想想看<br>
<br>
0:31:43.620,0:31:48.440<br>
他答錯的題目 weight 是 √3<br>
<br>
0:31:48.440,0:31:51.980<br>
它答對的題目 weight 是 1/√3 有三題<br>
<br>
0:31:51.980,0:31:55.900<br>
1/√3 * 3 也是 √3<br>
<br>
0:31:55.900,0:31:58.880<br>
所以答錯的題目跟答對的題目的 weight 是一樣<br>
<br>
0:31:59.000,0:32:06.220<br>
所以 f1 的 Error Rate 就變成 0.5<br>
<br>
0:32:06.220,0:32:10.020<br>
接下來在這組新的 training data 上面<br>
<br>
0:32:10.020,0:32:12.280<br>
這組新的 training data 可以讓 f1 整個爛掉<br>
<br>
0:32:12.280,0:32:15.940<br>
在這組新的 training data 上面再去訓練 f2<br>
<br>
0:32:15.980,0:32:24.660<br>
那 f2 因為它是看著這組新的 weight、新的配分去做練習的<br>
<br>
0:32:24.660,0:32:30.900<br>
所以新的 Error Rate 在這組 weight 上它的 error 會是小於 0.5<br>
<br>
0:32:30.900,0:32:33.280<br>
所以 f2 可以跟 f1 是互補<br>
<br>
0:32:33.280,0:32:42.580<br>
更詳細的證明之後會有<br>
今天都是講個精神<br>
<br>
0:32:45.460,0:32:56.400<br>
接下來講一下實際怎麼做 reweight 這件事情<br>
<br>
0:33:00.320,0:33:04.440<br>
如果某一筆 data x n<br>
<br>
0:33:04.440,0:33:06.760<br>
他會被 f1 分類錯<br>
<br>
0:33:06.760,0:33:14.080<br>
就把第 n 筆 data 的 weight u1 乘上一個值 d1 變成 u2<br>
<br>
0:33:14.080,0:33:16.640<br>
這個 d1 是大於 0 的值<br>
<br>
0:33:16.640,0:33:22.800<br>
也就是 x n 如果分類錯誤的話就那一個題目、那筆 data 的權重提高<br>
<br>
0:33:22.800,0:33:24.600<br>
乘上 d1 把它提高<br>
<br>
0:33:25.080,0:33:32.780<br>
如果 x n 是正確的被 f1 分類的話<br>
<br>
0:33:32.840,0:33:39.360<br>
就把 u1 除掉 d1，把它變小<br>
<br>
0:33:41.020,0:33:47.780<br>
所以對錯的就增加，對的就變小<br>
<br>
0:33:48.780,0:33:52.680<br>
f2 會在新的 weight u2 上面<br>
<br>
0:33:52.680,0:33:54.700<br>
進行訓練<br>
<br>
0:33:54.700,0:33:59.320<br>
再來的問題就是 d1 的值要設多少<br>
<br>
0:33:59.320,0:34:01.500<br>
這邊沒有甚麼高深的數學<br>
<br>
0:34:01.500,0:34:02.900<br>
就是推一下<br>
<br>
0:34:02.900,0:34:05.040<br>
要設甚麼樣的 d1<br>
<br>
0:34:05.040,0:34:10.040<br>
可以讓 u1 變成 u2 以後可以讓 f1 的 Error Rate 是 0.5<br>
<br>
0:34:10.040,0:34:16.520<br>
這邊就只是數學式比較繁瑣但其實很簡單的數學<br>
<br>
0:34:16.520,0:34:19.280<br>
這個數學是這樣<br>
<br>
0:34:19.280,0:34:23.740<br>
已經計算出 epsilon 1<br>
epsilon 1 的式子是這個樣子<br>
<br>
0:34:23.740,0:34:26.800<br>
現在希望把 u1 換成 u2<br>
<br>
0:34:26.800,0:34:30.800<br>
得到的 weight 是 0.5<br>
<br>
0:34:34.660,0:34:44.240<br>
原則就是如果第二筆 data 的分類是錯誤的那就乘上 d1<br>
<br>
0:34:44.240,0:34:48.420<br>
如果是正確的就除掉 d1<br>
<br>
0:34:48.420,0:34:52.980<br>
先看一下上面這邊<br>
<br>
0:34:52.980,0:34:59.160<br>
上面這邊是指 summation over 分類錯誤的那些 data<br>
<br>
0:34:59.160,0:35:04.500<br>
上面這邊是指先 summation over 分類錯誤的那些 data<br>
<br>
0:35:04.500,0:35:09.640<br>
所以上面的這些 u2 都是分類錯誤的，所以都會乘上 d1<br>
<br>
0:35:09.640,0:35:12.680<br>
所以上面分子的地方<br>
<br>
0:35:12.680,0:35:19.440<br>
可以寫成 summation over u1 乘上 d1<br>
<br>
0:35:19.440,0:35:26.300<br>
上面這些 u 每一筆都是 u1 乘上 d1<br>
<br>
0:35:26.300,0:35:29.040<br>
因為他們都是分類錯的<br>
<br>
0:35:29.040,0:35:31.800<br>
再來看分子的地方<br>
<br>
0:35:31.805,0:35:34.815<br>
分子的地方是 summation over u2<br>
<br>
0:35:34.820,0:35:36.300<br>
u2 有兩個 case<br>
<br>
0:35:36.300,0:35:41.560<br>
一個是如果 f1 會把這筆 data 分類錯誤的話<br>
<br>
0:35:41.560,0:35:44.800<br>
那 u2 是來自於 u1 * d1<br>
<br>
0:35:44.800,0:35:46.580<br>
如果是分類正確的話<br>
<br>
0:35:46.620,0:35:51.500<br>
那 u2 就是來自 u1/d1<br>
<br>
0:35:51.500,0:35:57.060<br>
所以這整個式子列出來的話就是這個樣子<br>
<br>
0:35:57.160,0:36:00.720<br>
然後把分子的地方<br>
<br>
0:36:00.720,0:36:02.040<br>
帶進去<br>
<br>
0:36:02.040,0:36:06.940<br>
分母的地方，這一項帶進去得到這個式子<br>
<br>
0:36:06.940,0:36:12.080<br>
這個式子是等於 0.5<br>
<br>
0:36:20.280,0:36:24.280<br>
然後把分子和分母倒過來<br>
<br>
0:36:24.280,0:36:27.340<br>
所以左邊分子和分母倒過來<br>
<br>
0:36:27.340,0:36:31.880<br>
右邊就從 0.5 變成 2<br>
<br>
0:36:32.240,0:36:40.500<br>
接下來發現分子和分母都有共同的這一項<br>
<br>
0:36:40.500,0:36:45.420<br>
所以知道這一項除以這一項<br>
<br>
0:36:45.420,0:36:48.560<br>
這一項除以這一項<br>
<br>
0:36:48.640,0:36:50.880<br>
會等於 1<br>
<br>
0:36:50.880,0:36:59.180<br>
這告訴我們 u1 除以 d1<br>
<br>
0:36:59.180,0:37:04.280<br>
把所有那些 f1 會答對的 data x n 拿出來<br>
<br>
0:37:04.280,0:37:07.500<br>
把他們的 u1 除以 d1<br>
<br>
0:37:07.500,0:37:14.380<br>
要等於所有 f1 會答錯的那些 x n 他們的 u1 乘以 d1<br>
<br>
0:37:14.380,0:37:21.500<br>
也不用剛才推導其實也可以很直覺地寫出這個式子<br>
<br>
0:37:21.500,0:37:26.740<br>
如果要讓 f1 在新的 weight 的 Error Rate 是 0.5 的話<br>
<br>
0:37:26.740,0:37:32.620<br>
當然他答對的部分的新的 weight<br>
<br>
0:37:32.640,0:37:36.280<br>
要等於答錯的部分的新的 weight<br>
<br>
0:37:36.700,0:37:50.380<br>
接下來就把 d1 提出去<br>
<br>
0:37:59.100,0:38:05.120<br>
epsilon 1 可以寫成這個樣子<br>
<br>
0:38:05.120,0:38:12.280<br>
epsilon 1 分子的地方是對那些答錯的 example x n 的 weight 的總和<br>
<br>
0:38:12.520,0:38:15.480<br>
然後再做 normalization，那這一項<br>
<br>
0:38:15.600,0:38:18.040<br>
出現在這個地方<br>
<br>
0:38:18.040,0:38:24.660<br>
所以可以把這一項用 epsilon 1 把它代換掉<br>
<br>
0:38:24.660,0:38:30.020<br>
所以這一項 = z1 * epsilon 1<br>
<br>
0:38:30.020,0:38:33.080<br>
那這一項呢<br>
<br>
0:38:33.080,0:38:39.980<br>
這一項是 Z1 *( 1 - epsilon 1 )<br>
<br>
0:38:39.980,0:38:48.460<br>
因為這一項加這一項會是 Z1<br>
<br>
0:38:48.460,0:38:50.420<br>
既然它是 Z1 * epsilon 1<br>
<br>
0:38:50.420,0:38:52.660<br>
它就是 Z1 *( 1 - epsilon 1)<br>
<br>
0:38:52.660,0:38:56.980<br>
總之經過一翻推導以後<br>
<br>
0:38:56.980,0:39:06.800<br>
你會算出來 d1 = 更號 1 除以 epsilon 1 除以 epsilon 1<br>
<br>
0:39:06.800,0:39:11.620<br>
拿這個 d1 去乘或者是除 u1<br>
<br>
0:39:11.620,0:39:18.200<br>
就可以製造一個 training dataset 它是會讓 f1 fail 掉的 training dataset<br>
<br>
0:39:18.260,0:39:23.500<br>
這個 d1 的值一定會大於 1<br>
<br>
0:39:23.540,0:39:26.275<br>
因為 epsilon 1 一定小於 0.5<br>
<br>
0:39:26.280,0:39:29.300<br>
所以在 d1 的更號項裡面<br>
<br>
0:39:29.300,0:39:37.840<br>
分子會大於分母所以 d1 都會大於 1<br>
<br>
0:39:42.580,0:39:49.140<br>
整個 Adaboost 的演算法可以講完這一頁就好<br>
<br>
0:39:49.140,0:39:53.340<br>
整個 Adaboost 的演算法看起來就是這個樣子<br>
<br>
0:39:53.340,0:40:04.240<br>
現在有一堆 training data，每一筆 training data 給它的初始的 weight 都是 1<br>
<br>
0:40:04.380,0:40:08.580<br>
接下來要跑大 T 個 iteration<br>
<br>
0:40:08.580,0:40:15.320<br>
每一個 iteration 都會給一個 weight 的 classifier ft<br>
<br>
0:40:15.320,0:40:21.260<br>
最後再把所有的 ft 集合起來就變成一個強的 classifier<br>
<br>
0:40:21.560,0:40:27.320<br>
在每一個 iteration 每一筆 training data 都有自己的 weight<br>
<br>
0:40:27.320,0:40:32.460<br>
這邊寫成 u 上標 1 下標 t <br>
到 u 上標 N 下標 t<br>
<br>
0:40:32.460,0:40:40.900<br>
用下標 t 代表那一個 iteration 的 weight<br>
<br>
0:40:44.700,0:40:49.320<br>
用這個 weight 訓練出 ft<br>
<br>
0:40:49.320,0:40:56.960<br>
然後計算 ft 在原來 weight 上面的 error epsilon t<br>
<br>
0:40:56.960,0:40:59.000<br>
計算 epsilon t 以後<br>
<br>
0:40:59.000,0:41:05.000<br>
就可以 reweight 每一筆 training data<br>
<br>
0:41:05.000,0:41:11.360<br>
如果 x n 它被 ft 分類錯誤的話<br>
<br>
0:41:11.360,0:41:18.080<br>
就把 u 上標 n 下標 t 乘上 d 下標 t<br>
<br>
0:41:18.280,0:41:22.360<br>
就把 u 上標 n 下標 t 乘上一個大於 1 的值<br>
<br>
0:41:22.360,0:41:27.920<br>
然後得到一組新的 weight<br>
這組新的 weight 會在下一個 iteration 的時候被使用<br>
<br>
0:41:27.920,0:41:32.000<br>
反之就把原來的 weight 除掉 dt<br>
<br>
0:41:32.000,0:41:37.020<br>
然後得到一組新的 weight<br>
這組新的 weight 要在下一個 iteration 的時候被使用<br>
<br>
0:41:38.780,0:41:46.460<br>
這個 dt 就等於 √ ( 1 - epsilon t ) / epsilon t<br>
<br>
0:41:46.500,0:41:54.640<br>
或者是可以寫成另外有一個變數叫 alpha t<br>
<br>
0:41:54.640,0:41:58.820<br>
這個 alpha t = ln√ ( 1 - epsilon t ) / epsilon t<br>
<br>
0:41:58.820,0:42:02.140<br>
這麼做是有涵義的<br>
這麼做的話<br>
<br>
0:42:02.140,0:42:06.240<br>
可以把 dt 換成 exp ( alpha t )<br>
<br>
0:42:06.260,0:42:10.120<br>
把除 dt 換成 乘以 exp ( - alpha t )<br>
<br>
0:42:10.120,0:42:11.560<br>
本來是有乘有除<br>
<br>
0:42:11.560,0:42:15.000<br>
現在變成一個是  * exp ( alpha t )<br>
<br>
0:42:15.000,0:42:17.920<br>
一個是 * exp ( - alpha t )<br>
<br>
0:42:17.920,0:42:22.220<br>
之所以這麼做是為了要表達式子的時候<br>
<br>
0:42:22.220,0:42:24.860<br>
可以更簡便一點<br>
<br>
0:42:24.860,0:42:26.580<br>
怎麼樣更簡便?<br>
<br>
0:42:26.580,0:42:30.400<br>
可以把這兩個式子合成一個式子<br>
<br>
0:42:32.740,0:42:37.200<br>
這兩個式子差的只有一個負號而已<br>
<br>
0:42:37.200,0:42:40.940<br>
都是要把原來的 weight 乘上 exp ( alpha t )<br>
<br>
0:42:40.940,0:42:45.660<br>
只是這個 alpha t 前面有時候是 +1 有時候是 -1<br>
<br>
0:42:45.660,0:42:49.700<br>
怎麼用一條式子決定 alpha t 前面應該是 +1 還是 -1<br>
<br>
0:42:49.820,0:42:57.480<br>
只需要把 y n hat * ft ( x n )<br>
<br>
0:42:57.600,0:43:01.880<br>
如果是 misclassified 的情況下<br>
<br>
0:43:02.320,0:43:06.920<br>
y hat 跟 ft( x ) 它是不一樣的<br>
<br>
0:43:06.920,0:43:09.440<br>
這兩個值是不一樣的所以它是 -1<br>
<br>
0:43:09.440,0:43:15.800<br>
-1 * -1，alpha t 前面就變成 1<br>
<br>
0:43:15.960,0:43:20.160<br>
如果是分類正確的情況下，這兩項是一樣的<br>
<br>
0:43:20.160,0:43:22.440<br>
這兩項相乘就是 +1<br>
<br>
0:43:22.440,0:43:26.960<br>
所以再乘上 -1 這一項就變成 -1<br>
<br>
0:43:26.960,0:43:34.660<br>
總之可以直接用這一個式子來表示這兩個式子<br>
<br>
0:43:35.380,0:43:45.180<br>
經過剛才的訓練以後就得到一把 classifier f1 到 fT<br>
<br>
0:43:45.180,0:43:50.260<br>
再來就是怎麼把這把 classifier 集合在一起<br>
<br>
0:43:50.380,0:43:56.680<br>
你可以用 uniform 的 weight<br>
現在有大 T 個 classifier<br>
<br>
0:43:56.780,0:43:59.680<br>
叫這大 T 個 classifier 都得到一個 output<br>
<br>
0:43:59.680,0:44:04.880<br>
把大 T 個 classifier 的 output 就加起來看是正的還是負的<br>
<br>
0:44:04.880,0:44:09.340<br>
如果是正的話就代表是 class 1<br>
如果是負的話就代表是 class 2<br>
<br>
0:44:09.400,0:44:14.380<br>
就把這大 T 個 classifier 的值通通加起來然後取正負號<br>
<br>
0:44:14.660,0:44:17.820<br>
這樣雖然可以但這樣不是最好的方法<br>
<br>
0:44:17.820,0:44:21.740<br>
因為這大 T 個 classifier 有好有壞<br>
<br>
0:44:21.740,0:44:26.180<br>
所以應該要給它不同的權重<br>
<br>
0:44:26.180,0:44:36.360<br>
怎麼給不同的權重<br>
在每一個 classifier output 前面都乘上一個權重 alpha t<br>
<br>
0:44:36.360,0:44:40.640<br>
然後在全部加起來以後<br>
<br>
0:44:40.640,0:44:45.320<br>
再取它的正負號這樣可以得到比較好的結果<br>
<br>
0:44:45.440,0:44:50.960<br>
alpha t 怎麼得到<br>
<br>
0:44:50.960,0:44:54.060<br>
這個 alpha t 在前一頁的式子有看過<br>
<br>
0:44:54.060,0:45:01.120<br>
這個 alpha t 就是拿來改變每一筆 training data 的 weight 的 alpha t<br>
<br>
0:45:01.160,0:45:03.440<br>
那個 alpha t 在前面看過<br>
<br>
0:45:03.440,0:45:05.840<br>
現在看一下 alpha t 的精神<br>
<br>
0:45:05.840,0:45:09.920<br>
如果某一個 classifier 的 epsilon t 是 0.1<br>
<br>
0:45:09.920,0:45:12.800<br>
是一個錯誤率比較低的 classifier<br>
<br>
0:45:12.800,0:45:17.700<br>
把 epsilon t = 0.1 帶到這個式子去算出 alpha t<br>
<br>
0:45:17.740,0:45:20.280<br>
它的 alpha t 就是 1.1<br>
<br>
0:45:20.280,0:45:24.420<br>
錯誤率低的 classifier 會有比較大的 alpha t<br>
<br>
0:45:24.420,0:45:31.060<br>
如果有另外一個 classifier 它的 epsilon t 是 0.4 代表它是個很爛的 classifier，錯誤率已經接近 0.5 了<br>
<br>
0:45:31.060,0:45:36.240<br>
把 epsilon t = 0.4 帶到這個式子裡面去算 alpha t 得到 alpha t 是 0.2<br>
<br>
0:45:36.240,0:45:39.160<br>
如果有一個比較正確的 classifier<br>
<br>
0:45:39.160,0:45:42.540<br>
錯誤率比較低的 classifier 它得到的 alpha t 的值是大的<br>
<br>
0:45:42.540,0:45:45.860<br>
如果爛的 classifier 它得到的 alpha t 的值是小的<br>
<br>
0:45:45.860,0:45:48.060<br>
也就是在做 weighted sum 的時候<br>
<br>
0:45:48.060,0:45:49.840<br>
如果有一個 classifier 的正確率<br>
<br>
0:45:49.900,0:45:53.020<br>
它當初訓練的時候錯誤率是比較大的<br>
<br>
0:45:53.020,0:45:54.720<br>
它的 weight 就比較小<br>
<br>
0:45:54.720,0:45:59.680<br>
它當初訓練的時候錯誤率比較小它的 weight 就比較大<br>
<br>
0:45:59.780,0:46:06.780<br>
這件事情是非常有道理的<br>
<br>
0:46:06.820,0:46:10.120<br>
這個 alpha t 是 make sense 的<br>
<br>
0:46:10.240,0:46:13.820<br>
我們很快把後面這個例子講過好了<br>
<br>
0:46:13.820,0:46:20.000<br>
如果這邊你覺得太快就回去自己看投影片<br>
我相信這個對大家非常容易<br>
<br>
0:46:20.000,0:46:24.340<br>
我講的這一段就請助教來講一下作業六<br>
<br>
0:46:24.420,0:46:27.775<br>
這個很簡單<br>
<br>
0:46:27.780,0:46:32.260<br>
剛剛的演算法如果沒有聽懂就看這個例子<br>
就知道它的意思了<br>
<br>
0:46:32.260,0:46:34.580<br>
假設大 T = 3<br>
0:46:34.580,0:46:39.980<br>
現在 weak 的 classifier 很 weak<br>
它不是 Decision Tree 也不是 Neural Network<br>
<br>
0:46:39.980,0:46:41.800<br>
它叫做 decision stump<br>
<br>
0:46:41.820,0:46:45.920<br>
decision stump 沒什麼好講的它太簡單了<br>
<br>
0:46:45.920,0:46:47.600<br>
它做的事情就是<br>
<br>
0:46:47.600,0:46:50.640<br>
假設 feature 都分佈在二維平面上<br>
<br>
0:46:50.680,0:46:54.360<br>
在二維平面上選一個 dimension 切一刀<br>
<br>
0:46:54.420,0:46:57.840<br>
其中一邊當作 class 1 另外一邊當作 class 2<br>
<br>
0:46:57.840,0:47:00.600<br>
結束，這個就叫做 decision stump<br>
<br>
0:47:00.780,0:47:03.700<br>
要做 Boosting 一定要找個 weak 的 classifier<br>
<br>
0:47:03.700,0:47:07.320<br>
decision stump 它夠 weak 所以把它用在這裡<br>
<br>
0:47:07.580,0:47:13.720<br>
一開始每一筆 training data 的 weight 都是一模一樣的都是 1.0<br>
<br>
0:47:16.160,0:47:19.420<br>
用 decision stump 找一個 function<br>
<br>
0:47:19.420,0:47:21.920<br>
這個 function 是 f1<br>
<br>
0:47:21.920,0:47:24.600<br>
它的 bounder 就切在這個地方<br>
<br>
0:47:24.600,0:47:27.300<br>
以左就說是 positive 的 example<br>
<br>
0:47:27.300,0:47:29.440<br>
就算是 positive 的<br>
<br>
0:47:29.440,0:47:31.820<br>
一邊 class 1 是 positive 的<br>
<br>
0:47:31.820,0:47:35.560<br>
往右就是粉紅色就是 negative 的<br>
<br>
0:47:35.560,0:47:40.120<br>
發現這邊有三筆 data 它的分類是錯的<br>
<br>
0:47:40.180,0:47:45.800<br>
計算一下有三筆 data 總共有十筆 data<br>
<br>
0:47:45.800,0:47:47.400<br>
有三筆 data 分類錯<br>
<br>
0:47:47.400,0:47:49.400<br>
所以 Error Rate 是 0.3<br>
<br>
0:47:49.400,0:47:54.500<br>
Error Rate 是 0.3 的話 d1 算出來就是 1.53 alpha 算出來就是 0.42<br>
<br>
0:47:54.500,0:48:00.840<br>
就帶前一頁投影片的公式就可以輕易地求出來了<br>
<br>
0:48:01.420,0:48:05.640<br>
現在已經算出 epsilon 1, d1, alpha 1 以後<br>
<br>
0:48:05.640,0:48:09.440<br>
接下來就是改變每一筆 training data 的 weight<br>
<br>
0:48:09.440,0:48:13.380<br>
分類正確的 weight 就要變小<br>
<br>
0:48:13.680,0:48:16.040<br>
分類錯誤的 weight 就要變大<br>
<br>
0:48:16.040,0:48:18.560<br>
分類錯誤的要乘 1.53<br>
<br>
0:48:18.560,0:48:20.660<br>
分類對的就要除 1.53<br>
<br>
0:48:20.700,0:48:25.420<br>
這三筆分類錯的 weight 就變大<br>
<br>
0:48:25.440,0:48:28.000<br>
分類對的 weight 就變小<br>
<br>
0:48:28.280,0:48:31.540<br>
有了一組新的 weight 以後<br>
<br>
0:48:31.540,0:48:37.460<br>
就可以再去找一次另外一個 decision stump<br>
<br>
0:48:37.500,0:48:41.120<br>
有一組新的 weight 找出來的 decision stump 就不一樣了<br>
<br>
0:48:41.120,0:48:44.840<br>
在新的 decision stump 切一刀切在這個地方<br>
<br>
0:48:44.840,0:48:49.080<br>
往左是 positive 往右是 negative<br>
<br>
0:48:49.080,0:48:52.115<br>
往左是藍色往右是紅色<br>
<br>
0:48:52.120,0:48:56.020<br>
會發現有三筆 data 分類是錯的<br>
<br>
0:48:56.020,0:48:59.200<br>
現在 f2 的 Error Rate 是多少<br>
<br>
0:48:59.200,0:49:02.360<br>
會根據每一筆 data 的 weight 進行計算<br>
<br>
0:49:02.360,0:49:07.060<br>
就會發現第二個 classifier 的 Error Rate 是 0.21<br>
<br>
0:49:07.060,0:49:11.140<br>
它的 d2 = 1.94, alpha 2 = 0.66<br>
<br>
0:49:11.260,0:49:14.880<br>
接下來這三筆 data 分類錯所以給他 weight 比較大<br>
<br>
0:49:14.880,0:49:17.800<br>
這三筆 data 要把它乘上 1.94<br>
<br>
0:49:17.860,0:49:21.480<br>
剩下的 data 把他除掉 1.94<br>
<br>
0:49:22.000,0:49:26.480<br>
就找到了第二個 classifier<br>
<br>
0:49:26.820,0:49:29.840<br>
每一個 classifier 的 weight 就是它 alpha 的值<br>
<br>
0:49:29.840,0:49:33.160<br>
把 alpha 的值寫再 classifier 的旁邊<br>
<br>
0:49:33.160,0:49:35.800<br>
接下來找第三個 classifier<br>
<br>
0:49:36.800,0:49:41.820<br>
第三個 classifier 上面是藍色下面是紅色<br>
<br>
0:49:42.140,0:49:47.240<br>
它這麼講會導致有三筆 data 錯誤<br>
<br>
0:49:47.240,0:49:49.700<br>
計算一下它的 Error Rate = 0.13<br>
<br>
0:49:49.700,0:49:53.660<br>
可以計算它的 d3 可以計算它的 alpha 3<br>
<br>
0:49:53.860,0:49:59.360<br>
如果有更多 iteration 的話會去重新 weight data<br>
<br>
0:49:59.440,0:50:02.980<br>
但現在只跑三個 iteration 跑完就結束了<br>
<br>
0:50:02.980,0:50:06.540<br>
得到三個 classifier 還有他們的 weight 就結束了<br>
<br>
0:50:06.540,0:50:10.720<br>
最後怎麼把這三個 classifier 組合起來<br>
<br>
0:50:10.720,0:50:15.180<br>
把每個 classifier 都乘上對應的 weight<br>
<br>
0:50:15.180,0:50:19.020<br>
通通加起來再取它的正負號<br>
<br>
0:50:19.280,0:50:25.480<br>
這個加起來的結果到底是怎麼回事<br>
<br>
0:50:25.480,0:50:29.600<br>
有三個 decision stump 這三個 decision stump<br>
<br>
0:50:29.680,0:50:34.300<br>
把整個二維的平面切成六塊<br>
<br>
0:50:34.300,0:50:39.560<br>
左上角三個 classifier 都覺得是藍的<br>
所以就藍色<br>
<br>
0:50:39.560,0:50:46.060<br>
中間這一塊他們兩個覺得是藍的，第一個覺得是紅的<br>
<br>
0:50:46.060,0:50:49.480<br>
但是他們兩個合起來的 weight 比較大<br>
<br>
0:50:49.480,0:50:52.220<br>
所以上面這組就是藍的<br>
<br>
0:50:52.220,0:50:56.840<br>
右上角第一個覺得是紅的第二個覺得是紅的第三個覺得是藍的<br>
<br>
0:50:56.840,0:51:03.340<br>
這兩個紅的 weight 合起來比藍的 weight 大<br>
所以又是紅的<br>
<br>
0:51:03.420,0:51:08.560<br>
左下角是第一個藍的第二個藍的第三個紅的<br>
<br>
0:51:08.620,0:51:11.740<br>
兩個藍的合起來比紅的大所以是藍的<br>
<br>
0:51:11.740,0:51:15.100<br>
下面這個紅的藍的紅的<br>
<br>
0:51:15.100,0:51:17.600<br>
兩個紅的加起來比藍的大所以是紅的<br>
<br>
0:51:17.620,0:51:24.520<br>
右下角三個 decision stump 都說是紅的所以是紅的<br>
<br>
0:51:24.520,0:51:32.340<br>
這三個 decision stump 沒有一個是 0% 的 Error Rate<br>
<br>
0:51:32.340,0:51:35.900<br>
他們都有犯一些錯<br>
<br>
0:51:35.900,0:51:39.700<br>
但把這三個 decision stump 組合起來的時候<br>
<br>
0:51:39.700,0:51:43.780<br>
它告訴我們這三個區塊屬於藍色、這三個區塊屬於紅色<br>
<br>
0:51:43.840,0:51:46.640<br>
而它的正確率是 100%<br>
<br>
0:51:46.640,0:51:51.500<br>
三個 weak 的 classifier 把它組合起來可以得到好的結果<br>
<br>
0:51:51.640,0:51:57.000<br>
接下來就請助教來講一下作業六<br>
<br>
0:52:05.760,0:52:09.400<br>
我們來繼續講 Adaboost<br>
<br>
0:52:09.460,0:52:16.020<br>
上次講的是 Adaboost 的 algorithm<br>
<br>
0:52:29.980,0:52:33.800<br>
現在要講的是理論上的證明<br>
<br>
0:52:33.800,0:52:40.200<br>
這邊要證明假設我們按照 Adaboost 的 algorithm<br>
<br>
0:52:40.200,0:52:44.260<br>
來產生最後的 classifier<br>
<br>
0:52:44.260,0:52:48.700<br>
這最後的 classifier 這邊寫成 H(x)<br>
<br>
0:52:48.740,0:52:57.440<br>
這個最後的 classifier H(x) 是由一堆 weak 的 classifier ft 所組成的<br>
<br>
0:52:57.500,0:53:01.700<br>
如果 Adaboost 的 algorithm 跑大 T 個 iteration 的話<br>
<br>
0:53:01.700,0:53:07.400<br>
就會得到大 T 個 weak 的 classifier f1 到 fT<br>
<br>
0:53:07.400,0:53:11.440<br>
每一個 weak 的 classifier 還有 weight 權重<br>
<br>
0:53:11.440,0:53:18.000<br>
這樣就可以知道哪一些 weak classifier 應該被參考多一點<br>
哪一些應該被參考少一點<br>
<br>
0:53:18.000,0:53:21.960<br>
這個權重就是 alpha t<br>
<br>
0:53:21.980,0:53:24.740<br>
把所有 weak classifier 的 output<br>
<br>
0:53:24.740,0:53:27.080<br>
假設切到 classifier 某一個 object x<br>
<br>
0:53:27.100,0:53:31.140<br>
就把 x 分別丟到每一個 weak 的 classifier ft 裡面<br>
<br>
0:53:31.140,0:53:34.900<br>
再把 x 的 output 乘上它的 weight alpha t<br>
<br>
0:53:34.900,0:53:39.040<br>
再 summation over 所有 weak 的 classifier 再取它的正負號<br>
<br>
0:53:39.040,0:53:41.880<br>
就可以得到最終的分類的結果<br>
<br>
0:53:41.940,0:53:44.900<br>
這個 alpha t 是甚麼<br>
<br>
0:53:44.900,0:53:49.700<br>
這個 alpha t 跟 epsilon t 有關<br>
<br>
0:53:49.700,0:53:52.020<br>
epsilon t 組成的 alpha t<br>
<br>
0:53:52.020,0:53:56.120<br>
epsilon t 是甚麼<br>
epsilon t 是 Error Rate<br>
<br>
0:55:35.100,0:55:43.220<br>
我想先講另外一件事情就是我們接下來的規劃<br>
<br>
0:55:43.720,0:55:53.340<br>
這周我們講 Boosting 跟 Structure Learning 的開頭<br>
<br>
0:55:53.340,0:55:57.560<br>
下周有一位 NVIDIA 的外賓要來<br>
<br>
0:55:57.560,0:56:00.460<br>
它要來告訴我們一些它做的研究<br>
<br>
0:56:00.460,0:56:05.420<br>
相比於我自己授課我比較喜歡請外賓來講<br>
<br>
0:56:05.420,0:56:08.040<br>
為甚麼? 因為我上課的內容都是有錄影的<br>
<br>
0:56:08.040,0:56:13.720<br>
所以請外賓來講比較能夠聽到不一樣的東西<br>
<br>
0:56:13.720,0:56:17.540<br>
如果我再講一次同樣的內容其實笑話都是差不多的<br>
<br>
0:56:19.460,0:56:24.120<br>
所以如果有外賓的話就盡量請外賓來講<br>
<br>
0:56:24.120,0:56:25.900<br>
沒有講的內容怎麼辦<br>
<br>
0:56:25.900,0:56:30.080<br>
還沒講 SVM 阿，那個上課都有錄影<br>
<br>
0:56:30.080,0:56:34.920<br>
都有上學期的錄影再把它放到課程網站就好<br>
<br>
0:56:34.920,0:56:37.880<br>
所以不用太擔心<br>
<br>
0:56:37.880,0:56:43.480<br>
在下下周講 Reinforcement Learning<br>
<br>
0:56:43.480,0:56:47.420<br>
上學期其實沒有講完 Reinforcement Learning<br>
這學期會把它講完<br>
<br>
0:56:47.500,0:56:53.280<br>
接下來就是 final，祝大家 final 做的順利<br>
<br>
0:56:53.340,0:56:59.980<br>
有人可能會想說沒講到機器學習理論的部分<br>
<br>
0:57:05.240,0:57:12.520<br>
剛才講到 alpha t 是跟 epsilon t 有關的<br>
<br>
0:57:14.600,0:57:17.460<br>
epsilon t 是 Error rate<br>
<br>
0:57:17.460,0:57:20.620<br>
是 classifier ft 的 Error Rate<br>
<br>
0:57:22.460,0:57:28.520<br>
現在要證明如果 weak 的 classifier 越多<br>
<br>
0:57:28.520,0:57:34.360<br>
或者換句話說 Adaboost 的 algorithm 跑越多的 iteration<br>
<br>
0:57:34.360,0:57:37.240<br>
在 training set 上的 error<br>
<br>
0:57:37.240,0:57:40.080<br>
會越來越小<br>
<br>
0:57:40.240,0:57:44.280<br>
所以這樣子就可以增加 weak 的 classifier<br>
<br>
0:57:44.280,0:57:48.720<br>
然後讓 model 在 training set 上的 performance 越來越好<br>
<br>
0:57:48.880,0:57:53.300<br>
怎麼證<br>
其實是滿簡單的<br>
<br>
0:57:53.300,0:57:58.480<br>
先算一下 H(x) 的 Error Rate<br>
<br>
0:57:58.540,0:58:01.880<br>
先把 H(x) 的 Error Rate 的式子列出來<br>
<br>
0:58:04.760,0:58:12.600<br>
其實就是 summation over n<br>
這個 x n 代表 training data<br>
<br>
0:58:12.600,0:58:16.700<br>
如果 H(x n) 不等於 y n hat<br>
<br>
0:58:16.700,0:58:19.660<br>
H(x n) 的 output 跟正確解答不一樣的話<br>
<br>
0:58:19.660,0:58:23.480<br>
那就有一筆 error，得到的 error 就是 1<br>
<br>
0:58:23.540,0:58:26.680<br>
反之如果 H 等於 y n hat 的話<br>
<br>
0:58:26.680,0:58:29.080<br>
那得到的 error 就是 0<br>
<br>
0:58:29.080,0:58:30.880<br>
然後再做一下平均<br>
<br>
0:58:30.880,0:58:33.720<br>
假設有大 N 筆 training data<br>
<br>
0:58:36.680,0:58:41.160<br>
這邊是先把大 T 個 weak classifier weighted sum 起來<br>
<br>
0:58:41.160,0:58:43.680<br>
再取它的正負號<br>
<br>
0:58:43.680,0:58:48.720<br>
括號裡面這一項用 g(x) 來表示<br>
<br>
0:58:48.720,0:58:54.900<br>
g(x) 代表大 T 個 classifier 的 weighted sum<br>
<br>
0:58:54.980,0:58:57.465<br>
所以 Error Rate 這一項<br>
<br>
0:58:57.465,0:59:00.380<br>
也可以寫成是 y hat<br>
<br>
0:59:00.380,0:59:04.140<br>
乘上 g(x) 看它是小於 0 還是大於 0<br>
<br>
0:59:04.140,0:59:15.820<br>
小於 0 代表 y hat 跟 g 異號所以是錯誤的<br>
<br>
0:59:15.840,0:59:17.840<br>
得到的 error 就是 1<br>
<br>
0:59:17.840,0:59:23.700<br>
如果他們是同號代表是正確的得到的 error 就是 0<br>
<br>
0:59:23.700,0:59:26.140<br>
這都沒有甚麼特別難的<br>
<br>
0:59:26.220,0:59:32.780<br>
最後一項我們說這個 Error Rate 有一個 upper bound<br>
<br>
0:59:32.780,0:59:35.320<br>
這個 upper bound 寫作這樣<br>
<br>
0:59:35.320,0:59:42.220<br>
這個 upper bound 是 exp ( - y hat * g(x))<br>
<br>
0:59:42.340,0:59:47.580<br>
如果把 y hat * g(x) 的這個值<br>
<br>
0:59:47.580,0:59:50.720<br>
把它畫出來就一目瞭然了<br>
<br>
0:59:52.100,1:00:05.220<br>
這個圖的橫軸是 y hat * g(x)<br>
<br>
1:00:05.320,1:00:14.860<br>
綠色的線代表的是 delta ( y hat * g(x) < 0) 這個 function 的值<br>
<br>
1:00:14.860,1:00:17.900<br>
所以 y hat * g(x) < 0 的話<br>
<br>
1:00:17.900,1:00:22.000<br>
這個 output 是 1 反之 delta output 是 0<br>
<br>
1:00:22.060,1:00:27.520<br>
綠色的 function 有一個 upper bound 就是藍色的 function<br>
<br>
1:00:27.520,1:00:31.240<br>
藍色的 function 是 exp ( - y hat * g(x))<br>
<br>
1:00:31.360,1:00:35.660<br>
exp ( - y hat * g(x)) 畫起來就是這個樣子<br>
<br>
1:00:35.740,1:00:39.600<br>
藍色的 function 是綠色的 function 的 upper bound<br>
<br>
1:00:39.600,1:00:42.820<br>
應該是沒有甚麼特別的問題<br>
<br>
1:00:44.960,1:00:51.260<br>
再來是要證明這個 upper bound 會越來越小<br>
<br>
1:00:51.260,1:00:57.620<br>
怎麼證 upper bound 會越來越小<br>
在直接證它之前來算另外一個式子<br>
<br>
1:00:57.620,1:01:02.080<br>
我們來算 Zt，甚麼是 Zt<br>
<br>
1:01:02.080,1:01:09.060<br>
在每一個 iteration 都會給 training data 一個 weight<br>
<br>
1:01:09.060,1:01:11.500<br>
每一筆 training data 都有一個 weight<br>
<br>
1:01:11.500,1:01:15.140<br>
用這些 weight 來算 ft<br>
<br>
1:01:15.320,1:01:20.060<br>
所謂的 Zt 就是把所有 training data 的 weight 加總起來<br>
<br>
1:01:20.060,1:01:22.380<br>
就是 Zt<br>
<br>
1:01:22.380,1:01:26.700<br>
等一下會說明 Zt 跟上面 upper bound 的關係<br>
<br>
1:01:26.700,1:01:30.840<br>
先不要管 upper bound，先算 Zt<br>
<br>
1:01:31.080,1:01:36.840<br>
要先來算的是 Z T+1<br>
<br>
1:01:36.840,1:01:41.420<br>
也就是當 T 個 iteration 跑完以後<br>
<br>
1:01:41.420,1:01:50.620<br>
假設接下來要算、要學 F T+1<br>
<br>
1:01:50.620,1:01:52.900<br>
第 T + 1 個 weak classifier<br>
<br>
1:01:52.900,1:01:59.400<br>
那 train 第 T + 1 個 weak classifier 的時候<br>
<br>
1:01:59.480,1:02:03.040<br>
那些 training data 的 weight 把它總合起來<br>
<br>
1:02:03.040,1:02:06.360<br>
應該是多少<br>
<br>
1:02:06.360,1:02:16.680<br>
Z T + 1 = summation over 所有的 training data 它的每一筆 training data 的 weight 總和<br>
<br>
1:02:16.720,1:02:20.020<br>
每一筆 training data 的 weight 又是多少<br>
<br>
1:02:20.020,1:02:24.940<br>
假設初始的時候、train 第一個  weak classifier 的時候<br>
<br>
1:02:24.940,1:02:29.200<br>
這個時候每一筆 training data 裡面的 weight 都是一樣<br>
都是 1<br>
<br>
1:02:29.200,1:02:32.540<br>
這是非常合理的假設<br>
<br>
1:02:32.540,1:02:37.180<br>
接下來在第 T + 1 個 iteration<br>
<br>
1:02:37.360,1:02:40.160<br>
要 train 第 T + 1 個 classifier 的時候<br>
<br>
1:02:40.260,1:02:42.100<br>
會把原來的 weight<br>
<br>
1:02:42.100,1:02:50.700<br>
在第 t 個 iteration 的 weight u t 乘上<br>
exp ( - y n hat * (ft) * (alpha t))<br>
<br>
1:02:51.740,1:02:56.580<br>
這件事情其實之前有講過了<br>
<br>
1:02:56.880,1:03:02.040<br>
如果第 n 筆 classifier<br>
<br>
1:03:02.360,1:03:04.160<br>
它被 classified 是正確的<br>
<br>
1:03:04.160,1:03:06.400<br>
它的 weight 就會被下降<br>
<br>
1:03:06.400,1:03:10.600<br>
如果它 classified 是錯誤的它的 weight 就會被上升<br>
<br>
1:03:10.600,1:03:16.160<br>
怎麼增加和減少它的 weight 靠的是乘後面這一項<br>
<br>
1:03:16.240,1:03:20.220<br>
前面在講 Adaboost 的 algorithm 的時候<br>
<br>
1:03:20.220,1:03:24.280<br>
有解釋過為甚麼這個式子長這個樣子<br>
<br>
1:03:24.280,1:03:27.280<br>
這 alpha t 跟 epsilon t 有關係<br>
<br>
1:03:27.280,1:03:29.600<br>
把它寫在右上角<br>
<br>
1:03:29.620,1:03:35.100<br>
總之第 t + 1 個時間點的 weight<br>
<br>
1:03:35.120,1:03:40.140<br>
跟第 t 個時間點的 weight 之間有甚麼樣的關係<br>
<br>
1:03:40.360,1:03:48.740<br>
如果要算第 T + 1 的 iteration 的時候的 weight<br>
<br>
1:03:48.740,1:03:52.460<br>
要 train 第 T + 1 個 weak classifier 的 weight<br>
<br>
1:03:52.460,1:03:54.740<br>
會不會算呢<br>
<br>
1:03:54.740,1:03:57.640<br>
因為第一項是 1<br>
<br>
1:03:57.640,1:04:01.660<br>
一開始是 1，接下來就一直乘 exponential 這一項<br>
<br>
1:04:01.660,1:04:04.400<br>
所以其實我們只是把<br>
<br>
1:04:04.400,1:04:11.660<br>
這些 exponential 這些項乘上 T 次而已<br>
<br>
1:04:11.740,1:04:14.940<br>
這個大家應該沒有問題吧<br>
<br>
1:04:15.200,1:04:18.200<br>
知道 t 跟 t + 1 的關係就是乘這一項<br>
<br>
1:04:18.300,1:04:22.300<br>
從 u1 到 u (T+1) 中間<br>
<br>
1:04:22.300,1:04:28.560<br>
就是乘了這個 exponential 項乘了 T 次<br>
<br>
1:04:29.740,1:04:34.120<br>
如果要算 Z 的話<br>
<br>
1:04:34.120,1:04:38.380<br>
Z 就是把每一筆 training data 的 u 通通 summation 起來<br>
<br>
1:04:38.400,1:04:44.280<br>
所以只是在這個式子前面、這個式子前面加了 summation 而已<br>
<br>
1:04:49.940,1:04:57.680<br>
接下來可以把連乘這一項放到 exponential 裡面<br>
<br>
1:04:57.800,1:05:03.400<br>
有一大堆 exponential 相乘等於指數項相加<br>
<br>
1:05:03.400,1:05:08.680<br>
所以可以把連乘這一項放到 exponential 裡面<br>
<br>
1:05:08.820,1:05:15.440<br>
那 y hat 是指第 n 筆 training data 正確答案<br>
<br>
1:05:15.520,1:05:18.880<br>
它跟 iteration 是完全沒有關係<br>
<br>
1:05:18.880,1:05:21.300<br>
label 它跟 iteration 完全沒有關係<br>
<br>
1:05:21.300,1:05:24.980<br>
所以 y hat 這一項可以被提出來<br>
<br>
1:05:25.140,1:05:31.660<br>
總之 Z ( T + 1 ) 會寫成右下角這個式子<br>
<br>
1:05:31.660,1:05:36.480<br>
右下角這個式子是甚麼<br>
<br>
1:05:37.860,1:05:44.240<br>
這一項其實就是 g(x)<br>
<br>
1:05:44.400,1:05:50.660<br>
所以這一項其實就是這一項<br>
<br>
1:05:50.940,1:05:55.760<br>
所以這 Z ( T + 1 ) 它的 upper bound 是非常有關係的<br>
<br>
1:05:55.760,1:06:02.300<br>
其實 training 的時候，error 的 upper bound 就是<br>
Z(T+1) / N<br>
<br>
1:06:02.440,1:06:10.060<br>
你會發現 training data 的 weight 的 summation 居然是跟 error 的 upper bound 是有關係的<br>
<br>
1:06:10.340,1:06:23.840<br>
接下來要證 weight 的 summation 會越來越小<br>
<br>
1:06:23.880,1:06:28.760<br>
所有的 training data 的 weight 的 summation 會越來越小<br>
<br>
1:06:28.760,1:06:33.020<br>
如果可以證明這件事的話<br>
<br>
1:06:33.300,1:06:35.920<br>
遊戲就結束了<br>
<br>
1:06:36.140,1:06:37.800<br>
Z1 是甚麼<br>
<br>
1:06:37.800,1:06:41.560<br>
Z1 在第一次 train 第一個 classifier 的時候<br>
<br>
1:06:41.560,1:06:47.260<br>
每一筆 training data 的 weight 都是 1<br>
總共有 N 筆 training data 所以 weight 是 N<br>
<br>
1:06:47.440,1:06:51.680<br>
所以 Z1 = N<br>
<br>
1:06:52.060,1:06:55.580<br>
那 Zt 呢<br>
<br>
1:06:55.960,1:06:59.860<br>
Zt 跟 Zt - 1 中間<br>
<br>
1:06:59.860,1:07:02.220<br>
有以下的這個關係<br>
<br>
1:07:02.220,1:07:08.660<br>
要從 Z( t - 1 ) 變到 Zt 只要做以下這個運算就好<br>
<br>
1:07:08.860,1:07:10.940<br>
這個運算是甚麼意思<br>
<br>
1:07:10.940,1:07:17.460<br>
這個運算是先找出 Z ( t - 1 ) 裡面<br>
<br>
1:07:17.900,1:07:19.880<br>
misclassified 的部分<br>
<br>
1:07:19.880,1:07:23.020<br>
misclassified 的部分、分類錯誤的部分<br>
<br>
1:07:23.020,1:07:26.120<br>
會被乘上 exp( alpha )<br>
<br>
1:07:26.480,1:07:32.600<br>
分類正確的部分會被乘上 exp( - alpha )<br>
<br>
1:07:34.060,1:07:38.360<br>
分類錯誤的部分有多少<br>
<br>
1:07:38.360,1:07:43.580<br>
假設 Error Rate 叫做 epsilon t<br>
<br>
1:07:43.580,1:07:48.880<br>
那分類錯誤的部分就是 Z ( t - 1 ) * epsilon t<br>
<br>
1:07:48.880,1:07:53.200<br>
分類正確的部分就是 Z ( t - 1 ) * ( 1 - epsilon t )<br>
<br>
1:07:53.200,1:07:56.940<br>
分類錯誤的部分會被乘上 exp( alpha t )<br>
<br>
1:07:56.940,1:08:00.800<br>
分類正確的部分會被乘上 exp ( - alpha t )<br>
<br>
1:08:00.800,1:08:05.340<br>
把這兩項加起來就得到 Zt<br>
<br>
1:08:05.420,1:08:10.660<br>
現在知道 alpha t 是多少<br>
alpha t 的式子就寫在這邊<br>
<br>
1:08:10.660,1:08:13.780<br>
把 alpha t 帶進去<br>
<br>
1:08:13.800,1:08:31.860<br>
就得到 Zt = Z( t - 1 ) * (εt) * √( 1 - εt) /εt<br>
+ Z ( t - 1 )(1 - εt)* √ εt / ( 1 - εt)<br>
<br>
1:08:34.660,1:08:36.940<br>
合起來就是這個太容易了<br>
<br>
1:08:37.060,1:08:41.440<br>
就把分子跟分母消一下<br>
<br>
1:08:41.440,1:08:47.740<br>
得到 Z(t - 1) * 2 √ εt ( 1 - εt )<br>
<br>
1:08:48.040,1:08:53.380<br>
其實從這一項就可以看出<br>
<br>
1:08:53.400,1:08:58.280<br>
Zt 會比 Z(t - 1) 還要小<br>
<br>
1:08:59.800,1:09:03.180<br>
εt 是 Error Rate<br>
<br>
1:09:03.180,1:09:07.220<br>
Error Rate 一定小於 0.5<br>
<br>
1:09:07.220,1:09:10.260<br>
最大就是 0.5<br>
<br>
1:09:10.300,1:09:15.120<br>
所以 Z(t - 1) 後面乘的這一項是多少<br>
<br>
1:09:15.120,1:09:19.380<br>
如果 εt = 0.5 的時候它最大<br>
<br>
1:09:19.400,1:09:27.740<br>
所以 2√ εt ( 1 - εt ) 最大就是 1<br>
它沒有辦法比 1 還要更大<br>
<br>
1:09:27.740,1:09:35.240<br>
Z( t - 1 ) 會乘上一個比 1 小的值變成 Zt<br>
所以 Zt 會小於 Z( t - 1)<br>
<br>
1:09:37.880,1:09:41.520<br>
Z ( T + 1 ) 算出來的話是多少<br>
<br>
1:09:41.520,1:09:45.720<br>
Z ( T + 1 ) 就是 Z1 的 N 乘上 T 項<br>
<br>
1:09:45.720,1:09:50.940<br>
每一項都是 2 √  εt ( 1 - εt )<br>
<br>
1:09:50.940,1:09:55.380<br>
所以 training 的 error 會越來越小<br>
<br>
1:09:55.380,1:09:59.580<br>
因為 2 √ εt ( 1 - εt ) 是小於 1 的<br>
<br>
1:09:59.580,1:10:02.220<br>
所以 Zt 會越來越小<br>
<br>
1:10:02.220,1:10:05.940<br>
Zt 就是 upper bound 所以 upper bound 會越來越小<br>
<br>
1:10:05.940,1:10:10.520<br>
所以 Error Rate 可能也會是越來越小<br>
<br>
1:10:13.135,1:10:15.925<br>
這個證明就到這邊<br>
<br>
1:10:15.925,1:10:20.100<br>
接下來講 Adaboost 神秘的現象<br>
<br>
1:10:20.100,1:10:23.800<br>
這個神祕的現象是這個樣子<br>
<br>
1:10:23.860,1:10:27.740<br>
這邊橫軸是 training 的 iteration<br>
<br>
1:10:27.740,1:10:31.320<br>
找多少個 weak 的 classifier 來幫忙<br>
<br>
1:10:31.320,1:10:34.940<br>
縱軸是 Error Rate<br>
<br>
1:10:37.640,1:10:44.240<br>
比較低的這條線是在 training data 上的 Error Rate<br>
<br>
1:10:44.280,1:10:49.180<br>
比較高的這條線是在 testing data 上的 Error Rate<br>
<br>
1:10:49.200,1:10:55.380<br>
但神奇的是 training data 的 Error Rate<br>
<br>
1:10:55.380,1:10:57.860<br>
其實很快就變成 0<br>
<br>
1:10:57.900,1:11:02.800<br>
大概在 5 個 iteration 之後<br>
找五個 weak 的 classifier combine 在一起以後<br>
<br>
1:11:02.800,1:11:06.240<br>
Error Rate 其實就已經是 0<br>
<br>
1:11:06.720,1:11:13.220<br>
雖然 Error Rate 是 0<br>
<br>
1:11:13.220,1:11:16.300<br>
五個 weak classifier 的 Error Rate 合起來是 0<br>
<br>
1:11:16.300,1:11:20.660<br>
這邊要強調一下是五個 weak classifier 的 Error Rate 合起來是 0<br>
<br>
1:11:20.660,1:11:24.760<br>
並不是單一一個 weak classifier 的 Error Rate 是 0<br>
<br>
1:11:24.760,1:11:29.120<br>
單一一個 weak classifier 都很弱<br>
要五個合起來以後 Error Rate 才是 0<br>
<br>
1:11:29.120,1:11:33.840<br>
事實上在 Adaboost 演算法裡面如果你想一下<br>
<br>
1:11:33.840,1:11:39.600<br>
如果 weak classifier 的 Error Rate train 在 training data 上就已經是 0 了<br>
<br>
1:11:39.640,1:11:44.100<br>
其實這演算法是會有問題的<br>
<br>
1:11:44.100,1:11:47.940<br>
算一下那個 alpha 會發現是 undefine<br>
<br>
1:11:48.020,1:11:58.700<br>
Adaboost 假設你的 train weak classifier 的 algorithm 沒有辦法讓 Error Rate 變 0<br>
<br>
1:11:58.700,1:12:03.400<br>
如果變 0 的話這個演算法是會有問題的<br>
<br>
1:12:10.020,1:12:14.440<br>
雖然加了更多 weak classifier 以後<br>
<br>
1:12:14.440,1:12:17.900<br>
整體的 Error Rate 在 training data 上沒已下降<br>
<br>
1:12:17.900,1:12:22.420<br>
但是在 testing data 上仍然是有下降<br>
<br>
1:12:22.420,1:12:26.120<br>
這又是一件還頗神奇的事情<br>
<br>
1:12:26.120,1:12:29.680<br>
在 training data 上的 error 已經沒有再下降了<br>
<br>
1:12:29.680,1:12:34.360<br>
但是在 testing data 上的 error 仍然可以繼續下降<br>
<br>
1:12:34.360,1:12:40.100<br>
classifier 已經可以把 training data 的每一筆都 classify 正確<br>
<br>
1:12:40.100,1:12:48.120<br>
感覺已經沒有可以學的東西了<br>
它可以把 training data 都 classify 正確<br>
<br>
1:12:48.220,1:12:51.055<br>
但是在加了更多的 weak classifier 以後<br>
<br>
1:12:51.060,1:12:55.340<br>
居然 testing data error 還可以再下降<br>
<br>
1:12:55.520,1:12:59.460<br>
為甚麼<br>
<br>
1:12:59.700,1:13:01.920<br>
我們來看一下這個式子<br>
<br>
1:13:04.040,1:13:07.540<br>
最後找到的 classifier 叫 H(x)<br>
<br>
1:13:07.540,1:13:10.380<br>
它是一大堆 weak classifier combine 後的結果<br>
<br>
1:13:10.380,1:13:15.160<br>
把 weak classifier combine 後的 output 叫作 g(x)<br>
<br>
1:13:15.160,1:13:18.300<br>
把 g(x) 乘上 y hat<br>
<br>
1:13:18.300,1:13:20.745<br>
定義為 margin<br>
<br>
1:13:20.745,1:13:25.020<br>
我們希望 g(x) 跟 y hat 是同號<br>
<br>
1:13:25.020,1:13:26.780<br>
如果是同號分類才正確<br>
<br>
1:13:26.780,1:13:32.180<br>
不只希望它同號，希望它相乘以後越大越好<br>
<br>
1:13:33.480,1:13:36.300<br>
不只是希望這個 g(x)<br>
<br>
1:13:36.300,1:13:39.560<br>
如果 x 是 positive<br>
<br>
1:13:39.560,1:13:41.320<br>
如果 y hat 是正的<br>
<br>
1:13:41.320,1:13:45.240<br>
不只希望 g(x) 就是稍微大於 0<br>
0.000001<br>
<br>
1:13:45.240,1:13:50.280<br>
希望它比 0 大的越多越好<br>
<br>
1:13:50.480,1:13:56.400<br>
如果 y hat 是正的，g(x) 是 0.00001<br>
<br>
1:13:56.400,1:13:59.920<br>
那一點的 error 就會讓分類錯誤<br>
<br>
1:13:59.920,1:14:03.980<br>
只要一點 training data、testing data mismatch 就會讓分類錯誤<br>
<br>
1:14:03.980,1:14:09.240<br>
但如果 y hat 是正的，而 g(x) 是一個非常大的正值<br>
<br>
1:14:09.280,1:14:11.640<br>
那 error 的影響就會比較小<br>
<br>
1:14:11.820,1:14:20.140<br>
如果從現象上來看一下 Adaboost margin 變化的話<br>
<br>
1:14:20.180,1:14:24.740<br>
會發現如果只有五個 weak classifier 合在一起<br>
<br>
1:14:24.740,1:14:29.780<br>
margin 的分佈是這個樣子<br>
<br>
1:14:29.780,1:14:33.700<br>
如果有一百個甚至一千個 weak classifier 結合在一起的時候<br>
<br>
1:14:33.700,1:14:37.320<br>
它的分佈就是黑色的實線<br>
<br>
1:14:37.320,1:14:42.180<br>
會發現雖然 training data 上的 error 已經不會再下降<br>
<br>
1:14:43.460,1:14:46.220<br>
五個 weak classifier 的時候就已經不會再下降<br>
<br>
1:14:46.220,1:14:51.200<br>
因為所有的 training data 它的 g(x) * y hat 都是大於 0<br>
<br>
1:14:51.200,1:14:54.340<br>
會發現 margin 的分布都是在右邊<br>
<br>
1:14:54.340,1:14:57.560<br>
也就是 y hat 跟所有的 g(x) 同號<br>
<br>
1:14:57.560,1:15:05.280<br>
但在加上 weak classifier 以後可以增加 margin<br>
<br>
1:15:05.340,1:15:09.420<br>
增加 margin 的好處是讓你的方法比較 robust<br>
<br>
1:15:09.480,1:15:13.160<br>
可以在 testing set 上得到比較好的 performance<br>
<br>
1:15:13.160,1:15:17.340<br>
其實 SVM 也有類似的效果<br>
<br>
1:15:17.340,1:15:20.620<br>
Adaboost 也有這個效果<br>
<br>
1:15:20.880,1:15:24.320<br>
為甚麼可以讓 margin 增加<br>
<br>
1:15:24.320,1:15:30.920<br>
這邊就是要說明一下為甚麼 Adaboost 可以讓 margin 增加<br>
<br>
1:15:30.920,1:15:35.160<br>
剛才已經把 Error Rate 的式子列出來<br>
這是 Error Rate 的式子<br>
<br>
1:15:35.160,1:15:37.320<br>
它是綠色這條線<br>
<br>
1:15:37.520,1:15:54.180<br>
這個 Error Rate 的式子有個 upper bound 是紅色這條線<br>
<br>
1:15:54.180,1:15:58.480<br>
這一項是紅色這條線<br>
<br>
1:16:04.860,1:16:08.460<br>
這個 upper bound 其實會越來越小<br>
<br>
1:16:08.460,1:16:11.960<br>
剛才證明對每一個 iteration 而言<br>
<br>
1:16:11.960,1:16:14.100<br>
這個 upper bound 會越來越小<br>
<br>
1:16:14.100,1:16:19.040<br>
雖然並沒有對 upper bound 做微分之類的事情<br>
<br>
1:16:19.040,1:16:23.740<br>
並沒有對 upper bound 做微分、做 Gradient Descent 等等的事情<br>
<br>
1:16:23.740,1:16:26.200<br>
但是會讓這個 upper bound 越來越小<br>
<br>
1:16:26.200,1:16:36.980<br>
所以可以把 upper bound 想成就是 Adaboost 的 Objective Function<br>
<br>
1:16:36.980,1:16:43.660<br>
所以 Adaboost 做的事情是 minimize 一個 Objective Function<br>
<br>
1:16:43.660,1:16:47.620<br>
而這個 Objective Function 是紅色的這條線<br>
<br>
1:16:47.640,1:16:52.220<br>
這邊還畫了別的方法的 Objective Function<br>
<br>
1:16:52.220,1:16:55.560<br>
黃色這條線是 SVM 的 Objective Function<br>
<br>
1:16:55.560,1:17:00.640<br>
綠色這條線是 Logistic Regression 的 Objective Function<br>
<br>
1:17:00.720,1:17:04.920<br>
Adaboost 的 Objective Function 是紅色這條線<br>
<br>
1:17:04.920,1:17:07.880<br>
紅色這條線有甚麼樣的特性<br>
<br>
1:17:10.880,1:17:13.120<br>
如果是考慮這條線<br>
<br>
1:17:13.120,1:17:20.600<br>
只要讓 y hat * g(x) 到這個圖的右邊 error 就是 0<br>
<br>
1:17:20.620,1:17:25.460<br>
到右邊以後如果讓 y hat * g(x) 再更靠右<br>
<br>
1:17:25.460,1:17:27.960<br>
也沒什麼好處 error 不會下降<br>
<br>
1:17:27.960,1:17:32.860<br>
但你看 Adaboost，其實 SVM 跟 Logistic Regression 也有同樣的效果<br>
<br>
1:17:32.860,1:17:38.940<br>
看 Adaboost 這條線，當 y hat、g(x) 同號在右邊的時候<br>
<br>
1:17:38.940,1:17:40.800<br>
error 並不是 0<br>
<br>
1:17:40.800,1:17:47.020<br>
可以把 y hat 跟 g(x) 繼續再往右還是可以得到越來越小的 error<br>
<br>
1:17:47.040,1:17:52.420<br>
所以就算是現在 Error Rate 算出來已經是 0 了<br>
<br>
1:17:52.420,1:17:55.420<br>
對 Adaboost 來說還沒有結束<br>
<br>
1:17:55.420,1:17:57.560<br>
還可以再做得更好<br>
<br>
1:17:57.560,1:18:04.160<br>
因為它可以把 g(x) * y hat 再更往右邊推然後得到更小的 error<br>
<br>
1:18:05.080,1:18:12.460<br>
這個是 Adaboost 為甚麼會 increase 這個 margin<br>
<br>
1:18:12.780,1:18:16.880<br>
最後這一頁是個實作<br>
<br>
1:18:16.880,1:18:20.460<br>
實作一下 Adaboost + Decision Tree<br>
<br>
1:18:20.460,1:18:23.600<br>
Decision Tree 的深度就設為 5<br>
<br>
1:18:23.600,1:18:27.240<br>
把很多深度只有 5 的 Decision Tree 集合起來<br>
<br>
1:18:27.260,1:18:30.520<br>
看看他們可以變甚麼樣子，之前有講過<br>
<br>
1:18:30.520,1:18:34.520<br>
深度是 5 的 Decision Tree 沒有辦法<br>
<br>
1:18:34.520,1:18:36.800<br>
我們之前用的是初音的 function<br>
<br>
1:18:36.800,1:18:39.620<br>
它沒有辦法 fit 一個初音的 function<br>
<br>
1:18:39.620,1:18:42.280<br>
就算做 Bagging、做 Random Forest<br>
<br>
1:18:42.280,1:18:44.660<br>
也沒有用 Random Forest 它本來要做的事情<br>
<br>
1:18:44.660,1:18:49.380<br>
就並不是要讓不同 weak classifier 之間可以互補<br>
<br>
1:18:49.380,1:18:52.520<br>
它只要讓強的 classifier varience 不要那麼大而已<br>
<br>
1:18:52.520,1:18:57.560<br>
但是 Adaboost 不一樣它可以讓 weak classifier 彼此之間是互補<br>
<br>
1:18:57.580,1:19:01.120<br>
所以就算是深度是 5 的 Decision Tree<br>
<br>
1:19:01.120,1:19:04.460<br>
一棵沒有辦法 fit 出一個 function<br>
<br>
1:19:04.460,1:19:08.960<br>
如果找了十棵，這邊 T = 10 代表 Adaboost iteration 跑了十次<br>
<br>
1:19:08.960,1:19:11.820<br>
有十棵深度是 5 的 Decision Tree<br>
<br>
1:19:11.820,1:19:16.620<br>
這些 Decision Tree 互相之間是互補的<br>
<br>
1:19:16.620,1:19:18.420<br>
跟 Random Forest 不一樣<br>
<br>
1:19:18.560,1:19:20.920<br>
這十棵 Decision Tree 是互補的<br>
<br>
1:19:20.920,1:19:25.040<br>
如果是 Random Forest 找十棵一百棵都 fit 不了初音這個 function<br>
<br>
1:19:25.040,1:19:31.240<br>
但是如果找十棵 tree 彼此之間是互補的就可以得到比較好的結果<br>
<br>
1:19:31.240,1:19:37.560<br>
這是個初音的 function，你可以看到初音的樣子，可是他的腳是歪的<br>
<br>
1:19:37.560,1:19:41.000<br>
如果有二十棵樹就可以做得好很多了<br>
<br>
1:19:41.000,1:19:43.920<br>
只是這邊腳的地方還是有點奇怪的東西<br>
<br>
1:19:43.920,1:19:47.560<br>
如果五十棵樹，幾乎就可以 fit 初音的 function<br>
<br>
1:19:47.560,1:19:50.580<br>
但其實這樣還沒有結束因為這邊其實要有一根毛<br>
<br>
1:19:50.580,1:19:52.420<br>
要把那根毛做出來才行<br>
<br>
1:19:52.420,1:19:58.880<br>
如果用一百棵樹的話就可以幾乎完美的 fit 初音的 function<br>
<br>
1:19:58.960,1:20:08.340<br>
從這個例子可以看到 Boosting 跟 Bagging 是不一樣的<br>
<br>
1:20:10.420,1:20:15.700<br>
接下來講的是 Gradient Boosting<br>
<br>
1:20:15.820,1:20:24.480<br>
Gradient Boosting 它是剛才那個 Boosting 演算法更 general 的版本<br>
<br>
1:20:24.480,1:20:27.980<br>
整個 Boosting 演算法 in general 可以看成是<br>
<br>
1:20:27.980,1:20:30.340<br>
以下這樣的 algorithm<br>
<br>
1:20:30.340,1:20:34.800<br>
現在跑 T 個 iteration<br>
<br>
1:20:34.940,1:20:38.420<br>
每次在 T 個 iteration 裡面<br>
<br>
1:20:38.420,1:20:43.440<br>
找一個 function ft 跟 alpha t<br>
<br>
1:20:43.440,1:20:47.340<br>
找一個 weak classifier ft 跟它的 weight alpha t<br>
<br>
1:20:47.440,1:20:53.140<br>
這些人合在一起會 improve 一個 g t-1<br>
<br>
1:20:53.140,1:20:54.620<br>
g t-1 是甚麼<br>
<br>
1:20:54.620,1:21:00.220<br>
g t-1 是把過去所有的已經找出來的 function<br>
<br>
1:21:01.600,1:21:06.120<br>
把過去所有已經找出來的 function 根據 weighted sum 的結果<br>
<br>
1:21:06.120,1:21:08.160<br>
就是這個 g t-1<br>
<br>
1:21:08.340,1:21:10.600<br>
已經有一個 g t-1 了<br>
<br>
1:21:10.600,1:21:15.840<br>
要找一個 ft 跟 alpha t 跟 g t-1 是互補的<br>
<br>
1:21:15.900,1:21:20.720<br>
把這個 ft 跟 alpha t 加到 g t-1 以後<br>
<br>
1:21:20.720,1:21:22.160<br>
變成了 gt 了<br>
<br>
1:21:22.160,1:21:24.700<br>
會比原來的 g t-1 更好<br>
<br>
1:21:24.700,1:21:29.100<br>
最後跑完 T 個 iteration 就得到 H(x)<br>
<br>
1:21:29.300,1:21:37.120<br>
問題是怎麼找到比較好的 g(x)<br>
<br>
1:21:39.000,1:21:47.720<br>
怎麼樣找到一個 ft 把它加到 g t-1以後得到的 gt 是比較好的<br>
<br>
1:21:48.000,1:21:53.200<br>
要為 g 設一個目標<br>
<br>
1:21:53.940,1:21:59.420<br>
在做 Machine Learning 的時候要設一個 Objective Function<br>
<br>
1:21:59.420,1:22:07.240<br>
接下來就是調整參數去 maximize Objective Function 或是 minimize Cost Function<br>
<br>
1:22:07.260,1:22:13.720<br>
現在要做的就是 minimize Cost Function<br>
<br>
1:22:14.220,1:22:16.140<br>
這 Cost Function 怎麼寫<br>
<br>
1:22:16.140,1:22:20.220<br>
對某一個 function g 它的 Cost Function 怎麼寫<br>
<br>
1:22:20.220,1:22:23.740<br>
寫成 summation over 所有的 training data n<br>
<br>
1:22:23.740,1:22:27.560<br>
小 L 是 Loss Function<br>
<br>
1:22:27.560,1:22:33.280<br>
小 L 這個 function 是算 y hat 跟 g(x) 他們之間的差異<br>
<br>
1:22:33.280,1:22:40.740<br>
比如說可以用 Cross Entropy 或 Mean Square Error 等等<br>
<br>
1:22:40.740,1:22:44.700<br>
來計算 y hat 和 g 之間的差異<br>
<br>
1:22:44.820,1:22:56.860<br>
把小 L 定為 exp( - y hat * g(x))<br>
<br>
1:22:56.880,1:22:58.640<br>
這個定義合不合理<br>
<br>
1:22:58.640,1:23:04.000<br>
這個定義應該是合理的，因為如果要 minimize 它的話<br>
<br>
1:23:04.000,1:23:07.280<br>
minimize exp( - y hat * g(x))<br>
<br>
1:23:07.280,1:23:11.080<br>
希望 y hat 跟 g(x) 儘量同號<br>
<br>
1:23:11.080,1:23:16.780<br>
而且他們同號相乘的時候要越大要越好<br>
<br>
1:23:17.720,1:23:22.880<br>
那怎麼 minimize 這個 function g<br>
<br>
1:23:22.900,1:23:26.840<br>
這一步可能需要稍微地想一下<br>
<br>
1:23:26.840,1:23:32.440<br>
這件事情比較抽象可是其實要用 Gradient Descent<br>
<br>
1:23:32.440,1:23:36.040<br>
來找一個新的 function gt<br>
<br>
1:23:36.100,1:23:41.280<br>
它可以 minimize Loss Function L<br>
<br>
1:23:46.440,1:23:53.640<br>
要把 g 這個 function 對 L 做微分<br>
<br>
1:23:53.640,1:23:56.720<br>
算出它的 Gradient<br>
<br>
1:24:02.540,1:24:05.540<br>
這邊 notation 好像沒有用的很好<br>
<br>
1:24:05.540,1:24:09.040<br>
這邊我應該是寫三角形比較對<br>
<br>
1:24:12.480,1:24:15.480<br>
沒關係大家知道我的意思<br>
<br>
1:24:15.480,1:24:23.280<br>
要把 function g 對 L 算它的 gradient<br>
<br>
1:24:23.340,1:24:29.500<br>
然後再用這個 gradient 去 update g t-1 得到 gt<br>
<br>
1:24:29.500,1:24:35.280<br>
這樣新的 gt 跟原來的 g t-1 比起來它會讓 Loss Function 比較小<br>
<br>
1:24:35.280,1:24:37.380<br>
這樣大家知道我的意思嗎<br>
<br>
1:24:37.380,1:24:41.480<br>
我猜這邊你一定一下又卡住了<br>
<br>
1:24:41.480,1:24:48.100<br>
甚麼叫拿一個 function 對 L 做 gradient<br>
<br>
1:24:48.100,1:24:52.080<br>
function g 又不是參數<br>
<br>
1:24:52.200,1:24:55.800<br>
對不對，如果是 Neural Network 參數 theta<br>
<br>
1:24:55.800,1:24:58.720<br>
你知道怎麼對 L 算 gradient<br>
<br>
1:24:58.720,1:25:01.640<br>
但是如果是一個 function g<br>
<br>
1:25:01.840,1:25:05.940<br>
它要怎麼對 L 做 gradient<br>
這個地方<br>
<br>
1:25:06.060,1:25:11.660<br>
你可以這樣想<br>
其實一個 function g(x)<br>
<br>
1:25:11.660,1:25:14.420<br>
假設橫坐標就是 x<br>
<br>
1:25:14.420,1:25:18.440<br>
它其實是高維不過這邊就意思一下<br>
<br>
1:25:18.580,1:25:23.600<br>
其實一個 function 比如說它長這個樣子就是 g(x)<br>
<br>
1:25:23.740,1:25:29.600<br>
你可以想成它的每一點就是一個參數<br>
<br>
1:25:29.840,1:25:32.200<br>
這樣大家可以想像嗎<br>
<br>
1:25:32.280,1:25:35.040<br>
取一個 x1<br>
<br>
1:25:35.640,1:25:38.880<br>
得到一個 g(x1)<br>
<br>
1:25:39.160,1:25:41.740<br>
取一個 x2<br>
<br>
1:25:41.740,1:25:45.160<br>
得到一個 g(x2)<br>
<br>
1:25:45.160,1:25:47.880<br>
假設 x 取的非常非常的密<br>
<br>
1:25:47.880,1:25:51.440<br>
那其實 g(x) 就是一個 vector<br>
<br>
1:25:51.540,1:25:55.340<br>
這個 g 就是一個 vector g(x1)<br>
<br>
1:25:55.460,1:25:57.260<br>
g(x2)<br>
<br>
1:25:57.520,1:25:59.220<br>
......<br>
<br>
1:25:59.300,1:26:04.380<br>
它就是一個 vector<br>
這個 vector 就是這個 function 的參數<br>
<br>
1:26:04.380,1:26:07.880<br>
可以調整參數就改變了這個 function 的形狀<br>
<br>
1:26:07.880,1:26:10.020<br>
你可以決定這個 function 的形狀是甚麼<br>
<br>
1:26:10.020,1:26:13.660<br>
你就調整這些參數這個 function 的形狀就變了<br>
<br>
1:26:13.660,1:26:19.480<br>
其實可以把一個 function 想成它其實就是有無窮多的參數<br>
<br>
1:26:19.540,1:26:21.360<br>
大家可以想像嗎<br>
<br>
1:26:25.720,1:26:33.640<br>
反正我既然可以接受它是參數的話<br>
就可以把它對 L 做偏微分<br>
<br>
1:26:33.660,1:26:38.900<br>
還是可以說我如果改變 g 在某一個點的位置<br>
<br>
1:26:38.900,1:26:41.620<br>
它對 L 的影響有多小、有多大<br>
<br>
1:26:41.620,1:26:47.280<br>
所以還是可以算出 g 對 L 的偏微分<br>
<br>
1:26:49.420,1:26:54.760<br>
這個是如果從 Gradient Descent 的角度來考慮的話是這樣子<br>
<br>
1:26:55.040,1:27:01.160<br>
如果從 Boosting 角度來看得話<br>
Boosting 做的事情是找一個 ft 跟 alpha t<br>
<br>
1:27:01.160,1:27:03.920<br>
加到 g t-1 後變 gt<br>
<br>
1:27:03.920,1:27:06.580<br>
怎麼找這個 ft 跟 alpha t<br>
<br>
1:27:06.580,1:27:11.560<br>
就會希望 ft 跟 alpha t 這一項<br>
<br>
1:27:11.720,1:27:14.700<br>
其實就是這一項<br>
<br>
1:27:14.780,1:27:18.240<br>
或者是至少他們的方向<br>
<br>
1:27:18.400,1:27:23.480<br>
要是一樣的，因為前面還有乘一個 Learning Rate<br>
<br>
1:27:23.480,1:27:27.040<br>
這兩個式子一模一樣其實沒有必要但是<br>
<br>
1:27:27.220,1:27:30.060<br>
希望他們的方向是一樣的<br>
<br>
1:27:30.060,1:27:36.400<br>
如果 ft 的方向跟這個微分的方向一致的話<br>
<br>
1:27:36.400,1:27:40.780<br>
把這個 ft 加給 g t-1<br>
<br>
1:27:40.860,1:27:45.080<br>
就可以讓新的 gt loss 變小<br>
<br>
1:27:45.260,1:27:48.300<br>
這個是比較抽象的部分<br>
<br>
1:27:50.880,1:28:02.120<br>
假設定義 L 就是長樣子的話那對 g 做偏微分得到的值是多少<br>
<br>
1:28:02.220,1:28:05.820<br>
把 L 對 g 做偏微分<br>
<br>
1:28:05.860,1:28:08.260<br>
這邊是 exp( - y n * g(x))<br>
<br>
1:28:08.260,1:28:12.280<br>
這邊是 exp (  - y n *g (t-1) )<br>
<br>
1:28:12.520,1:28:18.100<br>
把它對 g 做偏微分得到的值是多少<br>
<br>
1:28:18.100,1:28:23.140<br>
exponential 的部分做偏微分以後是不變的<br>
<br>
1:28:27.420,1:28:29.280<br>
g 其實是我們的參數<br>
<br>
1:28:29.280,1:28:32.660<br>
對 exponential 的指數項做微分的話<br>
<br>
1:28:32.660,1:28:35.080<br>
這邊得到的是 - y hat<br>
<br>
1:28:35.080,1:28:39.620<br>
前面這邊有一項符號我把它拿下來，這邊省略掉了 Learning Rate<br>
<br>
1:28:39.620,1:28:42.820<br>
負號是可以消掉的<br>
<br>
1:28:42.820,1:28:45.140<br>
所以得到了這樣的式子<br>
<br>
1:28:45.140,1:28:58.800<br>
會希望 ft 跟這個式子的方向越一致越好<br>
<br>
1:28:58.860,1:29:01.320<br>
所謂的方向越一致越好是甚麼意思<br>
<br>
1:29:01.320,1:29:07.680<br>
每一個 function 都可以把它想成是一個 vector<br>
只是這個 vector 有無窮多維<br>
<br>
1:29:07.680,1:29:12.840<br>
ft 是一個 vector 它有無窮多維<br>
<br>
1:29:12.920,1:29:17.960<br>
這個也是一個 vector 這個 vector 有無窮多維<br>
<br>
1:29:19.480,1:29:25.860<br>
如果你覺得無窮多維很難想像的話，可以只考慮 training data 有出現的 x<br>
<br>
1:29:25.980,1:29:31.640<br>
那它的維度就是有限的，training data 一百萬筆 data 它就是一百萬維<br>
<br>
1:29:31.640,1:29:38.020<br>
希望 ft 跟這個式子他們的的方向越一致越好<br>
<br>
1:29:38.020,1:29:40.060<br>
怎麼讓它越一致越好<br>
<br>
1:29:40.060,1:29:43.100<br>
因為 ft 是我們要找的目標<br>
<br>
1:29:43.100,1:29:45.920<br>
我們要找出 ft<br>
<br>
1:29:45.920,1:29:48.140<br>
要怎麼找 ft<br>
<br>
1:29:48.140,1:30:00.120<br>
要找這個 ft 希望如果把 ft 乘上這一項<br>
<br>
1:30:00.120,1:30:02.340<br>
這個值要越大越好<br>
<br>
1:30:02.340,1:30:07.200<br>
如果這個值越大越好就代表 ft 跟這個式子<br>
<br>
1:30:07.200,1:30:12.960<br>
他們的方向越一致<br>
<br>
1:30:13.680,1:30:16.960<br>
這個式子要怎麼看<br>
<br>
1:30:16.960,1:30:29.960<br>
這個式子可以想成對每一筆 training data 都希望 y hat 跟ft 他們是同號的<br>
<br>
1:30:29.960,1:30:35.440<br>
然後每一筆 training data 前面都乘上了一個 weight<br>
<br>
1:30:35.440,1:30:38.760<br>
這個 weight 是 u 上標 n 下標 t<br>
<br>
1:30:38.760,1:30:48.420<br>
都乘上一個 weight 這個 weight 是 exp( - y hat * g (t-1))<br>
<br>
1:30:51.640,1:31:02.920<br>
這個 weight exp ( - y hat * g t-1(x n )) 到底是甚麼<br>
<br>
1:31:02.920,1:31:06.040<br>
把 g t-1 的式子帶進去<br>
<br>
1:31:06.040,1:31:12.480<br>
g t-1 是一堆小 f 乘上它的 weight 的 summation<br>
<br>
1:31:12.480,1:31:16.460<br>
然後再把相加的這一項提出來<br>
<br>
1:31:16.460,1:31:18.480<br>
變成連乘<br>
<br>
1:31:18.480,1:31:30.340<br>
就會發現這個 weight exactly 就是 Adaboost 的 weight<br>
<br>
1:31:31.360,1:31:37.200<br>
所以找出來的這個 ft<br>
<br>
1:31:37.240,1:31:41.920<br>
其實就是 Adaboost 裡面找出來的 f<br>
<br>
1:31:41.920,1:31:47.380<br>
所以 Adaboost 裡面找一個 weak classifier ft 的時候<br>
<br>
1:31:47.400,1:31:51.620<br>
可以想成好像在做 Gradient Descent 一樣<br>
<br>
1:31:51.620,1:31:56.580<br>
有了這個 ft 把這個 ft 加到 g 裡面<br>
<br>
1:31:56.580,1:32:04.060<br>
會讓 g 的 loss 變小<br>
<br>
1:32:04.380,1:32:11.700<br>
再來的問題就是怎麼決定 alpha t<br>
<br>
1:32:12.300,1:32:18.340<br>
這個 alpha t 的作用就很像是 Learning Rate 一樣<br>
<br>
1:32:18.340,1:32:22.800<br>
在一般做 Gradient Descent、train Neural Network 的時候<br>
<br>
1:32:22.800,1:32:29.820<br>
Learning Rate 就是設個 fix 的值或者是用種種比如說<br>
<br>
1:32:29.820,1:32:34.200<br>
adaptive Learning Rate 的設法來設它<br>
<br>
1:32:35.860,1:32:40.880<br>
但是這邊做的事情是給定了 ft 以後<br>
<br>
1:32:40.880,1:32:45.920<br>
窮舉各種不同可能的 alpha、去試不同可能的 alpha<br>
<br>
1:32:45.920,1:32:49.860<br>
看看哪一個 alpha 可以讓 gt 最小<br>
<br>
1:32:49.860,1:32:53.880<br>
找完 ft 以後把 ft 固定下來試不同的 alpha<br>
<br>
1:32:53.880,1:33:04.540<br>
看哪一個 alpha 可以讓 gt 的 loss 更小<br>
<br>
1:33:04.720,1:33:08.280<br>
為甚麼這邊會選擇這樣的做法<br>
<br>
1:33:08.280,1:33:12.640<br>
因為在做 Gradient Descent、在 train Neural Network 的時候<br>
<br>
1:33:12.640,1:33:16.540<br>
算參數的 gradient 都是比較快的<br>
<br>
1:33:16.540,1:33:21.820<br>
所以你可能不會稀罕說你的 Learning Rate 設得好不好<br>
<br>
1:33:21.820,1:33:24.540<br>
如果 Learning Rate 設得太小<br>
<br>
1:33:24.540,1:33:27.640<br>
反正就多算幾次 gradient、多跑幾次就行<br>
<br>
1:33:27.700,1:33:31.020<br>
但是 Gradient Boosting 的方法裡面<br>
<br>
1:33:31.060,1:33:35.040<br>
ft 是一個 classifier<br>
<br>
1:33:35.040,1:33:39.840<br>
在找 ft 的過程中它的運算量可能就是很大的<br>
<br>
1:33:39.840,1:33:42.020<br>
甚至如果 ft 是個 Neural Network<br>
<br>
1:33:42.020,1:33:51.900<br>
要把 ft 找出來的時候本身就需要很多次的 Gradient Descent 的 iteration<br>
<br>
1:33:51.900,1:33:56.340<br>
既然找出了 ft 以後就要好好的珍惜它<br>
<br>
1:33:56.340,1:34:00.080<br>
把它的利用價值發揮到最大<br>
<br>
1:34:00.080,1:34:03.180<br>
這邊 Gradient Boosting 採取的方式是<br>
<br>
1:34:03.180,1:34:06.960<br>
既然已經找出 ft，固定住 ft 然後<br>
<br>
1:34:06.960,1:34:09.900<br>
硬調一個最好的 Learning Rate alpha t<br>
<br>
1:34:09.900,1:34:13.740<br>
窮舉所有的 Learning Rate alpha t<br>
<br>
1:34:13.740,1:34:19.580<br>
看哪一個 alpha t 可以讓 loss 掉最多<br>
<br>
1:34:21.300,1:34:26.120<br>
實際上不可能窮舉所有的 alpha t 一個一個去試試看<br>
<br>
1:34:26.240,1:34:27.940<br>
這邊做的事情其實就是<br>
<br>
1:34:28.060,1:34:31.355<br>
解一個 optimization 的 problem<br>
<br>
1:34:31.360,1:34:35.540<br>
看哪一個 alpha t 可以讓 loss 最小<br>
<br>
1:34:36.100,1:34:41.980<br>
怎麼做<br>
這邊就把 equation 略過<br>
<br>
1:34:41.980,1:34:43.720<br>
實際上做的事情就是<br>
<br>
1:34:43.720,1:34:48.080<br>
計算 alpha t 跟 L(g) 的微分<br>
<br>
1:34:48.080,1:34:53.320<br>
然後再看 alpha t 的值是多少的時候這個微分是 0<br>
<br>
1:34:53.320,1:34:57.500<br>
這樣就可以把極值找出來<br>
<br>
1:34:57.680,1:35:00.720<br>
巧合的是找出來的 alpha t<br>
<br>
1:35:00.720,1:35:04.360<br>
就是 ln√( 1 - εt)/εt<br>
<br>
1:35:04.360,1:35:08.800<br>
就是 Adaboost 裡面的那個 weight<br>
<br>
1:35:08.800,1:35:11.320<br>
所以 Adaboost 整件事情<br>
<br>
1:35:11.320,1:35:14.900<br>
就可以想成它也是在做 Gradient Descent<br>
<br>
1:35:14.900,1:35:18.180<br>
只是 Gradient 是一個 function<br>
<br>
1:35:18.180,1:35:21.900<br>
Learning Rate 有一個很好的方法<br>
<br>
1:35:21.900,1:35:24.240<br>
可以決定 Learning Rate<br>
<br>
1:35:27.420,1:35:30.420<br>
Gradient Boosting 有一個想法好的地方是<br>
<br>
1:35:30.500,1:35:34.780<br>
可以任意更改 Objective Function<br>
<br>
1:35:34.780,1:35:37.260<br>
我們剛才定了一個 Objective Function<br>
<br>
1:35:37.260,1:35:40.780<br>
是 exp(- y hat * g(x))<br>
<br>
1:35:40.780,1:35:44.800<br>
你永遠可以定其它的 Objective Function<br>
<br>
1:35:44.800,1:35:49.700<br>
就可以創造出不一樣的新的方法<br>
<br>
1:35:52.720,1:35:58.720<br>
最後一個我要講的 ensemble 方法是 Stacking<br>
<br>
1:35:58.720,1:36:02.660<br>
Stacking 是甚麼<br>
Stacking 非常實用我覺得<br>
<br>
1:36:02.660,1:36:05.560<br>
final project 裡面非常實用的方法<br>
<br>
1:36:05.620,1:36:09.020<br>
現在到了期末大家都很忙<br>
<br>
1:36:09.020,1:36:11.140<br>
一組其實有四個人<br>
<br>
1:36:11.140,1:36:16.920<br>
可能就四個人每個人都弄了一個自己的 model<br>
<br>
1:36:17.080,1:36:23.280<br>
選好一個 final project 題目後四個人都弄了一個自己的 model<br>
<br>
1:36:23.300,1:36:27.500<br>
但最後要怎麼讓 performance 再提升<br>
<br>
1:36:27.500,1:36:30.580<br>
就要把四個人的 model combine 起來<br>
<br>
1:36:30.580,1:36:35.720<br>
比如說把一把 data x 丟到四個 model 裡面<br>
<br>
1:36:35.720,1:36:38.740<br>
然後每一個 model 都會給你一個 output<br>
<br>
1:36:38.740,1:36:44.660<br>
再把這四個 output 想辦法把它合併起來得到最終的答案<br>
<br>
1:36:44.660,1:36:50.940<br>
假設是個分類的問題的話可以用 Majority Vote<br>
<br>
1:36:50.940,1:36:55.920<br>
最多系統選擇哪個 class 那那個 class 就是正確答案<br>
<br>
1:36:56.080,1:37:04.380<br>
今天會遇到的問題是並不是所有的系統都是好<br>
並不是所有的 model 都是好<br>
<br>
1:37:04.400,1:37:07.480<br>
有一些 model 可能是爛的，比如說可能小毛特別弱<br>
<br>
1:37:07.480,1:37:10.320<br>
它做的系統是跟 random 一樣<br>
<br>
1:37:10.340,1:37:14.960<br>
所以如果把它系統權重跟其它系統設一樣<br>
<br>
1:37:15.000,1:37:17.500<br>
那這樣不行這樣整個 performance 會壞掉<br>
<br>
1:37:17.500,1:37:22.500<br>
但如果你本來就知道小毛特別弱，把他的權重設的很低的話<br>
<br>
1:37:22.500,1:37:24.340<br>
就傷了他的自尊心<br>
<br>
1:37:24.600,1:37:29.160<br>
所以怎麼辦<br>
要去 learn 一個 classifier<br>
<br>
1:37:29.360,1:37:33.860<br>
這個 classifier 是這樣，它把前面這些 system 的 output<br>
<br>
1:37:33.920,1:37:36.900<br>
當作 input，也就是說這些 system 的 output<br>
<br>
1:37:36.980,1:37:42.140<br>
對最後這個 classifier 來說就好像是個 feature 一樣<br>
<br>
1:37:42.140,1:37:48.220<br>
它把這些 system 的 output 當作 feature 再決定最終的結果是甚麼<br>
<br>
1:37:48.260,1:37:51.400<br>
這個最終的 classifier 就不需要太複雜<br>
<br>
1:37:51.400,1:37:56.320<br>
最前面如果都已經用好幾個 Hidden Layer 的 Neural Network 了<br>
<br>
1:37:56.320,1:38:00.580<br>
也許 final classifier 就不需要再好幾個 Hidden Layer 的 Neural Network<br>
<br>
1:38:00.580,1:38:04.420<br>
它可以只是 Logistic Regression 就行了<br>
<br>
1:38:10.255,1:38:13.385<br>
那在做這個實驗的時候要注意<br>
<br>
1:38:13.385,1:38:18.240<br>
我們會把有 label 的 data 分成 training set 跟 validation set<br>
<br>
1:38:18.240,1:38:22.340<br>
在做 Stacking 的時候要把 training set 再分成兩部分<br>
<br>
1:38:22.340,1:38:27.260<br>
一部份的 training set 拿來 learn 這些 classifier<br>
<br>
1:38:27.260,1:38:32.460<br>
另外一部分的 training data 拿來 learn 這個 final classifier<br>
<br>
1:38:32.580,1:38:36.120<br>
為甚麼要這麼做? 因為有的 classifier<br>
<br>
1:38:36.120,1:38:38.940<br>
有的要來做 Stacking 的前面 classifier<br>
<br>
1:38:38.940,1:38:41.460<br>
它可能只是 fit training data<br>
<br>
1:38:41.460,1:38:45.560<br>
舉例來說可能小明的 code 就是亂寫的<br>
<br>
1:38:45.560,1:38:49.540<br>
其實它的 classifier 甚麼事都不會做<br>
它的 classifier 就是<br>
<br>
1:38:49.540,1:38:54.840<br>
如果有一筆 data 進來跟 training data 一樣它就把它 label 找出來<br>
<br>
1:38:54.840,1:38:57.480<br>
不然就甚麼事都沒有做<br>
<br>
1:38:57.600,1:39:02.620<br>
它可能寫一個很奇怪、很爛的、很異常 overfitting 的 code<br>
<br>
1:39:02.620,1:39:05.915<br>
如果 final classifier 的 training data<br>
<br>
1:39:05.920,1:39:09.860<br>
跟這些 system 用的 training data 是同一組的話<br>
<br>
1:39:09.860,1:39:12.800<br>
就會發現喔小明的 classifier 好強 Error Rate 是 100%<br>
<br>
1:39:12.800,1:39:14.960<br>
都參考小明的 classifier 就好<br>
<br>
1:39:15.040,1:39:18.000<br>
但其實小明的 classifier 其實甚麼事都沒有做<br>
<br>
1:39:18.000,1:39:20.840<br>
它只是硬把 training data 記起來而已<br>
<br>
1:39:20.840,1:39:27.860<br>
所以在 train final classifier 的時候必須要用另外一筆 training data 來 train final classifier<br>
<br>
1:39:27.860,1:39:30.880<br>
不能跟前面 train 系統的 classifier 一樣<br>
<br>
1:39:30.880,1:39:35.600<br>
如果有 final classifier 就可以給不同的系統不同的權重<br>
<br>
1:39:35.600,1:39:37.800<br>
如果小毛的系統特別差的話<br>
<br>
1:39:37.800,1:39:40.040<br>
那 final classifier 就會給它比較小的權重<br>
<br>
1:39:40.040,1:39:41.460<br>
比如說是 0 這樣<br>
<br>
1:39:41.460,1:39:44.360<br>
這樣小毛的自尊心其實還是會被傷害<br>
<br>
1:39:44.360,1:39:49.280<br>
只是它是被機器傷害所以就可以維護團隊和諧<br>
<br>
1:39:51.280,1:39:54.900<br>
最後這一頁講過了<br>
<br>
1:39:54.900,1:39:58.640<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
