<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.040,0:00:01.960<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:01.960,0:00:02.980<br>
我們來講 Semi-supervised learning<br>
<br>
0:00:02.980,0:00:04.000<br>
有一件事情是這樣子的<br>
<br>
0:00:04.000,0:00:06.380<br>
本來預計今天要公告作業四<br>
<br>
0:00:06.380,0:00:08.980<br>
但我想說，我們改到下周再公告好了<br>
<br>
0:00:09.980,0:00:12.540<br>
那你就先專心把作業三<br>
<br>
0:00:12.540,0:00:15.660<br>
做完，然後下周再開始做作業四<br>
<br>
0:00:15.660,0:00:18.980<br>
如果沒有意外的話，下周也會同時公告<br>
<br>
0:00:18.980,0:00:20.620<br>
final project<br>
<br>
0:00:23.120,0:00:26.480<br>
那我們先來講一下 Semi-supervised learning<br>
<br>
0:00:26.480,0:00:30.300<br>
就是我們作業三，要請大家稍微做一下的東西<br>
<br>
0:00:30.300,0:00:33.580<br>
因為如果只做 CIFAR-10 的辨識太簡單<br>
<br>
0:00:33.580,0:00:38.560<br>
去網路上 call 個 script，按個 enter，應該就可以得到結果<br>
<br>
0:00:38.560,0:00:41.400<br>
所以，我們增加一些挑戰性<br>
<br>
0:00:41.400,0:00:43.740<br>
那甚麼是 Semi-supervised learning<br>
<br>
0:00:43.740,0:00:45.800<br>
Supervised learning 大家都知道<br>
<br>
0:00:45.800,0:00:47.240<br>
在 Supervised learning 裡面<br>
<br>
0:00:47.240,0:00:49.620<br>
你就是有一大堆的 training data<br>
<br>
0:00:49.620,0:00:51.900<br>
這些 training data 的組成<br>
<br>
0:00:51.900,0:00:57.160<br>
是一個 function 的 input 跟 function 的 output 的 pair<br>
<br>
0:00:57.160,0:00:58.880<br>
假設你有 R 筆 training data<br>
<br>
0:00:58.880,0:01:00.240<br>
每一筆 training data 裡面<br>
<br>
0:01:00.240,0:01:02.660<br>
都有一個 x^r 代表 function 的 input<br>
<br>
0:01:02.660,0:01:05.540<br>
都有一個 y^r\head 代表 function 的 output<br>
<br>
0:01:05.540,0:01:08.300<br>
這個 x^r 代表是一張<br>
<br>
0:01:08.300,0:01:10.460<br>
舉例來說，在 homework 3 裡面<br>
<br>
0:01:10.460,0:01:12.240<br>
x^r 是一張 image<br>
<br>
0:01:12.240,0:01:15.820<br>
y^r\head 是 class 的 label<br>
<br>
0:01:15.820,0:01:18.620<br>
那所謂的 Semi-supervised learning 是甚麼呢<br>
<br>
0:01:18.620,0:01:20.380<br>
Semi-supervised learning 是說<br>
<br>
0:01:20.380,0:01:23.820<br>
在 labeled data 上面，我們有另外一組<br>
<br>
0:01:23.820,0:01:25.720<br>
unlabeled 的 data<br>
<br>
0:01:26.060,0:01:28.360<br>
那這一組 unlabeled 的 data，我們這邊<br>
<br>
0:01:28.360,0:01:30.580<br>
寫成 x^u<br>
<br>
0:01:30.580,0:01:34.000<br>
那在這些 unlabeled 的 data，它就只有 function 的 input<br>
<br>
0:01:34.000,0:01:36.680<br>
它沒有 output，在這邊呢<br>
<br>
0:01:36.680,0:01:40.240<br>
有 U 筆 unlabeled 的 data<br>
<br>
0:01:40.240,0:01:43.520<br>
通常我們在做 Semi-supervised learning 的時候<br>
<br>
0:01:43.520,0:01:46.620<br>
我們期待常見的 scenario 是<br>
<br>
0:01:46.620,0:01:50.600<br>
unlabeled 的數量遠大於 labeled 的數量<br>
<br>
0:01:50.600,0:01:52.740<br>
也就這邊的 U<br>
<br>
0:01:52.740,0:01:55.420<br>
是遠大於 R 的<br>
<br>
0:01:56.100,0:02:00.040<br>
那其實 Semi-supervised learning 可以分成兩種<br>
<br>
0:02:00.040,0:02:01.920<br>
一種叫做 Transductive learning<br>
<br>
0:02:01.920,0:02:03.660<br>
一種叫做 Inductive learning<br>
<br>
0:02:04.080,0:02:06.680<br>
那 Transductive learning 跟 Inductive learning<br>
<br>
0:02:06.680,0:02:08.560<br>
我認為最簡單的分法就是<br>
<br>
0:02:08.560,0:02:10.900<br>
在做 Transductive learning 的時候<br>
<br>
0:02:10.900,0:02:13.740<br>
你的 unlabeled data 就是你的 testing set<br>
<br>
0:02:13.740,0:02:15.420<br>
這樣大家懂我意思嗎<br>
<br>
0:02:15.420,0:02:19.120<br>
有人會說，這不是用了 testing set，這不是 cheating 嗎<br>
<br>
0:02:19.120,0:02:21.900<br>
其實不是，你用了 testing set 的<br>
<br>
0:02:21.900,0:02:23.720<br>
label 才是 cheating<br>
<br>
0:02:23.720,0:02:27.420<br>
你用了 testing set 的 feature<br>
<br>
0:02:27.420,0:02:30.020<br>
不是 cheating，這樣大家懂我意思嗎？<br>
<br>
0:02:30.020,0:02:33.500<br>
因為，那筆 testing set 的 feature<br>
<br>
0:02:33.500,0:02:35.380<br>
本來就在那邊了，所以<br>
<br>
0:02:35.380,0:02:37.400<br>
你是可以用它的<br>
<br>
0:02:37.400,0:02:41.480<br>
所以，如果你用了 testing set 的 feature 的話<br>
<br>
0:02:41.480,0:02:44.680<br>
這個叫做 Transductive learning<br>
<br>
0:02:44.680,0:02:47.320<br>
我已經跟助教確認過了<br>
<br>
0:02:47.320,0:02:50.640<br>
其實，只要在 Kaggle 上面載下來的 data 都是可以用的<br>
<br>
0:02:50.640,0:02:53.160<br>
所以，你其實可以用 testing set 的 image<br>
<br>
0:02:53.160,0:02:55.940<br>
你只是不能夠去找它的 label 出來而已<br>
<br>
0:02:55.940,0:02:58.940<br>
這樣大家了解我的意思嗎<br>
<br>
0:02:58.940,0:03:02.160<br>
那 Inductive learning 呢，Inductive learning 是說<br>
<br>
0:03:02.160,0:03:05.260<br>
我們不把 testing set 考慮進來<br>
<br>
0:03:05.260,0:03:09.120<br>
假設我們在 training 的時候<br>
<br>
0:03:09.120,0:03:13.520<br>
我們還不知道 testing set 會長什麼樣子<br>
<br>
0:03:13.520,0:03:16.460<br>
所以我沒有辦法事先跟據 testing set 去做任何事<br>
<br>
0:03:16.460,0:03:18.200<br>
那我們必須要先 learn 好一個 model<br>
<br>
0:03:18.200,0:03:20.540<br>
在  testing set 進來的時候<br>
<br>
0:03:20.540,0:03:21.880<br>
再去 classify 它<br>
<br>
0:03:21.880,0:03:24.860<br>
至於要用 Transductive learning 還是 Inductive learning<br>
<br>
0:03:24.860,0:03:28.060<br>
現在 testing set 是不是已經有給你了<br>
<br>
0:03:28.060,0:03:30.220<br>
有些比賽裡面，testing set 已經有給你了<br>
<br>
0:03:30.220,0:03:33.300<br>
你就確實可能可以用它了<br>
<br>
0:03:33.300,0:03:35.800<br>
不過還是跟主辦單位確認一下比較好<br>
<br>
0:03:35.800,0:03:39.040<br>
但是。在很多時候你是<br>
<br>
0:03:39.040,0:03:40.880<br>
你手上沒有那個 testing set 的<br>
<br>
0:03:40.880,0:03:42.520<br>
你要先 learn 好 model 以後<br>
<br>
0:03:42.520,0:03:44.300<br>
尤其是在真正你要用<br>
<br>
0:03:44.300,0:03:46.260<br>
machine learning 的 application 的時候<br>
<br>
0:03:46.260,0:03:48.380<br>
你並沒有 testing set 在你手上阿<br>
<br>
0:03:48.380,0:03:50.660<br>
你要先 learn 好 model 以後，再等 testing set 進來<br>
<br>
0:03:50.660,0:03:53.240<br>
這個時候你就只能做 Inductive learning<br>
<br>
0:03:53.240,0:03:56.740<br>
有人會說 Transductive learning <br>
不算是 Semi-supervised learning<br>
<br>
0:03:56.740,0:03:59.300<br>
不過，我覺得這個也算是一種 Semi-supervised learning<br>
<br>
0:03:59.300,0:04:02.060<br>
只是跟 Inductive learning 很不一樣就是了<br>
<br>
0:04:02.060,0:04:05.560<br>
為什麼做 Semi-supervised learning 呢<br>
<br>
0:04:05.560,0:04:09.300<br>
因為有人常會說我們沒有 data<br>
<br>
0:04:09.300,0:04:11.460<br>
其實，我們不會沒有 data<br>
<br>
0:04:11.460,0:04:14.900<br>
我們從來都不缺 data，我們只是<br>
<br>
0:04:14.900,0:04:17.100<br>
缺有 labeled 的 data<br>
<br>
0:04:17.100,0:04:20.440<br>
比如說，你要收集 image<br>
<br>
0:04:20.440,0:04:21.520<br>
其實是很容易的<br>
<br>
0:04:21.520,0:04:24.660<br>
我就放一個機器人每天在路上走來走去<br>
<br>
0:04:24.660,0:04:27.280<br>
一直拍照，它就收集到一大堆的image<br>
<br>
0:04:27.280,0:04:30.100<br>
只是這些 image 是沒有 label的<br>
<br>
0:04:30.100,0:04:31.420<br>
只有非常少量的 image<br>
<br>
0:04:31.420,0:04:33.160<br>
你才有可能僱人去 label<br>
<br>
0:04:33.160,0:04:36.580<br>
所以，labeled data 很少，unlabeled data 會很多<br>
<br>
0:04:36.580,0:04:39.640<br>
所以，Semi-supervised learning 如果你可以利用這些<br>
<br>
0:04:39.640,0:04:41.620<br>
unlabeled data 來做某些事的話<br>
<br>
0:04:41.620,0:04:43.300<br>
會是很有價值的<br>
<br>
0:04:43.300,0:04:45.320<br>
但事實上，對人類來說<br>
<br>
0:04:45.320,0:04:49.380<br>
我們人類可能也是一直在做 Semi-supervised learning<br>
<br>
0:04:49.380,0:04:53.660<br>
我們會從，比如說，小孩子會從父母那邊<br>
<br>
0:04:53.660,0:04:55.680<br>
得到一點點的 supervised<br>
<br>
0:04:55.680,0:04:56.940<br>
小孩在路上看到一條狗<br>
<br>
0:04:56.940,0:04:58.500<br>
他問他爸那是什麼，然後<br>
<br>
0:04:58.500,0:05:01.400<br>
他爸說是狗，他就認得說這個東西是狗<br>
<br>
0:05:01.400,0:05:04.800<br>
之後，他會再看到其他的東西<br>
<br>
0:05:04.800,0:05:08.720<br>
有狗啊、有貓啊，但是沒有人會告訴他其他的動物是什麼<br>
<br>
0:05:08.720,0:05:12.260<br>
他在他往後的人生裡面，看過很多其他奇奇怪怪的動物<br>
<br>
0:05:12.260,0:05:13.940<br>
那沒有人會去 label 那些動物<br>
<br>
0:05:13.940,0:05:17.520<br>
他必須要自己把它學出來<br>
<br>
0:05:17.520,0:05:19.280<br>
所以，對人類來說<br>
<br>
0:05:19.280,0:05:22.580<br>
我們也是在做 Semi-supervised learning<br>
<br>
0:05:22.580,0:05:26.600<br>
那為甚麼 Semi-supervised learning 有可能<br>
<br>
0:05:26.600,0:05:27.920<br>
會帶來幫助呢<br>
<br>
0:05:27.920,0:05:30.920<br>
假設我們現在要做一個分類的 task<br>
<br>
0:05:30.920,0:05:34.100<br>
我們要建一個貓跟狗的 classifier<br>
<br>
0:05:34.100,0:05:37.080<br>
我們同時，有一大堆有關貓跟狗的圖片<br>
<br>
0:05:37.080,0:05:38.420<br>
那這些圖片呢<br>
<br>
0:05:38.420,0:05:41.500<br>
是沒有 label 的，並不知道哪些是貓，哪些是狗<br>
<br>
0:05:42.000,0:05:46.360<br>
今天，假設我們只考慮這個<br>
<br>
0:05:46.360,0:05:48.740<br>
貓跟狗有 label 的 data 的話<br>
<br>
0:05:49.960,0:05:53.480<br>
假設你今天要畫一個 boundary<br>
<br>
0:05:53.480,0:05:56.780<br>
把貓跟狗的 training data 分開的話<br>
<br>
0:05:56.780,0:05:59.880<br>
你可能會想說，就畫在這邊<br>
<br>
0:05:59.880,0:06:04.060<br>
但是，假如那些 unlabeled data 的分布<br>
<br>
0:06:04.060,0:06:06.620<br>
是像灰色的點，這個樣子的話<br>
<br>
0:06:06.620,0:06:10.200<br>
這可能就會影響你的決定<br>
<br>
0:06:10.200,0:06:13.820<br>
unlabeled data，雖然它只告訴我們 function 的 input<br>
<br>
0:06:13.820,0:06:17.760<br>
但，unlabeled data 它的分布<br>
<br>
0:06:17.760,0:06:21.300<br>
可以告訴我們某一些事<br>
<br>
0:06:21.300,0:06:24.260<br>
比如說，在這個 example 裡面，你可能很直覺的<br>
<br>
0:06:24.260,0:06:27.320<br>
就會覺得說 boundary 應該切成這樣<br>
<br>
0:06:27.320,0:06:30.380<br>
但是，Semi-supervised learning<br>
<br>
0:06:30.380,0:06:34.040<br>
使用 unlabeled 的方式，往往伴隨著一些假設<br>
<br>
0:06:34.040,0:06:37.980<br>
所以，Semi-supervised learning 有沒有用就取決於<br>
<br>
0:06:38.360,0:06:41.560<br>
你這個假設符不符合實際<br>
<br>
0:06:41.560,0:06:43.980<br>
你這個假設精不精確，因為<br>
<br>
0:06:43.980,0:06:46.960<br>
你可能覺得說這個是貓吧<br>
<br>
0:06:46.960,0:06:49.040<br>
誰知道呢，搞不好這個是狗<br>
<br>
0:06:49.040,0:06:51.740<br>
它們看起來很像，是因為背景都是綠的<br>
<br>
0:06:51.740,0:06:54.740<br>
所以 Semi-supervised learning 有沒有用<br>
<br>
0:06:54.740,0:06:55.940<br>
不見得永遠都是有用<br>
<br>
0:06:55.940,0:06:59.400<br>
那 depend on 你現在的假設是不是合理的<br>
<br>
0:06:59.400,0:07:03.140<br>
我們這邊要講 4 件事，第一個我們會講說<br>
<br>
0:07:03.140,0:07:05.180<br>
在 Generative model 的時候<br>
<br>
0:07:05.180,0:07:07.260<br>
我們要怎麼用 Semi-supervised learning<br>
<br>
0:07:07.260,0:07:11.320<br>
然後，我們會講兩個還蠻通用的假設<br>
<br>
0:07:11.320,0:07:14.160<br>
一個是 Low-density 的 Separation Assumption<br>
<br>
0:07:14.160,0:07:16.120<br>
一個是 Smoothness Assumption<br>
<br>
0:07:16.120,0:07:19.580<br>
最後，我們會說 Semi-supervised learning 還有一招就是<br>
<br>
0:07:19.580,0:07:21.860<br>
找一個比較好的 Representation<br>
<br>
0:07:21.860,0:07:25.840<br>
這個我們會等到講 Supervised learning 的時候再講<br>
<br>
0:07:25.840,0:07:29.440<br>
我們來講一下在 Generative model 裡面<br>
<br>
0:07:29.440,0:07:31.740<br>
你怎麼做 Semi-supervised learning<br>
<br>
0:07:31.740,0:07:36.020<br>
我們都已經看過 <br>
Supervised learning 的 Generative model<br>
<br>
0:07:36.020,0:07:37.880<br>
在 Supervised learning 裡面呢<br>
<br>
0:07:37.880,0:07:40.320<br>
你有一堆 training 的 example<br>
<br>
0:07:40.320,0:07:42.960<br>
你知道它們分別屬於 class 1<br>
<br>
0:07:42.960,0:07:44.720<br>
還是屬於 class 2<br>
<br>
0:07:44.720,0:07:50.080<br>
那你會去估測 class 1 和 class 2 的 prior probability<br>
<br>
0:07:50.080,0:07:52.420<br>
你會去估測 P(C1)、P(C2)<br>
<br>
0:07:52.420,0:07:57.440<br>
然後，你會去估測 P(x|C1)、P(x|C2)<br>
<br>
0:07:57.440,0:07:59.760<br>
比如說，我假設你假設每一個 class<br>
<br>
0:07:59.760,0:08:02.740<br>
它的分佈都是一個 Gaussian distribution 的話<br>
<br>
0:08:02.740,0:08:05.600<br>
那你會估測說，這個 class 1<br>
<br>
0:08:05.600,0:08:08.260<br>
是從這個 mean 是 μ^1<br>
<br>
0:08:08.260,0:08:11.420<br>
covariance 是 Σ 的 Gaussian 估測出來的<br>
<br>
0:08:11.420,0:08:13.780<br>
那 class 2 是從 mean 是 μ^2<br>
<br>
0:08:13.780,0:08:20.800<br>
covariance matrix 也是 Σ 的 Gaussian 所估測出來的<br>
<br>
0:08:20.800,0:08:24.120<br>
之前講過說，如果你 share Gaussian<br>
<br>
0:08:24.120,0:08:26.600<br>
你的 performance 可能會是比較好的<br>
<br>
0:08:27.140,0:08:30.000<br>
那現在有了這些 prior probability<br>
<br>
0:08:30.000,0:08:33.160<br>
有了這些 mean，有了這些 covariance matrix<br>
<br>
0:08:33.160,0:08:36.980<br>
你就可以估測 given 一個新的 data<br>
<br>
0:08:36.980,0:08:39.820<br>
它是屬於 C1 的 posterior probability<br>
<br>
0:08:39.820,0:08:42.140<br>
然後，你就可以<br>
<br>
0:08:42.140,0:08:46.260<br>
看一筆 data 就做一些 classification<br>
<br>
0:08:46.260,0:08:50.000<br>
那你會決定一個 boundary 的位置在哪裡<br>
<br>
0:08:50.000,0:08:53.140<br>
但是，如果今天給了我們一些 unlabeled data<br>
<br>
0:08:53.140,0:08:55.140<br>
它就會影響你的決定<br>
<br>
0:08:55.140,0:08:58.320<br>
舉例來說，如果我們看這一筆 data<br>
<br>
0:08:58.320,0:09:01.680<br>
假設這些綠色的其實是 unlabeled data 的話<br>
<br>
0:09:01.680,0:09:03.700<br>
那如果你的 mean 跟 variance<br>
<br>
0:09:03.700,0:09:06.560<br>
是 μ^1, μ^2 跟 Σ，顯然就是不合理<br>
<br>
0:09:07.400,0:09:10.180<br>
今天這個 Σ，顯然可能<br>
<br>
0:09:10.180,0:09:11.620<br>
應該要比較接近圓圈<br>
<br>
0:09:11.620,0:09:13.500<br>
或許你在 sample 的時候有一些問題<br>
<br>
0:09:13.500,0:09:16.320<br>
所以，你 sample 到比較奇怪的 distribution<br>
<br>
0:09:16.320,0:09:19.220<br>
或許它應該比較接近圓形<br>
<br>
0:09:19.220,0:09:23.740<br>
而這個 class 2 的 μ 呢，或許不應該在這邊<br>
<br>
0:09:23.740,0:09:27.800<br>
它或許應該在其他的地方，或許應該在更下面，等等<br>
<br>
0:09:28.140,0:09:31.540<br>
如果你看這個 prior 的話<br>
<br>
0:09:31.540,0:09:34.900<br>
那 prior 可能也會受到影響，比如說，我們本來覺得說<br>
<br>
0:09:34.900,0:09:37.960<br>
positive，這兩個的 labeled data 是一樣多的<br>
<br>
0:09:37.960,0:09:39.660<br>
但是，看了這些 unlabeled data 以後<br>
<br>
0:09:39.660,0:09:43.700<br>
你或許會覺得 class 2 的 data 其實是比較多的<br>
<br>
0:09:43.700,0:09:46.100<br>
它的 prior probability 應該是比較大的<br>
<br>
0:09:46.100,0:09:48.740<br>
總之，看了這些 unlabeled data 以後<br>
<br>
0:09:48.740,0:09:51.200<br>
會影響你對 prior probability<br>
<br>
0:09:51.200,0:09:53.860<br>
對 mean，還有對 covariance 的估測<br>
<br>
0:09:53.860,0:09:54.940<br>
影響了這些估測<br>
<br>
0:09:54.940,0:09:57.100<br>
就影響了你 posterior probability 的式子<br>
<br>
0:09:57.100,0:10:00.280<br>
然後，就影響了你的 decision boundary<br>
<br>
0:10:00.900,0:10:04.480<br>
這個是，在直覺上是這麼做的<br>
<br>
0:10:04.480,0:10:07.040<br>
但是，實際上在 formulation 上怎麼做呢<br>
<br>
0:10:07.040,0:10:10.740<br>
我們先講操作的方式<br>
<br>
0:10:10.740,0:10:14.020<br>
然後再稍微講它的原理<br>
<br>
0:10:14.020,0:10:16.800<br>
這邊會講稍微比較快帶過去，因為<br>
<br>
0:10:16.800,0:10:19.700<br>
我猜在這個作業，你大概用不上<br>
<br>
0:10:19.700,0:10:22.200<br>
因為你也不是用 Generative model 做<br>
<br>
0:10:22.740,0:10:25.820<br>
那 step 1，step 1 是怎麼樣呢<br>
<br>
0:10:25.820,0:10:30.140<br>
我們先計算每一筆 unlabeled data 的<br>
<br>
0:10:30.140,0:10:32.380<br>
posterior probability<br>
<br>
0:10:32.820,0:10:36.820<br>
對每一筆 unlabeled data,  x^u<br>
<br>
0:10:36.820,0:10:42.780<br>
我們都去計算，我們要先初始化一組參數<br>
<br>
0:10:42.780,0:10:46.240<br>
先初始化兩個，假設我們做 binary classification 的話<br>
<br>
0:10:46.240,0:10:50.060<br>
先初始化 class 1 和 class 2 的 prior 的機率<br>
<br>
0:10:50.060,0:10:53.160<br>
先初始化 μ^1、μ^2 跟  Σ<br>
<br>
0:10:53.160,0:10:54.700<br>
那你說初始化這個值怎麼來<br>
<br>
0:10:54.700,0:10:58.000<br>
你可以 random 來，你可以用已經有 labeled 的 data<br>
<br>
0:10:58.000,0:10:59.620<br>
先估測一個值<br>
<br>
0:10:59.620,0:11:02.140<br>
總之，你就得到一組初始化的參數<br>
<br>
0:11:02.140,0:11:06.080<br>
我們把這些 prior probability、class dependent 的<br>
<br>
0:11:06.080,0:11:10.020<br>
μ^1、μ^2、Σ 統稱為參數 θ<br>
<br>
0:11:10.020,0:11:12.980<br>
那根據我們現在有的 θ<br>
<br>
0:11:12.980,0:11:17.920<br>
你可以估算每一筆 unlabeled data 屬於 class 1 的機率<br>
<br>
0:11:17.920,0:11:22.100<br>
當然這個機率算出來怎樣，是跟你的 model 的值有關的<br>
<br>
0:11:22.100,0:11:25.120<br>
算出這個機率以後，你就可以去<br>
<br>
0:11:25.120,0:11:26.700<br>
update 你的 model<br>
<br>
0:11:26.700,0:11:29.540<br>
這個是 update 的式子非常的直覺<br>
<br>
0:11:29.540,0:11:32.720<br>
怎麼個直覺法呢<br>
<br>
0:11:32.720,0:11:37.620<br>
現在 C1 的 prior probability 怎麼算呢？<br>
<br>
0:11:37.620,0:11:39.960<br>
原來如果沒有 unlabeled data 的時候<br>
<br>
0:11:39.960,0:11:41.580<br>
你的計算方法可能是<br>
<br>
0:11:41.580,0:11:44.560<br>
這個 N 是所有的 example<br>
<br>
0:11:45.040,0:11:49.080<br>
N1 是被標註為 C1 的 example<br>
<br>
0:11:49.080,0:11:53.140<br>
如果你要算 C1 的 prior probability，這些事情太直覺了<br>
<br>
0:11:53.140,0:11:57.460<br>
如果不考慮 unlabeled data 的話，感覺就是 N1/N<br>
<br>
0:11:57.460,0:12:01.080<br>
但是，我們現在需要考慮 unlabeled data<br>
<br>
0:12:01.080,0:12:03.320<br>
我們需要考慮 unlabeled data<br>
<br>
0:12:03.320,0:12:06.760<br>
根據 unlabeled data 告訴我們的資訊<br>
<br>
0:12:06.760,0:12:08.660<br>
C1 出現的次數是多少呢<br>
<br>
0:12:08.660,0:12:11.140<br>
C1 出現的次數就是<br>
<br>
0:12:11.140,0:12:13.520<br>
所有 unlabeled data<br>
<br>
0:12:13.520,0:12:17.160<br>
它是 C1 的 posterior probability 的和<br>
<br>
0:12:17.160,0:12:19.100<br>
所以，unlabeled data 並不是<br>
<br>
0:12:19.100,0:12:22.120<br>
hard design，它一定要屬於 C1 或 C2<br>
<br>
0:12:22.120,0:12:24.780<br>
根據它的 posterior probability 決定<br>
<br>
0:12:24.780,0:12:30.040<br>
它有百分之多少是屬於 C1，它有百分之多少是屬於 C2<br>
<br>
0:12:31.700,0:12:34.940<br>
那你就得到 C1 的 prior probability<br>
<br>
0:12:34.940,0:12:38.800<br>
根據 unsupervised data 影響你對 C1 的估測<br>
<br>
0:12:38.800,0:12:40.260<br>
那 μ^1 怎麼算呢<br>
<br>
0:12:40.260,0:12:43.480<br>
如果不考慮 unlabeled data 的時候，所謂的 μ^1<br>
<br>
0:12:43.480,0:12:46.600<br>
就是把所有屬於 C1 的 labeled data<br>
<br>
0:12:46.600,0:12:49.940<br>
都平均起來，就結束了，這個很直覺<br>
<br>
0:12:49.940,0:12:54.060<br>
如果今天要加上 unlabeled data 怎麼做呢<br>
<br>
0:12:54.060,0:12:56.180<br>
其實就只是把<br>
<br>
0:12:56.180,0:12:59.020<br>
unlabeled data 的那每一筆 data, x^u<br>
<br>
0:12:59.020,0:13:01.100<br>
根據它的 posterior probability<br>
<br>
0:13:01.100,0:13:02.880<br>
做 weighted sum<br>
<br>
0:13:02.880,0:13:06.720<br>
如果這個 x^u，它比較偏向 class1、C1 的話<br>
<br>
0:13:06.720,0:13:10.320<br>
它對 class 1 的影響就大一點，反之，就小一點<br>
<br>
0:13:10.320,0:13:13.220<br>
你就把所有 unlabeled data 根據它是<br>
<br>
0:13:13.220,0:13:16.200<br>
這個 C1 的 posterior probability 做 weighted sum<br>
<br>
0:13:16.200,0:13:20.540<br>
然後，再除掉所有 weight 的 和<br>
<br>
0:13:20.540,0:13:24.040<br>
做一個 normalization，就結束了<br>
<br>
0:13:24.040,0:13:27.340<br>
這件事情你幾乎不用解釋 ，因為太直覺了<br>
<br>
0:13:27.340,0:13:29.100<br>
直覺就是這麼做的<br>
<br>
0:13:29.100,0:13:31.960<br>
跟這個 C2 的 prior probability 阿<br>
<br>
0:13:31.960,0:13:33.560<br>
μ^1、μ^2、Σ<br>
<br>
0:13:33.560,0:13:35.180<br>
也都用同樣的方式算出來<br>
<br>
0:13:35.180,0:13:37.800<br>
接下來，你有了新的 model<br>
<br>
0:13:37.800,0:13:39.880<br>
你就會 Back to step 1<br>
<br>
0:13:39.880,0:13:41.420<br>
有了新的 model 以後<br>
<br>
0:13:41.420,0:13:43.160<br>
你的這個機率就不一樣<br>
<br>
0:13:43.160,0:13:45.600<br>
你這個機率就不一樣，在 step 2<br>
<br>
0:13:45.600,0:13:47.180<br>
你的 model 算出來就不一樣<br>
<br>
0:13:47.180,0:13:49.680<br>
接下來，你又可以去 update 你的機率<br>
<br>
0:13:49.680,0:13:52.960<br>
所以，就反覆地繼續下去<br>
<br>
0:13:53.540,0:13:56.360<br>
在理論上這個方法會收斂<br>
<br>
0:13:56.360,0:13:57.760<br>
可以保證它會收斂<br>
<br>
0:13:57.760,0:14:01.640<br>
但是，它的初始值<br>
<br>
0:14:01.640,0:14:02.980<br>
它就跟 Gradient Descent 一樣<br>
<br>
0:14:02.980,0:14:05.980<br>
初始值會影響你最後收斂的結果<br>
<br>
0:14:05.980,0:14:07.940<br>
事實上，這個 step 1<br>
<br>
0:14:07.940,0:14:10.440<br>
如果你聽過 EM algorithm 的話<br>
<br>
0:14:10.440,0:14:13.300<br>
這個 step 1 就是 E step<br>
<br>
0:14:13.300,0:14:16.960<br>
這個 step 2 就是 M step<br>
<br>
0:14:16.960,0:14:21.280<br>
我們來解釋一下<br>
<br>
0:14:21.280,0:14:26.040<br>
為什麼這個 algorithm 是這樣子做的<br>
<br>
0:14:26.040,0:14:28.380<br>
雖然這件事情實在是很直覺<br>
<br>
0:14:28.380,0:14:31.760<br>
但是它背後的理論，它為什麼要這樣做呢<br>
<br>
0:14:31.760,0:14:33.520<br>
這個想法是這樣子的<br>
<br>
0:14:34.400,0:14:37.200<br>
原來假設我們只有 labeled data 的時候<br>
<br>
0:14:37.200,0:14:38.840<br>
我們只有 labeled data 的時候<br>
<br>
0:14:38.840,0:14:41.500<br>
我們要做的事情<br>
<br>
0:14:41.500,0:14:44.520<br>
是要去 maximize 一個 likelihood 對不對<br>
<br>
0:14:44.520,0:14:47.000<br>
或者是 maximize Log 的 likelihood<br>
<br>
0:14:47.000,0:14:49.980<br>
這個意思是一樣的<br>
<br>
0:14:49.980,0:14:53.600<br>
那每一筆 training data<br>
<br>
0:14:53.600,0:14:56.960<br>
它的 likelihood，我們是可以算出來的<br>
<br>
0:14:56.960,0:14:58.800<br>
如果你給一個 θ<br>
<br>
0:14:58.800,0:15:02.200<br>
每一筆 training data、每一筆 labeled data 的 likelihood<br>
<br>
0:15:02.200,0:15:03.360<br>
我們是可以算出來的<br>
<br>
0:15:03.360,0:15:05.040<br>
每一筆 data 的 likelihood<br>
<br>
0:15:05.040,0:15:07.460<br>
就是 P(y^r\head)<br>
<br>
0:15:07.460,0:15:09.820<br>
那個 label、那個 class 出現的 prior<br>
<br>
0:15:09.820,0:15:12.960<br>
跟根據那個 class，generate 那筆 data 的機率<br>
<br>
0:15:12.960,0:15:15.980<br>
所以，給一個 θ，你可以把那個 likelihood 算出來<br>
<br>
0:15:15.980,0:15:17.860<br>
把所有的 labeled data<br>
<br>
0:15:17.860,0:15:19.860<br>
的這個 log likelihood 加起來<br>
<br>
0:15:19.860,0:15:22.780<br>
就是你的 total log likelihood<br>
<br>
0:15:22.780,0:15:25.060<br>
然後，你要去找一個 θ 去 maximize 它<br>
<br>
0:15:25.060,0:15:27.200<br>
那個 solution，是很直覺的<br>
<br>
0:15:27.200,0:15:28.740<br>
它有 Closed-form solution<br>
<br>
0:15:28.740,0:15:31.340<br>
代個式子，你就可以把它解出來<br>
<br>
0:15:31.780,0:15:34.060<br>
現在如果有 unlabeled data 的時候<br>
<br>
0:15:34.060,0:15:35.840<br>
式子有什麼不一樣呢<br>
<br>
0:15:38.800,0:15:41.340<br>
我有一個地方寫錯了，就是這邊<br>
<br>
0:15:41.340,0:15:44.720<br>
應該要有 y^r\head，就是這一項<br>
<br>
0:15:44.720,0:15:48.080<br>
是要考慮 labeled data，所以這一項<br>
<br>
0:15:48.080,0:15:51.420<br>
跟前面這個部分是一樣的<br>
<br>
0:15:51.420,0:15:54.220<br>
但是，unlabeled data 怎麼辦呢<br>
<br>
0:15:54.220,0:15:56.360<br>
unlabeled data 我們並不知道<br>
<br>
0:15:56.360,0:15:59.220<br>
它是來自於哪一個 class 阿<br>
<br>
0:15:59.220,0:16:02.760<br>
我們怎麼估測它的機率呢<br>
<br>
0:16:02.760,0:16:06.320<br>
那我們說一筆 unlabeled data, x^u<br>
<br>
0:16:06.320,0:16:08.340<br>
它出現的機率<br>
<br>
0:16:08.340,0:16:11.340<br>
因為我不知道它是從 C1 還是從 C2 來的<br>
<br>
0:16:11.340,0:16:14.460<br>
所以，它就是 C1、C2 都有可能<br>
<br>
0:16:14.460,0:16:17.460<br>
所以，一筆 unlabeled data 出現的機率<br>
<br>
0:16:17.460,0:16:20.540<br>
就是它在 C1 的 prior probability<br>
<br>
0:16:20.540,0:16:24.620<br>
跟 C1 這個 class 產生這筆 unlabeled data 的機率<br>
<br>
0:16:24.620,0:16:27.300<br>
加上 C2 的 prior probability<br>
<br>
0:16:27.300,0:16:33.800<br>
乘上C2 這個 class 產生這筆 unlabeled data 的機率<br>
<br>
0:16:33.800,0:16:35.340<br>
把他們通通合起來<br>
<br>
0:16:35.340,0:16:38.560<br>
就是這筆 unlabeled data 出現的機率<br>
<br>
0:16:38.560,0:16:40.900<br>
你問 x^u，它可以從 C1 來，它可以從 C2 來<br>
<br>
0:16:40.900,0:16:42.240<br>
我不知道它從哪裡來<br>
<br>
0:16:42.240,0:16:44.640<br>
所以，你就說它兩個都有可能<br>
<br>
0:16:44.640,0:16:46.240<br>
接下來，你要做的事情<br>
<br>
0:16:46.240,0:16:49.120<br>
就是要去 maximize 這個式子<br>
<br>
0:16:49.120,0:16:52.140<br>
不幸的是，這個式子它不是 convex<br>
<br>
0:16:52.140,0:16:54.060<br>
所以，你解它的時候呢<br>
<br>
0:16:54.060,0:16:56.940<br>
你變成要用 EM algorithm 解<br>
<br>
0:16:56.940,0:17:01.020<br>
其實，你就是要用，要 iterative 的去 solve 它<br>
<br>
0:17:01.020,0:17:04.020<br>
所以，我們剛才做的那個步驟<br>
<br>
0:17:04.020,0:17:06.540<br>
我在前一頁投影片裡面的那個 algorithm<br>
<br>
0:17:06.540,0:17:09.380<br>
它做的事情就是，在每一次循環的時候<br>
<br>
0:17:09.380,0:17:11.060<br>
你做完 step1，你做完 step 2<br>
<br>
0:17:11.060,0:17:15.180<br>
你就可以讓這個 Log likelihood<br>
<br>
0:17:15.180,0:17:17.140<br>
增加一點，然後跑到最後呢<br>
<br>
0:17:17.140,0:17:19.820<br>
它會收斂在一個 local minimum 的地方<br>
<br>
0:17:22.360,0:17:26.060<br>
那這個是 Generative 的 model<br>
<br>
0:17:26.060,0:17:28.720<br>
那我們等一下會講<br>
<br>
0:17:28.720,0:17:31.620<br>
我們接下來要講一個，比較 general 的方式<br>
<br>
0:17:31.620,0:17:34.180<br>
這邊基於的假設<br>
<br>
0:17:34.180,0:17:36.220<br>
是 Low-density 的 Separation<br>
<br>
0:17:36.220,0:17:40.560<br>
也就是說，這個世界是非黑即白的<br>
<br>
0:17:40.560,0:17:42.680<br>
什麼是非黑即白呢<br>
<br>
0:17:42.680,0:17:44.500<br>
非黑即白，意思就是說<br>
<br>
0:17:44.500,0:17:47.480<br>
假設我們現在，有一大堆的 data<br>
<br>
0:17:47.480,0:17:49.420<br>
有 labeled data、有 unlabeled data<br>
<br>
0:17:49.420,0:17:54.020<br>
在兩個 class 之間呢<br>
<br>
0:17:54.020,0:17:57.900<br>
它們會有一個非常明顯的鴻溝<br>
<br>
0:17:57.900,0:18:01.800<br>
就是說，如果現在給你這些 labeled data<br>
<br>
0:18:01.800,0:18:04.680<br>
給你這些 labeled data<br>
<br>
0:18:04.680,0:18:08.220<br>
你可以說，我的 boundary 要切在這邊也可以<br>
<br>
0:18:08.220,0:18:10.720<br>
我的 boundary 要切在這邊也可以<br>
<br>
0:18:10.720,0:18:13.980<br>
你就可以把這兩個 class 分開<br>
<br>
0:18:13.980,0:18:17.660<br>
它們在 training data 上的正確率都是100%<br>
<br>
0:18:17.660,0:18:20.100<br>
但是，如果你考慮 unlabeled data 的話<br>
<br>
0:18:20.100,0:18:22.940<br>
或許這一個 boundary 是比較好的<br>
<br>
0:18:22.940,0:18:24.620<br>
這個 boundary 是比較不好的<br>
<br>
0:18:24.620,0:18:26.800<br>
為什麼呢？因為今天基於的假設就是<br>
<br>
0:18:26.800,0:18:28.920<br>
這是一個非黑即白的世界<br>
<br>
0:18:28.920,0:18:30.680<br>
在這兩個 class 之間呢<br>
<br>
0:18:30.680,0:18:35.120<br>
會有一個很明顯的楚河漢界，會有一個鴻溝<br>
<br>
0:18:35.120,0:18:36.500<br>
會有一個地方<br>
<br>
0:18:36.500,0:18:38.560<br>
它之所以叫 Low-density Separation<br>
<br>
0:18:38.560,0:18:41.380<br>
意思就是說，在這兩個 class 的交界處<br>
<br>
0:18:41.380,0:18:42.900<br>
它的 density 是低的<br>
<br>
0:18:42.900,0:18:44.040<br>
這兩個 class 的交界處<br>
<br>
0:18:44.040,0:18:47.400<br>
data 量是很少，不會出現 data 的<br>
<br>
0:18:47.400,0:18:50.280<br>
所以，這個 boundary 可能就是比較合理的<br>
<br>
0:18:50.880,0:18:54.240<br>
那 Low-density Separation 最具代表性、最簡單的方法<br>
<br>
0:18:54.240,0:18:56.780<br>
就是 Self-training，但是 Self-training 太直覺了<br>
<br>
0:18:56.780,0:18:58.940<br>
我覺得這個沒什麼好講的<br>
<br>
0:18:58.940,0:19:01.720<br>
我相信大家都是秒 implement 這樣<br>
<br>
0:19:01.720,0:19:04.960<br>
我們就很快地講過去，Self-training 就是說<br>
<br>
0:19:04.960,0:19:07.600<br>
我們有一些 labeled data<br>
<br>
0:19:07.600,0:19:10.380<br>
有一些 unlabeled data<br>
<br>
0:19:10.380,0:19:14.860<br>
接下來，先從 labeled data 去<br>
<br>
0:19:14.860,0:19:20.200<br>
train 一個 model，這個 model 叫做 f*<br>
<br>
0:19:20.200,0:19:23.600<br>
那這邊其實，你的這個 training 的方法<br>
<br>
0:19:23.600,0:19:25.860<br>
Self-training 其實是一個很 general 的方法<br>
<br>
0:19:25.860,0:19:27.740<br>
你用什麼方法得到你的 f*<br>
<br>
0:19:27.780,0:19:29.280<br>
你用 neural network 是<br>
<br>
0:19:29.280,0:19:31.380<br>
用 Deep 的方法、是用 Shallow 的方法<br>
<br>
0:19:31.380,0:19:33.120<br>
還是用其他 machine learning 的方法<br>
<br>
0:19:33.120,0:19:36.300<br>
都可以，反正你就是 train 出一個 model, f*<br>
<br>
0:19:36.300,0:19:41.880<br>
根據這個 f*，你去 label 你的 unlabeled data<br>
<br>
0:19:41.880,0:19:44.380<br>
你就把 x^u 丟進 f*<br>
<br>
0:19:44.380,0:19:47.860<br>
看它吐出來的 y^u 是什麼<br>
<br>
0:19:47.860,0:19:49.540<br>
那就是你的 labeled data<br>
<br>
0:19:49.540,0:19:53.660<br>
這個東西，叫做 Pseudo-label<br>
<br>
0:19:54.320,0:19:55.940<br>
接下來呢<br>
<br>
0:19:55.940,0:20:00.940<br>
你要從你的 unlabeled data set 裡面拿出一些 data<br>
<br>
0:20:00.940,0:20:02.920<br>
把它加到 labeled data set 裡面<br>
<br>
0:20:02.920,0:20:04.880<br>
至於哪些 data 會被加進去<br>
<br>
0:20:04.880,0:20:06.700<br>
這就是 open question，你要自己<br>
<br>
0:20:06.700,0:20:11.540<br>
design 一些 heuristic 的 rule，自己想個辦法來解決<br>
<br>
0:20:11.540,0:20:14.620<br>
你甚至可以給每一筆 unlabeled data provide weight<br>
<br>
0:20:14.620,0:20:15.740<br>
那有一些比較 confidence<br>
<br>
0:20:15.740,0:20:17.260<br>
有一些 Pseudo-label 比較 confident<br>
<br>
0:20:17.260,0:20:19.580<br>
有一些 Pseudo-label 比較不 confident<br>
<br>
0:20:19.580,0:20:21.960<br>
那有了更多的 labeled data 以後<br>
<br>
0:20:21.960,0:20:23.600<br>
現在 labeled data 從 unlabeled data 那邊<br>
<br>
0:20:23.600,0:20:24.780<br>
得到額外的 data<br>
<br>
0:20:24.780,0:20:28.080<br>
你就可以回頭再去 train 你的 model, f*<br>
<br>
0:20:28.080,0:20:30.420<br>
這件事情，非常的直覺<br>
<br>
0:20:30.900,0:20:32.200<br>
那 Self-training 這麼簡單<br>
<br>
0:20:32.200,0:20:34.380<br>
你可能覺得自己非常的懂<br>
<br>
0:20:34.380,0:20:36.180<br>
那我來問大家一個問題<br>
<br>
0:20:36.640,0:20:38.140<br>
以下這個 process<br>
<br>
0:20:38.140,0:20:41.580<br>
如果我們用在 Regression 上面<br>
<br>
0:20:41.660,0:20:43.260<br>
會怎樣呢？<br>
<br>
0:20:44.120,0:20:48.080<br>
當然你永遠可以把 Regression 用在這邊，沒有什麼問題<br>
<br>
0:20:48.080,0:20:49.500<br>
程式也不會 segmentation fault<br>
<br>
0:20:49.500,0:20:51.420<br>
那問題就是<br>
<br>
0:20:51.420,0:20:54.720<br>
這一招在 Regression 上面<br>
<br>
0:20:54.720,0:20:57.140<br>
你覺得有可能會有用嗎<br>
<br>
0:20:59.360,0:21:01.740<br>
我們給大家5秒鐘想一下<br>
<br>
0:21:01.740,0:21:03.500<br>
你覺得這一招在 Regression 上<br>
<br>
0:21:03.500,0:21:05.760<br>
有可能會有用的，舉手一下<br>
<br>
0:21:06.700,0:21:08.880<br>
你覺得這一招在 Regression 上<br>
<br>
0:21:08.880,0:21:10.620<br>
一定沒有用的，舉手一下<br>
<br>
0:21:11.220,0:21:14.600<br>
那都沒有人舉手<br>
<br>
0:21:15.680,0:21:19.080<br>
你仔細想想看<br>
<br>
0:21:19.080,0:21:21.540<br>
你覺得這一招在 Regression 上會有用嗎？<br>
<br>
0:21:22.440,0:21:28.680<br>
Regression 大家知道，就是 output 一個數字<br>
<br>
0:21:28.680,0:21:32.520<br>
就是 output 一個 real number，那你有一個<br>
<br>
0:21:32.520,0:21:36.040<br>
x^u，然後，你 output 一個 real number<br>
<br>
0:21:36.040,0:21:38.560<br>
你把這筆 data 加到你的 data 裡面再 train<br>
<br>
0:21:38.560,0:21:40.840<br>
你會影響 f* 嗎？<br>
<br>
0:21:43.940,0:21:46.340<br>
其實不會影響 f*，對不對<br>
<br>
0:21:46.340,0:21:50.680<br>
所以，Regression 其實不能用這一招的<br>
<br>
0:21:54.000,0:21:56.160<br>
這樣大家有問題嗎？<br>
<br>
0:22:01.380,0:22:03.620<br>
那其實是這樣子的<br>
<br>
0:22:03.620,0:22:07.700<br>
你可能會覺得剛才這個 Self-training<br>
<br>
0:22:07.700,0:22:11.800<br>
它很像是，我們剛才在 Generative model 裡面<br>
<br>
0:22:11.800,0:22:13.040<br>
用的那個方法<br>
<br>
0:22:13.040,0:22:14.680<br>
它們唯一的差別是在<br>
<br>
0:22:14.680,0:22:16.300<br>
做 Self-training 的時候<br>
<br>
0:22:16.300,0:22:18.680<br>
你用的是Hard label<br>
<br>
0:22:18.680,0:22:21.880<br>
在做 Generative model 的時候，你用的是Soft label<br>
<br>
0:22:21.880,0:22:24.900<br>
在做 Self-training 的時候，我們會強制 assign<br>
<br>
0:22:24.900,0:22:27.520<br>
一筆 training data，它一定是屬於某一個 class<br>
<br>
0:22:27.520,0:22:29.840<br>
但是，在 Generative model 的時候<br>
<br>
0:22:29.840,0:22:33.560<br>
我們是說，根據它的 posterior probability<br>
<br>
0:22:33.560,0:22:36.080<br>
你可能有部分屬於 class 1，有部分屬於 class 2<br>
<br>
0:22:36.080,0:22:38.360<br>
所以，Self-training 是 Hard label<br>
<br>
0:22:38.360,0:22:41.980<br>
Generative model 的時候，我們用的是Soft label<br>
<br>
0:22:41.980,0:22:45.580<br>
那到底哪一個比較好呢<br>
<br>
0:22:45.580,0:22:48.220<br>
如果我們今天考慮的是<br>
<br>
0:22:48.220,0:22:49.900<br>
neural network 的話<br>
<br>
0:22:49.900,0:22:52.560<br>
你可以比較看看，到底哪一個方法比較好<br>
<br>
0:22:52.560,0:22:56.380<br>
假設我們用的是 neural network<br>
<br>
0:22:56.380,0:23:01.320<br>
那你從你的 labeled data，得到一組 network 的參數、θ*<br>
<br>
0:23:01.320,0:23:03.980<br>
那現在有一筆 unlabeled data，x^u<br>
<br>
0:23:03.980,0:23:06.840<br>
然後呢，你說<br>
<br>
0:23:06.840,0:23:09.980<br>
根據我們現在手上的參數，θ*<br>
<br>
0:23:09.980,0:23:11.240<br>
我把它分成兩類<br>
<br>
0:23:11.240,0:23:13.480<br>
它有 0.7 的機率屬於 class a<br>
<br>
0:23:13.480,0:23:16.260<br>
有 0.3 的機率屬於 class b<br>
<br>
0:23:16.580,0:23:17.900<br>
屬於 class 2<br>
<br>
0:23:17.900,0:23:22.060<br>
如果是 Hard label 的話，你就把它直接 label 成 class 1<br>
<br>
0:23:22.060,0:23:24.920<br>
然後你就說，因為它變成 class 1 了<br>
<br>
0:23:24.920,0:23:27.200<br>
所以，x^u 的新的 target<br>
<br>
0:23:27.200,0:23:28.940<br>
就是你拿 x^u 在 train neural network 的時候<br>
<br>
0:23:28.940,0:23:30.940<br>
它的 target 就是第一維是 1<br>
<br>
0:23:30.940,0:23:33.180<br>
第二維是 0<br>
<br>
0:23:33.180,0:23:35.120<br>
或是，你就把這個東西<br>
<br>
0:23:35.120,0:23:38.180<br>
跟你 neural network 的 output 去算 cross entropy<br>
<br>
0:23:38.180,0:23:41.160<br>
如果是做 soft 的話<br>
<br>
0:23:41.160,0:23:44.360<br>
那你就是說 70% 屬於 class 1<br>
<br>
0:23:44.360,0:23:47.100<br>
30% 屬於 class 2<br>
<br>
0:23:47.100,0:23:49.860<br>
然後你就說，新的 target 就是<br>
<br>
0:23:49.860,0:23:53.100<br>
0.7 跟 0.3<br>
<br>
0:23:55.600,0:24:00.640<br>
你覺得，如果我們今天用的是 neural network 的話<br>
<br>
0:24:00.640,0:24:04.580<br>
上面跟下面哪一個方法，有可能是有用的呢<br>
<br>
0:24:04.580,0:24:07.160<br>
你覺得下面這個方法<br>
<br>
0:24:07.160,0:24:09.760<br>
有可能有用的同學舉手一下<br>
<br>
0:24:10.720,0:24:12.060<br>
手放下<br>
<br>
0:24:12.060,0:24:15.840<br>
如果你覺得，下面這個方法完全不可能有用的舉手<br>
<br>
0:24:17.200,0:24:18.860<br>
手放下<br>
<br>
0:24:18.860,0:24:21.780<br>
比較多人覺得它完全不會有用<br>
<br>
0:24:21.780,0:24:23.580<br>
為什麼它完全不會有用呢<br>
<br>
0:24:23.580,0:24:25.060<br>
你仔細想想看<br>
<br>
0:24:25.060,0:24:27.400<br>
你現在 model 的 output 在這些 unlabeled data 上<br>
<br>
0:24:27.400,0:24:29.660<br>
用這個值，參數 output 是 0.7, 0.3<br>
<br>
0:24:29.660,0:24:31.960<br>
你說你把它的 target 又設成 0.7, 0.3<br>
<br>
0:24:31.960,0:24:34.400<br>
那不就是同一組參數可以做到一樣的事情嗎<br>
<br>
0:24:35.380,0:24:38.220<br>
所以如果你是做 neural network 的時候<br>
<br>
0:24:38.220,0:24:39.720<br>
你用一個 Soft label<br>
<br>
0:24:39.720,0:24:41.980<br>
結果是沒有用的<br>
<br>
0:24:41.980,0:24:46.080<br>
所以，這邊你一定要用 Hard label<br>
<br>
0:24:46.080,0:24:49.060<br>
那我們用 Hard label 是什麼意思呢<br>
<br>
0:24:49.060,0:24:50.860<br>
我們用  Hard label 的時候<br>
<br>
0:24:50.860,0:24:53.440<br>
我們用的就是 Low-density Separation 的概念<br>
<br>
0:24:53.440,0:24:56.420<br>
也就是說，今天我們看 x^u<br>
<br>
0:24:56.420,0:24:59.640<br>
它屬於 class 1 的機率只是比較高而已<br>
<br>
0:24:59.640,0:25:02.960<br>
我們沒有很確定，它一定是屬於 class 1<br>
<br>
0:25:02.960,0:25:05.000<br>
但是這是一個非黑即白的世界<br>
<br>
0:25:05.000,0:25:07.340<br>
所以，如果你看起來有點像 class 1<br>
<br>
0:25:07.340,0:25:09.240<br>
那你就一定是 class 1<br>
<br>
0:25:09.240,0:25:11.900<br>
所以，本來根據我的 model 是說<br>
<br>
0:25:11.900,0:25:13.620<br>
機率是 0.7 是 class 1<br>
<br>
0:25:13.620,0:25:15.000<br>
0.3 是 class 2<br>
<br>
0:25:15.000,0:25:18.120<br>
那用 Hard label<br>
<br>
0:25:18.120,0:25:21.040<br>
用 Low-density Separation Assumption<br>
<br>
0:25:21.040,0:25:23.520<br>
就改成說，它這邊是 class 1 的機率是 1<br>
<br>
0:25:23.520,0:25:26.800<br>
你就把它往 class 1 那邊推過去<br>
<br>
0:25:26.800,0:25:29.680<br>
它就完全不可能是屬於 class 2<br>
<br>
0:25:29.680,0:25:32.880<br>
那下面這個方法不會 work<br>
<br>
0:25:33.260,0:25:35.800<br>
我之前還有看過有 paper propose 就是<br>
<br>
0:25:35.800,0:25:38.520<br>
propose 在做 neural network 的時候<br>
<br>
0:25:38.520,0:25:39.740<br>
用一個甚麼 soft 的方法<br>
<br>
0:25:39.740,0:25:41.660<br>
那果然 performance 不 work<br>
<br>
0:25:41.660,0:25:43.400<br>
不用做，我就知道結果會怎樣<br>
<br>
0:25:47.280,0:25:50.260<br>
那剛才這一招<br>
<br>
0:25:50.260,0:25:52.080<br>
有一個進階版<br>
<br>
0:25:52.080,0:25:55.920<br>
叫做 Entropy-based Regularization<br>
<br>
0:25:55.920,0:25:57.460<br>
你可能會覺得說<br>
<br>
0:25:57.460,0:25:59.860<br>
直接看它有點像 class 1 就變 1<br>
<br>
0:25:59.860,0:26:02.420<br>
直接看它有點像 class 2 就變 2<br>
<br>
0:26:02.420,0:26:03.560<br>
有點太武斷了<br>
<br>
0:26:03.560,0:26:06.380<br>
那你可以用 Entropy-based 的這個方法<br>
<br>
0:26:06.380,0:26:08.040<br>
Entropy-based 這個方法是說<br>
<br>
0:26:08.040,0:26:10.900<br>
如果你用 neural network 的時候，你的 output<br>
<br>
0:26:10.900,0:26:13.620<br>
是一個 distribution<br>
<br>
0:26:13.620,0:26:16.900<br>
那我們不要限制說這個 output 一定要是<br>
<br>
0:26:16.900,0:26:18.560<br>
class 1，一定要是 class 2<br>
<br>
0:26:18.560,0:26:20.660<br>
但是，我們做的假設是這樣<br>
<br>
0:26:20.660,0:26:22.280<br>
這個 output 的 distribution<br>
<br>
0:26:22.280,0:26:24.120<br>
它一定要很集中<br>
<br>
0:26:24.120,0:26:26.200<br>
因為這是一個非黑即白的世界<br>
<br>
0:26:26.200,0:26:28.380<br>
所以，output 的 distribution 一定要很集中<br>
<br>
0:26:28.380,0:26:32.160<br>
也就是說你 output，假設我們現在做 5 個 class 的分類<br>
<br>
0:26:32.160,0:26:34.420<br>
那如果你的 output 都是<br>
<br>
0:26:34.420,0:26:37.640<br>
在 class 1 的機率很大，在其他 class 的機率很小<br>
<br>
0:26:37.640,0:26:39.220<br>
這個是好的<br>
<br>
0:26:39.220,0:26:41.740<br>
因為是 unlabeled data，所以我不知道它的 label 是什麼<br>
<br>
0:26:41.740,0:26:43.280<br>
但是，如果你的 model<br>
<br>
0:26:43.280,0:26:46.180<br>
可以讓這筆 data，在 class 1 的機率很大<br>
<br>
0:26:46.180,0:26:48.280<br>
在其他的機率很小，那是好的<br>
<br>
0:26:48.280,0:26:50.940<br>
如果它在 class 5 的機率很大<br>
<br>
0:26:50.940,0:26:52.140<br>
其他的機率都很小<br>
<br>
0:26:52.140,0:26:53.120<br>
這個也是好的<br>
<br>
0:26:53.120,0:26:54.900<br>
因為我也不知道它是 class 1 還是 class 5<br>
<br>
0:26:54.900,0:26:56.600<br>
所以，這樣是好的<br>
<br>
0:26:56.600,0:26:58.000<br>
甚麼狀況不好呢<br>
<br>
0:26:58.000,0:27:01.380<br>
如果今天分布是很平均的話<br>
<br>
0:27:01.380,0:27:04.600<br>
這樣是不好的，因為這是一個非黑即白的世界<br>
<br>
0:27:04.600,0:27:07.960<br>
這樣子不符合 Low-density Separation 的假設<br>
<br>
0:27:10.280,0:27:13.300<br>
但是，現在的問題就是我們要怎麼<br>
<br>
0:27:13.300,0:27:16.700<br>
用數值的方法來evaluate<br>
<br>
0:27:16.700,0:27:19.400<br>
這個 distribution 到底是好的還是不好<br>
<br>
0:27:19.400,0:27:22.520<br>
這個 distribution是集中的還是不集中的呢<br>
<br>
0:27:22.520,0:27:25.400<br>
這邊要用的東西 ，叫做Entropy<br>
<br>
0:27:25.400,0:27:28.380<br>
你就去算一個 distribution 的 Entropy<br>
<br>
0:27:28.380,0:27:30.120<br>
這個 distribution 的 Entropy 呢<br>
<br>
0:27:30.120,0:27:34.000<br>
告訴你說，這個 distribution 它到底是集中還是不集中<br>
<br>
0:27:34.000,0:27:38.760<br>
我們用一個值來表示，這個 distribution 是集中還是分散<br>
<br>
0:27:38.760,0:27:40.860<br>
這個怎麼算呢<br>
<br>
0:27:40.860,0:27:44.020<br>
其實就算你沒有修過 information theory 之類的<br>
<br>
0:27:44.020,0:27:46.080<br>
我相信你也是聽得懂的<br>
<br>
0:27:46.080,0:27:47.360<br>
就記一下它的式子<br>
<br>
0:27:47.360,0:27:50.280<br>
它的式子是這樣<br>
<br>
0:27:50.280,0:27:53.500<br>
某一個 distribution，它的 entropy 呢<br>
<br>
0:27:53.500,0:27:56.100<br>
就是負的<br>
<br>
0:27:56.100,0:27:59.980<br>
它對每一個 class 的機率<br>
<br>
0:27:59.980,0:28:02.760<br>
有 5 個 class，就 summation 1到 5<br>
<br>
0:28:02.760,0:28:04.500<br>
它對每一個 class 的機率<br>
<br>
0:28:04.500,0:28:07.440<br>
乘上 log(那一個 class 的機率)<br>
<br>
0:28:08.160,0:28:10.420<br>
如果我們今天把<br>
<br>
0:28:10.420,0:28:13.940<br>
這一個、第一個 distribution 的機率<br>
<br>
0:28:13.940,0:28:17.200<br>
代到這裡面去，它只有一個是 1<br>
<br>
0:28:17.200,0:28:18.560<br>
其他都是 0<br>
<br>
0:28:18.560,0:28:20.680<br>
那你得到的 entropy 是多少呢<br>
<br>
0:28:20.680,0:28:22.040<br>
你得到的 entropy<br>
<br>
0:28:22.040,0:28:24.920<br>
算出來會是 0<br>
<br>
0:28:24.920,0:28:29.680<br>
因為 1*ln1 是 0<br>
<br>
0:28:29.680,0:28:31.360<br>
0*ln0 也是 0，所以<br>
<br>
0:28:31.360,0:28:35.180<br>
這個就是 0，這沒有甚麼特別好講的<br>
<br>
0:28:35.180,0:28:39.560<br>
這個也是 0<br>
<br>
0:28:39.560,0:28:43.060<br>
那下面這個呢<br>
<br>
0:28:43.060,0:28:44.340<br>
這邊每一個機率<br>
<br>
0:28:44.340,0:28:48.240<br>
也就是這一邊每一個 (y 上標 u, 下標 m) 都是 1/5<br>
<br>
0:28:48.240,0:28:50.160<br>
所以，你就把這些值代進去<br>
<br>
0:28:50.160,0:28:52.620<br>
你就把這些 1/5 的值都代進去<br>
<br>
0:28:52.620,0:28:56.620<br>
你算出來就是 1- ln(1/5)<br>
<br>
0:28:56.620,0:28:58.700<br>
也就是 ln5，所以<br>
<br>
0:28:58.700,0:29:02.100<br>
它的 entropy 比較大，它是散佈比較開的<br>
<br>
0:29:02.100,0:29:03.400<br>
所以，它 entropy 比較大<br>
<br>
0:29:03.400,0:29:04.780<br>
它是散佈比較窄的<br>
<br>
0:29:04.780,0:29:06.480<br>
所以，它的 entropy 比較小<br>
<br>
0:29:06.480,0:29:08.500<br>
所以，我們需要做的事情是<br>
<br>
0:29:08.500,0:29:10.260<br>
我們希望這個 model 的 output<br>
<br>
0:29:10.260,0:29:12.520<br>
當然在 labeled data 上，它的分類要正確<br>
<br>
0:29:12.520,0:29:13.720<br>
但是在 unlabeled data 上，<br>
<br>
0:29:13.720,0:29:17.260<br>
它的 output、entropy 要越小越好<br>
<br>
0:29:17.260,0:29:19.960<br>
所以，根據這個假設<br>
<br>
0:29:19.960,0:29:24.640<br>
你就可以去重新設計你的 loss function<br>
<br>
0:29:24.640,0:29:26.560<br>
我們原來的 loss function 是說<br>
<br>
0:29:26.560,0:29:28.880<br>
我希望，找一組參數<br>
<br>
0:29:28.880,0:29:31.960<br>
讓我現在在 labeled data 上的 model 的 output<br>
<br>
0:29:31.960,0:29:33.160<br>
跟正確的 model 的 output<br>
<br>
0:29:33.160,0:29:34.880<br>
它的距離越近越好<br>
<br>
0:29:34.880,0:29:38.080<br>
你用 cross entropy 來 evaluate 它們之間的距離<br>
<br>
0:29:38.080,0:29:40.360<br>
這個是 labeled data 的部分<br>
<br>
0:29:40.360,0:29:42.200<br>
在 unlabeled data 的部分呢<br>
<br>
0:29:42.200,0:29:45.380<br>
你會加上每一筆 unlabeled data<br>
<br>
0:29:45.380,0:29:48.020<br>
它的 output distribution 的 entropy<br>
<br>
0:29:48.020,0:29:51.200<br>
那你會希望這些 unlabeled data 的 entropy<br>
<br>
0:29:51.200,0:29:53.520<br>
越小越好，那在這兩項中間呢<br>
<br>
0:29:53.520,0:29:56.000<br>
你其實可以乘一個 weight 來考慮說<br>
<br>
0:29:56.000,0:29:58.940<br>
你要偏向 unlabeled data 多一點<br>
<br>
0:29:58.940,0:30:00.300<br>
還是少一點<br>
<br>
0:30:00.300,0:30:02.080<br>
那在 training 的時候怎麼辦呢？<br>
<br>
0:30:02.080,0:30:03.800<br>
在 training 的時候就是<br>
<br>
0:30:03.800,0:30:06.640<br>
一句話就 train 下去這樣，懂嗎？<br>
<br>
0:30:06.640,0:30:08.280<br>
這個可以算微分<br>
<br>
0:30:08.280,0:30:10.740<br>
可以算微分就<br>
<br>
0:30:10.740,0:30:13.680<br>
就沒有甚麼問題，就用 Gradient Descent<br>
<br>
0:30:13.680,0:30:16.680<br>
來 minimize 這個式子而已<br>
<br>
0:30:16.680,0:30:18.020<br>
那這一件事情<br>
<br>
0:30:18.020,0:30:21.940<br>
它的角色就很像我們之前講的 Regularization<br>
<br>
0:30:21.940,0:30:24.580<br>
所以它稱之為 Entropy-based 的 Regularization<br>
<br>
0:30:24.580,0:30:26.100<br>
之前我們說 Regularization 的時候<br>
<br>
0:30:26.100,0:30:28.340<br>
我們說，我們在原來的 loss function 後面<br>
<br>
0:30:28.340,0:30:32.160<br>
加一個 parameter 的 1-norm 或 2-norm<br>
<br>
0:30:32.160,0:30:34.000<br>
讓它比較不會 overfitting<br>
<br>
0:30:34.000,0:30:35.540<br>
那現在加上一個<br>
<br>
0:30:35.540,0:30:38.200<br>
根據 unlabeled data 得到的 entropy<br>
<br>
0:30:38.200,0:30:40.400<br>
來讓它比較不會 overfitting<br>
<br>
0:30:41.760,0:30:44.420<br>
那還有別的 Semi-supervised learning 的方式<br>
<br>
0:30:44.420,0:30:46.600<br>
有一個很著名的叫做<br>
<br>
0:30:46.600,0:30:50.080<br>
Semi-supervised SVM，不過我們還沒有講 SVM<br>
<br>
0:30:50.080,0:30:53.100<br>
所以，這邊就是當作一個 outlook<br>
<br>
0:30:53.100,0:30:57.100<br>
這個 Semi-supervised SVM 它的精神是這樣<br>
<br>
0:30:57.100,0:30:59.060<br>
我們知道 SVM 做的事情就是<br>
<br>
0:30:59.060,0:31:01.680<br>
給你兩個 class 的 data，然後找一個 boundary<br>
<br>
0:31:01.680,0:31:05.720<br>
那這個 boundary，一方面它要有最大的 margin<br>
<br>
0:31:05.720,0:31:09.800<br>
所謂最大的 margin 就是讓這兩個 class 分的越開越好<br>
<br>
0:31:09.800,0:31:13.740<br>
同時，它也要有最小的<br>
<br>
0:31:13.740,0:31:15.700<br>
分類的錯誤<br>
<br>
0:31:15.700,0:31:18.800<br>
現在，假設有一些 unlabeled data<br>
<br>
0:31:18.800,0:31:21.560<br>
這個 Semi-supervised SVM 會怎麼處理這個問題呢<br>
<br>
0:31:21.560,0:31:24.660<br>
它會窮舉所有可能的 label<br>
<br>
0:31:24.660,0:31:26.820<br>
這邊有 4 筆 unlabeled data<br>
<br>
0:31:26.820,0:31:30.500<br>
每一筆它都可以是屬於 class 1，也可以是屬於 class 2<br>
<br>
0:31:30.500,0:31:32.840<br>
我們就窮舉它所有可能的 label<br>
<br>
0:31:32.840,0:31:34.740<br>
它可以是長這樣的<br>
<br>
0:31:34.740,0:31:38.120<br>
就是說這三筆是屬於藍色 class<br>
<br>
0:31:38.120,0:31:40.500<br>
這一筆是屬於橙色 class，它可以是長這樣<br>
<br>
0:31:40.500,0:31:41.920<br>
它可以是長這樣<br>
<br>
0:31:41.920,0:31:44.540<br>
這兩個是藍色 class，這兩個是橙色 class<br>
<br>
0:31:44.540,0:31:47.960<br>
它可以長這樣，這個是橙的，這個是藍的<br>
<br>
0:31:47.960,0:31:50.820<br>
這個是藍的，這個是橙的，有各種可能，有很多的可能<br>
<br>
0:31:50.820,0:31:53.520<br>
有很多可能<br>
<br>
0:31:53.520,0:31:56.840<br>
然後，對每一個可能的結果，你都去算一個<br>
<br>
0:31:56.840,0:31:59.180<br>
你都去做一個 SVM<br>
<br>
0:31:59.180,0:32:01.140<br>
如果是在這個可能，這個情況下<br>
<br>
0:32:01.140,0:32:02.660<br>
你的 SVM 的 boundary 切在這邊<br>
<br>
0:32:02.660,0:32:05.400<br>
然後，這個可能你的 SVM 的 boundary 切在這邊<br>
<br>
0:32:05.400,0:32:08.160<br>
這個可能你的 SVM 的 boundary 不得不切在這邊<br>
<br>
0:32:08.160,0:32:11.060<br>
因為找不到一個方法可以把兩個 class 分開<br>
<br>
0:32:11.060,0:32:13.920<br>
然後，你再去看說，哪一個<br>
<br>
0:32:13.920,0:32:17.260<br>
unlabeled data 的可能性<br>
<br>
0:32:17.260,0:32:21.460<br>
在窮舉所有的可能的 label 裡面，哪一個可能性<br>
<br>
0:32:21.460,0:32:24.700<br>
可以讓你的 margin 最大，同時又 minimize error<br>
<br>
0:32:24.700,0:32:26.520<br>
今天在這個 example 裡面呢<br>
<br>
0:32:26.520,0:32:29.720<br>
可能是這一個方法<br>
<br>
0:32:29.720,0:32:30.980<br>
可以讓你的 margin 最大<br>
<br>
0:32:30.980,0:32:32.520<br>
它的 margin 是小的<br>
<br>
0:32:32.520,0:32:35.080<br>
它的 margin 不只小，而且還有分類錯誤<br>
<br>
0:32:35.080,0:32:37.260<br>
它的 margin 大而且都分類對，所以<br>
<br>
0:32:37.260,0:32:40.900<br>
你可能最後就選擇這一個 boundary<br>
<br>
0:32:40.900,0:32:44.920<br>
那這個 SVM，我把它的 reference 放在下面<br>
<br>
0:32:44.920,0:32:46.300<br>
你可能會有一個問題說<br>
<br>
0:32:46.300,0:32:49.800<br>
窮舉所有 unlabeled data 的 label<br>
<br>
0:32:49.800,0:32:52.480<br>
這聽起來不 make sense 阿，我有一萬筆 unlabeled data<br>
<br>
0:32:52.820,0:32:57.080<br>
2 的一萬次方，可能沒辦法做啊<br>
<br>
0:32:57.080,0:32:58.440<br>
所以，這個 paper 裡面<br>
<br>
0:32:58.440,0:33:01.040<br>
它就提出了一個很 approximate 的方法<br>
<br>
0:33:01.040,0:33:02.980<br>
基本精神是<br>
<br>
0:33:02.980,0:33:06.480<br>
我們今天，先很快帶過<br>
<br>
0:33:06.480,0:33:08.840<br>
它的基本精神是你一開始先得到一些 label<br>
<br>
0:33:08.840,0:33:11.300<br>
然後，你每次改一筆 unlabeled data 的 label<br>
<br>
0:33:11.300,0:33:14.080<br>
看看可不可以讓你的 objective function 變大<br>
<br>
0:33:14.080,0:33:15.520<br>
變大的話就改這樣子<br>
<br>
0:33:19.080,0:33:23.400<br>
接下來我們要講的方法呢<br>
<br>
0:33:23.400,0:33:26.760<br>
叫做 Smoothness Assumption<br>
<br>
0:33:26.760,0:33:30.100<br>
它的精神就是：近朱者赤；近墨者黑<br>
<br>
0:33:30.100,0:33:33.900<br>
或者是，像勸學篇說的那個<br>
<br>
0:33:33.900,0:33:38.500<br>
蓬生麻中，不扶而直；白沙在涅，與之俱黑<br>
<br>
0:33:40.280,0:33:42.940<br>
它的假設是這樣子<br>
<br>
0:33:42.940,0:33:50.400<br>
如果 x 是像的，那它們的 label y 也就像<br>
<br>
0:33:50.400,0:33:52.540<br>
這個假設聽起來沒有甚麼，而且<br>
<br>
0:33:52.540,0:33:53.860<br>
光講這個假設<br>
<br>
0:33:53.860,0:33:56.620<br>
其實是不精確的，因為<br>
<br>
0:33:56.620,0:33:59.360<br>
你知道一個正常的 model，你給它一個像的 input<br>
<br>
0:33:59.360,0:34:01.880<br>
如果它不是很 deep 的話，output 就會很像阿<br>
<br>
0:34:01.880,0:34:05.140<br>
所以，這個這樣講其實是不夠精確的<br>
<br>
0:34:05.780,0:34:08.880<br>
真正精確的假設，應該是下面這個講法<br>
<br>
0:34:08.880,0:34:13.300<br>
x 的分布是不平均的<br>
<br>
0:34:13.300,0:34:16.140<br>
它在某些地方是很集中<br>
<br>
0:34:16.140,0:34:18.920<br>
某些地方又很分散<br>
<br>
0:34:19.700,0:34:22.700<br>
如果今天 x1 和 x2<br>
<br>
0:34:22.700,0:34:26.580<br>
它們在一個 high density 的 region<br>
<br>
0:34:26.580,0:34:28.680<br>
很 close 的話<br>
<br>
0:34:28.680,0:34:31.300<br>
x^1 的 label、y^1\head<br>
<br>
0:34:31.300,0:34:34.600<br>
跟 x^2 的 label、y^2\head，它們才會很像<br>
<br>
0:34:35.080,0:34:37.160<br>
這句話有點讓人不知道在說甚麼<br>
<br>
0:34:37.160,0:34:40.820<br>
甚麼叫做在 high density 的 region 下呢<br>
<br>
0:34:40.820,0:34:42.880<br>
這句話的意思就是說<br>
<br>
0:34:42.880,0:34:46.960<br>
它們可以用 high density 的 path 做 connection<br>
<br>
0:34:46.960,0:34:49.620<br>
這樣講你還是不知道我在說什麼，所以我直接<br>
<br>
0:34:49.620,0:34:52.280<br>
舉一個例子，假設這個是<br>
<br>
0:34:52.280,0:34:55.280<br>
我們 data 的分布，假設這個是 data 的分布<br>
<br>
0:34:55.280,0:34:57.960<br>
它分布就像是一個血輪眼的樣子<br>
<br>
0:35:00.640,0:35:03.920<br>
假設我們現在有 3 筆 data<br>
<br>
0:35:03.920,0:35:05.260<br>
有 3 筆 data<br>
<br>
0:35:05.260,0:35:07.560<br>
x^1、x^2 跟 x^3<br>
<br>
0:35:07.560,0:35:11.560<br>
如果我們只是考慮這個比較粗的假設<br>
<br>
0:35:11.560,0:35:14.460<br>
像的 x，它的 output 像<br>
<br>
0:35:14.460,0:35:16.180<br>
那它的 label 像，所以<br>
<br>
0:35:16.180,0:35:19.320<br>
感覺好像應該是，x^2 跟 x^3 的 label 應該比較像<br>
<br>
0:35:19.320,0:35:22.420<br>
x^1 跟 x^2 的 label 比較不像<br>
<br>
0:35:22.420,0:35:25.620<br>
但是，其實 Smoothness Assumption 的假設不是這樣<br>
<br>
0:35:25.620,0:35:27.320<br>
它更精確的假設是說<br>
<br>
0:35:27.320,0:35:29.680<br>
你的像要透過一個<br>
<br>
0:35:29.680,0:35:32.240<br>
high density 的 region 像<br>
<br>
0:35:32.240,0:35:34.020<br>
懂嗎？就是說<br>
<br>
0:35:34.020,0:35:37.220<br>
x^1 跟 x^2，它們中間有一個<br>
<br>
0:35:37.220,0:35:38.700<br>
high density 的 region<br>
<br>
0:35:38.700,0:35:42.880<br>
它們中間有很多很多很多的 data<br>
<br>
0:35:42.880,0:35:45.240<br>
所以，它們兩個相連的地方是<br>
<br>
0:35:45.240,0:35:47.820<br>
通過一個 high density 的 path 相連的<br>
<br>
0:35:47.820,0:35:51.740<br>
從 x^1 走到 x^2 中間都是點，都是人煙<br>
<br>
0:35:51.740,0:35:55.940<br>
然後 x^2 跟 x^3 中間沒有點，所以可以走過去<br>
<br>
0:35:55.940,0:35:57.660<br>
這樣懂我意思嗎？就是<br>
<br>
0:35:57.660,0:36:01.120<br>
假設藍色點是聚落的分布的話<br>
<br>
0:36:01.120,0:36:03.480<br>
這中間是平原，所以人很多<br>
<br>
0:36:03.480,0:36:05.480<br>
所以，從 x^1 走到 x^2 比較容易<br>
<br>
0:36:05.480,0:36:07.240<br>
x^2 跟 x^3 中間是個山<br>
<br>
0:36:07.240,0:36:09.200<br>
所以這邊沒有住人，所以你走不過去<br>
<br>
0:36:09.200,0:36:10.100<br>
所以<br>
<br>
0:36:10.100,0:36:13.580<br>
根據這個真正的 Smoothness Assumption 的假設<br>
<br>
0:36:13.580,0:36:15.940<br>
它要告訴我們的意思是說<br>
<br>
0:36:15.940,0:36:18.420<br>
x^1 跟 x^2 是會有<br>
<br>
0:36:18.420,0:36:19.960<br>
比較可能有一樣的 label<br>
<br>
0:36:19.960,0:36:24.560<br>
x^2 跟 x^3 比較可能有不一樣的 label<br>
<br>
0:36:24.560,0:36:28.820<br>
因為，它們中間沒有 high density 的 path<br>
<br>
0:36:28.820,0:36:33.980<br>
那為甚麼會有 Smoothness Assumption 這樣的假設？<br>
<br>
0:36:33.980,0:36:37.860<br>
因為在真實的情況下，這個假設很有可能是成立的<br>
<br>
0:36:37.860,0:36:40.100<br>
比如說，我們考慮這個例子<br>
<br>
0:36:40.100,0:36:43.040<br>
我們考慮手寫數字辨識的例子<br>
<br>
0:36:43.040,0:36:46.940<br>
我們現在看到這邊有兩個 2，這邊有一個 3<br>
<br>
0:36:46.940,0:36:51.200<br>
對人來說，你當然知道這兩個都是 2<br>
<br>
0:36:51.200,0:36:55.440<br>
但是，如果你是單純算他們 pixel 的相似度的話<br>
<br>
0:36:55.440,0:36:58.500<br>
搞不好，這兩個其實是比較不像的<br>
<br>
0:36:58.500,0:37:01.260<br>
這兩個搞不好還比較像，因為它這邊有一個圈圈<br>
<br>
0:37:01.260,0:37:03.600<br>
它沒有圈圈，它這邊有一個勾勾<br>
<br>
0:37:03.600,0:37:05.400<br>
它有一個這樣的勾勾<br>
<br>
0:37:05.400,0:37:07.220<br>
我看它們兩個搞不好還比較像<br>
<br>
0:37:07.220,0:37:09.800<br>
對不對，你這邊再稍微彎曲一點<br>
<br>
0:37:09.800,0:37:12.220<br>
就變成 3 了，所以它們搞不好還比較像<br>
<br>
0:37:12.220,0:37:15.940<br>
但是，如果你把你的 data 通通倒出來的話<br>
<br>
0:37:15.940,0:37:17.900<br>
你會發現，這個 2<br>
<br>
0:37:17.900,0:37:21.640<br>
和這個 2 中間，它們有很多連續的型態<br>
<br>
0:37:21.640,0:37:23.400<br>
就是這個 2 稍微變一下變它<br>
<br>
0:37:23.400,0:37:25.320<br>
再變一下變它、變一下變它這樣<br>
<br>
0:37:25.320,0:37:28.580<br>
它和它中間有很多連續的變化<br>
<br>
0:37:28.580,0:37:30.680<br>
所以，可以從這種生物演化成這種生物<br>
<br>
0:37:30.680,0:37:32.160<br>
但是沒有辦法演化成這種生物<br>
<br>
0:37:32.160,0:37:33.840<br>
中間沒有過渡的型態<br>
<br>
0:37:33.840,0:37:38.200<br>
所以說，它們中間有很多<br>
<br>
0:37:38.200,0:37:41.460<br>
不直接相連的相似<br>
<br>
0:37:41.460,0:37:45.760<br>
它們中間有很多 stepping stone，可以讓它這樣跳過去<br>
<br>
0:37:45.760,0:37:48.500<br>
所以，如果根據 Smoothness Assumption 的話<br>
<br>
0:37:48.500,0:37:50.560<br>
你就可以得到說，這個東西<br>
<br>
0:37:50.560,0:37:52.160<br>
和這個東西是比較像的<br>
<br>
0:37:52.160,0:37:55.160<br>
這個東西和這個東西，它們中間沒有<br>
<br>
0:37:55.160,0:37:57.600<br>
過渡的型態，所以它們其實是比較不像的<br>
<br>
0:37:57.600,0:37:59.540<br>
它們其實不應該是屬於同一個 class<br>
<br>
0:37:59.540,0:38:01.720<br>
它們其實是屬於同一個 class<br>
<br>
0:38:01.720,0:38:05.200<br>
如果你看人臉辨識的話呢<br>
<br>
0:38:05.200,0:38:07.560<br>
其實也是一樣，一個人的<br>
<br>
0:38:07.560,0:38:10.760<br>
一個人，如果從他的左臉照一張相<br>
<br>
0:38:10.760,0:38:13.120<br>
跟右臉照一張相，那差很多<br>
<br>
0:38:13.120,0:38:16.140<br>
你拿這一張相片，跟另外一個人的<br>
<br>
0:38:16.140,0:38:21.000<br>
這邊是甚麼，正側面，這也是正側面<br>
<br>
0:38:21.000,0:38:25.480<br>
你拿另外一張一樣是眼睛朝左的相片來比較的話<br>
<br>
0:38:25.480,0:38:27.680<br>
我看還比這個像<br>
<br>
0:38:27.680,0:38:30.460<br>
還比較像這個眼睛朝左的相片<br>
<br>
0:38:30.460,0:38:33.220<br>
跟這個眼睛朝右的相片相比的話<br>
<br>
0:38:33.220,0:38:36.760<br>
但是，假設你收集到夠多的 unlabeled data 的話<br>
<br>
0:38:36.760,0:38:41.000<br>
你會找到說，這一張臉和這一張臉中間呢<br>
<br>
0:38:41.000,0:38:43.680<br>
有很多過渡的型態<br>
<br>
0:38:43.680,0:38:47.440<br>
所以，這一張臉跟這一張臉可能是同一個人的臉<br>
<br>
0:38:48.140,0:38:50.660<br>
或者是，在這個<br>
<br>
0:38:50.660,0:38:52.920<br>
這招在文件分類上面呢<br>
<br>
0:38:52.920,0:38:55.320<br>
可能是會非常有用的<br>
<br>
0:38:55.320,0:38:57.560<br>
為甚麼呢？假設你現在<br>
<br>
0:38:57.560,0:39:02.280<br>
要分天文學跟旅遊的文章<br>
<br>
0:39:02.280,0:39:05.640<br>
那天文學的文章有一個它固定的 word distribution<br>
<br>
0:39:05.640,0:39:08.940<br>
比如說，它會出現這個<br>
<br>
0:39:08.940,0:39:13.400<br>
asteroid, bright，那如果旅遊的文章，它會出現<br>
<br>
0:39:13.400,0:39:15.600<br>
yellow stone 等等<br>
<br>
0:39:15.600,0:39:20.780<br>
如果今天，你的 unlabeled data<br>
<br>
0:39:20.780,0:39:24.440<br>
如果今天你的 unlabeled data 跟你的 labeled data<br>
<br>
0:39:24.440,0:39:26.800<br>
是有 overlapped 的<br>
<br>
0:39:26.800,0:39:31.000<br>
那你就很容易可以處理這個問題<br>
<br>
0:39:31.000,0:39:32.580<br>
但是，在真實的情況下<br>
<br>
0:39:32.580,0:39:36.360<br>
你的 unlabeled data 跟 labeled data，它們中間可能<br>
<br>
0:39:36.360,0:39:38.480<br>
沒有任何 overlapped 的 word<br>
<br>
0:39:38.480,0:39:41.980<br>
為甚麼呢？因為世界上的 word 很多<br>
<br>
0:39:41.980,0:39:43.540<br>
一篇文章裡面，你往往<br>
<br>
0:39:43.540,0:39:45.220<br>
你的詞彙不會太多<br>
<br>
0:39:45.220,0:39:46.620<br>
但是，世界上可能 word 很多<br>
<br>
0:39:46.620,0:39:49.980<br>
所以，每一篇文章它裡面的詞彙，其實是非常 sparse 的<br>
<br>
0:39:49.980,0:39:51.660<br>
它只提到非常少量的 word<br>
<br>
0:39:51.660,0:39:54.860<br>
所以，你拿兩篇文章出來，它們中間<br>
<br>
0:39:54.860,0:39:58.140<br>
有重複的 word 的比例，是沒有那麼多的<br>
<br>
0:39:58.140,0:40:00.740<br>
所以，很有可能你的 data<br>
<br>
0:40:00.740,0:40:05.680<br>
你的 unlabeled data 跟你的 labeled data 中間<br>
<br>
0:40:05.680,0:40:07.300<br>
是沒有任何 overlap 的<br>
<br>
0:40:07.300,0:40:11.220<br>
但是，如果你 collect 到夠多的 unlabeled data 的話<br>
<br>
0:40:11.220,0:40:14.340<br>
如果你 collect 到夠多的 unlabeled data 的話<br>
<br>
0:40:14.340,0:40:18.580<br>
你就可以說，這個是 d1 跟 d5 像<br>
<br>
0:40:18.580,0:40:23.060<br>
d5 又跟 d6 像，這個像就可以一路 propagate 過去<br>
<br>
0:40:23.060,0:40:26.160<br>
知道說 d1 跟 d3 一類，那 d2 跟 d9 像<br>
<br>
0:40:26.160,0:40:29.600<br>
d9 跟 d8 像，那你就會得到 d2 跟 d4 像<br>
<br>
0:40:29.600,0:40:32.680<br>
這個像也可以一路 propagate 過去<br>
<br>
0:40:33.380,0:40:36.500<br>
那如何實踐這個 Smoothness Assumption  呢<br>
<br>
0:40:36.500,0:40:37.860<br>
最簡單的方法這個<br>
<br>
0:40:37.860,0:40:42.920<br>
呃，電腦卡住了，沒有卡住<br>
<br>
0:40:42.920,0:40:45.520<br>
又回來了<br>
<br>
0:40:45.520,0:40:47.400<br>
是 Cluster and then Label<br>
<br>
0:40:47.400,0:40:50.260<br>
這個方法太簡單了，沒什麼可以講的<br>
<br>
0:40:50.260,0:40:53.740<br>
我們現在 data distribution 長這個樣子<br>
<br>
0:40:53.740,0:40:56.960<br>
橙色是 class 1 ，綠色是 class 2<br>
<br>
0:40:56.960,0:40:59.600<br>
藍色是 unlabeled data<br>
<br>
0:40:59.600,0:41:02.620<br>
接下來，你就做一下 clustering<br>
<br>
0:41:02.620,0:41:04.980<br>
你把這些所有的 data 拿來做 clustering<br>
<br>
0:41:04.980,0:41:07.120<br>
你可能就分成 3 個 class<br>
<br>
0:41:07.120,0:41:09.940<br>
3 個 class，3 個 cluster<br>
<br>
0:41:09.940,0:41:12.040<br>
然後，你就看出 cluster 1 裡面呢<br>
<br>
0:41:12.040,0:41:14.920<br>
橙色 class 1 的 label data 最多<br>
<br>
0:41:14.920,0:41:18.440<br>
所以，cluster 1 裡面所有的 data 都算 class 1<br>
<br>
0:41:18.440,0:41:21.100<br>
那 cluster 2 跟 cluster 3 都算 class 2<br>
<br>
0:41:21.160,0:41:23.820<br>
就結束了<br>
<br>
0:41:23.820,0:41:25.480<br>
那你把這些 data 拿去 learn 就結束了<br>
<br>
0:41:25.480,0:41:27.800<br>
那這個方法不一定有用，尤其是<br>
<br>
0:41:27.800,0:41:30.860<br>
在你的作業三裡面，你可以 implement 這個方法<br>
<br>
0:41:30.860,0:41:33.880<br>
因為我們只說，助教只說要實踐兩種方法<br>
<br>
0:41:33.880,0:41:36.060<br>
沒有說做完以後一定要進步嘛，所以<br>
<br>
0:41:39.480,0:41:41.860<br>
真的是這樣的阿，如果你今天在<br>
<br>
0:41:41.860,0:41:43.480<br>
就是說助教只提供兩個方法<br>
<br>
0:41:43.480,0:41:45.100<br>
一個是 self-learning<br>
<br>
0:41:45.100,0:41:46.840<br>
我們自己試過啦，是一定會進步的<br>
<br>
0:41:46.840,0:41:48.760<br>
如果你今天要做 Cluster and then Label<br>
<br>
0:41:48.760,0:41:50.520<br>
你這個 cluster 要很強<br>
<br>
0:41:51.120,0:41:53.160<br>
因為只有這一招 work 的假設就是<br>
<br>
0:41:53.160,0:41:55.460<br>
你可以把同一個 class 的東西 cluster 在一起<br>
<br>
0:41:55.460,0:41:56.940<br>
可是在 image 裡面<br>
<br>
0:41:56.940,0:41:58.680<br>
你要把同一個 class 的東西 cluster 在一起<br>
<br>
0:41:58.680,0:41:59.820<br>
其實是沒那麼容易的<br>
<br>
0:41:59.820,0:42:00.820<br>
我們之前有講過說<br>
<br>
0:42:00.820,0:42:03.380<br>
我們在前面的投影片講為甚麼要用 deep learning 的時候<br>
<br>
0:42:03.380,0:42:05.980<br>
不同 class，可能會長得很像<br>
<br>
0:42:05.980,0:42:07.400<br>
同一個 class，可能會長得很不像<br>
<br>
0:42:07.400,0:42:10.100<br>
你單純只用 pixel 來做 clustering<br>
<br>
0:42:10.100,0:42:11.460<br>
你結果八成是會壞掉<br>
<br>
0:42:11.460,0:42:14.020<br>
你沒有辦法把同一個 class 的 data cluster 在一起<br>
<br>
0:42:14.020,0:42:16.260<br>
那 unlabeled data 沒有什麼幫助<br>
<br>
0:42:16.260,0:42:17.360<br>
做出來就是會壞掉<br>
<br>
0:42:17.360,0:42:20.020<br>
所以，如果你要讓 Cluster and then Label 這個方法<br>
<br>
0:42:20.020,0:42:21.540<br>
有用，你的 cluster 要很強<br>
<br>
0:42:21.540,0:42:24.620<br>
你要有很好的方法，來描述你的一張 image<br>
<br>
0:42:24.620,0:42:29.240<br>
在我們自己試的時候，我們會用 Deep Autoencoder<br>
<br>
0:42:29.240,0:42:31.380<br>
我們還沒有講 Deep Autoencoder，所以<br>
<br>
0:42:31.380,0:42:34.240<br>
如果你覺得沒有辦法這實作，這個也是正常的<br>
<br>
0:42:34.240,0:42:35.900<br>
我們是用 Deep Autoencoder call feature<br>
<br>
0:42:35.900,0:42:38.260<br>
然後再 call clustering，這樣才會 work<br>
<br>
0:42:38.260,0:42:40.840<br>
如果你不這樣做的話，我覺得應該是不會 work 的<br>
<br>
0:42:41.500,0:42:44.500<br>
但是，你還是可以直接用 pixel 做 cluster<br>
<br>
0:42:45.300,0:42:48.880<br>
剛才講這個比較直覺的做法<br>
<br>
0:42:48.880,0:42:53.180<br>
另外一個方法是引入 Graph structure<br>
<br>
0:42:53.180,0:42:56.600<br>
我們用 Graph structure 呢<br>
<br>
0:42:56.600,0:43:01.500<br>
來表達 connected by a high density path 這件事情<br>
<br>
0:43:01.500,0:43:03.520<br>
就是說，我們現在呢<br>
<br>
0:43:03.520,0:43:10.720<br>
把所有的 data points，都建成一個 graph<br>
<br>
0:43:10.720,0:43:13.960<br>
每一個 data point, x，就是這個圖上的一個點<br>
<br>
0:43:13.960,0:43:18.000<br>
你要想辦法算它們之間的 singularity<br>
<br>
0:43:18.000,0:43:20.840<br>
你要想辦法它們之間的 edge 建出來<br>
<br>
0:43:20.840,0:43:22.600<br>
有了這個 graph 以後<br>
<br>
0:43:22.600,0:43:26.560<br>
你就可以說所謂的 high density path 的意思就是說<br>
<br>
0:43:26.560,0:43:29.180<br>
如果今天有兩個點<br>
<br>
0:43:29.180,0:43:31.820<br>
它們在這個 graph 上面是相連的<br>
<br>
0:43:31.820,0:43:34.180<br>
是走得到的<br>
<br>
0:43:34.180,0:43:36.820<br>
它們就是同一個 class，如果沒有相連<br>
<br>
0:43:36.820,0:43:38.400<br>
就算是實際上的距離呢<br>
<br>
0:43:38.400,0:43:40.460<br>
也不算太遠，那你也走不到<br>
<br>
0:43:41.340,0:43:43.420<br>
那怎麼建一個 graph<br>
<br>
0:43:43.420,0:43:44.840<br>
有些時候<br>
<br>
0:43:44.840,0:43:48.040<br>
這個 graph 的 representation 是很自然就可以得到<br>
<br>
0:43:48.040,0:43:50.520<br>
舉例來說，假設你現在要做的是<br>
<br>
0:43:50.520,0:43:52.300<br>
這個網頁的分類<br>
<br>
0:43:52.300,0:43:55.820<br>
而你有記錄網頁有網頁之間的 hyperlink<br>
<br>
0:43:55.820,0:44:00.220<br>
hyperlink 自然地就告訴你說，這些網頁先是如何連結的<br>
<br>
0:44:00.220,0:44:02.720<br>
或者是，你現在要做的是論文的分類<br>
<br>
0:44:02.720,0:44:05.460<br>
而論文和論文之間有引用的關係<br>
<br>
0:44:05.460,0:44:08.400<br>
這個引用的關係式也是另外一種 graph 的 edge<br>
<br>
0:44:08.400,0:44:12.300<br>
它也可以很自然地把這種圖畫出來給你<br>
<br>
0:44:12.300,0:44:17.980<br>
當然有時候，你需要自己想辦法建這個 graph<br>
<br>
0:44:17.980,0:44:20.100<br>
怎麼自己想辦法建這個 graph？<br>
<br>
0:44:20.100,0:44:22.040<br>
其實這邊<br>
<br>
0:44:22.040,0:44:24.300<br>
你的 graph 的好壞對你的結果<br>
<br>
0:44:24.300,0:44:26.260<br>
影響是非常的 critical 的<br>
<br>
0:44:26.260,0:44:28.580<br>
不過這個地方就非常的 heuristic<br>
<br>
0:44:28.580,0:44:30.240<br>
就是憑著經驗和直覺<br>
<br>
0:44:30.240,0:44:31.380<br>
你就覺得你怎麼做比較好<br>
<br>
0:44:31.380,0:44:34.280<br>
就選擇你覺得爽的方法做就是了<br>
<br>
0:44:34.280,0:44:37.420<br>
那這邊通常的做法是這個樣子<br>
<br>
0:44:37.420,0:44:39.660<br>
你要先定義兩個 object 之間<br>
<br>
0:44:39.660,0:44:42.080<br>
你怎麼算它們的相似度<br>
<br>
0:44:42.080,0:44:44.500<br>
影像的話，你可以 base on pixel 算相似度<br>
<br>
0:44:44.500,0:44:45.860<br>
performance 不太好<br>
<br>
0:44:45.860,0:44:49.240<br>
因為 base on autoencoder 抽出來的 feature 算相似度<br>
<br>
0:44:49.240,0:44:51.380<br>
可能 performance 就會比較好<br>
<br>
0:44:51.380,0:44:53.840<br>
算完相似度以後<br>
<br>
0:44:53.840,0:44:55.940<br>
你就可以建 graph 了<br>
<br>
0:44:55.940,0:44:58.440<br>
那 graph 有很多種，比如說你可以建<br>
<br>
0:44:58.440,0:45:00.460<br>
K nearest Neighbor 的 graph<br>
<br>
0:45:00.460,0:45:02.500<br>
所謂 K nearest Neighbor 的 graph，意思是說<br>
<br>
0:45:02.500,0:45:06.540<br>
我現在有一大堆的 data<br>
<br>
0:45:06.540,0:45:11.140<br>
那 data 和 data 之間，我都可以算出它的相似度<br>
<br>
0:45:11.140,0:45:13.500<br>
我就說<br>
<br>
0:45:13.500,0:45:15.720<br>
我 K nearest Neighbor 設 k = 3<br>
<br>
0:45:15.720,0:45:19.520<br>
那每一個 point 都跟它最近的、相似度最像的<br>
<br>
0:45:19.520,0:45:21.660<br>
三個點做相連<br>
<br>
0:45:21.660,0:45:23.220<br>
或者，你可以做<br>
<br>
0:45:23.220,0:45:26.880<br>
e-Neighborhood，所謂 e-Neighborhood 是甚麼意思呢<br>
<br>
0:45:26.880,0:45:29.820<br>
是說，每一個點<br>
<br>
0:45:29.820,0:45:32.780<br>
只有跟它相似度超過某一個 threshold<br>
<br>
0:45:32.780,0:45:34.900<br>
跟它相似大於 1 的那些點<br>
<br>
0:45:34.900,0:45:36.360<br>
才不會被黏起來<br>
<br>
0:45:36.360,0:45:38.140<br>
這都是很直覺的<br>
<br>
0:45:38.140,0:45:40.940<br>
那所謂的相連也不是只有<br>
<br>
0:45:40.940,0:45:43.620<br>
所謂的 edge 也不是只有相連和不相連<br>
<br>
0:45:43.620,0:45:45.540<br>
這樣 binary 的選擇而已<br>
<br>
0:45:45.540,0:45:48.660<br>
你可以給 edge 一些 weight<br>
<br>
0:45:48.660,0:45:51.600<br>
你可以讓你的 edge 跟你的<br>
<br>
0:45:51.600,0:45:54.920<br>
要被連接起來的兩個 data point 之間<br>
<br>
0:45:54.920,0:45:57.180<br>
相似度是成正比的<br>
<br>
0:45:57.180,0:46:01.220<br>
怎麼定義這個相似度呢<br>
<br>
0:46:01.460,0:46:04.200<br>
我會建議比較好的選擇<br>
<br>
0:46:04.200,0:46:09.780<br>
其實是用 RBM function 來訂這個相似度<br>
<br>
0:46:09.780,0:46:13.120<br>
然後，我就發現說我這邊寫錯了<br>
<br>
0:46:13.120,0:46:16.600<br>
這邊這個應該是 x 才對啊<br>
<br>
0:46:16.600,0:46:21.400<br>
這個其實應該是 x<br>
<br>
0:46:21.400,0:46:25.100<br>
怎麼算這個 function<br>
<br>
0:46:25.100,0:46:26.380<br>
你可以先算說<br>
<br>
0:46:26.380,0:46:30.160<br>
x^i 跟 x^j 如果你都把它們用 vector 來表示的話<br>
<br>
0:46:30.160,0:46:32.140<br>
算它們的 Euclidean distance<br>
<br>
0:46:32.140,0:46:35.420<br>
前面乘一個參數<br>
<br>
0:46:35.420,0:46:38.640<br>
然後，前面乘一個負號，再取 exponential<br>
<br>
0:46:38.640,0:46:41.640<br>
那其實取 exponential 這件事情呢<br>
<br>
0:46:41.640,0:46:45.800<br>
是我覺得還滿必要的，在經驗上<br>
<br>
0:46:45.800,0:46:50.300<br>
用這樣的 function，可以給你比較好的 performance<br>
<br>
0:46:50.300,0:46:52.480<br>
為甚麼用這樣子的 function<br>
<br>
0:46:52.480,0:46:54.220<br>
會給你比較好的 performance 呢？<br>
<br>
0:46:54.220,0:46:58.480<br>
因為你想想看這個 function，它下降的速度是非常快的<br>
<br>
0:46:58.480,0:47:00.820<br>
因為它有取 exponential<br>
<br>
0:47:00.820,0:47:04.740<br>
所以，只有當 x^i 跟 x^j 非常靠近的時候<br>
<br>
0:47:04.740,0:47:09.180<br>
它的 singularity 才會大<br>
<br>
0:47:09.180,0:47:10.980<br>
只要距離稍微遠一點<br>
<br>
0:47:10.980,0:47:14.180<br>
singularity 就會下降很快，變得很小<br>
<br>
0:47:14.180,0:47:17.780<br>
也就是說，如果你用這種 RBM function 的話<br>
<br>
0:47:17.780,0:47:20.740<br>
你才能夠製造，比如說，像這個圖上<br>
<br>
0:47:20.740,0:47:23.500<br>
這邊有兩個橙色的點，是距離很近的<br>
<br>
0:47:23.500,0:47:26.140<br>
這個綠色的點，其實它跟橙色的點的距離<br>
<br>
0:47:26.140,0:47:29.540<br>
也蠻近，只是稍微遠一點<br>
<br>
0:47:29.540,0:47:31.240<br>
但是，你用這種 exponential 的話<br>
<br>
0:47:31.240,0:47:33.240<br>
每一個點都只跟非常近的點連<br>
<br>
0:47:33.240,0:47:35.200<br>
跟它稍微遠一點，它就不連了<br>
<br>
0:47:35.200,0:47:37.520<br>
你要有這樣子的機制<br>
<br>
0:47:37.520,0:47:39.300<br>
才可以避免你連到這種<br>
<br>
0:47:39.300,0:47:45.600<br>
跨海溝的這種 link 這樣<br>
<br>
0:47:45.600,0:47:49.360<br>
所以如果你有 exponential，通常效果是會比較好的<br>
<br>
0:47:51.880,0:47:55.680<br>
所以，graph-based 的方法，它的精神<br>
<br>
0:47:55.680,0:47:56.980<br>
是這樣子的<br>
<br>
0:47:56.980,0:47:59.700<br>
如果我們現在，在這個 graph 上面<br>
<br>
0:47:59.700,0:48:02.060<br>
我有一些 labeled data<br>
<br>
0:48:02.060,0:48:03.600<br>
比如說，在這個 graph 上面<br>
<br>
0:48:03.600,0:48:07.680<br>
我們已經知道說，這筆 data 是屬於 class 1<br>
<br>
0:48:07.680,0:48:09.480<br>
這筆 data 是屬於 class 1<br>
<br>
0:48:09.480,0:48:12.200<br>
那跟它們有相連的<br>
<br>
0:48:12.200,0:48:15.980<br>
那些 data point，它是屬於 class 1 的機率也就會上升<br>
<br>
0:48:15.980,0:48:19.080<br>
比如說，這筆 data 它屬於 class 1 的機率也會上升<br>
<br>
0:48:19.080,0:48:22.700<br>
這筆 data 它屬於 class 1 的機率也會上升<br>
<br>
0:48:22.700,0:48:26.540<br>
所以，每一筆 data 它會去影響它的鄰居<br>
<br>
0:48:26.540,0:48:29.780<br>
光是會影響它的鄰居是不夠的<br>
<br>
0:48:29.780,0:48:32.880<br>
如果你只考慮光會影響它的鄰居的話，其實可能<br>
<br>
0:48:32.880,0:48:34.340<br>
幫助不會太大<br>
<br>
0:48:34.340,0:48:37.460<br>
為甚麼呢？因為如果說它們相連<br>
<br>
0:48:37.460,0:48:39.860<br>
本來就很像<br>
<br>
0:48:39.860,0:48:41.120<br>
你 train 一個 model<br>
<br>
0:48:41.120,0:48:43.700<br>
input 很像的東西，output 本來就很像的東西<br>
<br>
0:48:44.140,0:48:46.320<br>
所以，幫助不會太大<br>
<br>
0:48:46.320,0:48:49.180<br>
那 Graph-based 的 approach 真正會有幫助它的<br>
<br>
0:48:49.180,0:48:50.980<br>
這個醍醐味就是<br>
<br>
0:48:50.980,0:48:55.800<br>
它的這個 class 是會傳遞的<br>
<br>
0:48:55.800,0:48:58.620<br>
也就是說，本來 class 1<br>
<br>
0:48:58.620,0:49:00.780<br>
只有這個點有跟 class 1 相連<br>
<br>
0:49:00.780,0:49:02.180<br>
所以，它會變得比較像 class 1<br>
<br>
0:49:02.180,0:49:06.360<br>
但是，這件事情會像傳染病一樣傳遞過去<br>
<br>
0:49:06.360,0:49:08.020<br>
所以，這個點雖然它沒有<br>
<br>
0:49:08.020,0:49:10.320<br>
這個 data point，它雖然沒有真的跟任何<br>
<br>
0:49:10.320,0:49:12.260<br>
真的是 class 1 的點相連<br>
<br>
0:49:12.260,0:49:14.860<br>
但是，因為這件事情<br>
<br>
0:49:14.860,0:49:16.880<br>
像 class 1 這件事情是會感染的<br>
<br>
0:49:16.880,0:49:20.920<br>
所以，這件事情也會透過這個 graph 的 link 傳遞過來<br>
<br>
0:49:20.920,0:49:24.260<br>
所以，舉例來說，我們如果看這個例子<br>
<br>
0:49:24.260,0:49:27.280<br>
你把你所有的 data point 都建成一個 graph<br>
<br>
0:49:27.280,0:49:30.440<br>
當然這個是比較理想的例子<br>
<br>
0:49:30.440,0:49:33.140<br>
然後，你有一個藍色的點<br>
<br>
0:49:33.140,0:49:35.320<br>
是你 label 一筆 data 是屬於 class 1<br>
<br>
0:49:35.320,0:49:38.120<br>
你 label 一筆 data 是屬於 class 2<br>
<br>
0:49:38.120,0:49:39.620<br>
經過  Graph-based approach<br>
<br>
0:49:39.620,0:49:42.760<br>
如果你的 graph 是建得這麼漂亮的話<br>
<br>
0:49:42.760,0:49:45.260<br>
這邊就會通通是藍色<br>
<br>
0:49:45.260,0:49:47.700<br>
這邊就會通通是紅色<br>
<br>
0:49:47.700,0:49:50.480<br>
雖然說，這一點其實跟它的尾巴其實沒有接在一起<br>
<br>
0:49:50.480,0:49:55.320<br>
但是紅色這個 class 這件事情會一路 propagate 過去<br>
<br>
0:49:55.320,0:49:56.960<br>
會一路 propagate 過去<br>
<br>
0:49:56.960,0:49:59.120<br>
那如果你要讓 graph-based 這種<br>
<br>
0:49:59.120,0:50:01.480<br>
這種 Semi-supervised 的方法有用<br>
<br>
0:50:01.480,0:50:03.460<br>
你的一個 critical 的方法是你的 data 要夠多<br>
<br>
0:50:03.460,0:50:05.600<br>
如果你 data 不夠多<br>
<br>
0:50:05.600,0:50:08.620<br>
這個地方你沒收集到 data，這個點斷掉了<br>
<br>
0:50:08.620,0:50:11.500<br>
那你這 information就傳不過去<br>
<br>
0:50:13.540,0:50:15.020<br>
那剛才是<br>
<br>
0:50:15.020,0:50:19.060<br>
定性的說一下說，怎麼把<br>
<br>
0:50:19.060,0:50:22.460<br>
怎麼使用這個 graph，接下來是要說怎麼<br>
<br>
0:50:22.460,0:50:24.920<br>
定量的使用這個 graph<br>
<br>
0:50:24.920,0:50:27.040<br>
這個定量的使用方式<br>
<br>
0:50:27.040,0:50:30.440<br>
是我們在這個 graph 的 structure 上面<br>
<br>
0:50:30.440,0:50:32.340<br>
定一個東西，叫做<br>
<br>
0:50:32.340,0:50:35.060<br>
label 的 smoothness<br>
<br>
0:50:35.060,0:50:37.540<br>
我們會定義說，今天這個 label<br>
<br>
0:50:37.540,0:50:42.840<br>
有多符合我們剛才說的 Smoothness Assumption 的假設<br>
<br>
0:50:43.960,0:50:45.860<br>
怎麼定這個東西呢？<br>
<br>
0:50:45.860,0:50:48.480<br>
如果我們看這兩個例子<br>
<br>
0:50:48.480,0:50:50.700<br>
在這兩個例子裡面都有<br>
<br>
0:50:50.700,0:50:52.080<br>
4 個 data point<br>
<br>
0:50:52.080,0:50:55.960<br>
那 data point 和 data point 之間連接的數字<br>
<br>
0:50:55.960,0:50:58.920<br>
代表了這個 edge 的 weight<br>
<br>
0:50:58.920,0:51:02.320<br>
我們說，假設在左邊這個例子<br>
<br>
0:51:02.320,0:51:04.900<br>
左邊和右邊這兩個 graph 是一樣的，但是<br>
<br>
0:51:04.900,0:51:08.440<br>
我們現在給每一個 data 不同的 label<br>
<br>
0:51:08.440,0:51:10.080<br>
假設在這個 class 裡面<br>
<br>
0:51:10.080,0:51:13.080<br>
你給它的 label 是 1、1、1、0<br>
<br>
0:51:13.080,0:51:15.800<br>
再這個 example 裡面<br>
<br>
0:51:15.800,0:51:18.100<br>
給它的例子是 0、1、1、0<br>
<br>
0:51:18.100,0:51:22.920<br>
那誰比較 smooth 呢<br>
<br>
0:51:22.920,0:51:25.160<br>
給大家一秒鐘的時間考慮一下<br>
<br>
0:51:25.980,0:51:28.760<br>
你覺得左邊比較 smooth 的同學舉手<br>
<br>
0:51:29.440,0:51:31.560<br>
手放下<br>
<br>
0:51:31.560,0:51:34.460<br>
你覺得右邊比較 smooth 的同學舉手<br>
<br>
0:51:34.460,0:51:36.740<br>
沒有人，所以你覺得<br>
<br>
0:51:36.740,0:51:38.460<br>
多數人都覺得左邊比較 smooth<br>
<br>
0:51:38.460,0:51:40.760<br>
所以，大家的看法是非常一致的<br>
<br>
0:51:40.760,0:51:43.500<br>
左邊、這個三角形的地方都是 1<br>
<br>
0:51:43.500,0:51:44.540<br>
這邊是 0<br>
<br>
0:51:44.540,0:51:46.960<br>
這邊三角的地方有 0、有 1<br>
<br>
0:51:46.960,0:51:48.740<br>
這邊是 0，感覺這個比較<br>
<br>
0:51:48.740,0:51:50.840<br>
不符合 Smoothness Assumption 的假設<br>
<br>
0:51:50.840,0:51:52.000<br>
這個比較符合<br>
<br>
0:51:52.000,0:51:54.560<br>
但是，我們需要用一個數字來<br>
<br>
0:51:54.560,0:51:57.140<br>
定量的描述它說<br>
<br>
0:51:57.140,0:51:59.220<br>
它有多 smooth<br>
<br>
0:51:59.220,0:52:00.960<br>
那常見的作法是這個樣子<br>
<br>
0:52:00.960,0:52:02.980<br>
常見的作法是，你寫一個式子<br>
<br>
0:52:02.980,0:52:06.060<br>
這個式子你可以這樣寫，我們考慮<br>
<br>
0:52:06.060,0:52:09.700<br>
兩兩、有相連的這個<br>
<br>
0:52:09.700,0:52:13.700<br>
point，兩兩拿出來<br>
<br>
0:52:13.700,0:52:17.680<br>
summation over 所有的 data pair (i, j)<br>
<br>
0:52:17.680,0:52:21.220<br>
然後，我們計算 i, j 之間的 weight<br>
<br>
0:52:21.220,0:52:24.480<br>
跟 i 的 label<br>
<br>
0:52:24.480,0:52:28.240<br>
減掉 j 的 label 的平方<br>
<br>
0:52:28.240,0:52:32.320<br>
這邊是 summation over 所有的 data<br>
<br>
0:52:32.320,0:52:36.480<br>
不管它現在是有 label，還是沒有 label<br>
<br>
0:52:36.480,0:52:39.260<br>
所以你看左邊這個 case<br>
<br>
0:52:39.260,0:52:41.540<br>
(1 - 1) 的平方是 0<br>
<br>
0:52:41.540,0:52:43.940<br>
(1 - 1) 的平方是 0<br>
<br>
0:52:43.940,0:52:46.180<br>
(1 - 1) 的平方是 0<br>
<br>
0:52:46.180,0:52:49.580<br>
只有這邊是 (1 - 0) 的平方是 1<br>
<br>
0:52:49.580,0:52:53.300<br>
所以，你在 summation over 所有的 data pair 的時候<br>
<br>
0:52:53.300,0:52:55.860<br>
你只需要考慮 x^3 跟 x^4 這邊<br>
<br>
0:52:56.260,0:52:58.900<br>
那 (y^i - y^j) 的平方是 1<br>
<br>
0:52:58.900,0:53:03.560<br>
那 w (下標i, j) 是 1，再除 0.5，除 0.5 這件事情只是<br>
<br>
0:53:03.560,0:53:06.560<br>
為了等一下做某一個計算的時候<br>
<br>
0:53:06.560,0:53:10.440<br>
比較方便，它其實沒有甚麼真正的效用<br>
<br>
0:53:10.440,0:53:13.140<br>
這邊乘一個 0.5<br>
<br>
0:53:13.140,0:53:15.280<br>
最後得到的這個 smoothness<br>
<br>
0:53:15.280,0:53:16.360<br>
有多 smooth 呢？<br>
<br>
0:53:16.360,0:53:19.580<br>
evaluation 就是 0.5<br>
<br>
0:53:21.660,0:53:24.060<br>
那如果是右邊這一個 case<br>
<br>
0:53:24.060,0:53:26.180<br>
你自己回去算一下，到底有沒有算錯<br>
<br>
0:53:26.180,0:53:31.580<br>
根據這個定義，它算出來的 smoothness = 3<br>
<br>
0:53:31.580,0:53:34.940<br>
所以，這個值越小<br>
<br>
0:53:34.940,0:53:37.540<br>
它越 smooth<br>
<br>
0:53:37.540,0:53:40.120<br>
所以，你會希望你得出來的 label<br>
<br>
0:53:40.120,0:53:43.320<br>
根據這個 smoothness 的定義<br>
<br>
0:53:43.320,0:53:47.680<br>
它算出來，越小越好<br>
<br>
0:53:47.680,0:53:51.100<br>
其實，這邊可以很快告訴大家一件事情，這一項<br>
<br>
0:53:51.100,0:53:54.020<br>
可以稍微整理一下<br>
<br>
0:53:54.020,0:53:59.940<br>
寫成一個比較簡潔的式子，怎麼寫呢？<br>
<br>
0:53:59.940,0:54:03.940<br>
我們把 y 串成一個 vector<br>
<br>
0:54:03.940,0:54:07.640<br>
現在，y 是包括 labeled data，也包括 unlabeled data<br>
<br>
0:54:07.640,0:54:10.240<br>
每一筆  labeled data 跟 unlabeled data<br>
<br>
0:54:10.240,0:54:11.460<br>
都出一個值給你<br>
<br>
0:54:11.460,0:54:14.580<br>
所以，你現在有 (R + U) 個 dimension<br>
<br>
0:54:14.580,0:54:19.620<br>
串成一個 vector，寫成 y，我們粗體字來表示一個 vector<br>
<br>
0:54:19.620,0:54:21.360<br>
如果你這樣寫的話<br>
<br>
0:54:21.360,0:54:23.980<br>
這一個式子，可以寫成<br>
<br>
0:54:23.980,0:54:28.080<br>
y 這個 vector 的 transpose，乘上一個 matrix，叫做 L<br>
<br>
0:54:28.080,0:54:31.020<br>
再乘上 y<br>
<br>
0:54:31.020,0:54:33.520<br>
那這個 L，它是一個<br>
<br>
0:54:33.520,0:54:38.340<br>
因為 y 的 dimension 是 (R + U)<br>
<br>
0:54:38.340,0:54:42.320<br>
所以，這個 L 是一個 (R +U) * (R + U) 的 matrix<br>
<br>
0:54:42.320,0:54:45.300<br>
那這個 L，它是有名字的<br>
<br>
0:54:45.300,0:54:48.300<br>
它叫 Graph Laplacian，你可能有聽過這個名字<br>
<br>
0:54:48.300,0:54:50.700<br>
Graph Laplacian 就是指這個 L<br>
<br>
0:54:50.700,0:54:52.740<br>
這個是它的名字<br>
<br>
0:54:52.740,0:54:54.340<br>
那這個 L 的定義是甚麼呢？<br>
<br>
0:54:54.340,0:54:59.340<br>
它寫成兩個 matrix 的相減，就是 D - W<br>
<br>
0:54:59.340,0:55:03.260<br>
我們現在看 W 是什麼，W 就是<br>
<br>
0:55:04.100,0:55:08.260<br>
你把這些 data point 阿<br>
<br>
0:55:08.260,0:55:12.760<br>
兩兩之間 weight 的 connection 的關係建成一個 matrix<br>
<br>
0:55:12.760,0:55:15.480<br>
就是 W，這邊的<br>
<br>
0:55:15.480,0:55:19.460<br>
4 個 row 跟 4 個 column，分別就代表了<br>
<br>
0:55:19.460,0:55:21.500<br>
data x^1 到 data x^4<br>
<br>
0:55:21.500,0:55:23.800<br>
也就是說，你看現在 x^1 跟 x^2 之間的<br>
<br>
0:55:23.800,0:55:26.360<br>
connection 的 weight 是 2<br>
<br>
0:55:26.360,0:55:29.640<br>
這個 (1, 2) 這邊就是 2<br>
<br>
0:55:29.640,0:55:31.700<br>
那 x^1 跟 x^3 的 connection 是 3<br>
<br>
0:55:31.700,0:55:34.420<br>
(1, 3) 這邊就是 3，以此類推<br>
<br>
0:55:34.420,0:55:37.380<br>
就建出一個 matrix, W<br>
<br>
0:55:37.380,0:55:39.880<br>
D 是甚麼呢？ D 是這樣的<br>
<br>
0:55:39.880,0:55:42.640<br>
你把 W 的每一個 row<br>
<br>
0:55:42.640,0:55:46.280<br>
每一個 row 合起來，你把第一個 row：2 + 3 合起來<br>
<br>
0:55:46.280,0:55:48.640<br>
放在 diagonal 的地方，變成 5<br>
<br>
0:55:48.640,0:55:53.380<br>
2 + 1 變成 3，3 + 1 + 1 變成 5，1 變成 1<br>
<br>
0:55:53.380,0:55:56.200<br>
然後，把這些合起來的值放在 diagonal 的地方就是 D<br>
<br>
0:55:56.200,0:55:58.900<br>
然後，你把 D - W 就得到 Laplacian<br>
<br>
0:55:58.900,0:56:01.920<br>
你再把它放在這邊<br>
<br>
0:56:01.920,0:56:04.560<br>
這個左邊的式子，就會等於右邊的式子<br>
<br>
0:56:04.560,0:56:07.360<br>
你可能沒有辦法一下子看出來說<br>
<br>
0:56:07.360,0:56:09.960<br>
為什麼左邊的式子等於右邊的式子<br>
<br>
0:56:09.960,0:56:13.080<br>
這個證明其實很無聊<br>
<br>
0:56:13.080,0:56:14.800<br>
正確講你也不會覺得特別有趣<br>
<br>
0:56:14.800,0:56:16.560<br>
你就回去，把這個東西展開<br>
<br>
0:56:16.560,0:56:19.000<br>
你就知道左邊其實是等於右邊的<br>
<br>
0:56:20.780,0:56:24.200<br>
所以，現在我們知道這件事情了<br>
<br>
0:56:24.200,0:56:26.720<br>
我們可以用這一個式子<br>
<br>
0:56:26.720,0:56:33.220<br>
來 evaluate 說，我們現在得到的 label有多 smooth<br>
<br>
0:56:33.220,0:56:34.720<br>
那在這個式子裡面<br>
<br>
0:56:34.720,0:56:41.360<br>
我們會看到有 y，那這個 y 是 label<br>
<br>
0:56:41.360,0:56:45.580<br>
這個 label 的值，你的 neural network output 的值是<br>
<br>
0:56:45.580,0:56:49.080<br>
取決於你的 network 的 parameter，所以這一項<br>
<br>
0:56:49.080,0:56:51.720<br>
其實是 network 的 dependent<br>
<br>
0:56:51.720,0:56:55.640<br>
所以，你要把 graph 的 information<br>
<br>
0:56:55.640,0:56:57.600<br>
考慮到 neural network 的 training 的時候<br>
<br>
0:56:57.600,0:56:59.940<br>
你要做的事情，其實就是在<br>
<br>
0:56:59.940,0:57:02.300<br>
原來的 loss function 裡面，加一項<br>
<br>
0:57:02.300,0:57:05.420<br>
你原來的 loss function 是考慮 cross entropy 之類的<br>
<br>
0:57:05.420,0:57:09.960<br>
你就加另外一項，你加這一項是 smoothness 的值<br>
<br>
0:57:09.960,0:57:13.860<br>
乘上某一個你想要調的參數，λ<br>
<br>
0:57:13.860,0:57:17.940<br>
那這後面這一項，它其實就象徵了一個<br>
<br>
0:57:17.940,0:57:22.160<br>
Regularization，它就像是一個 Regularization 的 term<br>
<br>
0:57:22.160,0:57:26.860<br>
你不只要調你的參數讓你的那些 labeled data 的<br>
<br>
0:57:26.860,0:57:29.860<br>
你的 neural network 在那些 labeled data 的 output<br>
<br>
0:57:29.860,0:57:31.840<br>
跟真正的 label，越接近越好<br>
<br>
0:57:31.840,0:57:34.160<br>
你同時還要做到說<br>
<br>
0:57:34.160,0:57:36.420<br>
你 output 的這些 label<br>
<br>
0:57:36.420,0:57:39.040<br>
不管是在 labeled data 或 unlabeled data 上面<br>
<br>
0:57:39.040,0:57:41.400<br>
它都符合 Smoothness Assumption 的假設<br>
<br>
0:57:41.400,0:57:44.300<br>
Smoothness Assumption 的假設是由這個 x<br>
<br>
0:57:44.300,0:57:47.600<br>
所衡量出來的，所以你同時 minimize 這項<br>
<br>
0:57:47.600,0:57:50.840<br>
也要同時 minimize 這項，你可能會說，這件事怎麼做？<br>
<br>
0:57:50.840,0:57:53.320<br>
這件事沒有甚麼好講的<br>
<br>
0:57:53.320,0:57:55.480<br>
你就算一下它的 gradient<br>
<br>
0:57:55.480,0:58:00.300<br>
然後，做 gradient descent 就可以了<br>
<br>
0:58:00.300,0:58:03.680<br>
那其實，你也要算 smoothness 時候，不一定要算在<br>
<br>
0:58:03.680,0:58:06.840<br>
不一定要算在 output 的地方<br>
<br>
0:58:06.840,0:58:09.660<br>
不一定要算在 output 的地方，如果你今天是一個<br>
<br>
0:58:09.660,0:58:11.080<br>
deep neural network 的話<br>
<br>
0:58:11.080,0:58:14.520<br>
你可以把你的 smoothness 放在 network 的任何地方<br>
<br>
0:58:14.520,0:58:16.720<br>
你可以假設你的 output 是 smooth<br>
<br>
0:58:16.720,0:58:18.480<br>
你也可以同時說<br>
<br>
0:58:18.480,0:58:20.780<br>
我把某一個 hidden layer 接出來<br>
<br>
0:58:20.780,0:58:23.880<br>
再乘上一些別的 transform<br>
<br>
0:58:23.880,0:58:25.480<br>
它也要 smooth<br>
<br>
0:58:25.480,0:58:28.320<br>
你也可以說，每一個 hidden layer 的 output<br>
<br>
0:58:28.320,0:58:30.960<br>
都要是 smooth 的，都可以<br>
<br>
0:58:30.960,0:58:34.040<br>
你可以同時把這些 smooth <br>
通通都加到 neural network 上面去<br>
<br>
0:58:34.040,0:58:37.460<br>
最後呢，這個<br>
<br>
0:58:37.460,0:58:40.620<br>
最後的方法是 Better Representation<br>
<br>
0:58:40.620,0:58:44.040<br>
這個方法的精神是：是去蕪存菁、化繁為簡<br>
<br>
0:58:44.040,0:58:47.080<br>
這一部分我們會等到 unsupervised learning 的時候再講<br>
<br>
0:58:47.080,0:58:50.320<br>
那它的精神是這樣子的<br>
<br>
0:58:50.320,0:58:53.680<br>
我們觀察到的世界其實是比較複雜的<br>
<br>
0:58:53.680,0:58:55.660<br>
我們在我們觀察到的世界背後<br>
<br>
0:58:55.660,0:58:58.400<br>
其實有一些比較簡單的 vector<br>
<br>
0:58:58.400,0:59:03.600<br>
比較簡單的東西，在操控我們這個複雜的世界<br>
<br>
0:59:03.600,0:59:05.880<br>
所以，你只要能夠看透這個世界的假象<br>
<br>
0:59:05.880,0:59:09.620<br>
支持它的核心的話，你就可以讓 training 變得比較容易<br>
<br>
0:59:09.620,0:59:14.600<br>
舉例來說，這個圖是出自神鵰俠侶<br>
<br>
0:59:14.600,0:59:16.380<br>
這個大家知道什麼意思嗎<br>
<br>
0:59:16.380,0:59:19.320<br>
這個是楊過，他手上拿了一個剪刀<br>
<br>
0:59:19.320,0:59:21.900<br>
這個是樊一翁，這個是他的鬍子<br>
<br>
0:59:21.900,0:59:26.140<br>
然後，楊過跟樊一翁打的時候<br>
<br>
0:59:26.140,0:59:28.380<br>
他說我可以在三招之內，就剪掉你的鬍子<br>
<br>
0:59:28.380,0:59:30.980<br>
大家都不相信，楊過後來就真的在三招之內<br>
<br>
0:59:30.980,0:59:33.500<br>
剪掉他的鬍子，為甚麼呢？因為<br>
<br>
0:59:33.500,0:59:36.400<br>
楊過觀察到說，鬍子只是一個假相<br>
<br>
0:59:36.400,0:59:39.440<br>
雖然，鬍子的變化是比較複雜的<br>
<br>
0:59:39.440,0:59:41.200<br>
但是，鬍子是受到頭所操控<br>
<br>
0:59:41.200,0:59:44.020<br>
頭的變化是有限的，所以他看透這一件事情<br>
<br>
0:59:44.020,0:59:46.080<br>
以後，他就可以把鬍子剪掉<br>
<br>
0:59:46.080,0:59:48.420<br>
所以說，樊一翁他的鬍子就是 observation<br>
<br>
0:59:48.420,0:59:52.800<br>
而他的頭，就是你要找的 Better Representation<br>
<br>
0:59:52.800,0:59:54.980<br>
那就是我們下一堂課要講的東西<br>
<br>
0:59:54.980,0:59:56.600<br>
我們在這邊休息10分鐘<br>
<br>
0:59:56.600,0:59:59.200<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
