<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.860<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:03.120,0:00:05.560<br>
好，我們來講 Logistic Regression<br>
<br>
0:00:05.560,0:00:08.020<br>
在 Logistic Regression 裡面呢<br>
<br>
0:00:08.220,0:00:11.480<br>
我們在上一份投影片裡面，我們都已經知道說<br>
<br>
0:00:11.480,0:00:14.740<br>
我們要找的東西呢，是一個機率<br>
<br>
0:00:14.740,0:00:16.720<br>
是一個 Posterior probability<br>
<br>
0:00:16.720,0:00:20.640<br>
如果這個 Posterior probability > 0.5 的話<br>
<br>
0:00:20.640,0:00:23.900<br>
就 output C1，否則呢，就 output C2<br>
<br>
0:00:23.900,0:00:26.360<br>
我們知道這個 posterior probability<br>
<br>
0:00:26.360,0:00:29.900<br>
假設你覺得你想要用 Gaussian 的話<br>
<br>
0:00:29.900,0:00:32.080<br>
其實很多其他的 probability，化簡以後<br>
<br>
0:00:32.080,0:00:34.820<br>
也都可以得到同樣的結果<br>
<br>
0:00:34.920,0:00:37.260<br>
假設你覺得你想要用 Gaussian 的話<br>
<br>
0:00:37.260,0:00:39.600<br>
那你就說，你可以說這個<br>
<br>
0:00:39.600,0:00:44.340<br>
posterior probability 就是 σ(z)<br>
<br>
0:00:44.340,0:00:48.580<br>
它的 function 長的是右邊這個樣子<br>
<br>
0:00:48.580,0:00:51.940<br>
那這個 z 呢，是 w 跟 x 的 inner product 加上 b<br>
<br>
0:00:52.110,0:00:54.720<br>
所謂 w 跟 x 的 inner product 呢，是說<br>
<br>
0:00:54.720,0:00:57.860<br>
這個 w，它是一個 vector<br>
<br>
0:00:57.860,0:01:02.100<br>
它的每一個 dimension 我們就用下標 i 來表示<br>
<br>
0:01:02.440,0:01:04.560<br>
那這個 w 呢，是一個 vector<br>
<br>
0:01:04.560,0:01:09.520<br>
每一個 x，都有一個對應的 w 下標 i<br>
<br>
0:01:09.520,0:01:11.920<br>
你把所有的 xi 跟 (w 下標 i) 相乘<br>
<br>
0:01:12.220,0:01:14.700<br>
summation 起來再加上 b，就得到 z<br>
<br>
0:01:14.700,0:01:17.040<br>
代進這個 sigmoid function<br>
<br>
0:01:17.210,0:01:18.660<br>
就得到機率<br>
<br>
0:01:18.660,0:01:22.600<br>
所以，我們的 function set 是長這樣子<br>
<br>
0:01:22.600,0:01:26.720<br>
我們的 function set,  f(下標 w, b) (x)<br>
<br>
0:01:26.760,0:01:28.540<br>
這邊加下標 w, b 的意思就是說<br>
<br>
0:01:28.540,0:01:34.680<br>
我們現在的這個 function set 是受 w 和 b 所控制的<br>
<br>
0:01:34.680,0:01:38.500<br>
就是你可以選不同的 w 和 b，你就會得到不同 function<br>
<br>
0:01:38.500,0:01:41.920<br>
所有 w 和 b 可以產生的 function 集合起來<br>
<br>
0:01:42.140,0:01:44.500<br>
就是一個 function set，那這一項<br>
<br>
0:01:44.500,0:01:48.980<br>
它的涵意呢，就是一個 posterior probability<br>
<br>
0:01:48.980,0:01:52.620<br>
given x，它是屬於 C1 的機率<br>
<br>
0:01:53.460,0:01:56.180<br>
如果我們用圖像化的方式來表它的話呢<br>
<br>
0:01:56.180,0:01:57.700<br>
它長這樣<br>
<br>
0:01:57.700,0:02:03.520<br>
我們的 function 裡面，有兩組參數<br>
<br>
0:02:03.520,0:02:06.740<br>
一組是 w，我們稱之為 weights<br>
<br>
0:02:06.740,0:02:08.160<br>
weights 呢，有一整排<br>
<br>
0:02:08.160,0:02:10.620<br>
然後有一個 constant b<br>
<br>
0:02:10.620,0:02:12.580<br>
這個我們稱之為 bias<br>
<br>
0:02:12.580,0:02:15.060<br>
然後有一個 sigmoid function<br>
<br>
0:02:16.060,0:02:23.180<br>
如果我們今天的 input 是 x1, xi 到 xI<br>
<br>
0:02:23.180,0:02:28.820<br>
你就把 x1, xi, xI 分別乘上 w1, wi, wI<br>
<br>
0:02:28.820,0:02:32.900<br>
然後再加上 b 呢，你就得到 z，這個 z<br>
<br>
0:02:34.760,0:02:39.700<br>
我發現我寫錯了一個地方，後面這邊呢，應該是要 + b<br>
<br>
0:02:39.860,0:02:46.560<br>
你把  x1*w1 + xi*wi 加到 xI*wI，再加上 b，就得到 z<br>
<br>
0:02:46.560,0:02:49.340<br>
z 通過我們剛才得到的 sigmoid function<br>
<br>
0:02:49.380,0:02:54.460<br>
它 output 的值，就是機率，就是 posterior probability<br>
<br>
0:02:54.650,0:02:59.500<br>
這個是整個模型，長這個樣子<br>
<br>
0:02:59.730,0:03:03.940<br>
這件事呢，叫做 Logistic Regression<br>
<br>
0:03:03.940,0:03:06.920<br>
那我們可以把 Logistic Regression<br>
<br>
0:03:06.920,0:03:10.920<br>
跟我們在第一堂課就講的 Linear Regression 做一下比較<br>
<br>
0:03:10.940,0:03:14.900<br>
Logistic Regression，把每一個 feature 乘上一個 w<br>
<br>
0:03:14.900,0:03:18.080<br>
summation 起來再加上 b，再通過 sigmoid function<br>
<br>
0:03:18.080,0:03:20.120<br>
當作 function 的 output<br>
<br>
0:03:20.200,0:03:22.580<br>
那它的 output 因為有通過 sigmoid function<br>
<br>
0:03:22.580,0:03:25.700<br>
所以，一定是界於 0~1 之間<br>
<br>
0:03:25.800,0:03:30.260<br>
那 Linear Regression 呢？它就把 feature * w 再加上 b<br>
<br>
0:03:30.840,0:03:32.540<br>
它沒有通過 sigmoid function<br>
<br>
0:03:32.540,0:03:36.040<br>
所以，它的 output 就可以是任何值<br>
<br>
0:03:36.040,0:03:39.260<br>
可以是正的，可以是負的，負無窮大到正無窮大<br>
<br>
0:03:39.260,0:03:42.960<br>
那等一下呢，我們說 machine learning 就是 3 個 step<br>
<br>
0:03:42.960,0:03:44.320<br>
等一下我們會一個一個 step<br>
<br>
0:03:44.520,0:03:49.600<br>
比較 Logistic Regression 跟 Linear Regression 的差別<br>
<br>
0:03:50.340,0:03:55.200<br>
接下來呢，我們要決定一個 function 的好壞<br>
<br>
0:03:55.500,0:03:57.980<br>
那我們的 training data 呢<br>
<br>
0:03:57.980,0:04:00.340<br>
因為我們現在要做的是 Classification<br>
<br>
0:04:00.580,0:04:04.860<br>
所以我們的 training data，就是假設有 N 筆 training data<br>
<br>
0:04:04.960,0:04:08.140<br>
那每一筆 training data，你都要標說它屬於哪一個 class<br>
<br>
0:04:08.140,0:04:12.100<br>
比如說，x^1 屬於 C1，x^2 屬於 C1，x^3 屬於 C2<br>
<br>
0:04:12.100,0:04:15.200<br>
x^N 屬於 C1......等等，那接下來呢<br>
<br>
0:04:15.360,0:04:20.400<br>
我們假設這筆 training data 是從<br>
<br>
0:04:20.600,0:04:26.000<br>
我們的 function 所定義出來的這個 posterior probability<br>
<br>
0:04:26.000,0:04:29.180<br>
所產生的，就是這組 training data<br>
<br>
0:04:29.180,0:04:35.260<br>
是根據這個 posterior probability 所產生的<br>
<br>
0:04:35.880,0:04:39.940<br>
那給我們一個 w 和 b<br>
<br>
0:04:39.940,0:04:43.860<br>
我們就決定了這個 posterior probability<br>
<br>
0:04:43.860,0:04:47.760<br>
那我們就可以去計算，某一組 w 和 b<br>
<br>
0:04:47.760,0:04:52.360<br>
產生 N 筆 training data 的機率<br>
<br>
0:04:53.060,0:04:57.380<br>
某一組 w 和 b 產生 N 筆 training data 的機率怎麼算呢？<br>
<br>
0:04:57.880,0:05:01.700<br>
這個很容易，就是假設 x1 是屬於 C1<br>
<br>
0:05:01.700,0:05:06.140<br>
那它根據某一組 w 和 b，產生的機率就是<br>
<br>
0:05:06.140,0:05:08.240<br>
f(下標w, b) (x^1)<br>
<br>
0:05:08.300,0:05:12.680<br>
假設 x2 是屬於 C1，那它被產生的機率就是<br>
<br>
0:05:12.680,0:05:13.860<br>
f(下標w, b) (x^2)<br>
<br>
0:05:14.040,0:05:16.620<br>
假設 x3 是屬於 C2<br>
<br>
0:05:16.620,0:05:20.080<br>
我們知道 x3 如果屬於 C1 的機率<br>
<br>
0:05:20.080,0:05:23.720<br>
就是 f(下標w, b) (x^3)，因為我們這邊算的是 C1 的機率<br>
<br>
0:05:23.820,0:05:26.420<br>
那這個 x3 屬於 C2，所以他的機率就是<br>
<br>
0:05:26.420,0:05:30.880<br>
1 - f(下標w, b) (x^3)，以此類推<br>
<br>
0:05:30.880,0:05:36.040<br>
那最有可能的參數 w 跟 b<br>
<br>
0:05:36.040,0:05:39.060<br>
我們覺得最好的參數 w 跟 b<br>
<br>
0:05:39.060,0:05:43.820<br>
就是那個有最大的可能性、最大的機率<br>
<br>
0:05:43.820,0:05:49.360<br>
可以產生這個 training data 的 w 跟 b<br>
<br>
0:05:49.360,0:05:51.960<br>
我們把它叫做 w* 跟 b*<br>
<br>
0:05:51.960,0:05:59.420<br>
w* 跟 b* 就是那個可以最大化這一個機率的 w 跟 b<br>
<br>
0:06:00.740,0:06:05.060<br>
那我們在這邊，做一個數學式上的轉換<br>
<br>
0:06:05.120,0:06:07.500<br>
我們原來是要找一組 w 跟 b<br>
<br>
0:06:07.500,0:06:11.640<br>
最大化 L(w , b)<br>
<br>
0:06:11.640,0:06:13.520<br>
最大化這個 function<br>
<br>
0:06:13.520,0:06:16.240<br>
但是，這件事情等同於呢<br>
<br>
0:06:16.240,0:06:18.020<br>
我們找一個 w 跟 b<br>
<br>
0:06:18.020,0:06:20.940<br>
minimize 負 ln 這個 function<br>
<br>
0:06:20.980,0:06:24.440<br>
我們知道取 ln，它的這個 order 是不會變的<br>
<br>
0:06:24.500,0:06:29.700<br>
加上一個負號，就從找最大的，變成找最小的<br>
<br>
0:06:29.800,0:06:32.160<br>
所以，我們就是要找一組 w 跟 b<br>
<br>
0:06:32.160,0:06:35.760<br>
最小化 -ln L(w, b)<br>
<br>
0:06:35.760,0:06:39.340<br>
這個可以讓計算變得容易一點，左式跟右式是一樣的<br>
<br>
0:06:39.340,0:06:41.880<br>
根據左式跟右式找出來的 w 跟 b 呢<br>
<br>
0:06:41.880,0:06:45.900<br>
w* 跟 b* 呢，是同個 w* 跟 b*<br>
<br>
0:06:46.440,0:06:50.320<br>
那 -ln (這一項) 怎麼做呢？<br>
<br>
0:06:50.320,0:06:54.580<br>
你知道取 -ln 的好處就是，本來相乘，現在變成相加<br>
<br>
0:06:54.580,0:06:59.700<br>
然後，你就把它展開，所以這一項就是 -ln f(x^1)<br>
<br>
0:06:59.700,0:07:03.360<br>
-ln f(x^2), -ln (1 - f(x^3))<br>
<br>
0:07:03.560,0:07:05.180<br>
以此類推<br>
<br>
0:07:05.180,0:07:09.360<br>
那這件事情讓你，寫式子有點難寫<br>
<br>
0:07:09.360,0:07:12.520<br>
就是你沒有辦法寫一個 summation over<br>
<br>
0:07:12.520,0:07:16.160<br>
因為對不同的 x，如果屬於不同 class<br>
<br>
0:07:16.160,0:07:19.260<br>
我們就要用不同的方法來處理它<br>
<br>
0:07:19.260,0:07:22.020<br>
而且你沒有辦法 summation over x，那怎麼辦呢？<br>
<br>
0:07:22.020,0:07:25.060<br>
我們做一個符號上的轉換<br>
<br>
0:07:25.060,0:07:29.720<br>
我們說，如果某一個 x 它屬於 class 1<br>
<br>
0:07:29.720,0:07:32.800<br>
我們就說它的 target 是 1<br>
<br>
0:07:32.800,0:07:36.560<br>
如果它屬於 class 2，我們就說它的 target 是 0<br>
<br>
0:07:36.560,0:07:39.560<br>
我們之前在做 Linear Regression 的時候<br>
<br>
0:07:39.720,0:07:43.420<br>
每一個 x，它都有一個對應的 y\head，對不對？<br>
<br>
0:07:43.420,0:07:45.540<br>
然後那個對應的 y\head 是一個 real number<br>
<br>
0:07:45.540,0:07:49.420<br>
在這邊呢，每一個 x 也都有一個對應的 y\head<br>
<br>
0:07:49.540,0:07:52.260<br>
這個對應的 y\head，它的 number 就代表說<br>
<br>
0:07:52.260,0:07:54.220<br>
現在這個 x，屬於哪一個 class<br>
<br>
0:07:54.310,0:07:56.310<br>
如果屬於 class 1<br>
<br>
0:07:57.200,0:08:02.280<br>
欸，我怎麼會犯這麼弱智的錯誤<br>
<br>
0:08:02.800,0:08:05.600<br>
大家有發現嗎？這個投影片上，有一個錯啊<br>
<br>
0:08:06.880,0:08:09.840<br>
對，它應該是 1, 1 , 0，麼會犯這麼弱智的錯誤<br>
<br>
0:08:09.840,0:08:14.040<br>
這個屬於 class 1 就應該是 1，所以這個應該是<br>
<br>
0:08:14.040,0:08:18.920<br>
1, 1, 0 這樣，沒關係，你無視這邊<br>
<br>
0:08:18.920,0:08:20.380<br>
你就看這裡就好<br>
<br>
0:08:20.380,0:08:23.200<br>
屬於 class 1 就是 1，屬於 class 2 就是 0<br>
<br>
0:08:23.640,0:08:30.060<br>
如果你做這件事的話<br>
<br>
0:08:30.060,0:08:35.160<br>
那你就可以把這邊的每一個式子<br>
<br>
0:08:35.160,0:08:36.600<br>
都寫成這樣<br>
<br>
0:08:36.600,0:08:41.140<br>
這看起來有一點複雜，但你仔細算一下就會發現說<br>
<br>
0:08:41.140,0:08:43.740<br>
左邊和右邊，是相等的<br>
<br>
0:08:43.740,0:08:47.660<br>
每一個 -ln f(x)，你都可以寫成<br>
<br>
0:08:47.660,0:08:51.860<br>
負的中括號、負的括號<br>
<br>
0:08:51.860,0:08:58.260<br>
它的 y1\head、它的 y\head 乘上 ln f(x)<br>
<br>
0:08:58.260,0:09:02.680<br>
加上 (1 - y\head) * ln(1 - f(x))<br>
<br>
0:09:02.680,0:09:05.600<br>
你就實際上算一下，比如說<br>
<br>
0:09:05.600,0:09:07.840<br>
x1, x2 都是屬於 C1<br>
<br>
0:09:07.840,0:09:10.880<br>
所以呢，它對應的 y\head 是 1<br>
<br>
0:09:10.900,0:09:14.200<br>
所以這個 y1\head 跟 y2\head 是 1<br>
<br>
0:09:14.200,0:09:17.000<br>
(1 -  y1\head) 跟 (1 - y2\head) 就是 0<br>
<br>
0:09:17.000,0:09:21.460<br>
0 的話呢，它乘上後面那一項，你就不要管它，把它拿掉<br>
<br>
0:09:21.520,0:09:24.540<br>
所以，你會發現它等於它，它等於它這樣<br>
<br>
0:09:24.540,0:09:27.680<br>
因為空間的關係，我就把 w 跟 b 省略掉了<br>
<br>
0:09:27.680,0:09:30.520<br>
有時候放 w 跟 b，只是為了強調說<br>
<br>
0:09:30.520,0:09:33.700<br>
這個 f 是 w 跟 b 的 function<br>
<br>
0:09:33.880,0:09:37.100<br>
那因為這個寫不下，所以把它省略掉<br>
<br>
0:09:37.100,0:09:39.440<br>
好，那這個 y3 呢？<br>
<br>
0:09:39.840,0:09:42.740<br>
這個 x3 它屬於 C2，C2 是 0<br>
<br>
0:09:42.740,0:09:46.800<br>
所以 y3\head 是 0，(1 - y3\head) 就是 1<br>
<br>
0:09:46.800,0:09:49.160<br>
那前面這個部分可以拿掉，你會發現<br>
<br>
0:09:49.500,0:09:52.860<br>
右邊這個也是等於左邊這個<br>
<br>
0:09:52.860,0:09:55.500<br>
有了這些以後，我們把<br>
<br>
0:09:55.800,0:10:02.820<br>
這個 likelihood 的 function，取 -ln<br>
<br>
0:10:03.440,0:10:06.200<br>
然後呢，再假設說<br>
<br>
0:10:06.220,0:10:08.200<br>
class 1 就是 1，class 2 就是 0 以後<br>
<br>
0:10:08.200,0:10:12.780<br>
我們就可以把，我們要去 minimize 的對象<br>
<br>
0:10:12.780,0:10:14.040<br>
寫成一個 function<br>
<br>
0:10:14.040,0:10:16.040<br>
我們就可以把我們要去 minimize 的對象<br>
<br>
0:10:16.060,0:10:18.000<br>
寫成 summation over N<br>
<br>
0:10:18.000,0:10:28.380<br>
- [ y^n\head * ln f(x^n) + (1-y^n\head) * ln (1-f(x^n))]<br>
<br>
0:10:28.380,0:10:33.400<br>
那其實 summation over 的這一項阿<br>
<br>
0:10:33.400,0:10:38.140<br>
這個 Σ 後面的這一整項阿<br>
<br>
0:10:38.140,0:10:43.880<br>
它其實是兩個 Bernoulli distribution 的 Cross entropy<br>
<br>
0:10:45.400,0:10:48.700<br>
這一項其實是一個 Cross entropy<br>
<br>
0:10:48.700,0:10:52.180<br>
所以，等一下我們就會說它是 Cross entropy<br>
<br>
0:10:52.180,0:10:57.580<br>
雖然它的來源跟 information theory 沒有太直接的關係<br>
<br>
0:10:57.580,0:11:02.280<br>
但是，我們剛才看過它推導的過程<br>
<br>
0:11:02.280,0:11:04.920<br>
但是，如果你把<br>
<br>
0:11:05.800,0:11:08.460<br>
你假設有兩個 distribution，p 跟 q<br>
<br>
0:11:08.460,0:11:13.660<br>
這個 p 的 distribution，它是說 p(x =1) = y^n\head<br>
<br>
0:11:13.660,0:11:16.560<br>
p(x=0) =1 – y^n\head<br>
<br>
0:11:16.820,0:11:20.700<br>
q 的 distribution，它 q(x =1) 是 f(x^n)<br>
<br>
0:11:20.740,0:11:25.020<br>
q(x =0) 是 1 - f(x^n)<br>
<br>
0:11:25.020,0:11:28.820<br>
那你把這兩個 distribution 算 cross entropy<br>
<br>
0:11:29.640,0:11:32.120<br>
如果你不知道甚麼是 cross entropy 的話，沒有關係<br>
<br>
0:11:32.120,0:11:35.400<br>
反正就是，代一個式子<br>
<br>
0:11:35.400,0:11:39.520<br>
summation over 所有的 x (-Σ p(x)*ln(q(x))<br>
<br>
0:11:39.520,0:11:42.780<br>
前面有個負號，這個就是 cross entropy<br>
<br>
0:11:42.780,0:11:47.820<br>
如果你把這兩個 distribution，<br>
算他們之間的 cross entropy<br>
<br>
0:11:47.820,0:11:53.080<br>
cross entropy 的涵義是這兩個 distribution 有多接近<br>
<br>
0:11:53.080,0:11:57.060<br>
如果今天這兩個 distribution 一模一樣的話<br>
<br>
0:11:57.240,0:12:00.420<br>
那他們算出來的 cross entropy 就是 0<br>
<br>
0:12:00.420,0:12:03.680<br>
所以，你把這兩個 distribution 算一下 cross entropy<br>
<br>
0:12:03.680,0:12:08.040<br>
你把 y^n\head 乘上 ln(f(x^n))<br>
<br>
0:12:08.040,0:12:11.240<br>
(1 - y^n\head) 乘上 ln (1 - f(x^n))<br>
<br>
0:12:11.240,0:12:13.560<br>
你得到的，就是這項<br>
<br>
0:12:13.560,0:12:17.680<br>
如果你有修過 information theory 的話呢<br>
<br>
0:12:17.680,0:12:21.960<br>
它這個式子寫出來，跟 cross entropy 是一樣的<br>
<br>
0:12:22.980,0:12:26.480<br>
所以在 Logistic Regression 裡面<br>
<br>
0:12:26.480,0:12:30.100<br>
我們怎麼定義一個 function，它的好壞呢？<br>
<br>
0:12:30.100,0:12:31.720<br>
我們定義的方式是這樣<br>
<br>
0:12:31.720,0:12:36.780<br>
有一堆 training data，我們有(x^n, y^n\head)<br>
<br>
0:12:36.840,0:12:39.520<br>
有這樣的 pair，如果屬於 class 1 的話呢<br>
<br>
0:12:39.520,0:12:43.740<br>
y^n\head 就等於 1，如果屬於 class 2 的話，<br>
y^n\head 就等於 0<br>
<br>
0:12:43.900,0:12:49.640<br>
那我們定義的 loss function，我們要去 minimize 的對象<br>
<br>
0:12:49.640,0:12:54.280<br>
是所有的 example<br>
<br>
0:12:54.340,0:12:57.380<br>
它的 cross entropy 的總和<br>
<br>
0:12:57.380,0:13:02.300<br>
也就是說，假設你把 f(x^n) <br>
當作一個 Bernoulli distribution<br>
<br>
0:13:02.300,0:13:05.260<br>
把 y^n\head 當作另一個 Bernoulli distribution<br>
<br>
0:13:05.360,0:13:09.960<br>
它們的 cross entropy，你把它算出來<br>
<br>
0:13:09.960,0:13:13.860<br>
這個東西，是我們要去 minimize 的對象<br>
<br>
0:13:13.860,0:13:16.340<br>
所以，就直觀來講，我們要做的事情是<br>
<br>
0:13:16.380,0:13:21.880<br>
我們希望 function 的 output 跟它的 target<br>
<br>
0:13:21.880,0:13:24.860<br>
如果你都把它看作是 Bernoulli distribution 的話<br>
<br>
0:13:24.860,0:13:29.160<br>
這兩個 Bernoulli distribution，他們越接近越好<br>
<br>
0:13:30.120,0:13:32.840<br>
如果我們比較一下 Linear Regression 的話<br>
<br>
0:13:32.840,0:13:37.620<br>
Linear Regression 這邊，這個你大概很困惑啦<br>
<br>
0:13:37.620,0:13:39.140<br>
如果你今天是第一次聽到的話<br>
<br>
0:13:39.140,0:13:41.520<br>
你應該聽得一頭霧水，想說這個<br>
<br>
0:13:41.520,0:13:43.940<br>
哇，這個這麼複雜，到底是怎麼來的<br>
<br>
0:13:43.940,0:13:48.820<br>
如果你是看 Linear Regression 的話，這個很簡單<br>
<br>
0:13:48.820,0:13:53.760<br>
減掉它的 target，y^n\head的平方<br>
<br>
0:13:53.880,0:13:56.380<br>
就是我們要去 minimize 的對象<br>
<br>
0:13:56.380,0:13:58.180<br>
這個，比較單純<br>
<br>
0:13:58.260,0:14:00.420<br>
這個，不知道怎麼來的<br>
<br>
0:14:00.420,0:14:02.540<br>
因為你可能就會有一個想法說<br>
<br>
0:14:02.540,0:14:07.340<br>
為甚麼在 Logistic Regression 裡面<br>
<br>
0:14:07.440,0:14:10.780<br>
我們不跟 Linear Regression 一樣<br>
<br>
0:14:10.780,0:14:13.740<br>
用 square error 就好了呢？<br>
<br>
0:14:13.740,0:14:16.520<br>
這邊其實也可以用 square error 阿<br>
<br>
0:14:16.520,0:14:20.160<br>
沒有甚麼理由，你不能用 square error 不是嗎？<br>
<br>
0:14:20.160,0:14:24.180<br>
對不對，因為你完全可以算說<br>
<br>
0:14:24.180,0:14:28.980<br>
這個 f(x^n) 跟 (y^n\head) 的 square error<br>
<br>
0:14:28.980,0:14:33.520<br>
你就把這個 f(x^n) 跟 (y^n\head) 代到右邊去<br>
<br>
0:14:33.520,0:14:36.820<br>
你一樣可以定一個 loss function<br>
<br>
0:14:36.820,0:14:39.540<br>
這個 loss function 聽起來也是頗合理的<br>
<br>
0:14:39.540,0:14:43.400<br>
為甚麼不這麼做呢？<br>
<br>
0:14:43.400,0:14:47.420<br>
等一下，我們會試著給大家一點解釋<br>
<br>
0:14:48.920,0:14:54.180<br>
那到目前為止呢，這個東西，反正就是很複雜<br>
<br>
0:14:54.260,0:14:56.180<br>
你就先記得說，必須要這麼做<br>
<br>
0:14:57.160,0:15:00.140<br>
接下來呢，我們要做的事情就是<br>
<br>
0:15:00.140,0:15:03.380<br>
找一個最好的 function<br>
<br>
0:15:03.380,0:15:08.440<br>
就是要去 minimize，我們現在要 minimize 的對象<br>
<br>
0:15:08.440,0:15:12.100<br>
那怎麼做呢？你就用 Gradient Descent 就好了<br>
<br>
0:15:12.100,0:15:17.920<br>
很簡單，接下來都是一些數學式無聊的運算而已<br>
<br>
0:15:17.920,0:15:20.360<br>
我們就算<br>
<br>
0:15:20.360,0:15:25.360<br>
它對某一個 w 這個 vector，<br>
裡面的某一個 element 的微分<br>
<br>
0:15:25.360,0:15:30.020<br>
我們就算這個式子，對 wi 的微分就好<br>
<br>
0:15:30.020,0:15:33.480<br>
剩下的部分呢，其實就可以交給大家自己來做<br>
<br>
0:15:33.680,0:15:38.040<br>
那我們要算這個東西對 w 的偏微分<br>
<br>
0:15:38.040,0:15:39.860<br>
那我們只需要能夠算<br>
<br>
0:15:39.860,0:15:42.920<br>
ln(f(x^n) 對 w 的偏微分<br>
<br>
0:15:42.920,0:15:46.680<br>
跟 ln(1 - f(x^n)) 對 w 的偏微分就行了<br>
<br>
0:15:46.680,0:15:50.940<br>
那 ln(f(x^n) 對 w 的偏微分，怎麼算呢？<br>
<br>
0:15:50.940,0:15:54.980<br>
我們知道說，我們把這個 f 寫在下面<br>
<br>
0:15:54.980,0:15:56.160<br>
把 f 寫在下面<br>
<br>
0:15:56.160,0:16:01.740<br>
f 它受到 z 這個 variable 的影響<br>
<br>
0:16:01.740,0:16:03.420<br>
然後 z 這個 variable 呢？<br>
<br>
0:16:03.420,0:16:08.740<br>
是從 w, x, b 所產生的<br>
<br>
0:16:08.880,0:16:11.920<br>
所以，你就知道說，我們可以把<br>
<br>
0:16:11.920,0:16:14.380<br>
這個偏微分拆開<br>
<br>
0:16:14.380,0:16:18.360<br>
把 ∂(ln(f(x)) / ∂(wi) 拆解成呢<br>
<br>
0:16:18.360,0:16:22.000<br>
∂(ln(f(x)) / ∂(z)<br>
<br>
0:16:22.000,0:16:27.040<br>
乘上 ∂(z) / ∂(wi)<br>
<br>
0:16:28.240,0:16:31.140<br>
那這個 ∂(z) / ∂(wi) 是甚麼？<br>
<br>
0:16:31.140,0:16:34.600<br>
∂(z) / ∂(wi)，這個 z 的式子我寫在這邊了<br>
<br>
0:16:34.600,0:16:37.680<br>
只有一項是跟 wi 有關<br>
<br>
0:16:37.680,0:16:40.360<br>
只有 wi * xi 那一項是跟 wi 有關<br>
<br>
0:16:40.360,0:16:43.900<br>
所以 ∂(z) / ∂(wi) 就是 xi<br>
<br>
0:16:44.500,0:16:48.360<br>
那這一項是甚麼呢？這一項太簡單了<br>
<br>
0:16:48.360,0:16:51.020<br>
我們把 f(x) 換成 σ(z)<br>
<br>
0:16:51.020,0:16:52.740<br>
把這個換成 σ(z)<br>
<br>
0:16:52.740,0:16:54.640<br>
然後做一下微分<br>
<br>
0:16:54.640,0:16:59.920<br>
這個  ∂(ln σ(z)) / ∂(z) 做微分以後呢，1/σ(z)<br>
<br>
0:17:00.060,0:17:03.360<br>
然後，再算  ∂(σ(z))/σ(z)<br>
<br>
0:17:03.360,0:17:06.100<br>
那  ∂(σ(z))/σ(z) 是甚麼呢？<br>
<br>
0:17:06.100,0:17:08.260<br>
這個 σ(z) 是 sigmoid function<br>
<br>
0:17:08.260,0:17:11.600<br>
sigmoid function 的微分呢，其實你可以直接背起來<br>
<br>
0:17:11.600,0:17:14.560<br>
就是 σ(z) * (1 - σ(z))<br>
<br>
0:17:14.560,0:17:16.860<br>
如果你要看比較直觀的結果的話<br>
<br>
0:17:16.860,0:17:18.200<br>
你就把它的圖畫出來<br>
<br>
0:17:18.200,0:17:21.820<br>
σ(z) 這邊顏色可能有一點淡<br>
<br>
0:17:21.880,0:17:25.200<br>
是綠色這條線，σ(z) 是綠色這條線<br>
<br>
0:17:25.200,0:17:30.740<br>
橫軸是 z，那如果對 z 做偏微分的話<br>
<br>
0:17:30.740,0:17:36.440<br>
在接近頭跟尾的地方<br>
<br>
0:17:36.440,0:17:38.120<br>
它的斜率很小<br>
<br>
0:17:38.120,0:17:40.960<br>
所以對 z 做微分的時候，是接近於 0 的<br>
<br>
0:17:40.960,0:17:43.980<br>
在中間的地方，斜率最大<br>
<br>
0:17:43.980,0:17:45.320<br>
所以這個地方，斜率最大<br>
<br>
0:17:45.320,0:17:48.360<br>
所以把這一項對 z 做偏微分的話<br>
<br>
0:17:48.440,0:17:52.760<br>
你得到的結果是長得像這樣<br>
<br>
0:17:52.760,0:17:56.300<br>
那這一項，其實就是 σ(z) * (1 - σ(z))<br>
<br>
0:17:56.320,0:17:59.420<br>
那你就把 σ(z) 消掉<br>
<br>
0:17:59.420,0:18:06.060<br>
那你就得到說，這項就是 (1 - σ(z)) * xi<br>
<br>
0:18:06.060,0:18:09.320<br>
那 σ(z) 其實就是 f(x)<br>
<br>
0:18:09.320,0:18:16.640<br>
所以這一項就是 [1 - f(x^n)] * xi^n<br>
<br>
0:18:16.640,0:18:20.860<br>
右邊這一項呢，這個也是 trivial 阿<br>
<br>
0:18:20.860,0:18:25.740<br>
你把 ln(1-f(x)) 對 wi 做偏微分<br>
<br>
0:18:25.820,0:18:30.760<br>
那就可以拆成先對 z 做偏微分，wi 再對 z 做偏微分<br>
<br>
0:18:30.840,0:18:35.320<br>
右邊這一項， ∂(z)/∂(wi) 我們已經知道它就是 xi<br>
<br>
0:18:35.400,0:18:38.800<br>
左邊這一項<br>
<br>
0:18:38.880,0:18:43.000<br>
你就把 ln 裡面的值，放到分母<br>
<br>
0:18:43.000,0:18:47.940<br>
然後呢，這邊是 -σ(z)<br>
<br>
0:18:47.940,0:18:52.060<br>
前面有個負號，然後這邊要算 σ(z) 的偏微分<br>
<br>
0:18:53.100,0:18:56.960<br>
那 σ(z) 做偏微分以後，得到的結果是這樣<br>
<br>
0:18:56.960,0:19:00.800<br>
把 (1 - σ(z)) 消掉，就只剩下 σ(z)<br>
<br>
0:19:00.800,0:19:05.160<br>
所以，這一項就是 xi * σ(z)，把它放上來<br>
<br>
0:19:05.160,0:19:07.880<br>
就是這個，ok<br>
<br>
0:19:09.260,0:19:15.680<br>
那我們就把這一項放進來<br>
<br>
0:19:15.680,0:19:17.480<br>
把這一項放進來<br>
<br>
0:19:17.480,0:19:20.740<br>
整理一下以後，你得到的結果就是這樣<br>
<br>
0:19:21.840,0:19:26.840<br>
接下來呢，你整理一下，把 xi<br>
<br>
0:19:26.840,0:19:29.700<br>
提到右邊去<br>
<br>
0:19:29.700,0:19:34.780<br>
把 xi 提到右邊去，把括弧的部分展開<br>
<br>
0:19:34.780,0:19:37.580<br>
那裡面有一樣的，把它拿掉<br>
<br>
0:19:37.580,0:19:39.940<br>
最後，你得到一個直觀的結果<br>
<br>
0:19:39.940,0:19:43.980<br>
這個式子看起來有點複雜、有點崩潰<br>
<br>
0:19:43.980,0:19:46.880<br>
但是，你對它做偏微分以後<br>
<br>
0:19:46.880,0:19:52.240<br>
得到的值的結果，卻是容易理解的<br>
<br>
0:19:52.240,0:19:56.620<br>
你得到的值，它的結果呢，每一項都是負的<br>
<br>
0:19:56.620,0:20:00.560<br>
y^\head - f(x^n)<br>
<br>
0:20:00.560,0:20:06.900<br>
再乘上 x^n 的第 i 個 component<br>
<br>
0:20:08.040,0:20:10.640<br>
如果你用 Gradient Descent update 它的話<br>
<br>
0:20:10.640,0:20:12.320<br>
那你的式子就很單純，就這樣<br>
<br>
0:20:12.320,0:20:17.540<br>
wi 是原來的 wi - learning rate<br>
<br>
0:20:17.540,0:20:20.380<br>
乘上 summation over 所有的 training sample<br>
<br>
0:20:20.380,0:20:25.240<br>
[y^n\head - f(x^n)] * x^n 在 i 維的地方<br>
<br>
0:20:25.240,0:20:29.240<br>
這件事情，它代表了甚麼意思呢？<br>
<br>
0:20:29.240,0:20:34.680<br>
它代表甚麼涵義呢？如果你看括號內的式子的話<br>
<br>
0:20:34.680,0:20:36.460<br>
括號內的這個式子的話<br>
<br>
0:20:36.460,0:20:41.820<br>
現在，你的 w 的 update 取決於三件事<br>
<br>
0:20:41.820,0:20:44.900<br>
一個是 learning rate，這個是你自己調的<br>
<br>
0:20:44.900,0:20:50.400<br>
一個是 xi，這個是來自於 data<br>
<br>
0:20:50.400,0:20:55.220<br>
第三項呢，就是這個 y^n\head<br>
<br>
0:20:55.220,0:20:58.640<br>
y^n\head - f(x^n) 是甚麼意思呢？<br>
<br>
0:20:58.640,0:21:04.280<br>
y^n\head - f(x^n) 代表說你現在這個 f 的 output<br>
<br>
0:21:04.540,0:21:10.680<br>
跟理想的這個目標<br>
<br>
0:21:10.680,0:21:12.580<br>
它的差距有多大<br>
<br>
0:21:12.580,0:21:14.820<br>
y^n\head 是目標<br>
<br>
0:21:14.840,0:21:19.320<br>
f(x^n) 是現在你的 model 的 output<br>
<br>
0:21:19.320,0:21:23.960<br>
這兩項之間的差距，這兩個相減的差呢<br>
<br>
0:21:23.960,0:21:30.500<br>
就代表說，他們的差距有多大<br>
<br>
0:21:30.500,0:21:33.380<br>
那如果今天，你離目標越遠<br>
<br>
0:21:33.380,0:21:37.760<br>
那你 update 的量就應該越大<br>
<br>
0:21:37.760,0:21:42.460<br>
所以，這個結果看起來是匹頗為合理的<br>
<br>
0:21:44.580,0:21:48.160<br>
那接下來呢<br>
<br>
0:21:48.340,0:21:53.820<br>
我們就來比較一下 Linear Regression 跟 Logistic Regression<br>
<br>
0:21:53.820,0:21:56.460<br>
Logistic Regression 跟 Linear Regression<br>
<br>
0:21:56.460,0:21:59.680<br>
他們在做 Gradient Descent 的時候，參數 update 的方式<br>
<br>
0:21:59.680,0:22:01.920<br>
我們已經看到 Logistic Regression<br>
<br>
0:22:01.920,0:22:03.700<br>
它 update 的式子長這樣子<br>
<br>
0:22:03.700,0:22:05.880<br>
那神奇的是 Linear Regression<br>
<br>
0:22:05.880,0:22:09.280<br>
大家應該都順利做完作業一了<br>
<br>
0:22:09.280,0:22:11.060<br>
Linear Regression 的這個<br>
<br>
0:22:11.060,0:22:14.040<br>
Gradient Descent update 的式子，你應該是很熟<br>
<br>
0:22:14.040,0:22:18.400<br>
他們其實是一模一樣的<br>
<br>
0:22:18.920,0:22:22.560<br>
你看哦，他們都算<br>
<br>
0:22:22.560,0:22:25.380<br>
y^n\head - f(x^n)<br>
<br>
0:22:25.380,0:22:27.780<br>
他們都算 y^n\head - f(x^n)<br>
<br>
0:22:27.780,0:22:29.340<br>
唯一不一樣的地方是<br>
<br>
0:22:29.340,0:22:32.840<br>
Logistic Regression 你的 target 一定是 0 或 1<br>
<br>
0:22:32.840,0:22:34.480<br>
你的 target 一定是 0 或 1<br>
<br>
0:22:34.480,0:22:37.640<br>
你的這個 f 呢。一定是介於 0~1 之間<br>
<br>
0:22:37.640,0:22:40.460<br>
但是如果是 Linear Regression 的話，你的 target y\head<br>
<br>
0:22:40.460,0:22:43.600<br>
它可以是任何 real number<br>
<br>
0:22:43.600,0:22:46.400<br>
而你這個 output 也可以是任何 value<br>
<br>
0:22:46.400,0:22:50.000<br>
但是他們 update 的這個方式，是一樣的<br>
<br>
0:22:50.000,0:22:52.620<br>
作業二我們需要做 Logistic Regression<br>
<br>
0:22:52.620,0:22:55.720<br>
你甚至八成都不用改 code<br>
<br>
0:22:55.720,0:22:58.760<br>
秒做就可以把它做出來這樣子<br>
<br>
0:22:58.900,0:23:00.980<br>
大家作業一做的還順利嗎？<br>
<br>
0:23:00.980,0:23:04.680<br>
我相信你應該是遇到了種種特別的問題啦<br>
<br>
0:23:04.680,0:23:07.540<br>
比如說，如果你在做 Gradient Descent 的話<br>
<br>
0:23:07.540,0:23:08.660<br>
你會發現說<br>
<br>
0:23:08.660,0:23:10.940<br>
雖然教科書上跟你講 Gradient Descent 的時候<br>
<br>
0:23:10.940,0:23:13.000<br>
你對他是不屑一顧的<br>
<br>
0:23:13.000,0:23:16.900<br>
然後，你覺得說一個 complex 的這個 surface<br>
<br>
0:23:16.900,0:23:19.940<br>
我應該用 Gradient Descent 可以輕易地找到它的最佳解<br>
<br>
0:23:19.940,0:23:21.980<br>
但是，你會發現說實際上做起來<br>
<br>
0:23:21.980,0:23:23.620<br>
是沒有那麼容易的<br>
<br>
0:23:23.680,0:23:28.280<br>
對不對，我其實可以出一個那種教科書上的問題<br>
讓你們來做 Linear Regression<br>
<br>
0:23:28.280,0:23:31.600<br>
但是，我們用真實的 example 就可以讓你知道說<br>
<br>
0:23:31.600,0:23:34.580<br>
在真實的世界，你會碰到怎麼樣的問題<br>
<br>
0:23:34.580,0:23:37.200<br>
事實上，因為今天我們做的是 Linear Regression<br>
<br>
0:23:37.200,0:23:40.060<br>
我看你八成可以用這個<br>
<br>
0:23:40.060,0:23:44.680<br>
解 least square error 的方式，偷偷找一下最佳解<br>
<br>
0:23:44.700,0:23:48.100<br>
然後再從那個最佳解當作 initialization 開始找對吧<br>
<br>
0:23:48.100,0:23:50.120<br>
大家聽得懂我在說甚麼嗎？<br>
<br>
0:23:50.120,0:23:50.740<br>
呵呵<br>
<br>
0:23:50.740,0:23:53.280<br>
有這麼做的人舉手一下<br>
<br>
0:23:53.280,0:23:55.960<br>
沒有人這麼做，還是不敢舉手這樣子<br>
<br>
0:23:57.340,0:24:00.560<br>
要是我就這麼做<br>
<br>
0:24:00.560,0:24:03.460<br>
大家知道我意思嗎？<br>
<br>
0:24:03.460,0:24:06.740<br>
好，但是如果你做到 deep learning 的時候<br>
<br>
0:24:06.740,0:24:07.940<br>
你就不能這麼做啦<br>
<br>
0:24:07.940,0:24:10.900<br>
因為 deep learning 你沒有任何方法可以去<br>
<br>
0:24:10.900,0:24:14.000<br>
找它的最佳解，到時候你才會真正的卡翻<br>
<br>
0:24:16.560,0:24:19.680<br>
那在下課之前，我想要講一下<br>
<br>
0:24:20.260,0:24:21.980<br>
我們今天的計畫是這樣子啦<br>
<br>
0:24:21.980,0:24:24.660<br>
我們講完 Logistic Regression 以後<br>
<br>
0:24:24.660,0:24:26.140<br>
我們就會進入 deep learning<br>
<br>
0:24:26.140,0:24:29.700<br>
然後等一下第三堂課，助教就會來講一下作業二<br>
<br>
0:24:30.880,0:24:33.500<br>
那我們現在要問的問題是這樣<br>
<br>
0:24:33.500,0:24:36.800<br>
為什麼 Logistic Regression 不能加 square error？<br>
<br>
0:24:36.800,0:24:39.800<br>
我為甚麼不能用 square error，<br>
當然可以用 square error 阿<br>
<br>
0:24:39.800,0:24:42.080<br>
我們如果用 square error 的話會怎樣？<br>
<br>
0:24:42.180,0:24:45.420<br>
我們做 Logistic Regression 的時候，我們的式子長這樣<br>
<br>
0:24:45.420,0:24:47.580<br>
我當然可以做 square error 阿<br>
<br>
0:24:47.580,0:24:50.580<br>
我把我的 function 的 output 減掉 y^n\head 的平方<br>
<br>
0:24:50.580,0:24:52.820<br>
summation 起來當作我的 loss function<br>
<br>
0:24:52.820,0:24:55.480<br>
我一樣用 Gradient Descent 去 minimize 它<br>
<br>
0:24:55.480,0:24:59.540<br>
有什麼不可以呢？當然沒什麼不可以這樣子<br>
<br>
0:24:59.980,0:25:02.940<br>
如果我們算一下這個微分的話<br>
<br>
0:25:02.940,0:25:05.640<br>
你會發現說，如果我們把括號裡面<br>
<br>
0:25:05.640,0:25:07.620<br>
summation 後面這個式子<br>
<br>
0:25:07.800,0:25:10.100<br>
對 wi 做偏微分的話<br>
<br>
0:25:10.100,0:25:12.560<br>
它得到的結果呢，是這樣子<br>
<br>
0:25:12.560,0:25:14.920<br>
然後這個 2 呢，提到前面去<br>
<br>
0:25:14.920,0:25:17.560<br>
所以，2(f(x) - y\head)<br>
<br>
0:25:17.560,0:25:20.500<br>
然後，把 f(x) 對 z 做偏微分<br>
<br>
0:25:20.500,0:25:22.960<br>
把 w 對 z 做偏微分<br>
<br>
0:25:22.960,0:25:25.000<br>
把他們都乘起來<br>
<br>
0:25:25.000,0:25:28.740<br>
然後，這一項<br>
<br>
0:25:30.340,0:25:34.700<br>
欸，這個地方<br>
<br>
0:25:34.770,0:25:37.100<br>
沒有寫錯，好，就是<br>
<br>
0:25:37.100,0:25:40.720<br>
這一項，就是這一項<br>
<br>
0:25:40.720,0:25:43.620<br>
把 z 對 f(x) 做偏微分<br>
<br>
0:25:43.620,0:25:45.780<br>
因為 f(x) 是 sigmoid function<br>
<br>
0:25:45.780,0:25:49.120<br>
所以，做偏微分以後，就是 f(x) * (1 - f(x))<br>
<br>
0:25:49.640,0:25:52.640<br>
∂(z) / ∂(wi) 就是 xi<br>
<br>
0:25:52.640,0:25:55.160<br>
當然你可以<br>
<br>
0:25:55.160,0:25:58.260<br>
就用 Gradient Descent 去 update 你的參數<br>
<br>
0:25:58.580,0:26:01.980<br>
但是，你現在會發現你遇到一個問題<br>
<br>
0:26:01.980,0:26:04.220<br>
假設 y^n\head  = 1<br>
<br>
0:26:04.220,0:26:07.420<br>
假設第 n 筆 data 是 class 1<br>
<br>
0:26:07.860,0:26:12.020<br>
當我的 f(x) 已經等於 1 的時候<br>
<br>
0:26:12.020,0:26:16.580<br>
當我的第 n 筆 data 是 class 1<br>
<br>
0:26:16.580,0:26:18.460<br>
而我的 f(x) 已經等於 1 的時候<br>
<br>
0:26:18.520,0:26:20.780<br>
我已經達到 perfect 的狀態了<br>
<br>
0:26:20.780,0:26:23.280<br>
這個時候，沒有甚麼問題<br>
<br>
0:26:23.280,0:26:26.000<br>
因為你 f(x) = 1、y^n\head = 1 的時候<br>
<br>
0:26:26.000,0:26:30.040<br>
你把這兩個數值代進這個 function 裡面<br>
<br>
0:26:30.040,0:26:32.500<br>
你會發現說，至少這一項<br>
<br>
0:26:32.500,0:26:34.840<br>
f(x) - y^n\head 是 0<br>
<br>
0:26:34.840,0:26:37.400<br>
所以你的微分，會變成 0<br>
<br>
0:26:37.400,0:26:39.440<br>
這件事情是很合理<br>
<br>
0:26:39.440,0:26:41.620<br>
但是，如果今天是另一個狀況<br>
<br>
0:26:41.620,0:26:43.620<br>
f(x^n) = 0<br>
<br>
0:26:44.200,0:26:47.180<br>
意味著說，你現在離你的目標<br>
<br>
0:26:47.180,0:26:49.240<br>
仍然非常的遠<br>
<br>
0:26:49.240,0:26:52.480<br>
因為你的目標是希望 f(x^n) 的目標是 1<br>
<br>
0:26:52.480,0:26:53.960<br>
但你現在 output 是 0<br>
<br>
0:26:53.960,0:26:55.800<br>
你離目標還很遠<br>
<br>
0:26:55.800,0:27:00.080<br>
但是，如果你把這個式子代到這裡面的話<br>
<br>
0:27:00.080,0:27:03.460<br>
你會發現說，這邊有乘一個 f(x^n)，而 f(x^n)  = 0<br>
<br>
0:27:03.460,0:27:07.000<br>
這時候，你會變成你微分的結果算出來也是 0<br>
<br>
0:27:07.000,0:27:09.900<br>
所以，如果你離目標很近<br>
<br>
0:27:09.900,0:27:12.560<br>
微分算出來是 0 沒有問題，但是，如果你離目標很遠<br>
<br>
0:27:12.560,0:27:14.640<br>
微分算出來也是 0<br>
<br>
0:27:14.640,0:27:16.840<br>
這個是 class 1 的例子<br>
<br>
0:27:16.840,0:27:20.120<br>
如果我們舉 class 2 的例子，看起來結果也是一樣<br>
<br>
0:27:20.120,0:27:22.100<br>
假設 y^n\head  = 0<br>
<br>
0:27:22.640,0:27:25.120<br>
假設現在距離目標很遠<br>
<br>
0:27:25.120,0:27:27.600<br>
假設距離目標很遠的時候<br>
<br>
0:27:27.600,0:27:30.600<br>
f(x^n) = 1 你代進去，至少最後這個式子是 0<br>
<br>
0:27:30.600,0:27:32.300<br>
你微分算出來也是 0<br>
<br>
0:27:32.300,0:27:35.180<br>
距離目標很近的時候，微分算出來也是 0<br>
<br>
0:27:35.180,0:27:37.020<br>
這會造成甚麼問題呢？<br>
<br>
0:27:37.020,0:27:40.140<br>
如果我們把<br>
<br>
0:27:40.140,0:27:44.940<br>
參數的變化對 total loss 作圖的話<br>
<br>
0:27:44.940,0:27:47.580<br>
你會發現說，如果你選擇 cross entropy<br>
<br>
0:27:47.580,0:27:50.740<br>
跟你選擇 square error<br>
<br>
0:27:50.740,0:27:54.260<br>
參數的變化跟 loss 的變化<br>
<br>
0:27:54.260,0:27:55.960<br>
串起來是這樣子的<br>
<br>
0:27:55.960,0:27:58.900<br>
黑色的是 cross entropy<br>
<br>
0:27:58.900,0:28:01.940<br>
紅色的是 square error<br>
<br>
0:28:01.940,0:28:07.480<br>
我們剛才講說 cross entropy 在距離目標很近的地方<br>
<br>
0:28:07.540,0:28:11.080<br>
假設現在中心最低的點就是<br>
<br>
0:28:11.080,0:28:13.760<br>
距離目標很近的地方<br>
<br>
0:28:13.760,0:28:16.300<br>
你的微分值就很小<br>
<br>
0:28:16.300,0:28:19.260<br>
但是，距離目標很遠的地方<br>
<br>
0:28:19.260,0:28:21.280<br>
你的微分值也是很小的<br>
<br>
0:28:21.280,0:28:23.740<br>
所以，在距離目標很遠的地方呢<br>
<br>
0:28:23.740,0:28:26.800<br>
你會非常的平坦<br>
<br>
0:28:26.800,0:28:28.560<br>
這會造成甚麼問題呢？<br>
<br>
0:28:28.560,0:28:31.800<br>
如果是 cross entropy 的話<br>
<br>
0:28:31.800,0:28:35.160<br>
你距離目標越遠，你的微分值就越大<br>
<br>
0:28:35.160,0:28:37.700<br>
那沒有問題，所以你距離目標越遠<br>
<br>
0:28:37.700,0:28:40.660<br>
你參數 update 的時候就越快<br>
<br>
0:28:40.660,0:28:43.100<br>
你參數更新的速度就越快<br>
<br>
0:28:43.180,0:28:45.840<br>
你參數 update 的時候，變化量就越大<br>
<br>
0:28:45.840,0:28:48.620<br>
這個沒有問題，距離你的目標越遠<br>
<br>
0:28:48.620,0:28:50.960<br>
你的步伐當然要踏越大一點<br>
<br>
0:28:50.960,0:28:54.080<br>
但是，如果你選 square error 的話，你就會很卡<br>
<br>
0:28:54.080,0:28:56.760<br>
因為，當你距離目標遠的時候<br>
<br>
0:28:56.840,0:29:00.740<br>
你的微分是非常非常小的<br>
<br>
0:29:00.740,0:29:03.480<br>
就變成說，你離目標遠的時候<br>
<br>
0:29:03.480,0:29:07.500<br>
你移動的速度是非常慢<br>
<br>
0:29:07.500,0:29:10.400<br>
所以，如果你用隨機<br>
<br>
0:29:10.400,0:29:13.600<br>
你 random 找一個初始值<br>
<br>
0:29:13.600,0:29:16.720<br>
那通常你離目標的距離，是非常遠的<br>
<br>
0:29:16.720,0:29:20.260<br>
那如果你今天是用 square error<br>
<br>
0:29:20.260,0:29:21.920<br>
你其實可以自己在作業裡面試試看<br>
<br>
0:29:21.920,0:29:24.120<br>
如果你用 square error，你選一個起始值<br>
<br>
0:29:24.120,0:29:26.160<br>
你算出來的微分很小，你一開始就卡住了<br>
<br>
0:29:26.160,0:29:29.060<br>
它的參數都不 update，你就永遠卡在那邊<br>
<br>
0:29:29.060,0:29:32.300<br>
它的參數 update 的速度很慢<br>
<br>
0:29:32.300,0:29:35.060<br>
你等了好幾個小時了，它都跑不出來這樣<br>
<br>
0:29:35.060,0:29:37.280<br>
那你可能會想說<br>
<br>
0:29:37.400,0:29:39.920<br>
那我們可以說看到這個<br>
<br>
0:29:39.920,0:29:43.260<br>
微分值很小的時候，就把它的<br>
<br>
0:29:43.320,0:29:45.500<br>
那個 learning rate 設大一點阿<br>
<br>
0:29:45.500,0:29:47.180<br>
可使問題是微分值很小的時候<br>
<br>
0:29:47.180,0:29:49.960<br>
你也有可能距離你的目標很近阿<br>
<br>
0:29:49.960,0:29:51.360<br>
如果距離目標很近的時候<br>
<br>
0:29:51.360,0:29:53.620<br>
這個時候你應該把它的微分值設小一點<br>
<br>
0:29:53.620,0:29:56.000<br>
但是，你現在搞不清楚說<br>
<br>
0:29:56.000,0:29:59.680<br>
到底 Gradient 小的時候，微分值算出來小的時候<br>
<br>
0:29:59.680,0:30:02.960<br>
你是距離目標很近，還是距離目標很遠<br>
<br>
0:30:02.960,0:30:04.320<br>
因為做 Gradient Descent 的時候<br>
<br>
0:30:04.320,0:30:06.260<br>
你是在玩世紀帝國這個遊戲<br>
<br>
0:30:06.260,0:30:08.860<br>
你不知道你距離目標是很近，還是很遠<br>
<br>
0:30:08.860,0:30:09.960<br>
所以你就會卡翻了<br>
<br>
0:30:09.960,0:30:12.420<br>
不知道你的 learning rate 應該設大還是設小<br>
<br>
0:30:12.420,0:30:14.500<br>
所以你選 square error<br>
<br>
0:30:14.500,0:30:16.140<br>
在實做上，你當然可以這麼做<br>
<br>
0:30:16.140,0:30:17.460<br>
那你可以在作業裡面試試看<br>
<br>
0:30:17.460,0:30:20.320<br>
你是不容易得到好的結果的<br>
<br>
0:30:20.320,0:30:24.260<br>
用 cross entropy 可以讓你的 training 順很多<br>
<br>
0:30:24.260,0:30:27.080<br>
我們在這邊呢，休息 10 分鐘<br>
<br>
0:30:29.400,0:31:19.620<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:31:21.060,0:31:23.320<br>
我們來上課吧<br>
<br>
0:31:23.320,0:31:25.820<br>
那我們接下來要講的是<br>
<br>
0:31:25.820,0:31:28.780<br>
這個 Logistic Regression 的方法阿<br>
<br>
0:31:28.780,0:31:32.440<br>
我們稱它為 discriminative 的方法<br>
<br>
0:31:32.440,0:31:36.460<br>
而剛才呢，我們用 Gaussian<br>
<br>
0:31:36.540,0:31:40.320<br>
來描述 posterior probability 這件事呢<br>
<br>
0:31:40.320,0:31:43.220<br>
我們稱之為 generative 的方法<br>
<br>
0:31:43.220,0:31:47.120<br>
實際上，他們的 function<br>
<br>
0:31:47.380,0:31:51.820<br>
他們的這個 model、function set 呢，是一模一樣的<br>
<br>
0:31:51.820,0:31:56.740<br>
不管你是用我們在這份投影片講的 Logistic Regression<br>
<br>
0:31:56.740,0:32:00.860<br>
還是前一份投影片講的機率模型<br>
<br>
0:32:00.860,0:32:03.220<br>
只要你在做機率模型的時候<br>
<br>
0:32:03.220,0:32:05.840<br>
你把 covariance matrix 設成是 share 的<br>
<br>
0:32:05.840,0:32:08.920<br>
那他們的 model 其實是一模一樣的<br>
<br>
0:32:08.920,0:32:12.560<br>
都是 σ(w * x + b)<br>
<br>
0:32:12.560,0:32:17.620<br>
那你可以找不同的 w 跟不同的 b，<br>
就得到不同的 function<br>
<br>
0:32:18.220,0:32:20.980<br>
那我們<br>
<br>
0:32:22.680,0:32:27.300<br>
我們可以直接去把 w 跟 b 找出來<br>
<br>
0:32:27.300,0:32:30.560<br>
如果你今天是用 Logistic Linear Regression 的話<br>
<br>
0:32:30.560,0:32:33.380<br>
你可以直接把 w 跟 b 找出來<br>
<br>
0:32:33.440,0:32:35.440<br>
就用 Gradient Descent 的方法<br>
<br>
0:32:35.440,0:32:38.400<br>
如果今天是 generative model 的話<br>
<br>
0:32:38.440,0:32:40.640<br>
那首先呢，我們會去算<br>
<br>
0:32:40.640,0:32:45.300<br>
μ^1, μ^2 跟 Σ 的 inverse<br>
<br>
0:32:45.300,0:32:47.340<br>
然後呢，我們就去把<br>
<br>
0:32:47.340,0:32:51.060<br>
我們一樣把 w 算出來、把 b 算出來<br>
<br>
0:32:51.060,0:32:56.180<br>
你算出 μ^1, μ^2 跟 covariance matrix<br>
<br>
0:32:56.180,0:32:59.620<br>
接下來呢，你就把這些項代到<br>
<br>
0:32:59.620,0:33:01.980<br>
這裡面，代到這裡面<br>
<br>
0:33:01.980,0:33:05.820<br>
這邊這個 Σ^1, Σ^2 呢，應該都是等於 Σ<br>
<br>
0:33:05.820,0:33:07.420<br>
這邊應該都把它代成 Σ<br>
<br>
0:33:07.420,0:33:11.600<br>
那就把它算出來就可以得到 w 跟 b<br>
<br>
0:33:11.600,0:33:13.460<br>
現在的問題來了<br>
<br>
0:33:13.460,0:33:18.260<br>
如果我們比較左邊跟右邊求 w 跟求 b 的方法<br>
<br>
0:33:20.960,0:33:25.520<br>
我們找出來的 w 跟 b 會是同一組嗎？<br>
<br>
0:33:25.520,0:33:27.760<br>
會是同一組嗎？<br>
<br>
0:33:28.380,0:33:31.340<br>
你覺得它是同一組的同學舉手一下<br>
<br>
0:33:32.100,0:33:35.780<br>
你覺得它是不同的同學舉手一下<br>
<br>
0:33:35.780,0:33:37.080<br>
好，謝謝，手放下<br>
<br>
0:33:37.080,0:33:39.140<br>
多數同學覺得它是不同<br>
<br>
0:33:39.140,0:33:42.680<br>
沒錯，你找出來的結果不會是一樣的<br>
<br>
0:33:42.680,0:33:46.960<br>
所以，今天當我們用 Logistic Regression<br>
<br>
0:33:46.960,0:33:51.360<br>
還是用剛才的 probabilistic 的 generative model<br>
<br>
0:33:51.360,0:33:55.180<br>
我們用的其實是同一個 model<br>
<br>
0:33:55.180,0:33:57.360<br>
其實是同一個 function set<br>
<br>
0:33:57.360,0:33:59.540<br>
也就是我們的 function 的那個 pool 阿<br>
<br>
0:33:59.540,0:34:01.900<br>
我們可以挑的那個 function 的 candidate 阿<br>
<br>
0:34:01.900,0:34:03.480<br>
其實是同一個 set<br>
<br>
0:34:03.480,0:34:07.580<br>
但是 ，因為我們做了不同的假設<br>
<br>
0:34:07.580,0:34:10.140<br>
所以，我們最後找出來的<br>
<br>
0:34:10.140,0:34:13.680<br>
根據同一組 training data 找出來的參數<br>
<br>
0:34:13.680,0:34:14.920<br>
會是不一樣的<br>
<br>
0:34:14.920,0:34:17.340<br>
在這個 Logistic Regression 裡面<br>
<br>
0:34:17.340,0:34:19.480<br>
其實我們就沒有做任何假設<br>
<br>
0:34:19.480,0:34:24.280<br>
我們沒有對這個 probability distribution 有任何的描述<br>
<br>
0:34:24.280,0:34:27.200<br>
我們就是單純去找一個 w 跟 b<br>
<br>
0:34:27.200,0:34:30.720<br>
那在 generative model 裡面<br>
<br>
0:34:30.720,0:34:33.960<br>
我們對 probability distribution 是有假設的<br>
<br>
0:34:33.960,0:34:36.320<br>
比如說，假設它是 Gaussian<br>
<br>
0:34:36.320,0:34:38.600<br>
假設它是 Bernoulli<br>
<br>
0:34:38.600,0:34:43.300<br>
假設它是不是 Naive Bayes...... 等等<br>
<br>
0:34:43.300,0:34:45.420<br>
我們做了種種的假設<br>
<br>
0:34:45.420,0:34:49.580<br>
根據這些假設，我們可以找到另外一組 w 跟 b<br>
<br>
0:34:49.580,0:34:55.320<br>
左右兩邊找出來的 w 跟 b 呢，不會是同一組<br>
<br>
0:34:55.320,0:35:01.040<br>
那問題就是，哪一組找出來的 w 跟 b 是比較好的呢？<br>
<br>
0:35:01.040,0:35:02.440<br>
如果我們比較<br>
<br>
0:35:02.840,0:35:06.980<br>
Generative model 跟 Discriminative model 的話<br>
<br>
0:35:07.020,0:35:09.100<br>
那我們先看一下我們之前講的<br>
<br>
0:35:09.100,0:35:12.240<br>
defense 跟 special defense 的例子<br>
<br>
0:35:12.240,0:35:14.420<br>
如果用 generative model 的話<br>
<br>
0:35:14.420,0:35:16.360<br>
我們的這兩個 class<br>
<br>
0:35:16.360,0:35:21.020<br>
就藍色的是水系的神奇寶貝，紅色的是一般系的寶可夢<br>
<br>
0:35:21.020,0:35:23.140<br>
他們之間 boundary 是這一條<br>
<br>
0:35:23.400,0:35:25.760<br>
如果你是用 Logistic Regression 的話呢<br>
<br>
0:35:25.760,0:35:28.020<br>
你找出來的 boundary 是這一條<br>
<br>
0:35:28.140,0:35:31.280<br>
其實從這個結果上，你很難看出來說<br>
<br>
0:35:31.280,0:35:32.900<br>
誰比較好啦<br>
<br>
0:35:33.060,0:35:34.940<br>
但是，如果我們比較說<br>
<br>
0:35:34.940,0:35:38.760<br>
我們都用 7 個 feature 的這個 class<br>
<br>
0:35:38.760,0:35:41.960<br>
我們會發現說，如果用 generative model 的話<br>
<br>
0:35:41.960,0:35:45.600<br>
我們剛才說，我們得到的正確率呢，是 73%<br>
<br>
0:35:45.600,0:35:48.520<br>
如果是用 discriminative model 的話<br>
<br>
0:35:48.620,0:35:52.620<br>
在同樣的 data set 上面，我們只是用<br>
<br>
0:35:52.620,0:35:56.480<br>
不同的假設，所以找了不同的 w 跟 b<br>
<br>
0:35:56.480,0:35:58.340<br>
但是我們找出來的結果比較好的<br>
<br>
0:35:58.420,0:36:01.540<br>
我們找出來的正確率有 79% 這樣<br>
<br>
0:36:01.540,0:36:05.160<br>
那我相信在文獻上，會常常聽到有人說<br>
<br>
0:36:05.160,0:36:09.640<br>
discriminative model 會比 generative model<br>
<br>
0:36:09.640,0:36:12.220<br>
常常會 performance 的更好<br>
<br>
0:36:12.220,0:36:13.680<br>
為甚麼會這樣呢？<br>
<br>
0:36:13.680,0:36:16.600<br>
我們來舉一個 toy 的 example<br>
<br>
0:36:16.600,0:36:20.280<br>
現在假設呢，你有一筆 training data<br>
<br>
0:36:20.280,0:36:22.320<br>
你有兩個 class<br>
<br>
0:36:22.320,0:36:24.360<br>
那你這筆 training data 裡面呢<br>
<br>
0:36:24.360,0:36:25.780<br>
總共有<br>
<br>
0:36:27.650,0:36:31.280<br>
每一筆 data 呢，有兩個 feature<br>
<br>
0:36:31.280,0:36:32.610<br>
然後呢<br>
<br>
0:36:32.660,0:36:39.260<br>
你總共有 1 + 4 + 4 + 4，總共有 13 筆 data<br>
<br>
0:36:39.260,0:36:42.740<br>
第一筆 data 是兩個 feature 的 value 都是 1<br>
<br>
0:36:42.740,0:36:44.980<br>
接下來呢，有 4 筆 data<br>
<br>
0:36:44.980,0:36:47.360<br>
第一個 feature 是 1，第二個 feature 是 0<br>
<br>
0:36:47.360,0:36:50.100<br>
接下來 4 筆 data，是第一個 feature 是 0，<br>
第二個 feature 是 1<br>
<br>
0:36:50.100,0:36:52.740<br>
接下來 4 筆 data，是兩個 feature 都是 0<br>
<br>
0:36:53.020,0:36:57.120<br>
然後呢，我們給第一筆 data 的 label 是 1<br>
<br>
0:36:57.120,0:37:01.060<br>
我們給剩下 12 筆 data 的 label 呢<br>
<br>
0:37:01.060,0:37:03.900<br>
都是 class 2<br>
<br>
0:37:03.900,0:37:09.240<br>
那假設你現在不做機器學習，做人類的學習<br>
<br>
0:37:09.240,0:37:11.120<br>
給你一個 testing data<br>
<br>
0:37:11.120,0:37:15.260<br>
它的兩個 feature 都是 1，<br>
你覺得它是 class 1 還是 class 2 呢？<br>
<br>
0:37:16.220,0:37:18.160<br>
我們來問一下大家的意見吧<br>
<br>
0:37:18.160,0:37:20.540<br>
如果你覺得它是 class 1 的同學，舉手一下<br>
<br>
0:37:21.480,0:37:25.000<br>
手放下，你覺得它是 class 2 的同學，舉手一下<br>
<br>
0:37:25.000,0:37:28.540<br>
沒有人覺得是 class 2，大家都覺得是 class 1<br>
<br>
0:37:28.540,0:37:31.660<br>
那如果我們來問一下 Naive Bayes<br>
<br>
0:37:31.660,0:37:35.160<br>
它覺得是 class 1 還是 class 2，它會怎麼說呢？<br>
<br>
0:37:35.160,0:37:38.680<br>
所謂的 Naive Bayes 就是，我們假設<br>
<br>
0:37:38.680,0:37:42.260<br>
所有的 feature，它產生的機率是 independent<br>
<br>
0:37:42.260,0:37:48.560<br>
所以 P(x|Ci)，P of x 從某一個 class 產生出來的機率<br>
<br>
0:37:48.560,0:37:51.840<br>
等於從某一個 class 產生 x1 的機率<br>
<br>
0:37:51.840,0:37:55.460<br>
乘上從某一個 class 產生 x2 的機率<br>
<br>
0:37:55.920,0:38:00.720<br>
那我們用 Naive Bayes 來算一下<br>
<br>
0:38:00.720,0:38:03.920<br>
首先，算一下 prior 的 probability<br>
<br>
0:38:04.520,0:38:06.840<br>
class 1 它出現的 probability 是多少？<br>
<br>
0:38:06.840,0:38:09.640<br>
總共 13 筆 data，只 sample 到一次是 class 1<br>
<br>
0:38:09.640,0:38:11.220<br>
所以是 1/13<br>
<br>
0:38:11.220,0:38:12.960<br>
class 2 的機率是多少呢？<br>
<br>
0:38:12.960,0:38:16.380<br>
總共 13 筆 data，有 12 筆是 class 2<br>
<br>
0:38:16.380,0:38:19.140<br>
所以它是 12/13，它比較多<br>
<br>
0:38:19.140,0:38:21.760<br>
那接下來呢，我們算說<br>
<br>
0:38:21.760,0:38:25.660<br>
在 class 1 裡面，x1 = 1 的機率<br>
<br>
0:38:25.660,0:38:30.180<br>
在 class 1 裡面，x1 = 1 的機率 就是 1<br>
<br>
0:38:30.180,0:38:34.780<br>
在 class 2 裡面，x1 = 1 的機率也是 1<br>
<br>
0:38:34.780,0:38:35.860<br>
class 1 就這筆 data 嘛<br>
<br>
0:38:35.860,0:38:37.980<br>
它 x1 是 1，x2 是 1<br>
<br>
0:38:37.980,0:38:40.340<br>
所以，如果你用機率來統計的話<br>
<br>
0:38:40.340,0:38:44.080<br>
那在 class 1 裡面，x1 = 1 的機率是 1<br>
<br>
0:38:44.080,0:38:47.240<br>
在 class 1 裡面，x2 = 1 的機率也是 1<br>
<br>
0:38:47.240,0:38:49.040<br>
接下來我們看<br>
<br>
0:38:49.740,0:38:53.640<br>
我發現我犯了一個錯，這邊應該是 C2，不好意思<br>
<br>
0:38:53.640,0:38:56.400<br>
這邊應該是 C2，這邊也應該是 C2<br>
<br>
0:38:56.400,0:39:00.160<br>
如果我們看 class 2 的話<br>
<br>
0:39:00.160,0:39:03.720<br>
如果我們看右邊這 12 筆 class 2 的 data<br>
<br>
0:39:04.460,0:39:08.060<br>
在 class 2 裡面，x1 = 1的機率是多少呢？<br>
<br>
0:39:08.060,0:39:14.020<br>
是 1/3 對不對，只有 1/3 的 data 是 x1(老師講錯) = 1 的<br>
<br>
0:39:14.300,0:39:16.600<br>
是 x1 = 1 的<br>
<br>
0:39:16.600,0:39:20.580<br>
那再來我們看 x2<br>
<br>
0:39:20.580,0:39:23.360<br>
x2= 1 的機率在 class 2 裡面有多少呢？<br>
<br>
0:39:23.360,0:39:25.520<br>
在 class 2 裡面只有 1/3 的 data<br>
<br>
0:39:25.520,0:39:29.460<br>
x2 = 1，所以它的機率是 1/3<br>
<br>
0:39:29.460,0:39:32.380<br>
如果我們把這些機率通通算出來以後<br>
<br>
0:39:32.380,0:39:35.060<br>
給你一個 testing data<br>
<br>
0:39:35.060,0:39:39.120<br>
你就可以去初估測它是來自 class 1 的機率<br>
<br>
0:39:39.240,0:39:41.300<br>
你就可以初估測它是來自 class 2 的機率<br>
<br>
0:39:41.300,0:39:44.080<br>
我們就算這筆 training data x 呢<br>
<br>
0:39:44.080,0:39:45.900<br>
它來自 class 1 的機率是多少<br>
<br>
0:39:45.900,0:39:48.040<br>
那你就把它代到這個<br>
<br>
0:39:48.040,0:39:53.260<br>
Bayesian 的 function 裡面，算一下<br>
<br>
0:39:53.680,0:39:56.420<br>
C1 的 prior probability 是 1/3<br>
<br>
0:39:56.420,0:40:01.000<br>
P(x|C1) 的機率是 1*1<br>
<br>
0:40:01.000,0:40:02.020<br>
甚麼意思呢？<br>
<br>
0:40:02.020,0:40:05.340<br>
這筆 data x，從 C1 裡面 generate 出來的機率<br>
<br>
0:40:05.340,0:40:09.140<br>
等於這個機率乘上這個機率<br>
<br>
0:40:09.660,0:40:13.400<br>
下面這項你算過，這是 1/13，這是 1*1<br>
<br>
0:40:13.520,0:40:16.640<br>
那這一項呢，P(C2) 是 12/13<br>
<br>
0:40:16.640,0:40:20.740<br>
P(x|C2)，從 C2 裡面 sample 出<br>
<br>
0:40:20.740,0:40:24.620<br>
根據 C2 的 distribution ，<br>
sample 出這筆 data 的機率是多少呢？<br>
<br>
0:40:24.760,0:40:27.020<br>
是 1/3 * 1/3<br>
<br>
0:40:27.020,0:40:29.780<br>
因為 x1 = 1 的機率在 C2 裡面是 1/3<br>
<br>
0:40:29.780,0:40:35.520<br>
x2 = 1 的機率在 C2 裡面是 1/3<br>
<br>
0:40:35.660,0:40:38.820<br>
所以這一項是 1/3 * 1/3<br>
<br>
0:40:38.820,0:40:41.880<br>
如果你實際去做一下運算<br>
<br>
0:40:41.880,0:40:45.160<br>
實際算一發就知道說<br>
<br>
0:40:45.160,0:40:48.740<br>
這一個是小於 0.5 的<br>
<br>
0:40:48.740,0:40:51.300<br>
所以對 Naive Bayes 來說<br>
<br>
0:40:51.300,0:40:55.040<br>
給它這樣子的 training data<br>
<br>
0:40:55.040,0:40:58.040<br>
它認為這一筆 testing data<br>
<br>
0:40:58.040,0:41:00.640<br>
應該是屬於 class 2<br>
<br>
0:41:00.640,0:41:02.740<br>
而不是 class 1<br>
<br>
0:41:02.740,0:41:09.040<br>
所以這跟大家的直覺比起來，是相反的<br>
<br>
0:41:09.040,0:41:10.260<br>
是相反的<br>
<br>
0:41:10.260,0:41:13.940<br>
其實我們很難知道說<br>
<br>
0:41:13.940,0:41:16.800<br>
怎麼知道說這筆 data 的產生到底是來自<br>
<br>
0:41:16.800,0:41:18.900<br>
class 1 還是 class 2<br>
<br>
0:41:18.900,0:41:20.260<br>
比較合理的假設<br>
<br>
0:41:20.260,0:41:22.280<br>
你會覺得說，因為 class 1 裡面<br>
<br>
0:41:22.300,0:41:25.820<br>
x1 和 x2 通通都是等於 1 的<br>
<br>
0:41:25.820,0:41:29.380<br>
所以，這筆 data 應該是要來自 class 1 才對吧<br>
<br>
0:41:29.380,0:41:31.320<br>
可是對 Naive Bayes 來說<br>
<br>
0:41:31.320,0:41:35.240<br>
它不考慮不同 dimension 之間的 correlation<br>
<br>
0:41:35.240,0:41:37.400<br>
所以，對 Naive Bayes 來說<br>
<br>
0:41:37.400,0:41:39.820<br>
這兩個 dimension 是 independent 所產生的<br>
<br>
0:41:39.820,0:41:43.220<br>
在 class 2 裡面，之所以沒有<br>
<br>
0:41:43.220,0:41:46.920<br>
sample 到這樣的 data，觀察到這樣的 data<br>
<br>
0:41:46.920,0:41:48.940<br>
是因為你 sample 的不夠多<br>
<br>
0:41:48.940,0:41:50.460<br>
如果你 sample 的夠多<br>
<br>
0:41:50.460,0:41:54.920<br>
搞不好就有都是 1 的 data<br>
<br>
0:41:56.020,0:41:57.440<br>
就都是 1 的 data<br>
<br>
0:41:57.440,0:41:59.540<br>
也是有機率被產生出來的<br>
<br>
0:41:59.540,0:42:02.180<br>
只是，因為我們 data 不夠多<br>
<br>
0:42:02.280,0:42:05.040<br>
所以，沒有觀察到這件事而已<br>
<br>
0:42:05.600,0:42:08.840<br>
所以，今天這個 generative model<br>
<br>
0:42:08.840,0:42:12.680<br>
跟 discriminative model 的差別就在於<br>
<br>
0:42:12.720,0:42:14.840<br>
這個 generative model<br>
<br>
0:42:14.840,0:42:17.300<br>
它有做了某些假設<br>
<br>
0:42:17.420,0:42:20.880<br>
假設你的 data 來自於一個機率模型<br>
<br>
0:42:20.880,0:42:22.580<br>
它做了某些假設<br>
<br>
0:42:22.580,0:42:26.200<br>
也就是說，它其實做了腦補這件事情<br>
<br>
0:42:26.200,0:42:28.300<br>
腦補是什麼，大家知道嗎？<br>
<br>
0:42:28.300,0:42:30.720<br>
就是，如果你看了一部動漫<br>
<br>
0:42:30.720,0:42:33.320<br>
那裡面沒有發生某一些事情<br>
<br>
0:42:33.320,0:42:37.340<br>
比如說，兩個男女主角其實沒有在一起<br>
<br>
0:42:37.400,0:42:40.560<br>
但是，你心裡想像他們是在一起，這個就是腦補<br>
<br>
0:42:41.420,0:42:45.680<br>
所以，這個 generative model 它做的事情<br>
<br>
0:42:45.680,0:42:47.120<br>
就是腦補<br>
<br>
0:42:47.160,0:42:49.540<br>
如果，我們在 data 裡面明明沒有觀察到<br>
<br>
0:42:49.540,0:42:51.180<br>
在 class 2 裡面<br>
<br>
0:42:51.180,0:42:54.460<br>
有都是 1 的這樣的 example 出現<br>
<br>
0:42:54.460,0:42:58.020<br>
但是，對 Naive Bayes 來說<br>
<br>
0:42:58.020,0:43:00.200<br>
它想像它看到了這件事情<br>
<br>
0:43:00.300,0:43:04.720<br>
所以，它就會做出一個跟我們人類直覺想法不一樣的<br>
<br>
0:43:04.720,0:43:06.760<br>
判斷的結果<br>
<br>
0:43:06.760,0:43:10.120<br>
那到底腦補是不是一件好的事情呢？<br>
<br>
0:43:10.120,0:43:12.540<br>
通常腦補可能不是一件好的事情<br>
<br>
0:43:12.540,0:43:14.540<br>
因為你的 data 沒有告訴你這一件事情<br>
<br>
0:43:14.540,0:43:17.040<br>
你卻腦補出這樣的結果<br>
<br>
0:43:17.040,0:43:20.360<br>
但是，如果今天在 data 很少的情況下<br>
<br>
0:43:20.360,0:43:22.760<br>
腦補有時候也是有用的<br>
<br>
0:43:22.760,0:43:27.080<br>
如果你得到的情報很少，腦補可以給你更多的情報<br>
<br>
0:43:27.080,0:43:28.480<br>
所以，其實<br>
<br>
0:43:28.860,0:43:33.040<br>
discriminative model 並不是在所有的情況下<br>
<br>
0:43:33.040,0:43:35.840<br>
都可以贏過 generative model <br>
<br>
0:43:35.840,0:43:38.640<br>
有些時候 generative model 也是有優勢的<br>
<br>
0:43:38.640,0:43:40.120<br>
甚麼時候會有優勢呢？<br>
<br>
0:43:40.120,0:43:42.600<br>
如果你今天的 training data 很少<br>
<br>
0:43:42.600,0:43:46.020<br>
你可以比較說，在同一個 problem 下<br>
<br>
0:43:46.020,0:43:50.420<br>
你給 discriminative model 和 generative model <br>
<br>
0:43:50.420,0:43:52.600<br>
不同量的 training data<br>
<br>
0:43:52.600,0:43:56.260<br>
你會發現說，這個 discriminative model<br>
<br>
0:43:56.260,0:43:58.120<br>
因為它完全沒有做任何假設<br>
<br>
0:43:58.120,0:44:00.080<br>
它是看著 data 說話<br>
<br>
0:44:00.080,0:44:04.560<br>
所以它的 performance 的變化量，<br>
會受你的 data 量影響很大<br>
<br>
0:44:04.560,0:44:07.600<br>
假設，現在由左到右是 data 越來越多<br>
<br>
0:44:07.600,0:44:10.900<br>
然後，縱軸是 error rate<br>
<br>
0:44:10.900,0:44:14.380<br>
discriminative model 它受到 data 影響很大<br>
<br>
0:44:14.380,0:44:18.420<br>
所以 data 越來越多，它的 error 就越來越小<br>
<br>
0:44:18.420,0:44:21.320<br>
如果你是看 generative model 的話<br>
<br>
0:44:21.320,0:44:25.820<br>
它受 data 的影響是比較小的<br>
<br>
0:44:25.820,0:44:28.280<br>
因為它有一個它自己的假設<br>
<br>
0:44:28.280,0:44:32.580<br>
它有時候會無視那個 data，而遵從它內心自己的假設<br>
<br>
0:44:32.580,0:44:34.480<br>
自己內心腦補的結果<br>
<br>
0:44:34.480,0:44:36.340<br>
所以如果你看 data 量<br>
<br>
0:44:36.340,0:44:40.140<br>
所以如果你看 data 量的影響的話，在 data 少的時候<br>
<br>
0:44:40.140,0:44:44.520<br>
generative model 有時候是可以<br>
贏過 discriminative model 的<br>
<br>
0:44:44.520,0:44:46.940<br>
只有在 data 慢慢增加的時候<br>
<br>
0:44:46.940,0:44:50.280<br>
generative model 才會輸給 discriminative model<br>
<br>
0:44:50.280,0:44:52.180<br>
這個其實是 case by case<br>
<br>
0:44:52.180,0:44:58.680<br>
你可以在作業裡面做做實驗，<br>
看看你能不能觀察到這樣的現象<br>
<br>
0:44:59.560,0:45:01.940<br>
那有時候 generative model 是有用的<br>
<br>
0:45:01.940,0:45:04.160<br>
可能是<br>
<br>
0:45:04.160,0:45:06.980<br>
你今天的 data 是 noise 的<br>
<br>
0:45:06.980,0:45:09.580<br>
你的 label 本身就有問題<br>
<br>
0:45:09.580,0:45:12.140<br>
因為你的 label 本身就有問題<br>
<br>
0:45:12.140,0:45:14.220<br>
你自己做一些腦補、做一些假設<br>
<br>
0:45:14.220,0:45:17.960<br>
反而可以把 data 裡面有問題的部分呢<br>
<br>
0:45:17.960,0:45:19.600<br>
忽視掉<br>
<br>
0:45:20.300,0:45:23.500<br>
那我們在做 discriminative model 的時候<br>
<br>
0:45:23.500,0:45:27.540<br>
我們是直接假設一個 posterior probability<br>
<br>
0:45:27.540,0:45:30.320<br>
然後去找 posterior probability 裡面的參數<br>
<br>
0:45:30.320,0:45:32.820<br>
但是我們在做 generative model 的時候<br>
<br>
0:45:32.820,0:45:36.200<br>
我們把整個 formulation 裡面拆成<br>
<br>
0:45:36.200,0:45:39.900<br>
prior 跟 class-dependent 的 probability 這兩項，對不對<br>
<br>
0:45:39.900,0:45:43.160<br>
那這樣做有時候是有好處的<br>
<br>
0:45:43.160,0:45:46.880<br>
如果你把你的整個 function 拆成<br>
<br>
0:45:47.280,0:45:52.700<br>
prior 跟 class-dependent 的 probability 這兩項的話<br>
<br>
0:45:52.700,0:45:54.360<br>
有時候會有幫助<br>
<br>
0:45:54.360,0:45:57.360<br>
因為，這個 prior 跟 class-dependent 的 probability <br>
<br>
0:45:57.400,0:46:01.460<br>
它們可以是來自於不同的來源<br>
<br>
0:46:01.500,0:46:05.420<br>
舉例來說，以語音辨識為例<br>
<br>
0:46:05.420,0:46:08.280<br>
大家可能都知道說呢<br>
<br>
0:46:08.280,0:46:10.900<br>
語音辨識現在都是用<br>
<br>
0:46:10.900,0:46:14.240<br>
neural network，它是一個discriminative 的方法<br>
<br>
0:46:14.240,0:46:16.640<br>
但事實上，整個語音辨識的系統<br>
<br>
0:46:16.640,0:46:18.600<br>
是一個 generative 的 system<br>
<br>
0:46:18.600,0:46:20.520<br>
DNN 只是其中一塊而已<br>
<br>
0:46:20.520,0:46:23.160<br>
所以說，該怎麼說呢<br>
<br>
0:46:24.740,0:46:27.880<br>
所以說，就全部都是用 DNN 這件事情呢<br>
<br>
0:46:27.880,0:46:31.140<br>
並不是那麼的精確，它整個 model 其實是 discriminative<br>
<br>
0:46:31.140,0:46:32.580<br>
為甚麼會這樣呢<br>
<br>
0:46:32.580,0:46:34.960<br>
因為它還是要去算一個 prior probability<br>
<br>
0:46:34.960,0:46:38.300<br>
因為 prior probability 是某一句話<br>
<br>
0:46:38.300,0:46:39.940<br>
被說出來的機率<br>
<br>
0:46:39.940,0:46:42.860<br>
而你要 estimate 某一句話被說出來的機率<br>
<br>
0:46:42.860,0:46:46.100<br>
你並不需要有聲音的 data<br>
<br>
0:46:46.100,0:46:49.340<br>
你只要去網路上爬很多很多的文字<br>
<br>
0:46:49.340,0:46:52.260<br>
你就可以計算某一段文字出現的機率<br>
<br>
0:46:52.260,0:46:54.260<br>
你不需要聲音的 data<br>
<br>
0:46:54.260,0:46:56.220<br>
這個就是 language model<br>
<br>
0:46:56.220,0:46:57.740<br>
所以在語音辨識裡面<br>
<br>
0:46:57.740,0:47:00.000<br>
我們整個 model 反而是 generative 的<br>
<br>
0:47:00.000,0:47:04.080<br>
因為你可以把 class-dependent 的部分跟 prior 的部分 <br>
<br>
0:47:04.080,0:47:05.940<br>
拆開來考慮，而 prior 的部分<br>
<br>
0:47:05.940,0:47:07.520<br>
你就用文字的 data 來處理<br>
<br>
0:47:07.520,0:47:10.620<br>
而 class-dependent 的部分，才需要聲音和文字的配合<br>
<br>
0:47:10.620,0:47:13.660<br>
這樣你可以把 prior estimate 的更精確<br>
<br>
0:47:13.660,0:47:16.420<br>
這一件事情在語音辨識裡面是很關鍵的<br>
<br>
0:47:16.500,0:47:20.600<br>
現在幾乎沒有辦法擺脫這個架構<br>
<br>
0:47:21.400,0:47:25.760<br>
那現在我們要講，我們剛剛舉的例子通通都是<br>
<br>
0:47:25.760,0:47:28.460<br>
只有兩個 class 的例子<br>
<br>
0:47:28.460,0:47:30.340<br>
接下來我們要講的是<br>
<br>
0:47:30.340,0:47:34.180<br>
如果是有兩個以上的 class，我們等一下舉的例子是<br>
<br>
0:47:34.180,0:47:37.400<br>
3 個 class 的，那應該要怎麼做呢？<br>
<br>
0:47:37.400,0:47:41.660<br>
那我們等一下就只講過程、不講原理<br>
<br>
0:47:41.660,0:47:44.760<br>
如果你想要知道原理的話<br>
<br>
0:47:44.760,0:47:46.800<br>
你可以看一下 Bishop 的教科書<br>
<br>
0:47:46.940,0:47:49.900<br>
那這個原理跟我們剛才從<br>
<br>
0:47:49.900,0:47:54.640<br>
只有兩個Class的情況呢<br>
<br>
0:47:54.640,0:47:56.680<br>
幾乎是一模一樣的<br>
<br>
0:47:56.800,0:47:58.560<br>
我相信你自己就可以推導出來<br>
<br>
0:47:58.560,0:48:02.040<br>
所以我就不想重複一個你覺得很 trivial 的東西<br>
<br>
0:48:02.040,0:48:04.300<br>
那我們就直接看它的操作怎麼做<br>
<br>
0:48:04.300,0:48:06.900<br>
假設我有三個 class<br>
<br>
0:48:06.900,0:48:08.800<br>
C1, C2 跟 C3<br>
<br>
0:48:08.800,0:48:13.840<br>
每一組 class 都有自己的 weight<br>
<br>
0:48:13.840,0:48:15.840<br>
和自己的 bias<br>
<br>
0:48:15.840,0:48:21.280<br>
這邊 w^1, w^2, w^3 分別代表 3 個 vector<br>
<br>
0:48:21.280,0:48:25.200<br>
b1, b2, b3 呢，代表 3 個 scalar<br>
<br>
0:48:26.540,0:48:27.880<br>
那接下來呢<br>
<br>
0:48:27.880,0:48:31.540<br>
input  一個 x，這個是你要分類的對象<br>
<br>
0:48:31.620,0:48:34.640<br>
你把 x 跟 w^1 做 inner product 加上 b1<br>
<br>
0:48:34.640,0:48:37.060<br>
x 跟 w^2 做 inner product 加上 b2<br>
<br>
0:48:37.060,0:48:40.200<br>
x 跟 w^3 做 inner product 加上 b3<br>
<br>
0:48:40.200,0:48:42.960<br>
你得到 z1, z2 跟 z3<br>
<br>
0:48:42.960,0:48:47.440<br>
這個 z1, z2 跟 z3 呢，它可以是任何值<br>
<br>
0:48:47.440,0:48:51.640<br>
它可以是負無窮大到正無窮大的任何值<br>
<br>
0:48:51.840,0:48:56.740<br>
接下來呢，我們把 z1, z2, z3 <br>
<br>
0:48:56.820,0:48:59.620<br>
丟進一個 Softmax 的 function<br>
<br>
0:48:59.620,0:49:02.900<br>
這個 Softmax function 它做的事情是這樣<br>
<br>
0:49:02.900,0:49:06.600<br>
把 z1, z2, z3 都取 exponential<br>
<br>
0:49:06.720,0:49:10.320<br>
得到 e^(z1), e^(z2), e^(z3)<br>
<br>
0:49:10.320,0:49:14.300<br>
接下來，把 e^(z1), e^(z2), e^(z3)<br>
<br>
0:49:14.300,0:49:16.460<br>
summation 起來<br>
<br>
0:49:16.570,0:49:19.340<br>
你得到它們的 total sum<br>
<br>
0:49:19.340,0:49:23.020<br>
然後你再把這個 total sum 分別除掉這 3 項<br>
<br>
0:49:23.020,0:49:25.100<br>
把 total sum 分別除掉這 3 項<br>
<br>
0:49:25.440,0:49:29.540<br>
得到 Softmax function 的 output, y1, y2 跟 y3<br>
<br>
0:49:29.540,0:49:33.280<br>
如果覺得有一點複雜的話，我舉一個數字<br>
<br>
0:49:33.280,0:49:38.320<br>
假設 z1 = 3, z2 = 1, z3 = -3<br>
<br>
0:49:39.360,0:49:40.780<br>
做完 exponential 以後<br>
<br>
0:49:40.780,0:49:43.200<br>
e^3 是很大的，是 20<br>
<br>
0:49:43.200,0:49:44.980<br>
e^1 是 2.7<br>
<br>
0:49:45.100,0:49:47.900<br>
e^(-3) 很小，是 0.05<br>
<br>
0:49:47.900,0:49:52.200<br>
接下來呢，你把這 3 項合起來<br>
<br>
0:49:52.200,0:49:56.720<br>
再分別去除掉，也就是做 normalization<br>
<br>
0:49:56.720,0:50:00.360<br>
那你得到的結果呢，20 就變成 0.88<br>
<br>
0:50:00.360,0:50:04.380<br>
2.7 就變成 0.12，0.05 就趨近於 0<br>
<br>
0:50:04.380,0:50:07.580<br>
那當你做完 Softmax 以後<br>
<br>
0:50:07.580,0:50:11.420<br>
原來 input z1, z2, z3 它可以是任何值<br>
<br>
0:50:11.420,0:50:15.200<br>
但是，做完 Softmax 以後，你的 output 會被限制住<br>
<br>
0:50:15.200,0:50:18.960<br>
第一個，你的 output 的值一定是介於 0~1 之間<br>
<br>
0:50:18.960,0:50:20.660<br>
首先，你 output 值一定是正的<br>
<br>
0:50:20.660,0:50:24.580<br>
不管你 z1, z2, z3 是正的還是負的，開exponential 以後<br>
<br>
0:50:24.580,0:50:26.800<br>
都變成是正的<br>
<br>
0:50:28.100,0:50:30.980<br>
那今天它的 total sum <br>
<br>
0:50:30.980,0:50:33.080<br>
一定是 1，你 output 的和一定是 1<br>
<br>
0:50:33.260,0:50:37.240<br>
因為在這個地方做了一個 normalization，<br>
所以你的 total sum 一定是 1<br>
<br>
0:50:37.240,0:50:40.700<br>
為甚麼這個東西叫 Softmax 呢？因為如果你做<br>
<br>
0:50:40.700,0:50:43.920<br>
如果是 max 的話，就是取最大值嘛<br>
<br>
0:50:44.040,0:50:45.940<br>
但是，你做 Softmax 的意思呢<br>
<br>
0:50:45.940,0:50:50.380<br>
是說你會對最大的值做強化<br>
<br>
0:50:50.380,0:50:53.920<br>
因為今天，你有取了 exponential<br>
<br>
0:50:53.920,0:50:56.120<br>
你取了 exponential 以後呢<br>
<br>
0:50:56.120,0:50:59.460<br>
大的值和小的值，他們之間的差距呢<br>
<br>
0:50:59.460,0:51:01.560<br>
會被拉得更開<br>
<br>
0:51:01.560,0:51:05.980<br>
強化它的值，這件事情呢，叫做 Softmax<br>
<br>
0:51:05.980,0:51:10.460<br>
那你就可以把這邊的每一個 y 呢<br>
<br>
0:51:10.460,0:51:13.920<br>
yi 呢，當作 input x<br>
<br>
0:51:13.920,0:51:18.100<br>
input 這個 x 是第 i 個 class 的 posterior probability<br>
<br>
0:51:18.100,0:51:20.720<br>
所以今天假設說，你 y1 是 0.88<br>
<br>
0:51:20.720,0:51:25.980<br>
也就是說，你 input x 屬於 class 1 的機率是 88%<br>
<br>
0:51:25.980,0:51:27.960<br>
屬於 class 2 的機率是 12%<br>
<br>
0:51:27.960,0:51:30.380<br>
屬於 class 3 的機率是趨近於 0<br>
<br>
0:51:30.380,0:51:32.520<br>
這個 Softmax 的 output<br>
<br>
0:51:32.520,0:51:35.660<br>
就是拿來當 z 的posterior probability<br>
<br>
0:51:35.660,0:51:37.600<br>
那你可能會問說<br>
<br>
0:51:37.680,0:51:40.600<br>
為甚麼會這樣呢？<br>
<br>
0:51:40.600,0:51:42.640<br>
事實上這件事情<br>
<br>
0:51:42.920,0:51:44.980<br>
是有辦法推導的<br>
<br>
0:51:44.980,0:51:48.580<br>
如果有人在外面演講，問我說為什麼是用 exponential<br>
<br>
0:51:48.580,0:51:50.980<br>
我就會回答說，你也可以用別的<br>
<br>
0:51:50.980,0:51:53.980<br>
因為我用別的，你也會問同樣的問題這樣子<br>
<br>
0:51:55.860,0:52:00.260<br>
但是，這個事情是有辦法講的<br>
<br>
0:52:00.260,0:52:02.280<br>
你可以去翻一下 Bishop 的教科書<br>
<br>
0:52:02.280,0:52:03.880<br>
這件事情是可以解釋的<br>
<br>
0:52:03.880,0:52:06.240<br>
如果，你今天有 3 個 class<br>
<br>
0:52:06.240,0:52:10.360<br>
假設這 3 個 class，通通都是 Gaussian distribution<br>
<br>
0:52:10.360,0:52:13.700<br>
他們共用同一個 covariance matrix<br>
<br>
0:52:13.860,0:52:17.460<br>
在這個情況下，你做一般推導以後<br>
<br>
0:52:17.460,0:52:21.620<br>
你得到的就會是這個 Softmax 的 function<br>
<br>
0:52:21.620,0:52:25.840<br>
這個就留給大家，自己做<br>
<br>
0:52:26.660,0:52:29.440<br>
如果你想要知道更多，你還可以 google 一個叫做<br>
<br>
0:52:29.640,0:52:32.100<br>
 maximum entropy 的東西<br>
<br>
0:52:32.100,0:52:35.260<br>
maximum entropy 也是一種 classify<br>
<br>
0:52:35.260,0:52:38.300<br>
但它其實跟 Logistic Regression 是一模一樣的東西<br>
<br>
0:52:38.300,0:52:40.280<br>
你只是換個名字而已<br>
<br>
0:52:40.280,0:52:42.640<br>
那它是從另外一個觀點呢<br>
<br>
0:52:42.640,0:52:47.860<br>
來切入為甚麼我們的 classifier 長這樣子<br>
<br>
0:52:47.860,0:52:50.640<br>
我們剛才說，我們可以從機率的觀點<br>
<br>
0:52:50.640,0:52:54.860<br>
假設我們用的是 Gaussian distribution，<br>
經過一般推導以後<br>
<br>
0:52:54.860,0:52:56.780<br>
你可以得到 Softmax 的 function<br>
<br>
0:52:56.780,0:52:58.780<br>
那你可以從另外一個角度<br>
<br>
0:52:58.780,0:53:01.780<br>
從 information theory 的角度去推導<br>
<br>
0:53:01.780,0:53:03.620<br>
你會得到 Softmax 這個 function<br>
<br>
0:53:03.840,0:53:06.080<br>
這個就留給大家<br>
<br>
0:53:06.080,0:53:10.000<br>
自己去研究，google maximum entropy 你會找到答案<br>
<br>
0:53:10.620,0:53:14.660<br>
所以，我們複習一下剛才做的事情<br>
<br>
0:53:14.680,0:53:16.660<br>
你就有一個 x 當作 input<br>
<br>
0:53:16.660,0:53:20.080<br>
所以你乘上 3 組不同的 weight<br>
<br>
0:53:20.080,0:53:24.200<br>
加上 3 組不同的 bias，得到 3 個不同的 z<br>
<br>
0:53:24.200,0:53:26.000<br>
通過 Softmax function <br>
<br>
0:53:26.000,0:53:27.460<br>
你就得到<br>
<br>
0:53:27.460,0:53:32.220<br>
y1, y2, y3 分別是這 3 個 class 的 posterior probability<br>
<br>
0:53:32.220,0:53:35.800<br>
可以把它合起來呢，當作是 y<br>
<br>
0:53:36.000,0:53:39.620<br>
那你在訓練呢，你要有一個 target<br>
<br>
0:53:39.620,0:53:41.140<br>
它的 target 是甚麼呢？<br>
<br>
0:53:41.240,0:53:44.220<br>
它的 target 是 y\head<br>
<br>
0:53:44.220,0:53:48.220<br>
每一維、你用 3 個 class，你 output 就是 3 維<br>
<br>
0:53:48.220,0:53:52.800<br>
這 3 維分別對應到 y1\head, y2\head 跟 y3\head<br>
<br>
0:53:52.800,0:53:55.000<br>
我們要去 minimize 的對象<br>
<br>
0:53:55.000,0:53:58.840<br>
是 y 所形成的這個 probability distribution<br>
<br>
0:53:58.840,0:54:00.500<br>
它是一個 probability distribution 嘛<br>
<br>
0:54:00.500,0:54:02.620<br>
在做完 Softmax 的時候<br>
<br>
0:54:02.620,0:54:03.880<br>
它就變成了<br>
<br>
0:54:03.880,0:54:06.480<br>
你把它當一個 probability distribution 來看待<br>
<br>
0:54:06.560,0:54:12.260<br>
你可以去計算這個 y 跟 y\head 他們之間的 cross entropy<br>
<br>
0:54:12.260,0:54:15.040<br>
它的這個 cross entropy 的式子呢<br>
<br>
0:54:15.040,0:54:17.360<br>
我就發現我寫錯了<br>
<br>
0:54:17.360,0:54:21.440<br>
這邊前面應該要有一個負號，不好意思<br>
<br>
0:54:21.570,0:54:24.800<br>
這前面應該要有一個負號<br>
<br>
0:54:24.800,0:54:26.220<br>
好<br>
<br>
0:54:27.220,0:54:30.220<br>
所以，這兩個 probability 的 cross entropy<br>
<br>
0:54:30.220,0:54:33.200<br>
它們的式子就是 y1\head<br>
<br>
0:54:33.200,0:54:35.120<br>
乘上 ln(y1)<br>
<br>
0:54:35.120,0:54:38.120<br>
y2\head * ln(y2) + y3\head * ln(y3)<br>
<br>
0:54:38.120,0:54:40.220<br>
前面再加一個負號<br>
<br>
0:54:40.220,0:54:43.040<br>
就是它們之間的 cross entropy<br>
<br>
0:54:43.080,0:54:46.660<br>
如果我們要計算 y 跟 y\head 的 cross entropy 的話<br>
<br>
0:54:46.660,0:54:50.340<br>
y\head 顯然也要是一個 probability distribution<br>
<br>
0:54:50.340,0:54:52.140<br>
我們才能夠算 cross entropy<br>
<br>
0:54:52.140,0:54:54.140<br>
怎麼算呢？<br>
<br>
0:54:55.100,0:54:58.640<br>
假設 x 是屬於 class 1 的話<br>
<br>
0:54:58.640,0:55:01.740<br>
在 training data 裡面，我們知道 x 是屬於 class 1 的話<br>
<br>
0:55:01.740,0:55:04.740<br>
它的 target 就是 [1 0 0]<br>
<br>
0:55:04.740,0:55:08.460<br>
如果是屬於 class 2 的話，它的 target 就是 [0 1 0]<br>
<br>
0:55:08.460,0:55:12.840<br>
如果是屬於 class 3 的話，它的 target 就是 [0 0 1]<br>
<br>
0:55:12.840,0:55:15.000<br>
我們之前有講過說<br>
<br>
0:55:15.000,0:55:17.300<br>
如果你設 class 1 的 target 是 1<br>
<br>
0:55:17.300,0:55:19.200<br>
class 2 的 target 是 2，class 3 的 target 是 3<br>
<br>
0:55:19.200,0:55:20.460<br>
這樣會有問題<br>
<br>
0:55:20.500,0:55:23.280<br>
你是假設說，1 跟 2 比較近、2 跟 3 比較近<br>
<br>
0:55:23.280,0:55:26.080<br>
1 跟 3 比較遠，這樣做會有問題<br>
<br>
0:55:26.080,0:55:28.740<br>
但是，如果你今天是<br>
<br>
0:55:28.740,0:55:31.720<br>
換一個假設<br>
<br>
0:55:31.720,0:55:33.880<br>
你今天是假設<br>
<br>
0:55:33.880,0:55:37.140<br>
如果 x 是屬於 class 1 的話，它的目標是 [1 0 0]<br>
<br>
0:55:37.140,0:55:39.860<br>
屬於 class 2是 [0 1 0]，屬於 class 3是 [0 0 1]<br>
<br>
0:55:39.860,0:55:42.280<br>
那你就不用假設 class 和 class 之間<br>
<br>
0:55:42.280,0:55:44.680<br>
誰跟誰比較近，誰跟誰比較遠<br>
<br>
0:55:44.680,0:55:46.360<br>
的問題這樣<br>
<br>
0:55:46.740,0:55:50.020<br>
至於這個式子，哪來的呢？<br>
<br>
0:55:50.020,0:55:54.020<br>
其實這個式子也是去 maximum likelihood<br>
<br>
0:55:54.020,0:55:58.800<br>
我們剛才在講這個 binary 的 case 的時候<br>
<br>
0:55:58.800,0:56:02.020<br>
我們講說，我們的這個 cross entropy 這個 function<br>
<br>
0:56:02.020,0:56:04.420<br>
minimize cross entropy 這件事情<br>
<br>
0:56:04.420,0:56:07.940<br>
其實是來自 maximize likelihood<br>
<br>
0:56:07.940,0:56:10.460<br>
那在有多個 head 的情況下<br>
<br>
0:56:10.460,0:56:13.400<br>
也是一模一樣的<br>
<br>
0:56:13.440,0:56:15.940<br>
它是一模一樣的<br>
<br>
0:56:15.940,0:56:19.000<br>
你就把 max likelihood 那個 function 列出來<br>
<br>
0:56:19.000,0:56:22.660<br>
那經過一番整理，你也會得到 minimize cross entropy<br>
<br>
0:56:22.660,0:56:26.040<br>
那這件事呢，就交給大家自己做<br>
<br>
0:56:26.040,0:56:28.360<br>
接下來，我要講的是<br>
<br>
0:56:28.480,0:56:31.840<br>
這個 Logistic Regression 阿，其實它是有<br>
<br>
0:56:31.840,0:56:34.420<br>
非常強的限制<br>
<br>
0:56:34.420,0:56:36.220<br>
怎麼樣的限制呢？<br>
<br>
0:56:36.240,0:56:38.380<br>
我們今天假設這樣一個 class<br>
<br>
0:56:38.380,0:56:40.300<br>
假設這樣一個 class<br>
<br>
0:56:40.300,0:56:41.960<br>
現在有 4 筆 data<br>
<br>
0:56:42.120,0:56:44.660<br>
它們每一筆 data 都有兩個 feature<br>
<br>
0:56:44.660,0:56:47.020<br>
那它們都是 binary 的 feature<br>
<br>
0:56:47.020,0:56:49.060<br>
那 class 2 呢，有兩筆 data<br>
<br>
0:56:49.060,0:56:51.600<br>
分別是 (0, 0)、(1, 1)<br>
<br>
0:56:51.600,0:56:54.960<br>
class 1 有兩筆 data，分別是 (0, 1)、(1, 0)<br>
<br>
0:56:54.960,0:56:56.620<br>
如果把它畫出來的話<br>
<br>
0:56:56.740,0:56:59.220<br>
class 1 的兩筆 data 是在這裡、在這裡<br>
<br>
0:56:59.220,0:57:02.960<br>
class 2 的兩筆 data 是在這跟這<br>
<br>
0:57:02.960,0:57:07.080<br>
如果我們想要用 Logistic Regression<br>
<br>
0:57:07.080,0:57:10.100<br>
對它做分類的話<br>
<br>
0:57:10.100,0:57:12.700<br>
我們能做到這件事情嗎？<br>
<br>
0:57:12.700,0:57:14.540<br>
我們能做到這件事情嗎？<br>
<br>
0:57:14.540,0:57:17.440<br>
你會發現說<br>
<br>
0:57:17.440,0:57:19.740<br>
這件事情<br>
<br>
0:57:19.740,0:57:21.420<br>
我們是辦不到的<br>
<br>
0:57:21.420,0:57:24.760<br>
Logistic Regression 的話，我們會希望說<br>
<br>
0:57:24.760,0:57:27.680<br>
對 Logistic Regression 的 output 而言<br>
<br>
0:57:27.680,0:57:31.960<br>
這兩個屬於 class 1 的 data，它的機率<br>
<br>
0:57:31.960,0:57:34.020<br>
要大於 0.5<br>
<br>
0:57:34.020,0:57:36.400<br>
另外兩個屬於 class 2 的 data<br>
<br>
0:57:36.400,0:57:38.740<br>
它的機率要小於 0.5<br>
<br>
0:57:38.740,0:57:42.800<br>
但這件事情，對 Logistic Regression 來說呢<br>
<br>
0:57:42.800,0:57:45.300<br>
它卡翻了，它沒有辦法做到這件事<br>
<br>
0:57:45.300,0:57:48.880<br>
因為 Logistic Regression 兩個 class 之間 boundary<br>
<br>
0:57:48.880,0:57:51.000<br>
就是一條直線<br>
<br>
0:57:51.200,0:57:53.080<br>
它的 boundary 就是一條直線<br>
<br>
0:57:53.080,0:57:56.240<br>
所以，你要分兩個 class 的時候，你只能在你的<br>
<br>
0:57:56.330,0:57:59.840<br>
feature 的平面上，畫一條直線<br>
<br>
0:57:59.840,0:58:02.460<br>
要馬畫這邊，要馬畫這邊<br>
<br>
0:58:02.460,0:58:03.880<br>
那不管你怎麼畫<br>
<br>
0:58:03.880,0:58:05.260<br>
你都沒有辦法把<br>
<br>
0:58:05.260,0:58:07.740<br>
紅色的放一邊、藍色的放一邊<br>
<br>
0:58:07.740,0:58:10.720<br>
不管你怎麼畫，你都沒有辦法把<br>
紅色的放一邊、藍色的放一邊<br>
<br>
0:58:10.720,0:58:12.760<br>
這直線可以隨便亂畫、你可以調整<br>
<br>
0:58:12.760,0:58:15.800<br>
w 跟 b，你的 weight 跟 bias<br>
<br>
0:58:16.060,0:58:19.920<br>
使得你的某些 Regression，兩個 class 之間的 boundary<br>
<br>
0:58:20.020,0:58:23.140<br>
可以是任何樣，可以是這樣、是這樣，怎麼畫都可以<br>
<br>
0:58:23.140,0:58:26.160<br>
這個 boundary 是一條直線，怎麼畫都可以<br>
<br>
0:58:26.160,0:58:28.000<br>
但你永遠沒有辦法把<br>
<br>
0:58:28.000,0:58:32.460<br>
今天這個 example 的紅色點跟藍色點分成兩邊<br>
<br>
0:58:33.300,0:58:35.200<br>
怎麼辦呢？<br>
<br>
0:58:35.200,0:58:38.060<br>
假設你還是堅持要用 Logistic Regression 的話<br>
<br>
0:58:38.060,0:58:40.620<br>
有一招叫做 Feature Transformation<br>
<br>
0:58:40.620,0:58:43.340<br>
就是你剛才的 feature 定的不好<br>
<br>
0:58:43.340,0:58:46.520<br>
原來 x1, x2 的 feature 定的不好<br>
<br>
0:58:46.520,0:58:50.260<br>
我們可以做一些轉化以後<br>
<br>
0:58:50.390,0:58:53.840<br>
找一個比較好的 feature space<br>
<br>
0:58:53.840,0:58:57.920<br>
這一個比較好的 feature space，<br>
讓 Logistic Regression 是可以處理<br>
<br>
0:58:57.920,0:59:00.840<br>
我們把 x1 跟 x2 呢<br>
<br>
0:59:00.840,0:59:03.360<br>
轉到另外一個 space 上面<br>
<br>
0:59:03.360,0:59:05.940<br>
轉到 x1' 跟 x2' 上面<br>
<br>
0:59:05.940,0:59:07.480<br>
x1' 是<br>
<br>
0:59:07.480,0:59:10.100<br>
怎麼做 feature transformation 是<br>
<br>
0:59:10.100,0:59:12.860<br>
這是很 heuristic and ad hoc的東西<br>
<br>
0:59:12.930,0:59:15.260<br>
想一個你喜歡的方式<br>
<br>
0:59:15.260,0:59:20.400<br>
舉例來說，我這邊定 x1' 就是某一個點到 (0,0) 的距離<br>
<br>
0:59:21.020,0:59:25.680<br>
x2' 就是某一個點到 (1,1) 的距離<br>
<br>
0:59:26.020,0:59:32.980<br>
如果我們把它畫出來的話<br>
<br>
0:59:33.920,0:59:37.640<br>
如果我們把它畫出來，我們先看<br>
<br>
0:59:37.640,0:59:40.060<br>
先看左下角這個點好了<br>
<br>
0:59:40.060,0:59:42.320<br>
如果我們看左下角這個點<br>
<br>
0:59:42.320,0:59:44.720<br>
它的 x1' 應該是 0<br>
<br>
0:59:44.720,0:59:47.420<br>
因為它跟 (0,0) 的距離就是 0<br>
<br>
0:59:47.840,0:59:50.860<br>
它跟 (1,1) 的距離，(0,0) 跟 (1,1) 的距離是<br>
<br>
0:59:50.860,0:59:54.140<br>
根號 2，所以 x2' 是 sqrt(2)<br>
<br>
0:59:54.140,0:59:56.900<br>
所以，經過這個 transformation，(0,0) 這個點<br>
<br>
0:59:56.900,0:59:58.240<br>
跑到這邊<br>
<br>
0:59:58.330,1:00:01.680<br>
那經過這個 transformation，(1,1) 這個點<br>
<br>
1:00:01.680,1:00:03.340<br>
跑到右下角<br>
<br>
1:00:03.380,1:00:07.240<br>
因為它跟 (0,0) 的距離是 sqrt(2)，跟 (1,1) 的距離是 0<br>
<br>
1:00:07.240,1:00:09.480<br>
經過這個 transformation，(0,1)<br>
<br>
1:00:09.680,1:00:14.220<br>
這邊又寫錯了，這邊應該是 (1,0)<br>
<br>
1:00:14.220,1:00:19.160<br>
(0,1) 和 (1,0)它們跟 (0,0) 和 (1,1) 之間的距離<br>
<br>
1:00:19.160,1:00:20.280<br>
都是一樣的<br>
<br>
1:00:20.280,1:00:22.700<br>
(0,1) 和 (0,0) 之間的距離是 1<br>
<br>
1:00:22.740,1:00:25.060<br>
(0,1) 和 (1,1) 之間的距離是 1<br>
<br>
1:00:25.060,1:00:26.900<br>
所以，經過這個 transform 以後<br>
<br>
1:00:27.190,1:00:30.380<br>
這兩個紅色的點會map，重疊在一起<br>
<br>
1:00:30.380,1:00:32.150<br>
都變成是 1<br>
<br>
1:00:32.280,1:00:34.320<br>
這個時候，對 Logistic Regression 來說<br>
<br>
1:00:34.320,1:00:35.860<br>
它可以處理這個問題了<br>
<br>
1:00:35.860,1:00:38.600<br>
因為它可以找一個 boundary，比如說，可能在這個地方<br>
<br>
1:00:38.600,1:00:42.220<br>
把藍色的點跟紅色的點分開<br>
<br>
1:00:42.480,1:00:46.280<br>
但是，麻煩的問題是這樣<br>
<br>
1:00:46.280,1:00:47.940<br>
麻煩的問題是<br>
<br>
1:00:47.940,1:00:50.540<br>
我們不知道怎麼做 feature transformation<br>
<br>
1:00:50.540,1:00:52.600<br>
如果我們花太多力氣在做 feature transformation<br>
<br>
1:00:52.880,1:00:56.100<br>
那就不是機器學習了，不是人工智慧了<br>
<br>
1:00:56.100,1:00:57.700<br>
就都是人的智慧了<br>
<br>
1:00:57.700,1:01:00.760<br>
所以，有時候我們不知道要怎麼找一個<br>
<br>
1:01:00.760,1:01:02.340<br>
好的 transformation<br>
<br>
1:01:02.340,1:01:03.780<br>
所以，我們會希望說<br>
<br>
1:01:03.780,1:01:08.180<br>
這個 transformation 是由機器自己產生的<br>
<br>
1:01:08.180,1:01:11.840<br>
怎麼讓機器自己產生這樣的 transformation 呢？<br>
<br>
1:01:11.840,1:01:15.700<br>
我們就把很多個 Logistic Regression 呢<br>
<br>
1:01:15.700,1:01:17.340<br>
cascade 起來<br>
<br>
1:01:17.500,1:01:19.560<br>
把很多 Logistic Regression 接起來<br>
<br>
1:01:19.560,1:01:21.700<br>
我們就可以做到這一的事情<br>
<br>
1:01:21.700,1:01:26.000<br>
假設 input 是 x1, x2<br>
<br>
1:01:26.280,1:01:29.420<br>
我們有一個 Logistic Regression 的 model<br>
<br>
1:01:29.420,1:01:32.760<br>
我們這邊就把 bias 省略掉，讓圖看起來比較簡單一點<br>
<br>
1:01:32.760,1:01:35.260<br>
這邊有一個 Logistic Regression 的 model<br>
<br>
1:01:35.260,1:01:37.460<br>
它的 x1 乘一個 weight，跟 x2 乘一個 weight<br>
<br>
1:01:37.590,1:01:39.660<br>
加起來以後得到 z 的 sigmoid function<br>
<br>
1:01:39.660,1:01:42.160<br>
它的 output 我們就說是<br>
<br>
1:01:42.160,1:01:44.660<br>
新的 transform 的第一維，x1'<br>
<br>
1:01:45.680,1:01:48.580<br>
我們有另外一個 Logistic Regression 的 model<br>
<br>
1:01:48.580,1:01:52.340<br>
它跟 x1 乘上一個 weight，對 x2 乘一個 weight，得到 z2<br>
<br>
1:01:52.340,1:01:55.340<br>
再通過 sigmoid function，得到 x2'<br>
<br>
1:01:55.340,1:01:58.420<br>
我們說它是 transform 後的另外一維<br>
<br>
1:01:59.040,1:02:01.900<br>
如果我們把 x1 跟 x2 經過<br>
<br>
1:02:01.900,1:02:05.860<br>
這兩個 Logistic Regression mode 的 transform<br>
<br>
1:02:05.860,1:02:10.540<br>
得到 x1' 跟 x2'，而在這個新的 transform 上面<br>
<br>
1:02:10.540,1:02:13.800<br>
class 1 和 class 2 是可以用一條直線分開的<br>
<br>
1:02:13.800,1:02:17.740<br>
那麼最後，只要再接<br>
另外一個 Logistic Regression 的 model<br>
<br>
1:02:17.740,1:02:20.220<br>
它的 input 就是 x1' 和 x2'<br>
<br>
1:02:20.220,1:02:22.800<br>
對它來說， x1' 和 x2' 就是<br>
<br>
1:02:22.800,1:02:27.300<br>
每一個 example 的 feature，不是 x1 和 x2，是 x1' 和 x2'<br>
<br>
1:02:27.300,1:02:30.380<br>
那根據 x1' 和 x2' 這個新的 feature<br>
<br>
1:02:30.380,1:02:34.500<br>
它就可以把 class 1 和 class 2 分開<br>
<br>
1:02:34.500,1:02:36.780<br>
所以前面這兩個<br>
<br>
1:02:37.660,1:02:40.280<br>
Logistic Regression 做的事情<br>
<br>
1:02:40.280,1:02:42.820<br>
就是做 feature transform 這件事情<br>
<br>
1:02:42.880,1:02:45.040<br>
它先把 feature transform 好以後<br>
<br>
1:02:45.040,1:02:49.380<br>
再由後面的，紅色的 Logistic Regression 的 model 呢<br>
<br>
1:02:49.380,1:02:52.120<br>
來做分類<br>
<br>
1:02:52.120,1:02:54.560<br>
如果舉比較實際的例子的話<br>
<br>
1:02:54.560,1:02:56.520<br>
我們看剛才那個例子<br>
<br>
1:02:56.520,1:03:00.800<br>
我們在 x1 和 x2 平面上，有 4 個點<br>
<br>
1:03:00.800,1:03:04.940<br>
我們可以調整藍色這個 Logistic Regression<br>
<br>
1:03:04.940,1:03:06.760<br>
它的 weight 的參數<br>
<br>
1:03:06.760,1:03:09.920<br>
讓它的 posterior probability 的 output 呢<br>
<br>
1:03:09.920,1:03:13.020<br>
長得像是這個圖上的顏色這樣子<br>
<br>
1:03:13.020,1:03:17.060<br>
因為這個 boundary 一定是一條直線嘛<br>
<br>
1:03:17.060,1:03:19.440<br>
所以，這個 posterior probability 的 output 呢<br>
<br>
1:03:19.440,1:03:23.220<br>
一定是長這樣子的，它的等高線一定是直的<br>
<br>
1:03:23.220,1:03:25.040<br>
在左上教的地方<br>
<br>
1:03:25.040,1:03:28.480<br>
output 的值比較大，在右下角的地方<br>
<br>
1:03:28.560,1:03:30.580<br>
output 的值比較小<br>
<br>
1:03:30.580,1:03:35.000<br>
你可以調整參數，讓這個藍色的<br>
<br>
1:03:35.320,1:03:37.640<br>
這個 Logistic Regression<br>
<br>
1:03:37.640,1:03:40.320<br>
它 input x1, x2 的時候，對這 4 個點<br>
<br>
1:03:40.320,1:03:44.740<br>
它的 output 是 0.73, 0.27, 0.27, 0.05<br>
<br>
1:03:44.740,1:03:47.220<br>
這件事情呢，是它做得到的<br>
<br>
1:03:47.220,1:03:49.420<br>
對綠色這個點來說<br>
<br>
1:03:49.420,1:03:52.320<br>
你也可以調整它的參數<br>
<br>
1:03:52.320,1:03:55.980<br>
讓它對右下角紅色這個點的 output 是 0.73<br>
<br>
1:03:55.980,1:03:58.640<br>
對藍色的點是 0.27, 0.27<br>
<br>
1:03:58.640,1:04:02.500<br>
對左上角這個點，它是 0.05<br>
<br>
1:04:02.500,1:04:04.740<br>
Logistic Regression 它可以<br>
<br>
1:04:04.740,1:04:07.620<br>
它的 boundary 一定是一條直線<br>
<br>
1:04:07.620,1:04:09.560<br>
那這個直線可以有任何的畫法<br>
<br>
1:04:09.560,1:04:11.560<br>
你可以是左邊高、右邊低<br>
<br>
1:04:11.560,1:04:15.960<br>
你可以是左上高、右下低，也可以是右下高、左上低<br>
<br>
1:04:16.000,1:04:19.820<br>
這都是做得到的，只要調整參數，都做得到這些事情<br>
<br>
1:04:19.820,1:04:21.440<br>
所以，現在呢<br>
<br>
1:04:21.440,1:04:24.980<br>
有了前面這兩個 Logistic Regression 以後<br>
<br>
1:04:24.980,1:04:27.520<br>
我們就可以把 input 的每一筆 data<br>
<br>
1:04:27.520,1:04:31.940<br>
做 feature transform得到另外一組 feature<br>
<br>
1:04:32.540,1:04:36.100<br>
有就是說呢，原來左上角這個點<br>
<br>
1:04:36.100,1:04:37.940<br>
原來左上角這個點<br>
<br>
1:04:38.000,1:04:42.200<br>
它本來在 x1, x2 的平面上是 (0, 1)<br>
<br>
1:04:42.200,1:04:45.740<br>
但是在 x1', x2' 的平面上<br>
<br>
1:04:45.740,1:04:49.100<br>
它變成是 (0.73, 0.05)<br>
<br>
1:04:49.140,1:04:52.640<br>
如果我們看右下角，這個紅色的點<br>
<br>
1:04:52.640,1:04:55.100<br>
在 x1', x2' 的平面上<br>
<br>
1:04:55.240,1:04:59.420<br>
它就是 (0.05, 0.73)<br>
<br>
1:04:59.420,1:05:05.140<br>
呃，這個 x1 跟 x2 是不是畫反了呢？<br>
<br>
1:05:05.500,1:05:07.820<br>
我看一下，對，畫反了<br>
<br>
1:05:07.820,1:05:11.800<br>
不好意思，這個 x1 跟 x2 是畫反的<br>
<br>
1:05:11.800,1:05:14.940<br>
因為你看這個是 0.05<br>
<br>
1:05:14.940,1:05:17.520<br>
然後，這個縱軸呢<br>
<br>
1:05:17.520,1:05:19.940<br>
才是比較小的，才是 0.05<br>
<br>
1:05:19.940,1:05:23.340<br>
所以，x1 跟 x2 呢，這邊的 label<br>
<br>
1:05:23.340,1:05:27.100<br>
應該要是反過來的<br>
<br>
1:05:30.580,1:05:33.520<br>
然後，我們現在把<br>
<br>
1:05:33.520,1:05:38.100<br>
紅色的點變到 (0.73, 0.05) 的位置<br>
<br>
1:05:38.100,1:05:40.980<br>
紅色的點變到 (0.05, 0.73) 的位置<br>
<br>
1:05:40.980,1:05:44.340<br>
把這兩個藍色的點變到 (0.27, 0.27) 的位置<br>
<br>
1:05:44.340,1:05:46.420<br>
我們做了這樣的轉換以後呢<br>
<br>
1:05:46.420,1:05:51.820<br>
我們就可以用紅色的這個 Logistic Regression<br>
<br>
1:05:51.820,1:05:54.600<br>
畫一條 boundary<br>
<br>
1:05:54.600,1:05:59.240<br>
把藍色的點和紅色的點分開<br>
<br>
1:05:59.240,1:06:03.460<br>
所以，如果我們只有一個 Logistic Regression<br>
<br>
1:06:03.460,1:06:07.280<br>
我們沒有辦法把這個 example 處理好<br>
<br>
1:06:07.280,1:06:10.500<br>
但是，如果我們有 3 個 Logistic Regression<br>
<br>
1:06:10.500,1:06:12.320<br>
他們被接在一起的話<br>
<br>
1:06:12.320,1:06:14.860<br>
那我們就可以把這件事情處理好<br>
<br>
1:06:14.860,1:06:18.620<br>
所以，把這些 Logistic Regression 的 model 疊在一起<br>
<br>
1:06:18.620,1:06:21.260<br>
它還滿有用的，我們可以有<br>
<br>
1:06:21.260,1:06:24.160<br>
某一個 Logistic Regression 它的 model、它的 input<br>
<br>
1:06:24.160,1:06:27.880<br>
是來自於其他 Logistic Regression 的 output<br>
<br>
1:06:27.880,1:06:30.440<br>
而某一個 Logistic Regression 的 output<br>
<br>
1:06:30.500,1:06:34.800<br>
它也可以是其他 Logistic Regression 的 input<br>
<br>
1:06:34.800,1:06:37.820<br>
我們可以把它前後相連起來<br>
<br>
1:06:37.820,1:06:40.040<br>
就變得呢，很 powerful<br>
<br>
1:06:40.040,1:06:42.660<br>
那我們呢，可以給它一個新的名字<br>
<br>
1:06:42.660,1:06:44.760<br>
我們把每一個 Logistic Regression<br>
<br>
1:06:44.760,1:06:46.440<br>
叫做一個 Neuron<br>
<br>
1:06:46.440,1:06:48.780<br>
把這些 Logistic Regression 串起來<br>
<br>
1:06:48.780,1:06:51.400<br>
所成的 network，就叫做 Neural Network<br>
<br>
1:06:51.400,1:06:53.020<br>
就叫做類神經網路<br>
<br>
1:06:53.020,1:06:56.220<br>
換了一個名字以後，它整個就潮起來了<br>
<br>
1:06:57.420,1:06:59.940<br>
你就可以騙麻瓜<br>
<br>
1:06:59.940,1:07:04.140<br>
你就可以跟麻瓜講說，我們是在模擬人類大腦的運作<br>
<br>
1:07:04.140,1:07:06.700<br>
而麻瓜就會覺得，你做的東西實在是太棒了<br>
<br>
1:07:06.700,1:07:08.880<br>
這個東西就是 deep learning 這樣<br>
<br>
1:07:08.980,1:07:10.480<br>
所以我們呢，就進入 deep learning<br>
<br>
1:07:10.480,1:07:14.300<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
