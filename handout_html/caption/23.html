<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.180<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.180,0:00:04.760<br>
接下來，我們要講的是 Neighbor Embedding<br>
<br>
0:00:04.760,0:00:08.680<br>
我們在這個作業，最後一個作業的投影片裡面啊<br>
<br>
0:00:08.680,0:00:10.420<br>
助教有提到一個東西呢<br>
<br>
0:00:10.420,0:00:12.660<br>
叫做 t-SNE<br>
<br>
0:00:12.660,0:00:17.780<br>
t-SNE 我相信你在很多不同的場合也常常聽到<br>
<br>
0:00:17.780,0:00:19.780<br>
但是如果有人問你說，<br>
<br>
0:00:19.780,0:00:21.780<br>
t-SNE 是什麼的時候<br>
<br>
0:00:21.780,0:00:23.660<br>
大家往往都答不出來<br>
<br>
0:00:23.680,0:00:26.920<br>
現在我們要來講一下 t-SNE 是什麼<br>
<br>
0:00:26.920,0:00:32.980<br>
那 t-SNE 的 “ＮＥ” <br>
其實就是 Neighbor Embedding 的縮寫<br>
<br>
0:00:32.980,0:00:35.320<br>
那我們現在要做的事情呢<br>
<br>
0:00:35.320,0:00:37.960<br>
其實就是我們之前講過的降維<br>
<br>
0:00:37.960,0:00:43.960<br>
只是我們要做的事情是「非線性」的降維<br>
<br>
0:00:43.960,0:00:45.960<br>
我們想要做到的事情是說<br>
<br>
0:00:45.960,0:00:47.520<br>
我們知道說 Data Point<br>
<br>
0:00:47.520,0:00:51.520<br>
它可能是在高維空間裡面的一個 Manifold<br>
<br>
0:00:51.520,0:00:56.460<br>
也就是說， Data Point 的分佈<br>
它其實是分佈在一個低維的空間裡面<br>
<br>
0:00:56.460,0:01:00.640<br>
只是被扭曲了塞到一個高維空間裡面<br>
<br>
0:01:00.640,0:01:03.940<br>
那，講到 Manifold 的時候<br>
<br>
0:01:03.940,0:01:08.560<br>
常常舉的例子就是這個「地球」這樣子<br>
<br>
0:01:08.560,0:01:11.920<br>
地球的表面就是一個 Manifold<br>
<br>
0:01:11.920,0:01:13.920<br>
他是一個二維的平面<br>
<br>
0:01:13.920,0:01:17.900<br>
但是被塞到了一個三維的空間裡面<br>
<br>
0:01:17.900,0:01:20.660<br>
那，在一個這樣子的情況下呢<br>
<br>
0:01:20.660,0:01:22.660<br>
在一個 Manifold 裡面呢<br>
<br>
0:01:22.660,0:01:26.040<br>
只有，這個，很近的距離的點<br>
<br>
0:01:26.040,0:01:28.040<br>
在很近的距離的情況下<br>
<br>
0:01:28.040,0:01:31.580<br>
Euclidean distance 才會成立<br>
<br>
0:01:31.580,0:01:34.940<br>
如果今天距離很遠的時候<br>
<br>
0:01:34.940,0:01:39.180<br>
這個，歐式幾何他就不一定成立<br>
<br>
0:01:39.180,0:01:43.100<br>
也就是說，假如我們在這個 S 形的空間裡面<br>
<br>
0:01:43.100,0:01:46.160<br>
取某一個點放在這邊<br>
<br>
0:01:46.160,0:01:49.940<br>
那我們比較，我們用 Euclidean distance 比較<br>
<br>
0:01:49.940,0:01:50.820<br>
這個點<br>
<br>
0:01:50.820,0:01:52.820<br>
跟這個點之間的距離<br>
<br>
0:01:52.820,0:01:55.780<br>
比較他跟他之間的距離<br>
和他跟他之間的距離<br>
<br>
0:01:55.780,0:01:57.780<br>
或許這件事情是 make sense 的<br>
<br>
0:01:57.780,0:02:00.940<br>
他跟他比較不像<br>
他跟他比較像<br>
<br>
0:02:00.940,0:02:04.460<br>
但是，如果今天是距離比較遠的時候<br>
<br>
0:02:04.460,0:02:06.460<br>
你要比較，這個點<br>
<br>
0:02:06.460,0:02:08.460<br>
跟這個點的相似程度<br>
<br>
0:02:08.460,0:02:10.460<br>
跟這個點跟這個點的相似的程度<br>
<br>
0:02:10.460,0:02:11.580<br>
你在高維的空間中<br>
<br>
0:02:11.580,0:02:14.060<br>
直接算他們的 Euclidean distance<br>
<br>
0:02:14.060,0:02:16.060<br>
就變得不 make sense 了<br>
<br>
0:02:16.060,0:02:19.640<br>
因為，如果根據 Euclidean distance<br>
<br>
0:02:19.640,0:02:21.640<br>
他跟他比較近，他跟他比較遠<br>
<br>
0:02:21.640,0:02:25.920<br>
但是，實際上呢<br>
<br>
0:02:25.920,0:02:28.520<br>
很有可能，他跟他是比較近的<br>
<br>
0:02:28.520,0:02:30.520<br>
他跟他是比較像的<br>
<br>
0:02:30.520,0:02:32.520<br>
他跟他是比較不像<br>
<br>
0:02:32.520,0:02:35.380<br>
所以， Manifold Learning 要做的事情<br>
<br>
0:02:35.380,0:02:40.180<br>
是把 S 形的這塊東西展開<br>
<br>
0:02:40.180,0:02:44.180<br>
把塞在高維空間裡面的低維空間「攤平」<br>
<br>
0:02:44.180,0:02:46.180<br>
攤平的好處就是<br>
<br>
0:02:46.180,0:02:49.720<br>
現在，如果我們把這個低維空間攤平以後<br>
<br>
0:02:49.720,0:02:50.660<br>
我們做降維<br>
<br>
0:02:50.660,0:02:55.840<br>
然後，把這個塞在高維空間裡面的 Manifold 攤平以後<br>
<br>
0:02:55.840,0:02:58.820<br>
那我們就可以在這個 Manifold 上面<br>
<br>
0:02:58.820,0:03:00.820<br>
用 Euclidean distance來<br>
<br>
0:03:00.820,0:03:02.820<br>
我們就可以在這個平面上用 Euclidean distance<br>
<br>
0:03:02.820,0:03:06.140<br>
在降維以後，我們就可以用 Euclidean distance<br>
<br>
0:03:06.140,0:03:08.140<br>
來計算點和點之間的距離<br>
<br>
0:03:08.140,0:03:11.320<br>
這會對接下來如果你要做 clustering<br>
<br>
0:03:11.320,0:03:14.640<br>
或者是，你要做接下來的 supervised learning<br>
<br>
0:03:14.640,0:03:16.640<br>
都是會有幫助的<br>
<br>
0:03:16.640,0:03:18.640<br>
那類似方法有很多<br>
<br>
0:03:18.640,0:03:20.640<br>
那我們今天就很快的介紹幾種方法<br>
<br>
0:03:20.640,0:03:24.020<br>
在最後，講一下t-SNE<br>
<br>
0:03:24.920,0:03:28.400<br>
那有個方法叫做 Locally Linear Embedding<br>
<br>
0:03:28.400,0:03:30.000<br>
這個方法意思是說<br>
<br>
0:03:30.080,0:03:33.500<br>
在原來的空間裡面<br>
你的點的分佈是長這個樣子<br>
<br>
0:03:33.500,0:03:36.060<br>
那有某一個點，叫做 xi<br>
<br>
0:03:36.060,0:03:40.060<br>
然後我們先選出這個 xi 的 neighbor<br>
<br>
0:03:40.060,0:03:42.060<br>
然後我們叫做 xj<br>
<br>
0:03:42.060,0:03:47.540<br>
那接下來呢，我們要找 xi 跟 xj的關係<br>
<br>
0:03:47.540,0:03:52.240<br>
那 xi 跟 xj 的關係，我們寫做 “wij”<br>
<br>
0:03:52.240,0:03:56.220<br>
wij 代表 xi 和 xj 的關係<br>
<br>
0:03:56.220,0:03:58.840<br>
這個 wij 是怎麼找出來的呢？<br>
<br>
0:03:58.840,0:04:00.840<br>
wij 是這個樣子<br>
<br>
0:04:00.840,0:04:02.480<br>
我們假設說<br>
<br>
0:04:02.480,0:04:08.460<br>
每一個 xi，都可以用它的 neighbor<br>
<br>
0:04:08.460,0:04:10.460<br>
做 linear combination 以後<br>
<br>
0:04:10.460,0:04:12.460<br>
組合而成<br>
<br>
0:04:12.460,0:04:14.460<br>
這個 wij 就是<br>
<br>
0:04:14.460,0:04:20.100<br>
拿 j 去組合 i 的時候<br>
<br>
0:04:20.100,0:04:27.960<br>
wij 就是拿 xj 去組合 xi 的時候的 <br>
linear combination 的 weight<br>
<br>
0:04:27.960,0:04:30.140<br>
那要找這組 wij 怎麼做呢<br>
<br>
0:04:30.140,0:04:34.580<br>
也就是說，我們現在要找一組 wij<br>
<br>
0:04:34.580,0:04:41.540<br>
這組 wij 對 xi 的所有 neighbor xj <br>
做 weighted sum的時候<br>
<br>
0:04:41.540,0:04:44.820<br>
他可以跟 xi 越接近越好<br>
<br>
0:04:44.820,0:04:46.640<br>
或者是 xi 減掉<br>
<br>
0:04:46.640,0:04:49.820<br>
summation over 所有的 j<br>
wij 乘以 xj<br>
<br>
0:04:49.820,0:04:53.000<br>
他的 two norm 是越接近越好的<br>
<br>
0:04:53.000,0:04:57.380<br>
xi 跟 xj 的 linear combination 他們是越接近越好<br>
<br>
0:04:57.380,0:05:02.880<br>
然後 summation over 所有的 data point i<br>
<br>
0:05:02.880,0:05:07.020<br>
然後，接下來我們就要做 dimension reduction<br>
<br>
0:05:07.020,0:05:12.120<br>
把原來所有的 xi 跟 xj 轉成 zi 和 zj<br>
<br>
0:05:12.120,0:05:14.940<br>
但是，現在這邊的原則就是<br>
<br>
0:05:14.940,0:05:18.760<br>
從 xi 跟 xj 轉成 zi 跟 zj<br>
<br>
0:05:18.760,0:05:22.900<br>
他們中間的關係 "wij"，是不變的<br>
<br>
0:05:22.900,0:05:24.900<br>
這個東西，就是那個<br>
<br>
0:05:24.900,0:05:28.980<br>
白居易的長恨歌裡面講的這句話這樣子<br>
<br>
0:05:28.980,0:05:30.660<br>
我想那句話叫什麼<br>
<br>
0:05:30.660,0:05:33.660<br>
這句話是出自長恨歌的結尾，它是<br>
<br>
0:05:35.100,0:05:40.620<br>
臨別殷勤重寄詞，詞中有誓兩心知。<br>
七月七日長生殿，夜半無人私語時。<br>
<br>
0:05:40.620,0:05:46.140<br>
在天願作比翼鳥，在地願為連理枝。<br>
天長地久有時盡，此恨綿綿無絕期。<br>
<br>
0:05:47.820,0:05:51.960<br>
並不是我國文特好，謝謝～<br>
<br>
0:05:53.240,0:05:55.760<br>
你不要誤會可能是我國文特別強，我其實沒有很強<br>
<br>
0:05:55.760,0:05:58.260<br>
為什麼我會背，其實我可以從頭背到尾，為什麼呢？<br>
<br>
0:05:59.100,0:06:02.320<br>
因為小時候我不知道犯了什麼錯，被老師懲罰背長恨歌<br>
<br>
0:06:02.520,0:06:04.980<br>
然後我就整首背起來了<br>
<br>
0:06:05.460,0:06:07.580<br>
你小時候背的東西，到長大其實不會忘<br>
<br>
0:06:10.100,0:06:11.740<br>
這個是什麼意思呢？所謂的<br>
<br>
0:06:12.300,0:06:17.300<br>
在天就是 xi 跟 xj，在原來的 space 上面<br>
<br>
0:06:17.820,0:06:20.380<br>
比翼鳥就是 wij<br>
<br>
0:06:20.920,0:06:24.460<br>
在地就是把 xi 跟 xj transform 到另外一個 space<br>
<br>
0:06:24.460,0:06:26.460<br>
就是 zi 跟 zj<br>
<br>
0:06:26.460,0:06:29.420<br>
連理枝就是比翼鳥就等於連理枝<br>
<br>
0:06:29.420,0:06:32.140<br>
所以他們關係還是 Wij<br>
<br>
0:06:33.500,0:06:36.680<br>
所以 LLE 它做的事情是這樣的<br>
<br>
0:06:38.360,0:06:42.120<br>
首先 wij 在原來的 space 上面找完以後<br>
<br>
0:06:42.120,0:06:44.120<br>
就 fix 住它，不要去動它<br>
<br>
0:06:44.740,0:06:48.200<br>
接下來，你為每一個 xi 跟 xj<br>
<br>
0:06:48.600,0:06:50.600<br>
找另外一個 vector<br>
<br>
0:06:50.600,0:06:52.600<br>
我們現在要做 dimension reduction<br>
<br>
0:06:52.600,0:06:55.540<br>
所以你新找的 vector 要比原本的 dimension 還要小<br>
<br>
0:06:55.540,0:06:58.780<br>
原本 100 維，降維後要比較小，10 維、2維之類的<br>
<br>
0:07:00.020,0:07:02.680<br>
zi 跟 zj 是另外的 vector<br>
<br>
0:07:03.500,0:07:04.900<br>
那我們要找這個<br>
<br>
0:07:05.880,0:07:07.880<br>
zi 跟 zj 它可以 minimize 什麼呢<br>
<br>
0:07:08.360,0:07:10.360<br>
它可以 minimize 下面這個 function<br>
<br>
0:07:10.960,0:07:14.680<br>
也就是說原來這個 xi <br>
它可以做 linear combination 產生 x<br>
<br>
0:07:15.380,0:07:18.140<br>
原來這些 xj 可以做 linear combination 產生 xi<br>
<br>
0:07:18.880,0:07:24.480<br>
這些 zj 也可以用同樣的 linear combination產生 zi<br>
<br>
0:07:25.580,0:07:28.420<br>
這些 zj 也可以用同樣的 linear combination 產生 zi<br>
<br>
0:07:28.420,0:07:31.500<br>
我們就是要找這組 z 可以滿足 wij 給我們的 constraint<br>
<br>
0:07:31.980,0:07:34.080<br>
所以，現在在這個式子裡面<br>
<br>
0:07:34.300,0:07:36.060<br>
wij 變成是已知的<br>
<br>
0:07:36.740,0:07:40.520<br>
但是我們要找一組 z，讓 zj 透過 wij<br>
<br>
0:07:41.000,0:07:44.080<br>
做 weighted sum 以後，它可以跟 zi 越接近越好<br>
<br>
0:07:44.180,0:07:48.360<br>
zj 用這組 weight 做 linear combination 後，它可以跟 zi 越接近越好<br>
<br>
0:07:48.360,0:07:51.180<br>
然後 summation over 所有的 datapoint<br>
<br>
0:07:52.440,0:07:56.500<br>
那這個 LLE 你要注意一下，其實它並沒有一個<br>
<br>
0:07:57.340,0:07:59.160<br>
明確的 function<br>
<br>
0:07:59.840,0:08:02.700<br>
告訴你說我們怎麼做 dimension reduction<br>
<br>
0:08:02.700,0:08:04.700<br>
不像我們在做 Auto-encoder 的時候<br>
<br>
0:08:05.540,0:08:06.940<br>
你 learn 出一個 encoder 的 network<br>
<br>
0:08:06.940,0:08:11.000<br>
input 一個新的 datapoint，然後可以找到它 dimension 的結果<br>
<br>
0:08:11.580,0:08:12.860<br>
今天在 LLE 裡面<br>
<br>
0:08:13.060,0:08:17.180<br>
你並不會找到一個很明確的 function，沒有一個 function 告訴我們說<br>
<br>
0:08:17.560,0:08:19.560<br>
怎麼從 x 變到 z<br>
<br>
0:08:20.260,0:08:22.520<br>
z 就是完全憑空找出來的<br>
<br>
0:08:24.860,0:08:28.000<br>
其實如果你用 LLE 這種方法<br>
<br>
0:08:28.000,0:08:30.500<br>
如果用 LLE 或其他類似的方法會有一個好處<br>
<br>
0:08:30.500,0:08:33.380<br>
就算是你原來這個 xi、xj 你不知道<br>
<br>
0:08:33.380,0:08:35.380<br>
你只知道 wij<br>
<br>
0:08:35.380,0:08:37.660<br>
你不知道 xi、xj 要用什麼 vector 來描述它<br>
<br>
0:08:37.660,0:08:39.660<br>
你只知道他們的關係<br>
<br>
0:08:39.660,0:08:42.520<br>
你其實也可以用 LLE 這種方法<br>
<br>
0:08:42.520,0:08:47.940<br>
那 LLE 呢，你其實需要好好的調一下你的 neighbor 選幾個<br>
<br>
0:08:47.940,0:08:53.240<br>
你 neighbor 選的數目要剛剛好才會得到好的結果<br>
<br>
0:08:53.240,0:08:57.360<br>
那這邊，是從原始的 paper 裡面截出來的圖<br>
<br>
0:08:57.360,0:08:59.620<br>
原始的 paper 它的題目實在是太潮了<br>
<br>
0:08:59.620,0:09:04.140<br>
他是 “Think Globally, Fit Locally” 這樣子<br>
<br>
0:09:04.140,0:09:06.560<br>
太精彩了，很好的一個題目<br>
<br>
0:09:06.560,0:09:10.180<br>
然後，它調了不同的 K<br>
<br>
0:09:10.180,0:09:12.720<br>
如果你今天 K 太小<br>
<br>
0:09:12.720,0:09:14.260<br>
你得出來的結果會不太好<br>
<br>
0:09:14.260,0:09:16.260<br>
K 太大，得到的結果也不太好<br>
<br>
0:09:16.260,0:09:18.260<br>
為什麼 K 太大得到的結果不太好呢？<br>
<br>
0:09:18.260,0:09:20.260<br>
因為我們之前的假設<br>
<br>
0:09:20.260,0:09:24.260<br>
就是每一個這個 Euclidean distance<br>
<br>
0:09:24.260,0:09:28.280<br>
只在很近的距離之內，可以被這樣想<br>
<br>
0:09:28.280,0:09:31.800<br>
所以，這個點和點之間的<br>
<br>
0:09:31.800,0:09:34.140<br>
data point 和 data point 之間的關係<br>
<br>
0:09:34.140,0:09:36.980<br>
在 transform前後可以被 keep 住<br>
<br>
0:09:36.980,0:09:38.980<br>
只有在距離很近的時候<br>
<br>
0:09:38.980,0:09:40.400<br>
才能夠成立<br>
<br>
0:09:40.400,0:09:42.080<br>
當你 K 選很大的時候<br>
<br>
0:09:42.080,0:09:43.720<br>
你會考慮一些距離很遠的點<br>
<br>
0:09:43.720,0:09:45.720<br>
你會考慮一些 transform 以後<br>
<br>
0:09:45.720,0:09:47.720<br>
relation 沒有辦法 keep 住的點<br>
<br>
0:09:47.720,0:09:50.600<br>
有一些關係太弱了，transform 以後沒有辦法 keep 住<br>
<br>
0:09:50.600,0:09:53.420<br>
那它不是這個比翼鳥或連理枝的關係<br>
<br>
0:09:53.420,0:09:54.740<br>
它太弱了，無法 keep 住<br>
<br>
0:09:54.740,0:09:56.040<br>
但是你不應該把它考慮進來<br>
<br>
0:09:56.040,0:09:58.560<br>
所以你的 K，要選一個適當的值<br>
<br>
0:10:00.060,0:10:03.280<br>
那另外一個方法叫 Laplacian Eigenmap<br>
<br>
0:10:03.280,0:10:04.940<br>
它的想法是這樣子<br>
<br>
0:10:04.940,0:10:06.940<br>
我們之前有說<br>
<br>
0:10:06.940,0:10:10.940<br>
我們之前在講這個 Semi-supervised learning 的時候<br>
<br>
0:10:10.940,0:10:13.720<br>
我們有講過 smoothness assumption<br>
<br>
0:10:13.720,0:10:17.160<br>
我們有說，如果你想要比較這個點跟這個點之間的距離<br>
<br>
0:10:17.160,0:10:20.840<br>
只算它的 Euclidean distance 是不足夠的<br>
<br>
0:10:20.840,0:10:26.660<br>
你要看的是它們在這個 high density 的 region <br>
之間的 distance<br>
<br>
0:10:26.660,0:10:27.800<br>
如果兩個點之間<br>
<br>
0:10:27.800,0:10:30.700<br>
它們有 high density 的 connection<br>
<br>
0:10:30.700,0:10:33.960<br>
那它們才是真正的接近<br>
<br>
0:10:34.980,0:10:35.560<br>
這件事情<br>
<br>
0:10:35.560,0:10:39.960<br>
你可以用一個 graph 來描述這件事情<br>
<br>
0:10:39.960,0:10:41.540<br>
也就是說你把你的 data point<br>
<br>
0:10:41.540,0:10:43.000<br>
construct 成一個 graph<br>
<br>
0:10:43.000,0:10:45.380<br>
你算你的 data point 兩兩之間的相似度<br>
<br>
0:10:45.380,0:10:47.600<br>
如果相似度超過一個 thereshold<br>
<br>
0:10:47.600,0:10:49.700<br>
就把它們 connect 起來<br>
<br>
0:10:49.700,0:10:50.960<br>
而建一個 graph 的方法有很多<br>
<br>
0:10:50.960,0:10:53.080<br>
有很多不同的方法<br>
<br>
0:10:53.080,0:10:54.560<br>
總之，你就把比較近的點<br>
<br>
0:10:54.560,0:10:56.560<br>
把它們連起來變成一個 graph<br>
<br>
0:10:57.720,0:11:00.740<br>
那你把點變成 graph 以後<br>
<br>
0:11:00.740,0:11:06.020<br>
你考慮這個 smoothness 的距離<br>
<br>
0:11:06.020,0:11:11.300<br>
就可以被這個 graph 上面的 connection 來 approximate<br>
<br>
0:11:11.300,0:11:14.760<br>
那我們之前在講 Semi-supervised learning 的時候<br>
<br>
0:11:14.760,0:11:15.840<br>
我們是這樣說的<br>
<br>
0:11:15.840,0:11:20.940<br>
如果今天 x1 跟  x2 在 high density 的 region 上面<br>
<br>
0:11:20.940,0:11:22.940<br>
它們是相近的<br>
<br>
0:11:22.940,0:11:24.940<br>
那它們的 label<br>
<br>
0:11:24.940,0:11:28.340<br>
y1\head 和 y2\head，很有可能是一樣的<br>
<br>
0:11:28.340,0:11:31.380<br>
雖然說 Semi-supervised learning 的時候<br>
<br>
0:11:31.380,0:11:32.360<br>
我們可以這麼做<br>
<br>
0:11:32.360,0:11:38.180<br>
我們有一項是考慮有 label 的 data 的項<br>
<br>
0:11:38.180,0:11:41.960<br>
有另外一項是跟 labeled data 沒有關係的<br>
<br>
0:11:41.960,0:11:45.420<br>
我們只可以利用這個 unlabeled 的 data<br>
<br>
0:11:45.420,0:11:47.020<br>
那這一項的作用<br>
<br>
0:11:47.020,0:11:51.140<br>
它是要考慮說我們現在得到的 label 是不是 smooth 的<br>
<br>
0:11:51.140,0:11:53.960<br>
它的作用很像一個 regularization 的 term<br>
<br>
0:11:53.960,0:11:55.960<br>
那這個 S 這一項啊<br>
<br>
0:11:55.960,0:11:57.960<br>
這個 S 這一項啊<br>
<br>
0:11:57.960,0:12:02.820<br>
它等於 yi 減 yj 的這個<br>
<br>
0:12:02.820,0:12:06.580<br>
它跟 yi 跟 yj 的這個距離<br>
<br>
0:12:06.580,0:12:08.200<br>
然後乘上 wi, j<br>
<br>
0:12:08.200,0:12:10.840<br>
也就是說，那個 wi, j 是什麼呢<br>
<br>
0:12:10.840,0:12:17.300<br>
wi, j 是說，如果今天兩個 data point xi 跟 xj<br>
<br>
0:12:17.300,0:12:18.720<br>
他們在圖上是相連的<br>
<br>
0:12:18.720,0:12:21.240<br>
那 wi, j 就是他們的相似成度<br>
<br>
0:12:21.240,0:12:23.240<br>
如果在圖上是沒有相連的<br>
<br>
0:12:23.240,0:12:24.140<br>
他就是 0<br>
<br>
0:12:24.140,0:12:25.470<br>
然後我們說<br>
<br>
0:12:25.470,0:12:29.980<br>
如果今天wi 跟 wj 他們很相近的話<br>
<br>
0:12:29.980,0:12:31.980<br>
那 wi, j 就有一個很大的值<br>
<br>
0:12:31.980,0:12:34.480<br>
就是希望yi 跟 yj 越接近越好<br>
<br>
0:12:34.480,0:12:36.160<br>
反之如果 wi, j 是 0<br>
<br>
0:12:36.160,0:12:39.820<br>
那 yi 跟 yj 要是什麼值都可以<br>
<br>
0:12:39.820,0:12:40.860<br>
那這個 S<br>
<br>
0:12:40.860,0:12:45.580<br>
就是evaluate 說，你現在得到的 label 有多麽的 smooth<br>
<br>
0:12:45.580,0:12:46.400<br>
那麼還說<br>
<br>
0:12:46.460,0:12:51.880<br>
這個 S 可以寫成一個 y 的 vector 乘上 L 再乘上 y<br>
<br>
0:12:51.880,0:12:54.520<br>
這個 L 就是 graph 的 laplacian<br>
<br>
0:12:54.520,0:12:56.240<br>
L 等於 E 減 W<br>
<br>
0:12:56.240,0:12:58.700<br>
這個大家回去 check 一下之前的投影片就知道了<br>
<br>
0:12:58.700,0:13:03.540<br>
那同樣的道理可以被 apply 在完全<br>
unsupervised 的 test 上面<br>
<br>
0:13:03.540,0:13:04.300<br>
我們可以說<br>
<br>
0:13:04.300,0:13:08.420<br>
如果 x1 跟 x2 在 high density 的 region 他們是 close 的<br>
<br>
0:13:08.420,0:13:13.640<br>
那我們就會希望， z1 跟 z2 他們也是相近的<br>
<br>
0:13:13.640,0:13:17.920<br>
所以我們可以一樣把剛才那個<br>
smoothness 的式子寫出來<br>
<br>
0:13:17.920,0:13:18.980<br>
我們可以寫說<br>
<br>
0:13:18.980,0:13:21.920<br>
我這邊寫平方可能是比較不好啦<br>
<br>
0:13:21.920,0:13:25.740<br>
我這邊應該寫  Euclidean distance 比較好<br>
<br>
0:13:25.740,0:13:28.340<br>
或者寫他的這個 two norm distance 比較好<br>
<br>
0:13:28.340,0:13:30.040<br>
因為我只是從前面投影片 copy 過來<br>
<br>
0:13:30.040,0:13:32.040<br>
只是把 y 改成 z 而已<br>
<br>
0:13:32.040,0:13:33.340<br>
就忘了把它改過來<br>
<br>
0:13:33.340,0:13:41.320<br>
好沒關係，那今天這個把 x1 跟 x2 變成 z1 跟 z2 以後<br>
<br>
0:13:41.320,0:13:46.580<br>
你的這個 z1 跟 z2 應該是長什麼樣子呢<br>
<br>
0:13:46.580,0:13:47.960<br>
你今天應該是這樣子<br>
<br>
0:13:47.960,0:13:49.960<br>
如果今天 xi 跟 xj<br>
<br>
0:13:49.960,0:13:53.440<br>
如果這個 i 跟 j 的這兩個 datapoint 他們之間的 wi, j 很像<br>
<br>
0:13:53.440,0:13:57.560<br>
那 zi 跟 zj 做完 dimension reduction 以後他們的距離就很近<br>
<br>
0:13:57.560,0:14:00.980<br>
反之呢，如果他們的 wi, j 很小<br>
<br>
0:14:00.980,0:14:03.600<br>
那他們的距離要怎樣就都可以<br>
<br>
0:14:03.600,0:14:05.920<br>
你覺得，這樣 ok 嗎？<br>
<br>
0:14:05.920,0:14:14.960<br>
我們就找一個 zi 跟 zj minimize 這個 S 的值<br>
<br>
0:14:14.960,0:14:18.000<br>
你不覺得這麼做是有問題的嗎<br>
<br>
0:14:19.140,0:14:22.100<br>
給大家五秒鐘想想看，這個問題出在哪裡<br>
<br>
0:14:24.720,0:14:26.640<br>
其實，這個問題是這樣子<br>
<br>
0:14:26.640,0:14:29.620<br>
你不需要告訴我 wi, j 是什麼<br>
<br>
0:14:29.620,0:14:31.260<br>
我一秒就可以告訴你說<br>
<br>
0:14:31.260,0:14:34.000<br>
要 minimize x 的時候，我應該選什麼值<br>
<br>
0:14:34.000,0:14:36.000<br>
心算就可以得到了<br>
<br>
0:14:36.000,0:14:36.880<br>
選什麼值呢？<br>
<br>
0:14:36.880,0:14:39.680<br>
我把所有的 zi 跟 zj 通通設一樣的值就好了<br>
<br>
0:14:39.680,0:14:40.520<br>
結束<br>
<br>
0:14:40.520,0:14:44.700<br>
把所有 zi 跟 zj 我通通都設一樣的值，比如說通通都設0<br>
<br>
0:14:44.700,0:14:46.980<br>
那 S 就等於 0<br>
<br>
0:14:46.980,0:14:49.980<br>
這個問題就結束了<br>
<br>
0:14:49.980,0:14:53.640<br>
所以，光是有這個式子是不夠的<br>
<br>
0:14:53.640,0:14:56.940<br>
那你可能說剛剛在 Semi-supervised learning 的時候<br>
你怎麼不講這句話呢<br>
<br>
0:14:56.940,0:14:58.840<br>
之前在 Semi-supervised learning 的時候<br>
<br>
0:14:58.840,0:15:03.380<br>
我們還有 supervised learning，"supervise" 那個 labeled data給我們的那一項<br>
<br>
0:15:03.380,0:15:09.000<br>
所以，如果你把所有的 y，所有的 label 都設成一樣的<br>
<br>
0:15:09.000,0:15:12.320<br>
那你在 supervise 那一項，你得到的 lost 就會很大<br>
<br>
0:15:12.320,0:15:17.640<br>
那我們要同時 balance supervise 那一項<br>
跟 semi-supervise 那一項<br>
<br>
0:15:17.640,0:15:21.340<br>
我們要同時 balance supervise 那一項的 cost <br>
跟 regularization 的 term<br>
<br>
0:15:21.340,0:15:25.020<br>
所以你不會選擇讓所有的 y 通通都是一樣的<br>
<br>
0:15:25.020,0:15:27.120<br>
但在這邊少了 supervise 的東西<br>
<br>
0:15:27.120,0:15:30.320<br>
所以變成選擇所有的 z 都是一樣<br>
<br>
0:15:30.320,0:15:32.320<br>
反而是一個最好的 solution<br>
<br>
0:15:32.320,0:15:34.380<br>
所以，這件事情是行不通的<br>
<br>
0:15:34.380,0:15:36.380<br>
怎麼辦呢？<br>
<br>
0:15:36.380,0:15:39.520<br>
你要給你的 z 一些 constraint<br>
<br>
0:15:39.520,0:15:41.240<br>
什麼樣的 constraint 呢？<br>
<br>
0:15:41.240,0:15:43.240<br>
會給的 constraint 是這樣<br>
<br>
0:15:43.240,0:15:48.960<br>
如果今天 z 他降維以後的空間是 M 維的空間<br>
<br>
0:15:48.960,0:15:52.380<br>
是 M 維的空間，比方說 M 是 2 之類的<br>
<br>
0:15:52.380,0:15:54.380<br>
那你不會希望說<br>
<br>
0:15:54.380,0:16:00.940<br>
你的 z 他還分佈在一個比 M 還要小的 dimension 裡面<br>
<br>
0:16:00.940,0:16:02.620<br>
因為我們現在要做的事情<br>
<br>
0:16:02.620,0:16:06.920<br>
是希望把高維空間中塞進去的低維空間展開<br>
<br>
0:16:06.920,0:16:09.340<br>
那我們不希望說我們展開以後<br>
<br>
0:16:09.340,0:16:12.840<br>
其實展開的結果他在一個更低維的空間裡面<br>
<br>
0:16:12.840,0:16:16.020<br>
所以，今天假如你的 z 的 dimension 是 M 的話<br>
<br>
0:16:16.020,0:16:19.720<br>
你會希望你找出來的那些點<br>
<br>
0:16:19.720,0:16:21.720<br>
假設現在總共有 N 個點<br>
<br>
0:16:21.720,0:16:24.000<br>
z1 到 zN<br>
<br>
0:16:24.000,0:16:28.020<br>
他們做 span 以後，會等於 RM<br>
<br>
0:16:28.020,0:16:32.940<br>
也就是說 z，他不是活在一個比 M 維更低維的空間裡面<br>
<br>
0:16:32.940,0:16:36.960<br>
他就會佔據整個 M 維的空間這樣子<br>
<br>
0:16:36.960,0:16:39.660<br>
其實，如果你要解這個式子的話<br>
<br>
0:16:39.660,0:16:41.660<br>
你解一解就會發現說<br>
<br>
0:16:41.660,0:16:49.460<br>
你解出來的這個 z 跟我們前面看到的那個<br>
graph laplacian L 是有關係的<br>
<br>
0:16:49.460,0:16:55.140<br>
他其實就是 graph laplacian 的 eigenvector<br>
<br>
0:16:55.140,0:17:00.580<br>
對應到比較小的 eigenvalue 的那些 eigenvector<br>
<br>
0:17:00.580,0:17:03.160<br>
這個大家再自己 check 一下文件就好<br>
<br>
0:17:03.160,0:17:05.860<br>
所以他叫做 Laplacian Eigenmap<br>
<br>
0:17:05.860,0:17:09.460<br>
因為我們找的是那個 laplacian matrix 的 eigenvector<br>
<br>
0:17:09.460,0:17:11.460<br>
如果你今天先找出 z 以後<br>
<br>
0:17:11.460,0:17:13.460<br>
你再去做 cluster<br>
<br>
0:17:13.460,0:17:16.220<br>
你先找出 z，再用 K-means 做 clustering 的話<br>
<br>
0:17:16.220,0:17:20.940<br>
這一招有一個很潮的名字，叫做 spectral clustering<br>
<br>
0:17:20.940,0:17:25.060<br>
那接下來呢，我們就要講 t-SNE<br>
<br>
0:17:25.060,0:17:31.320<br>
那 t-SNE 他是 T-distributed Stochastic Neighbor Embedding 的縮寫<br>
<br>
0:17:31.320,0:17:34.880<br>
那 t-SNE 他解決什麼樣的問題呢<br>
<br>
0:17:34.880,0:17:38.020<br>
前面那些問題啊，有一個最大的問題就是<br>
<br>
0:17:38.020,0:17:42.520<br>
他只假設相近的點應該要是接近的<br>
<br>
0:17:42.520,0:17:47.780<br>
但他們沒有假設說，不相近的點沒有要接近<br>
<br>
0:17:47.780,0:17:50.840<br>
他沒有假設說，不相近的點要分開<br>
<br>
0:17:50.840,0:17:54.780<br>
所以比如說你用 LLE 在 MNIST 上<br>
<br>
0:17:54.780,0:17:56.260<br>
你會遇到這樣的情形<br>
<br>
0:17:56.260,0:17:59.920<br>
他確實會把不同的點呢<br>
<br>
0:17:59.920,0:18:05.060<br>
他確實會把，<br>
這個不同的顏色代表不同的 digit 不同的 class，<br>
<br>
0:18:05.060,0:18:10.000<br>
他確實會把不同的 class都塞在一起<br>
<br>
0:18:10.000,0:18:13.980<br>
然後他確實會把同個 class 的點都聚集在一起<br>
<br>
0:18:13.980,0:18:18.940<br>
但他沒有防止說不同 class 的點不要疊成一團<br>
<br>
0:18:18.940,0:18:20.940<br>
如果他們都疊成一團的時候<br>
<br>
0:18:20.940,0:18:22.120<br>
沒有這個顏色<br>
<br>
0:18:22.120,0:18:25.320<br>
這些點還是擠在一起，你還是沒有辦法分開<br>
<br>
0:18:25.320,0:18:27.320<br>
這是另外一個例子<br>
<br>
0:18:27.320,0:18:29.780<br>
做在這個 COIL-20上面<br>
<br>
0:18:29.780,0:18:33.960<br>
COIL 是一個 image 的 corpus<br>
<br>
0:18:33.960,0:18:36.920<br>
他裡面的 image 就是某一個圖<br>
<br>
0:18:36.920,0:18:39.160<br>
比如說一個汽車、一個玩具車<br>
<br>
0:18:39.160,0:18:45.480<br>
然後把它轉一圈，然後拍很多張不同的照片這樣子<br>
<br>
0:18:45.480,0:18:49.340<br>
這邊同樣的顏色代表同一個 object<br>
<br>
0:18:49.340,0:18:53.000<br>
你會發現說，它找到一些圈圈<br>
<br>
0:18:53.000,0:18:54.700<br>
這個圈圈代表什麼意思呢？<br>
<br>
0:18:54.700,0:19:00.480<br>
這個圈圈代表，同一個 object 在做旋轉的時候<br>
<br>
0:19:00.480,0:19:02.720<br>
它每一張圖都很像<br>
<br>
0:19:02.720,0:19:07.720<br>
但是，旋轉的時候每一張圖都很像<br>
<br>
0:19:07.720,0:19:11.120<br>
但是你看一個東西的正面和側面，他卻是非常不一樣<br>
<br>
0:19:11.120,0:19:15.620<br>
所以，這個圈圈就顯示你把某一張圖做旋轉以後<br>
<br>
0:19:15.620,0:19:16.480<br>
你所得到<br>
<br>
0:19:16.480,0:19:18.860<br>
在做 dimension reduction 以後<br>
<br>
0:19:18.860,0:19:20.960<br>
你所得到的結果<br>
<br>
0:19:20.960,0:19:22.020<br>
但是一樣<br>
<br>
0:19:22.020,0:19:26.260<br>
你會發現說，不同的 object 其實他們是擠成一團的<br>
<br>
0:19:26.260,0:19:27.640<br>
沒有辦法分開<br>
<br>
0:19:27.640,0:19:31.420<br>
所以如果做 t-SNE的話，做 t-SNE 是怎樣呢？<br>
<br>
0:19:31.440,0:19:33.440<br>
做 t-SNE，我們一樣是要做降維<br>
<br>
0:19:33.440,0:19:35.520<br>
把原來的 data point x<br>
<br>
0:19:35.520,0:19:38.900<br>
變成比較 low dimension 的 vector z<br>
<br>
0:19:38.900,0:19:41.840<br>
那在原來的 x 這個 space 上面<br>
<br>
0:19:41.840,0:19:47.040<br>
我們會計算所有的點的 pair，xi 和 xj 之間的 similarity<br>
<br>
0:19:47.040,0:19:51.240<br>
這裡先寫成 S(xi, xj)<br>
<br>
0:19:51.240,0:19:55.680<br>
接下來，會做一個 normalization<br>
<br>
0:19:55.680,0:19:59.040<br>
我們會計算一個 P(xj | xi)<br>
<br>
0:19:59.040,0:20:03.920<br>
這個 P(xj | xi)，他是從 xi 跟 xj 的 similarity 來的<br>
<br>
0:20:03.920,0:20:07.100<br>
我們等下會說這個 similarity 要怎麼算<br>
<br>
0:20:07.100,0:20:12.800<br>
這個 P(xj | xi)，在分子的地方是 xi 跟 xj 的 similarity<br>
<br>
0:20:12.800,0:20:14.280<br>
然後分母的地方<br>
<br>
0:20:14.280,0:20:21.760<br>
就是 summation over 除了 xi 以外，所有其他的點<br>
<br>
0:20:21.760,0:20:24.680<br>
和 xi 之間所算出來的距離<br>
<br>
0:20:24.680,0:20:25.660<br>
所以你會發現說<br>
<br>
0:20:25.660,0:20:30.000<br>
xi 對其他所有的 data point<br>
<br>
0:20:30.000,0:20:33.240<br>
它所算出來的這個 P(xj | xi)<br>
<br>
0:20:33.240,0:20:36.060<br>
它的 summation 應該要是 1<br>
<br>
0:20:36.060,0:20:42.980<br>
另外假設我們今天已經找出了一個 low dimension 的 representation 就是 zi 跟 zj 的話<br>
<br>
0:20:42.980,0:20:44.840<br>
我們已經把 x 變成 z 的話<br>
<br>
0:20:44.840,0:20:47.960<br>
那我們也可以計算 zi 和 zj 之間的 similarity<br>
<br>
0:20:47.960,0:20:50.680<br>
這邊 similarity 我們寫成 S'<br>
<br>
0:20:50.680,0:20:55.000<br>
那一樣你可以計算一個 Q( zj | zi)<br>
<br>
0:20:55.000,0:20:59.020<br>
他的分子的地方就是 S'( zi, zj)<br>
<br>
0:20:59.020,0:21:06.780<br>
分母的地方就是 summation over zi<br>
跟所有 database 裡面的 data point zk 之間的距離<br>
<br>
0:21:06.780,0:21:13.260<br>
那今天有做這個 normalization 其實感覺是必要的<br>
<br>
0:21:13.260,0:21:15.880<br>
因為如果你又做這個 normalization 的話<br>
<br>
0:21:15.880,0:21:18.120<br>
因為你不知道在高維空間中算出來的距離<br>
<br>
0:21:18.120,0:21:24.840<br>
S(xi, xj) 跟 S’(zi, zj)，它們的 scale 是不是一樣的<br>
<br>
0:21:24.840,0:21:27.040<br>
如果，你今天有做這個 normalization<br>
<br>
0:21:27.040,0:21:31.340<br>
那你最後就可以把他們都變成機率<br>
<br>
0:21:31.340,0:21:34.940<br>
那這個時候他們值都會介於 0 到 1 之間<br>
<br>
0:21:34.940,0:21:37.240<br>
他們的 scale 會是一樣的<br>
<br>
0:21:37.240,0:21:39.240<br>
那接下來我們要做的事情就是<br>
<br>
0:21:39.240,0:21:44.100<br>
我們現在還不知道 zi 跟 zj 他們的值到底是多少<br>
<br>
0:21:44.100,0:21:46.100<br>
這是我們要被找出來的<br>
<br>
0:21:46.100,0:21:48.640<br>
那麼希望找一組 zi 跟 zj<br>
<br>
0:21:48.640,0:21:53.000<br>
它可以讓這一個 distribution 跟這一個 distribution<br>
<br>
0:21:53.000,0:21:54.800<br>
越接近越好<br>
<br>
0:21:54.800,0:21:57.060<br>
我們要讓原來在這個<br>
<br>
0:21:57.060,0:22:01.020<br>
根據 similarity 在 S 這個原來的 space 算出來 distribution<br>
<br>
0:22:01.020,0:22:04.860<br>
跟在這個 dimension reduction 以後的 space 算出來的 distribution<br>
<br>
0:22:04.860,0:22:06.860<br>
越接近越好<br>
<br>
0:22:06.860,0:22:09.800<br>
怎麼衡量兩個 distribution 之間的相似度呢？<br>
<br>
0:22:09.800,0:22:14.560<br>
可以很直覺的衡量兩個 distribution 之間的相似度的方法<br>
<br>
0:22:14.560,0:22:18.040<br>
就是我們之前看到的 KL divergence<br>
<br>
0:22:18.040,0:22:20.860<br>
所以，我們今天要做的事情<br>
<br>
0:22:20.860,0:22:22.860<br>
就是找一組 z<br>
<br>
0:22:22.860,0:22:24.920<br>
它可以做到這個<br>
<br>
0:22:24.920,0:22:31.720<br>
xi 的這個 distribution，跟 xi 對其他 point 的 distribution<br>
<br>
0:22:31.720,0:22:34.140<br>
跟 zi 對其他 point 的 distribution<br>
<br>
0:22:34.140,0:22:37.660<br>
這兩個 distribution 之間的 KL divergence 越小越好<br>
<br>
0:22:37.660,0:22:40.080<br>
summation over 所有的 data point<br>
<br>
0:22:40.080,0:22:45.140<br>
然後你要使得這一項，它的值越小越好<br>
<br>
0:22:45.140,0:22:48.960<br>
然後，我把這個值就寫在這邊<br>
<br>
0:22:48.960,0:22:52.700<br>
這件事情要怎麼做呢？<br>
<br>
0:22:52.700,0:22:53.500<br>
要怎麼做<br>
<br>
0:22:53.500,0:22:56.680<br>
其實這件事情，實際上並沒有很困難<br>
<br>
0:22:56.680,0:22:58.680<br>
實際上你就是用 Gradient descent 做的<br>
<br>
0:22:58.680,0:23:03.720<br>
你想想看，假設你知道這個 similarity 的 matrix<br>
<br>
0:23:03.720,0:23:06.100<br>
他這個式子長什麼樣子，然後這個式子長什麼樣子<br>
<br>
0:23:06.100,0:23:11.540<br>
那你把這些式子代進去，你把這些式子代進去<br>
<br>
0:23:11.540,0:23:17.600<br>
然後，接下來你只要對 z 做微分，然後做 Gradient Descent<br>
<br>
0:23:17.600,0:23:20.020<br>
就搞定這個問題，就結束了<br>
<br>
0:23:21.960,0:23:24.840<br>
那今天有一個問題就是你在做 t-SNE 的時候<br>
<br>
0:23:24.840,0:23:28.360<br>
他會計算所有 data point 之間的 similarity<br>
<br>
0:23:28.360,0:23:31.120<br>
所以它的運算量有點大，所以 t-SNE 有點麻煩<br>
<br>
0:23:31.120,0:23:36.220<br>
如果 data point 比較多的時候，你這電腦會跑不動<br>
<br>
0:23:36.220,0:23:39.320<br>
一個常見的做法是你會先做降維<br>
<br>
0:23:39.320,0:23:41.558<br>
比如說，你原來的 dimension 很大<br>
<br>
0:23:41.560,0:23:44.780<br>
你不會直接從很高的 dimension 直接做 t-SNE<br>
<br>
0:23:44.780,0:23:47.300<br>
因為你這樣子計算 similarity 的時間太長<br>
<br>
0:23:47.300,0:23:50.560<br>
你通常會先做，用比較快的方法比如說 PCA<br>
<br>
0:23:50.560,0:23:53.680<br>
先用 PCA 做降維，比如說，先降到 50 維<br>
<br>
0:23:53.680,0:23:57.160<br>
然後再用 t-SNE 從 50 維降到 2 維<br>
<br>
0:23:57.160,0:23:59.060<br>
這個是比較常見的做法<br>
<br>
0:23:59.060,0:24:01.260<br>
那其實像我們今天講的這些方法啊<br>
<br>
0:24:01.260,0:24:02.640<br>
你會發現說<br>
<br>
0:24:02.640,0:24:07.560<br>
比如說，像 t-SNE，如果你給他一個新的 data point<br>
<br>
0:24:07.560,0:24:09.720<br>
它是沒辦法做的，對不對<br>
<br>
0:24:09.720,0:24:12.800<br>
他只能夠，你給他一大群<br>
<br>
0:24:12.800,0:24:16.900<br>
已經先給他一大堆 x，他幫你把每一個 x 的 z 都找出來<br>
<br>
0:24:16.900,0:24:18.620<br>
但你找完這些 z 以後<br>
<br>
0:24:18.620,0:24:21.140<br>
再給他一個新的 x<br>
<br>
0:24:21.140,0:24:25.500<br>
你要重新跑一遍這一整套演算法，很麻煩<br>
<br>
0:24:25.500,0:24:28.240<br>
所以，一般你不會<br>
<br>
0:24:28.240,0:24:36.780<br>
一般 t-SNE 的作用比較不是用在這種 training testing 的這種 base 上面<br>
<br>
0:24:36.780,0:24:38.780<br>
通常比較常用的做法是<br>
<br>
0:24:38.780,0:24:40.320<br>
拿來做 visualization<br>
<br>
0:24:40.320,0:24:44.020<br>
如果你已經有一大堆的 x，他是 high dimensional 的<br>
<br>
0:24:44.020,0:24:47.180<br>
那你想要 visualize 他們在二維空間的分佈上是什麼樣子<br>
<br>
0:24:47.180,0:24:48.080<br>
你用 t-SNE<br>
<br>
0:24:48.080,0:24:50.400<br>
那 t-SNE 往往可以給你不錯的結果<br>
<br>
0:24:50.400,0:24:55.340<br>
那 t-SNE 現在可能是最多人使用的一種選擇<br>
<br>
0:24:55.340,0:25:01.380<br>
好那我們再來要講 t-SNE 一個非常有趣的地方<br>
<br>
0:25:01.380,0:25:06.860<br>
就是他的這個 similarity 的選擇是非常的神妙的<br>
<br>
0:25:06.860,0:25:10.960<br>
我們在這個原來的 data point 的 space 上面<br>
<br>
0:25:10.960,0:25:12.980<br>
你的 similarity 的選擇<br>
<br>
0:25:12.980,0:25:16.280<br>
它是選擇 RBF 的 function<br>
<br>
0:25:16.280,0:25:19.780<br>
就選擇說，我們要讓 xi，<br>
<br>
0:25:19.780,0:25:22.440<br>
我們這個 evaluate similarity 的方式<br>
<br>
0:25:22.440,0:25:25.640<br>
是計算 xi 跟 xj 的 Euclidean distance<br>
<br>
0:25:25.640,0:25:29.620<br>
然後取一個負號，再取 exponential<br>
<br>
0:25:29.620,0:25:32.740<br>
那我們之前有說如果你要在 graph 上算 similarity 的話<br>
<br>
0:25:32.740,0:25:34.120<br>
用這種方法比較好<br>
<br>
0:25:34.120,0:25:38.260<br>
因為它可以確保說只有非常相近的點才有值<br>
<br>
0:25:38.260,0:25:40.220<br>
那 exponential 他掉得非常快<br>
<br>
0:25:40.220,0:25:44.780<br>
所以只要距離一拉開，similarity 就會變得很小<br>
<br>
0:25:44.780,0:25:49.440<br>
那在 t-SNE之前有一個方法叫做 SNE<br>
<br>
0:25:49.440,0:25:51.160<br>
就把前面的 t 拿掉<br>
<br>
0:25:51.160,0:25:53.740<br>
SNE 就是一個很直覺的想法<br>
<br>
0:25:53.740,0:25:57.580<br>
在 data point 原來的 space 上用這個 evaluation 的 measure<br>
<br>
0:25:57.580,0:26:02.520<br>
當然在新的 space 上你用同樣的 measure 就好啦<br>
<br>
0:26:02.520,0:26:08.200<br>
選不同 measure，呃，你直覺就選一樣的 measure 嘛<br>
<br>
0:26:08.200,0:26:10.380<br>
那 t-SNE 神妙的地方就是<br>
<br>
0:26:10.380,0:26:13.660<br>
他在你 dimension reduction 以後的 space<br>
<br>
0:26:13.660,0:26:16.680<br>
他選的 measure 跟原來的 space 是不一樣的<br>
<br>
0:26:16.680,0:26:18.820<br>
他在 dimension reduction 以後選的 space<br>
<br>
0:26:18.820,0:26:22.820<br>
是這個 t-distribution 的其中一種<br>
<br>
0:26:22.820,0:26:25.680<br>
t-distribution 裡面有參數你可以調他<br>
<br>
0:26:25.680,0:26:27.600<br>
可以產生很多種不同的 distribution<br>
<br>
0:26:27.600,0:26:29.600<br>
t-distribution 的其中⼀一種<br>
<br>
0:26:29.600,0:26:47.140<br>
他是 1 除以 1 加 Euclidean distance 的平⽅<br>
<br>
0:26:47.140,0:26:51.200<br>
那你可能問說，為什麼要這樣做呢<br>
<br>
0:26:51.200,0:26:53.740<br>
我可以提供⼀個很直覺的理由<br>
<br>
0:26:53.740,0:26:59.140<br>
假設橫軸代表了在原來 space 上的 Euclidean distance<br>
<br>
0:26:59.140,0:27:00.720<br>
或者是<br>
<br>
0:27:00.720,0:27:08.700<br>
做 dimension reduction 以後的 Euclidean distance<br>
<br>
0:27:08.700,0:27:14.200<br>
那這個紅⾊這條線，是這⼀項<br>
<br>
0:27:14.200,0:27:17.560<br>
藍色這條線，是這⼀項<br>
<br>
0:27:17.560,0:27:20.320<br>
紅色這條線是 RBF function<br>
<br>
0:27:20.320,0:27:22.900<br>
藍色這條線是 t-distribution<br>
<br>
0:27:22.900,0:27:25.320<br>
你會發現說呢<br>
<br>
0:27:25.320,0:27:30.080<br>
如果原來在這個點跟這個點<br>
<br>
0:27:30.080,0:27:35.020<br>
之後做 Dimension reduction 以後<br>
<br>
0:27:35.020,0:27:38.100<br>
要怎麼才能維持它原來的 space 呢?<br>
<br>
0:27:38.100,0:27:41.180<br>
那你就把它變成這個樣⼦<br>
<br>
0:27:41.180,0:27:44.880<br>
它就可以維持它原來的 space<br>
<br>
0:27:44.880,0:27:48.280<br>
如果你要維持他們原來之間的距離<br>
<br>
0:27:48.280,0:27:55.880<br>
如果你今天要維持他們原來的機率的話<br>
<br>
0:27:55.880,0:27:58.320<br>
要維持原來他們之間相對的關係的話<br>
<br>
0:27:58.320,0:28:02.140<br>
那你就把他變成這樣<br>
<br>
0:28:02.140,0:28:03.100<br>
那你會發現說<br>
<br>
0:28:03.100,0:28:07.780<br>
如果本來距離比較近，他們的影響是比較⼩的<br>
<br>
0:28:07.780,0:28:09.780<br>
如果本來就已經有一段距離<br>
<br>
0:28:09.780,0:28:14.540<br>
那從原來的這個 distribution 變到 t-distribution 以後<br>
<br>
0:28:14.540,0:28:15.900<br>
他會被拉得很遠<br>
<br>
0:28:15.900,0:28:18.340<br>
t-distribution 他的尾巴特別長<br>
<br>
0:28:18.340,0:28:20.600<br>
所以如果你本來距離比較遠的話<br>
<br>
0:28:21.500,0:28:24.580<br>
變到 t-distribution 以後，他會變得更遠<br>
<br>
0:28:24.580,0:28:27.640<br>
也就是說，原來在高維空間裡面<br>
<br>
0:28:27.640,0:28:29.640<br>
如果距離很近<br>
<br>
0:28:29.640,0:28:32.840<br>
那做完 transform 以後他還是很近<br>
<br>
0:28:33.180,0:28:35.400<br>
如果原來就已經有一段距離了，有⼀個 gap<br>
<br>
0:28:35.400,0:28:38.740<br>
那做完 transform 以後他就會被拉得很遠<br>
<br>
0:28:38.740,0:28:43.260<br>
所以你會發現說 t-SNE，他畫出來的圖往往長得像這樣<br>
<br>
0:28:43.260,0:28:48.540<br>
他會把你的 data point 聚集成一群、一群、一群<br>
<br>
0:28:48.540,0:28:51.540<br>
因為你的 data point 之間本來只要有一個 gap<br>
<br>
0:28:51.540,0:28:56.940<br>
做完 t-SNE 以後，它就會把 gap 強化，gap 就會變得特別明顯<br>
<br>
0:28:56.940,0:29:00.680<br>
所以這是 t-SNE 做在 MNIST 上面的結果<br>
<br>
0:29:00.680,0:29:03.940<br>
那其實這個不是直接做在 MNIST 的 pixel 上⾯<br>
<br>
0:29:03.940,0:29:06.700<br>
直接做在 pixel 上面的 performance 看起來沒有這麼好<br>
<br>
0:29:06.700,0:29:14.920<br>
這是先對 pixel 做 PCA 降維以後，再做在 MNIST 上⾯<br>
<br>
0:29:14.920,0:29:17.760<br>
那不同顏色代表的是不同的數字<br>
<br>
0:29:17.760,0:29:20.920<br>
那你會發現，不同的數字會變成一群一群的<br>
<br>
0:29:21.720,0:29:26.080<br>
那 t-SNE 在 COIL-20 上的結果，其實還蠻驚人的<br>
<br>
0:29:26.080,0:29:35.420<br>
這邊的每一個圈圈就代表一個 object，他只是轉了一圈以後的結果<br>
<br>
0:29:35.420,0:29:40.380<br>
你會發現這邊有一些被扭曲的圈<br>
<br>
0:29:41.320,0:29:42.740<br>
這扭曲的圈圈是什麼意思呢？<br>
<br>
0:29:42.740,0:29:45.640<br>
那是因為有⼀些東西，比如說杯⼦<br>
<br>
0:29:46.320,0:29:48.280<br>
他的這個面長這樣<br>
<br>
0:29:49.160,0:29:52.200<br>
它轉 180 度過來以後，他還是長得一模⼀樣<br>
<br>
0:29:52.920,0:29:55.900<br>
所以你把那個杯子轉 360 度轉一圈的時候<br>
<br>
0:29:55.900,0:29:59.360<br>
你就會發現說，它在中間某一個地方是很像的 image<br>
<br>
0:29:59.760,0:30:03.860<br>
所以，你會看到說這邊有很多看起來很像 8 的符號<br>
<br>
0:30:04.440,0:30:05.980<br>
那這邊這很多條線<br>
<br>
0:30:06.240,0:30:09.320<br>
其實好像是說，有 4 部都是⼩汽⾞<br>
<br>
0:30:09.320,0:30:11.880<br>
4 部小汽車看起來是蠻像的<br>
<br>
0:30:11.880,0:30:15.300<br>
所以這邊有四條線是被 align 在一起的<br>
<br>
0:30:16.140,0:30:20.060<br>
這邊有一個從網路上找到的 t-SNE 動畫<br>
<br>
0:30:20.060,0:30:22.580<br>
他長得是這個樣⼦<br>
<br>
0:30:24.420,0:30:26.680<br>
你看，因為他是用 gradient descent train 的<br>
<br>
0:30:26.680,0:30:29.940<br>
所以你會看到，隨著 iteration 的 process<br>
<br>
0:30:29.940,0:30:34.800<br>
點會被分得越來越開<br>
<br>
0:30:35.120,0:30:37.980<br>
然後不同的點之間呢<br>
<br>
0:30:37.980,0:30:41.620<br>
這其實不是做在 MNIST 上，是做在另外⼀個手寫數字辨識的 corpus 上⾯<br>
<br>
0:30:42.020,0:30:46.680<br>
你會發現說不同的 digit 之間，他是會被分得很開的<br>
<br>
0:30:49.120,0:30:54.660<br>
那 t-SNE 的地方呢，我們就講到這邊<br>
<br>
0:30:54.660,0:30:57.060<br>
臺灣大學⼈⼯智慧中心<br>
科技部⼈工智慧技術暨全幅健康照護聯合研究中⼼<br>
http://aintu.tw<br>
