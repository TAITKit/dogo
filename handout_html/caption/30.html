<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
0:00:00.000,0:00:06.020<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
<br>
0:00:07.540,0:00:09.980<br>
那我們上次呢，講到 LSTM<br>
<br>
0:00:10.380,0:00:14.040<br>
總之就是一個複雜的東西<br>
<br>
0:00:15.080,0:00:20.140<br>
再來的問題是，像 Recurrent Neural Network 這種架構<br>
<br>
0:00:20.220,0:00:27.200<br>
他要如何做 learning 呢？<br>
我們之前有說過說，如果要做 learning 的話<br>
<br>
0:00:27.200,0:00:29.960<br>
你要定一個 cost function<br>
<br>
0:00:29.960,0:00:35.480<br>
來 evaluate 你的 model 的 parameter 是好還是不好<br>
<br>
0:00:35.480,0:00:39.960<br>
然後你選一個 model parameter 可以讓這個 lost 最小<br>
<br>
0:00:41.020,0:00:45.680<br>
那在 RNN 裡面，你會怎麼定這個 lost 呢？<br>
<br>
0:00:46.620,0:00:50.520<br>
以下我們就不寫算式，直接舉個例子<br>
<br>
0:00:50.520,0:00:54.840<br>
假設我們現在要做的事情是 Slot Filling<br>
<br>
0:00:54.840,0:01:00.720<br>
那你會有 training data ，<br>
這個 training data 是說，給你一些 sentence<br>
<br>
0:01:02.000,0:01:36.320<br>
解決投影機問題中<br>
<br>
0:01:38.740,0:01:43.720<br>
這個是 sentence, ok~<br>
<br>
0:01:43.760,0:01:51.120<br>
你要給 sentence label，告訴 machine  說，<br>
第一個 word 它屬於 other 這個 slot<br>
<br>
0:01:51.120,0:01:54.980<br>
然後台北屬於destination 這個 slot， on 屬於 other slot<br>
<br>
0:01:55.760,0:02:02.860<br>
November 2nd 屬於抵達時間的 slot<br>
<br>
0:02:03.180,0:02:07.980<br>
接下來你希望你的 cost 會怎麼定呢？<br>
<br>
0:02:08.160,0:02:12.220<br>
把 arrive 丟到 RNN<br>
<br>
0:02:12.620,0:02:15.980<br>
RNN 會得到一個 output 'y1'<br>
<br>
0:02:16.480,0:02:25.860<br>
接下來 'y1' 會和一個 Reference 的 vector 算它的 cross entropy<br>
<br>
0:02:26.140,0:02:36.380<br>
希望說如果我們現在丟進去的是 arrive <br>
那 'y1' 的reference 的 vector 應該是對應到 other 那個 slot<br>
<br>
0:02:36.620,0:02:39.280<br>
的 dimension value 是 1 其他是 0<br>
<br>
0:02:39.500,0:02:44.000<br>
這個 reference vector的長度，就是你 slot 的數目<br>
<br>
0:02:44.140,0:02:49.880<br>
比如說你定了 40 個 slot，<br>
那這個 reference vector 的長度，dimension 就是40<br>
<br>
0:02:50.140,0:02:55.480<br>
假設現在 input 的這個 word，<br>
它應該是對應到 other 這個 slot 的話<br>
<br>
0:02:56.040,0:02:58.680<br>
那對應到 other 那個 dimension<br>
<br>
0:02:59.020,0:03:01.480<br>
就是 1 其他就是 0<br>
<br>
0:03:02.140,0:03:09.100<br>
那現在你把 Taipei  丟進去的時候，因為 Taipei 屬於 destination 這個 slot<br>
<br>
0:03:09.260,0:03:13.060<br>
所以你就會希望說把 'x2' 丟進去的時候<br>
<br>
0:03:13.120,0:03:21.360<br>
'y2' 它要跟 reference 的 vector 距離越近越好<br>
<br>
0:03:21.580,0:03:27.360<br>
那 'y2' 的 reference vector 是對應到 destination 那個 slot 是 1 其他是 0<br>
<br>
0:03:27.500,0:03:32.520<br>
但這邊要注意的是，你在丟 'x2' 之前，一定要先丟 'x1'<br>
<br>
0:03:33.160,0:03:36.980<br>
你在把 Taipei 丟進去之前，一定要先把 arrive 丟進去<br>
<br>
0:03:37.160,0:03:42.980<br>
不然，你就不知道存在memory 裡面的值是多少<br>
<br>
0:03:43.350,0:03:48.020<br>
在做 training 的時候，<br>
你也不能把你的 *****  裡面的<br>
<br>
0:03:48.220,0:03:53.900<br>
這些 word sequence 打散來看，<br>
word sequence 仍然要當做一個整體來看<br>
<br>
0:03:54.200,0:03:56.760<br>
同樣的道理，把 on 丟進去<br>
<br>
0:03:58.200,0:04:02.480<br>
它的 reference vector 對應到 other 是 1<br>
<br>
0:04:02.480,0:04:07.020<br>
對應到 other 那個 dimension 是 1，其他是0<br>
<br>
0:04:07.020,0:04:10.760<br>
所以你的 cost ，就是每一個時間點的<br>
<br>
0:04:11.720,0:04:16.680<br>
RNN 的 output 跟 reference vector 的 cross entropy 和<br>
<br>
0:04:16.820,0:04:20.320<br>
就是你要去 minimize 的對象<br>
<br>
0:04:21.800,0:04:29.160<br>
那現在有了這個 lost function 之後，training 要怎麼做呢？<br>
<br>
0:04:29.860,0:04:34.120<br>
training 其實也是用 gradient decent<br>
<br>
0:04:34.500,0:04:38.860<br>
也就是說如果我們現在已經定出了 lost function 大 L<br>
<br>
0:04:39.320,0:04:45.560<br>
我要 update 這一個 network 裡面的某一個參數 w，我要怎麼做呢？<br>
<br>
0:04:45.640,0:04:50.600<br>
你就計算 w 對 L 的偏微分，把這個偏微分計算出來之後<br>
<br>
0:04:50.920,0:04:55.520<br>
就用 gradient decent 的方式去 update 每一個參數<br>
<br>
0:04:55.620,0:04:59.900<br>
那我們之前在講 Neural Network 的時候，<br>
已經講過很多次<br>
<br>
0:04:59.900,0:05:06.540<br>
那在講之前這個 feed forward network 的時候，我們說gradient decent<br>
<br>
0:05:06.600,0:05:11.240<br>
用在 feed forward network 裡面，<br>
你要用一個比較有效的演算法，叫做 Back propagation<br>
<br>
0:05:11.420,0:05:16.420<br>
在 RNN 裡面，gradient decent 的原理是一模一樣的<br>
<br>
0:05:17.700,0:05:23.480<br>
但是為了要計算方便，<br>
也有開發一套演算法，這套演算法呢<br>
<br>
0:05:23.980,0:05:30.040<br>
是 Back propagation 的進階版，叫做BPTT<br>
<br>
0:05:30.440,0:05:33.980<br>
那它跟 Back propagation 其實是很類似的<br>
<br>
0:05:34.040,0:05:42.540<br>
只是因為 RNN 是在 time sequence 上運作，所以 BPTT 它需要考慮時間的information<br>
<br>
0:05:42.900,0:05:50.640<br>
那我們在這邊就不講 BPTT ，反正你只要知道 RNN 是用 gradient decent train 的<br>
<br>
0:05:50.640,0:05:53.260<br>
它是可以 train 的，這樣就行了<br>
<br>
0:05:56.640,0:06:02.300<br>
然而不幸的，RNN 的 training 是比較困難的<br>
<br>
0:06:11.880,0:06:15.160<br>
RNN 的 training 是比較困難的<br>
<br>
0:06:15.940,0:06:23.840<br>
一般而言，你在做 training 的時候，<br>
你會期待你的 learning curve 是像藍色這條線<br>
<br>
0:06:23.920,0:06:32.420<br>
這邊的縱軸是 Total Loss，<br>
橫軸呢，是 Epoch, training 時的 Epoch 的數目<br>
<br>
0:06:33.040,0:06:34.740<br>
你會希望說呢<br>
<br>
0:06:34.940,0:06:41.440<br>
隨著 Epoch 越來越多，隨著參數不斷的被 update，loss 應該就是慢慢，慢慢的下降<br>
<br>
0:06:41.600,0:06:44.040<br>
最後趨向收斂<br>
<br>
0:06:44.180,0:06:49.960<br>
但不幸的，當你在訓練 RNN 時，你有時候會看到<br>
<br>
0:06:50.160,0:06:52.720<br>
綠色這條線<br>
<br>
0:06:52.960,0:06:58.380<br>
這很重要，如果你是第一次 train RNN<br>
<br>
0:06:58.500,0:07:02.660<br>
你看到綠色這樣的 learning curve<br>
<br>
0:07:02.940,0:07:10.980<br>
這個 learning curve ，非常劇烈的抖動，<br>
然後抖到某個地方，就突然 NaN，<br>
然後你的程式就 segmentation fault<br>
<br>
0:07:11.380,0:07:15.980<br>
這個時候你會有什麼想法呢？<br>
我相信你的第一個想法就是<br>
<br>
0:07:16.340,0:07:21.320<br>
程式有 bug 啊！<br>
<br>
0:07:22.520,0:07:26.980<br>
今年春天我邀那個國外學者來台灣<br>
<br>
0:07:27.080,0:07:30.260<br>
他們就是發明哪個 **** 的人<br>
<br>
0:07:30.440,0:07:34.760<br>
他跟我分享他當時開發 RNN 的心得<br>
<br>
0:07:36.140,0:07:44.360<br>
最早開始做 RNN 的 language model 的人<br>
大概在 09 年就開始做了<br>
<br>
0:07:44.700,0:07:49.700<br>
很長一段時間，<br>
只有他能把 RNN  language model train 起來<br>
<br>
0:07:49.700,0:07:51.400<br>
其他人都 train 不起來<br>
<br>
0:07:55.680,0:08:01.520<br>
你知道那個年代，不像現在有什麼 tensor flow 啊<br>
<br>
0:08:01.680,0:08:06.880<br>
那個年代做什麼，<br>
都是要徒手刻的，所以他徒手刻了一個 RNN<br>
<br>
0:08:07.260,0:08:10.020<br>
然後 train 完以後就發生這樣一個現象<br>
<br>
0:08:10.300,0:08:16.940<br>
他第一個想法就是，程式有 bug ...<br>
努力 debug 後，果然有很多 bugs<br>
<br>
0:08:19.860,0:08:24.400<br>
他後來就把 bugs 修掉<br>
<br>
0:08:24.440,0:08:26.540<br>
但這現象還是在<br>
<br>
0:08:26.700,0:08:33.600<br>
所以他就覺得很困惑<br>
其他人就跟他說放棄啦，不 work<br>
<br>
0:08:33.820,0:08:37.740<br>
可是他想知道結果為何會這樣<br>
<br>
0:08:37.740,0:08:42.720<br>
所以他就做分析，等一下這圖是來自於他的 paper<br>
<br>
0:08:42.960,0:08:47.360<br>
他分析了 RNN 的性質，他發現說 RNN 的 error surface<br>
<br>
0:08:47.360,0:08:54.500<br>
所謂 error surface 就是 Total Loss 對參數的變化，<br>
是非常的陡峭，非常崎嶇的<br>
<br>
0:08:55.240,0:09:00.180<br>
所謂崎嶇的意思是說，這個 error surface 它有些地方非常平坦<br>
<br>
0:09:00.400,0:09:03.860<br>
有一些地方，非常的陡峭<br>
<br>
0:09:03.860,0:09:08.980<br>
就像是有懸崖峭壁<br>
<br>
0:09:08.980,0:09:13.640<br>
投影片上是一個示意圖，縱軸是 Total Loss<br>
<br>
0:09:17.000,0:09:21.460<br>
x 軸跟 y 軸代表兩個參數 w1 and w2<br>
<br>
0:09:21.920,0:09:25.300<br>
圖上顯示的就是 w1 跟 w2 兩個參數<br>
<br>
0:09:25.420,0:09:29.340<br>
對 Total Loss，那發現說對很多地方<br>
是非常平坦的<br>
<br>
0:09:29.340,0:09:34.660<br>
然後在某些地方，非常的陡峭<br>
<br>
0:09:35.560,0:09:37.780<br>
這個會造成什麼樣的問題呢？<br>
<br>
0:09:37.960,0:09:42.180<br>
假設你從橙色那個點，當作你初始的點<br>
<br>
0:09:42.480,0:09:45.320<br>
用 gradient decent 開始調整你的參數<br>
<br>
0:09:45.520,0:09:50.560<br>
橙色那個點，你算一下 gradient<br>
然後update 你的參數<br>
<br>
0:09:50.900,0:09:55.300<br>
跳到下一個橙色的點，再算一下 gradient<br>
再 update 你的參數<br>
<br>
0:09:55.600,0:09:58.820<br>
你可能正好就跳過一個懸崖<br>
<br>
0:09:58.860,0:10:06.080<br>
所以你的 Loss 會突然暴增，<br>
你就會看到 Loss 上下非常劇烈震盪<br>
<br>
0:10:06.980,0:10:11.680<br>
有時候可能會遇到另外一個更慘的狀況，<br>
就是你正好就踩在<br>
<br>
0:10:11.680,0:10:16.100<br>
你一腳踩在這懸崖上<br>
<br>
0:10:17.740,0:10:22.640<br>
那你踩在這懸崖上，會發生什麼事情呢？<br>
你踩在懸崖上，因為在懸崖上 gradient 很大<br>
<br>
0:10:23.800,0:10:28.260<br>
然後呢，之前的 gradient 都很小，所以你措手不及<br>
<br>
0:10:28.260,0:10:33.100<br>
因為之前 gradient 很小，<br>
所以你可能把 learning rate 調得比較大<br>
<br>
0:10:33.220,0:10:39.460<br>
但 gradient 突然很大，很大的 gradient 再乘上很大的 learning rate<br>
<br>
0:10:39.460,0:10:43.720<br>
結果參數就 update 很多<br>
然後整個參數就飛出去了<br>
<br>
0:10:44.360,0:10:48.640<br>
所以你就 NaN<br>
程式就 segmentation fault<br>
<br>
0:10:49.340,0:10:53.180<br>
他們就想說怎麼辦呢？<br>
<br>
0:10:53.180,0:10:58.800<br>
他說他不是一個數學家，<br>
所以他要用工程師的想法來解決這問題<br>
<br>
0:10:59.100,0:11:05.940<br>
他就想了一招，這一招應該蠻關鍵的<br>
<br>
0:11:05.940,0:11:12.960<br>
讓很長一段時間，<br>
只有他的 code 可以把 RNN train 出來<br>
<br>
0:11:13.100,0:11:17.640<br>
很長一段時間，人們是不知道這一招的<br>
因為這一招他實在覺得太沒什麼<br>
<br>
0:11:18.000,0:11:22.860<br>
所以沒寫在 paper 裡面<br>
直到他在寫博士論文時<br>
<br>
0:11:22.860,0:11:27.640<br>
博士論文是比較長的<br>
所以有些東西 trivial 很可能還是會寫進去<br>
<br>
0:11:28.800,0:11:35.020<br>
直到他在寫博士論文的時候，大家才發現這個秘密<br>
<br>
0:11:35.580,0:11:36.900<br>
這個秘密是什麼呢？<br>
<br>
0:11:36.900,0:11:39.440<br>
這一招說穿了就不值錢<br>
<br>
0:11:39.440,0:11:41.440<br>
這一招叫做 clipping<br>
<br>
0:11:41.440,0:11:46.160<br>
clipping 是說，當 gradient 大於某一個threshold 時後<br>
<br>
0:11:46.400,0:11:52.140<br>
就不要讓他超過那個 threshold<br>
<br>
0:11:52.140,0:11:57.940<br>
當 gradient 大於 15 的時候就等於15<br>
結束...<br>
<br>
0:11:58.660,0:12:04.520<br>
因為 gradient 現在不會太大，假如你做 clipping 的時候<br>
<br>
0:12:04.600,0:12:08.780<br>
就算是踩在這個懸崖上，也沒有關係，<br>
你的參數就不會飛出去<br>
<br>
0:12:08.860,0:12:14.840<br>
他會飛到一個比較近的地方，<br>
這樣你仍然可以繼續做 RNN 的 training<br>
<br>
0:12:18.700,0:12:23.480<br>
那接下來的問題就是<br>
<br>
0:12:23.480,0:12:26.840<br>
為什麼 RNN 會有這種奇特的特性呢？<br>
<br>
0:12:32.880,0:12:37.420<br>
為什麼 RNN 會有這種奇特的特性呢？<br>
<br>
0:12:37.460,0:12:42.560<br>
有人可能會說是不是因為來自於 Sigmoid function<br>
<br>
0:12:42.780,0:12:47.300<br>
我們之前有講過，在講 ReLU activation function 的時候呢<br>
<br>
0:12:47.300,0:12:50.620<br>
我們有講過一個問題叫做 gradient vanishing 的問題<br>
<br>
0:12:50.900,0:12:55.000<br>
那我們說這個問題是從 Sigmoid function 來的<br>
<br>
0:12:55.660,0:12:59.320<br>
因為 Sigmoid 的關係，所以有 gradient vanishing 這個問題<br>
<br>
0:12:59.320,0:13:08.100<br>
RNN 會有這種很小的，很平滑的 error surface<br>
<br>
0:13:08.460,0:13:13.100<br>
是因為來自於 gradient vanish，gradient vanish 是因為來自於 Sigmoid function<br>
<br>
0:13:13.100,0:13:15.720<br>
這件事情，我覺得不是真的<br>
<br>
0:13:15.780,0:13:20.820<br>
想想看如果這個問題是來自於 Sigmoid function，換成 ReLU 就解決這個問題啦<br>
<br>
0:13:20.820,0:13:22.160<br>
所以不是這個問題<br>
<br>
0:13:22.260,0:13:26.820<br>
跟大家講一個秘密，如果你用 ReLU 你會發現說呢<br>
<br>
0:13:27.180,0:13:31.660<br>
一般在 train Neural Network 的時候呢，<br>
很少用 ReLU 當作 activation function<br>
<br>
0:13:31.660,0:13:39.860<br>
為什麼呢？因為如果你把 Sigmoid 換成 ReLU，其實在 RNN 上面 performance 通常是比較差的<br>
<br>
0:13:39.980,0:13:47.300<br>
所以 activation function 並不是這個地方的關鍵點<br>
<br>
0:13:50.560,0:13:54.460<br>
如果說我們今天有講 <br>
Back propagation through time 的話<br>
<br>
0:13:54.460,0:13:58.820<br>
從式子裡面，你會比較容易看出為何會有這個問題<br>
<br>
0:13:58.820,0:14:00.800<br>
今天沒有講 Back propagation through time<br>
<br>
0:14:00.800,0:14:01.780<br>
沒有關係<br>
<br>
0:14:01.780,0:14:07.040<br>
我們有一個更直觀的方法，<br>
可以來知道一個 gradient 的大小<br>
<br>
0:14:07.640,0:14:14.580<br>
這個更直觀的方法，你把某一個參數做小小的變化，<br>
看他對 network output 的變化有多大<br>
<br>
0:14:14.740,0:14:18.780<br>
就可以測出這個參數的 gradient 大小<br>
<br>
0:14:18.780,0:14:23.260<br>
我們這邊呢舉一個很簡單的 RNN 當作我們的例子<br>
<br>
0:14:24.260,0:14:29.440<br>
今天有一個全世界最簡單的 RNN，他只有一個 Neuron<br>
這個 Neuron 是 linear<br>
<br>
0:14:31.480,0:14:35.100<br>
他只有一個 input 沒有 bias<br>
input 的 weight 是 1<br>
<br>
0:14:35.820,0:14:38.780<br>
output weight 也是 1<br>
<br>
0:14:38.820,0:14:43.600<br>
transition 部分的 weight 是 w<br>
<br>
0:14:43.600,0:14:50.740<br>
也就是從 memory 接到 Neuron 的 input weight 是 w<br>
<br>
0:14:51.540,0:14:57.040<br>
現在假設我給這個network 的輸入是 [1 0 0 0 0 0]<br>
<br>
0:14:57.960,0:15:01.180<br>
只有第一個時間點輸入 1  ，接下來都輸入 0<br>
<br>
0:15:01.180,0:15:03.460<br>
那這個 network 的 output 會長什麼樣子呢？<br>
<br>
0:15:04.100,0:15:10.440<br>
比如說這個 network 在最後一個時間點，<br>
第 1000 個時間點的 output 值會是多少<br>
<br>
0:15:10.440,0:15:12.640<br>
我相信大家都可以馬上回答我<br>
<br>
0:15:12.640,0:15:17.380<br>
他的值是 w 的 999 次方，對吧？<br>
<br>
0:15:17.380,0:15:23.300<br>
你把 1 輸入進去，再乘上 w，再乘上 w...<br>
乘了 999  次 w<br>
<br>
0:15:23.680,0:15:28.260<br>
輸出就是 w 的 999 次方<br>
後面的輸入都是 0 當然不影響<br>
<br>
0:15:28.260,0:15:33.500<br>
只有一開始的 1 有影響<br>
但他會通過 999 次的 w<br>
<br>
0:15:34.340,0:15:38.440<br>
那我們現在來看，假設 w，是我們要認的參數<br>
<br>
0:15:38.760,0:15:41.060<br>
我們想要知道他的 gradient<br>
<br>
0:15:41.060,0:15:44.820<br>
所以我們想要知道，當我們改變 w 的值的時候<br>
<br>
0:15:44.820,0:15:47.360<br>
對 network 的 output 有多大的影響<br>
<br>
0:15:47.580,0:15:51.080<br>
現在我們假設 w = 1<br>
<br>
0:15:51.700,0:15:56.280<br>
y 1000，<br>
network 在最後時間點的 output，也是 1<br>
<br>
0:15:57.060,0:16:02.980<br>
假設 w = 1.01，那 y 1000 是多少呢？<br>
<br>
0:16:03.220,0:16:08.260<br>
y 1000 是 1.01 的 999 次方，<br>
1.01 的 999 次方是多少呢？<br>
<br>
0:16:11.560,0:16:14.240<br>
是 20000，是一個很大的值<br>
<br>
0:16:14.240,0:16:19.920<br>
這就跟蝴蝶效應一樣，這個 w 有一點小小的變化<br>
<br>
0:16:20.500,0:16:24.400<br>
對他的 output 影響是非常大的<br>
<br>
0:16:24.940,0:16:30.940<br>
所以 w 有很大的 gradient<br>
<br>
0:16:31.340,0:16:33.280<br>
想說這很大的 gradient 也沒有什麼<br>
<br>
0:16:33.380,0:16:37.120<br>
我們只要把他的 learning rate 設小一點就好<br>
<br>
0:16:37.700,0:16:42.620<br>
但是事實上，如果把 w 設成 0.99<br>
<br>
0:16:42.840,0:16:45.720<br>
那 y 1000 就等於 0<br>
<br>
0:16:45.840,0:16:51.060<br>
如果把 w 設 0.01，那 y 1000 還是等於 0<br>
<br>
0:16:51.540,0:17:01.080<br>
也就是說在 1 這個地方有很大的 gradient <br>
但在 0.99 的地方 gradient 就突然變得非常非常的小<br>
<br>
0:17:01.740,0:17:04.840<br>
這個時候你又需要一個很大的 learning rate<br>
<br>
0:17:04.840,0:17:09.300<br>
就會造成你設 learning rate 很麻煩，<br>
你的 error surface 很崎嶇<br>
<br>
0:17:09.300,0:17:13.120<br>
因為這 gradient 是時大時小的<br>
<br>
0:17:13.120,0:17:18.160<br>
而且在非常短的區域內，gradient 就會有很大的變化<br>
<br>
0:17:21.540,0:17:25.560<br>
所以從這個例子，你可以看出來說，為什麼 RNN<br>
<br>
0:17:26.080,0:17:29.540<br>
會有問題，RNN training 的問題<br>
<br>
0:17:29.900,0:17:33.540<br>
其實是來自於，他把同樣的東西<br>
<br>
0:17:33.680,0:17:38.200<br>
在 transition 的時候呢，<br>
在時間和時間轉換的時候，反覆使用<br>
<br>
0:17:38.700,0:17:41.920<br>
從 memory 接到 Neuron 的那一組 weight<br>
<br>
0:17:41.920,0:17:45.200<br>
在不同的時間點，都是反覆被使用<br>
<br>
0:17:45.200,0:17:49.120<br>
所以這個 w 只要一有變化<br>
<br>
0:17:49.720,0:17:53.440<br>
他有可能完全沒有造成任何影響，<br>
像這邊的例子<br>
<br>
0:17:53.440,0:17:57.940<br>
一但他可以造成影響，那個影響都會是天崩地裂的影響<br>
<br>
0:17:58.200,0:18:01.420<br>
所以他有時候 gradient 很大，有時很小<br>
<br>
0:18:07.060,0:18:11.160<br>
RNN 會不好訓練的原因，<br>
並不是來自於 activation function<br>
<br>
0:18:11.380,0:18:17.520<br>
而是來自於他有 time sequence，<br>
同樣的 weight，在不同的時間點<br>
<br>
0:18:17.520,0:18:20.920<br>
被反覆的，不斷的被使用<br>
<br>
0:18:21.520,0:18:27.120<br>
有什麼樣的技巧可以幫助我們解決這問題呢？<br>
<br>
0:18:31.060,0:18:36.400<br>
其實現在廣泛被使用的技巧呢，就是 LSTM<br>
<br>
0:18:37.320,0:18:42.000<br>
LSTM 可以讓你的 error surface 不要那麼崎嶇<br>
<br>
0:18:44.160,0:18:49.040<br>
他會把那些比較平坦的地方拿掉，<br>
他可以解決 gradient vanishing 的問題<br>
<br>
0:18:49.240,0:18:51.300<br>
但他不會解決 gradient explode 的問題<br>
<br>
0:18:51.300,0:18:55.300<br>
可能你有些地方，仍然是會非常崎嶇的<br>
<br>
0:18:55.300,0:19:00.580<br>
你有些地方，仍然變化會是非常劇烈的<br>
<br>
0:19:00.580,0:19:02.900<br>
但是不會有特別平坦的地方<br>
<br>
0:19:02.980,0:19:04.980<br>
因為如果你在做 LSTM 的時候<br>
<br>
0:19:04.980,0:19:07.620<br>
大部分的地方都變化很劇烈<br>
<br>
0:19:07.620,0:19:13.320<br>
所以當你在做 LSTM 的時候<br>
<br>
0:19:13.380,0:19:17.560<br>
你可以放心的把你的 learning rate 設的小一點<br>
<br>
0:19:17.920,0:19:23.480<br>
而他要在 learning rate 特別小的情況下進行訓練<br>
<br>
0:19:24.100,0:19:32.360<br>
那為什麼 LSTM 可以做到 <br>
handle gradient vanish 的問題呢？<br>
<br>
0:19:32.400,0:19:37.940<br>
為什麼他可以避免讓 gradient 特別小呢？<br>
<br>
0:19:38.260,0:19:48.060<br>
我聽說有人在面試某家國際大廠的時候，<br>
就被問這個問題<br>
<br>
0:19:48.380,0:19:53.840<br>
但這問題怎麼樣答比較好呢？<br>
<br>
0:19:53.860,0:19:58.000<br>
他那個問題是這樣，為什麼我們把 RNN 換成 LSTM?<br>
<br>
0:19:58.300,0:20:02.760<br>
如果你的答案是因為 LSTM 比較潮，<br>
因為 LSTM 比較複雜<br>
<br>
0:20:02.760,0:20:04.300<br>
這個都太弱了<br>
<br>
0:20:04.300,0:20:10.220<br>
真正的理由是 LSTM 可以 handle gradient vanishing 的問題<br>
<br>
0:20:11.020,0:20:17.080<br>
但接下來人家就會問說，為什麼 LSTM 可以 handle gradient vanishing 的問題呢？<br>
<br>
0:20:17.220,0:20:19.386<br>
我在這邊來試著回答看看<br>
<br>
0:20:19.386,0:20:26.700<br>
之後假如有人口試再被問到，<br>
你可以想想你有沒有辦法回答<br>
<br>
0:20:26.920,0:20:33.160<br>
如果你想看看 RNN 跟 LSTM，<br>
它們在面對 memory 的時候<br>
<br>
0:20:33.260,0:20:37.740<br>
它們處理的 operation<br>
其實是不一樣的<br>
<br>
0:20:38.440,0:20:43.180<br>
你想想看，在 RNN 裡面，在每一個時間點<br>
<br>
0:20:43.180,0:20:47.100<br>
其實 memory 裡面的資訊，都會被洗掉<br>
<br>
0:20:47.560,0:20:52.060<br>
你們看每一個時間點，<br>
Neuron 的 output 都會被放到 memory 裡面去<br>
<br>
0:20:52.100,0:20:56.400<br>
所以在每一個時間點，<br>
memory 裡面的資訊都會被覆蓋掉<br>
<br>
0:20:56.400,0:20:58.800<br>
都會被完全洗掉<br>
<br>
0:21:00.180,0:21:02.600<br>
但在 LSTM 裡面不一樣<br>
<br>
0:21:02.600,0:21:06.860<br>
他是把原來 memory 裡面的值，乘上一個值<br>
<br>
0:21:06.860,0:21:09.740<br>
再把 input 的值，加起來<br>
<br>
0:21:09.900,0:21:13.880<br>
放到 cell 裡面去<br>
<br>
0:21:14.320,0:21:19.560<br>
所以他的 memory 和 input 是相加的<br>
<br>
0:21:21.540,0:21:25.860<br>
今天他和 RNN 不同的地方是<br>
<br>
0:21:25.860,0:21:31.680<br>
如果你的 weight 可以影響到 memory 的值的話<br>
<br>
0:21:32.100,0:21:36.800<br>
一但發生影響，這個影響會永遠都存在<br>
<br>
0:21:36.880,0:21:40.740<br>
不像 RNN 在每一個時間點，值都會被 format 掉<br>
<br>
0:21:41.620,0:21:45.180<br>
只要影響一被 format 掉，他就消失了<br>
<br>
0:21:45.300,0:21:49.740<br>
但在 LSTM 裡面，一但能對 memory 造成影響<br>
<br>
0:21:51.420,0:21:55.360<br>
那個影響會永遠留著，除非 forget gate 被開<br>
<br>
0:21:55.360,0:22:00.720<br>
除非 forget gate 被使用，<br>
除非 forget get 決定要把 memory 值洗掉<br>
<br>
0:22:00.720,0:22:04.520<br>
不然一但 memory 有改變的時候，<br>
每次都只會有新的東西加進來<br>
<br>
0:22:04.560,0:22:08.320<br>
而不會把原來存在 memory 裡面的值洗掉<br>
<br>
0:22:08.860,0:22:12.760<br>
所以他不會有 gradient vanishing 的問題<br>
<br>
0:22:14.100,0:22:16.140<br>
那你可能會想說，現在有 forget gate 啊<br>
<br>
0:22:16.140,0:22:19.700<br>
forget gate 就是會把過去存的值洗掉啊<br>
<br>
0:22:19.700,0:22:24.040<br>
事實上 LSTM 在 97 年就被 proposed 了<br>
<br>
0:22:24.580,0:22:30.340<br>
LSTM 第一個版本就是<br>
為了解決 gradient vanishing 的問題<br>
<br>
0:22:30.820,0:22:35.600<br>
所以他是沒有 forget gate 的，<br>
forget gate 是後來才加上去的<br>
<br>
0:22:35.940,0:22:39.600<br>
那甚至現在有一個傳言是<br>
<br>
0:22:39.600,0:22:45.060<br>
你在訓練 LSTM 時，不要給 forget gate 特別大的 bias<br>
<br>
0:22:45.240,0:22:49.540<br>
你要確保 forget gate 在多數的情況下是開啟的<br>
<br>
0:22:49.540,0:22:52.780<br>
只有少數情況會被 format 掉<br>
<br>
0:22:53.780,0:23:04.960<br>
現在有另一個版本，用 gate 操控 memory 的 cell<br>
<br>
0:23:05.100,0:23:07.540<br>
叫做 Gated Recurrent Unit<br>
<br>
0:23:07.560,0:23:10.660<br>
LSTM 有 3 個 gate<br>
<br>
0:23:10.660,0:23:14.680<br>
這個 GRU，他只有兩個 gate<br>
<br>
0:23:16.800,0:23:24.420<br>
所以 GRU 相較於 LSTM，他的 gate 只有 2 個<br>
<br>
0:23:25.120,0:23:29.960<br>
所以他需要的參數量是比較少的<br>
<br>
0:23:29.960,0:23:34.640<br>
因為他需要的參數量是比較少的<br>
<br>
0:23:34.640,0:23:40.020<br>
所以他在 training 是比較 robust 的<br>
<br>
0:23:40.700,0:23:45.580<br>
所以你今天在 train LSTM 的時候，<br>
你覺得 over fitting 情況很嚴重<br>
<br>
0:23:45.660,0:23:49.240<br>
你可以試一下用 GRU<br>
<br>
0:23:49.720,0:23:53.100<br>
GRU 的精神就是，他怎麼拿掉一個 gate<br>
<br>
0:23:53.100,0:23:55.620<br>
我們今天就不講 GRU 的詳細原理<br>
<br>
0:23:55.660,0:23:59.060<br>
他的精神就是舊的不去，新的不來<br>
<br>
0:23:59.200,0:24:05.980<br>
他會把 input gate 跟 forget gate 連動起來<br>
<br>
0:24:06.320,0:24:14.320<br>
當 input gate 被打開的時候，<br>
forget gate 就會被自動的關閉<br>
<br>
0:24:14.320,0:24:17.916<br>
當 input gate 被打開的時候，<br>
forget gate 就會被洗掉<br>
<br>
0:24:17.920,0:24:21.280<br>
就會 format 掉，存在 memory 裡面的值<br>
<br>
0:24:21.600,0:24:26.720<br>
當 forget gate 沒有要 format 值，<br>
input gate 就會被關起來<br>
<br>
0:24:26.840,0:24:31.600<br>
也就是你要把存在 memory 裡面的值清掉，<br>
才可以把新的值放進來<br>
<br>
0:24:35.480,0:24:40.540<br>
其實還有很多其他 techniques，<br>
是來 handle gradient vanishing 這個問題<br>
<br>
0:24:41.240,0:24:48.640<br>
比如說是 Clockwise RNN 或是 SCRN，等等<br>
<br>
0:24:48.640,0:24:51.800<br>
我們把 reference 留在這邊，讓大家參考<br>
<br>
0:24:51.940,0:24:58.660<br>
最後，有一個蠻有趣的 paper<br>
<br>
0:24:58.780,0:25:06.240<br>
是 Hinton proposed，<br>
<br>
0:25:06.660,0:25:09.720<br>
他用一般的 RNN，不是用 LSTM<br>
<br>
0:25:10.080,0:25:17.440<br>
一般 RNN，他用 identity matrix ，<br>
來initialize transition 的weight<br>
<br>
0:25:17.660,0:25:20.600<br>
然後在使用 ReLU 的 activation function 的時候<br>
<br>
0:25:20.760,0:25:23.560<br>
他可以得到很好的 performance<br>
<br>
0:25:23.820,0:25:27.440<br>
有人說那 ReLU 的 performance 不是比較差嗎？<br>
<br>
0:25:27.440,0:25:32.540<br>
如果你是一般 training 的方法，<br>
你 initialization 的 weight 是 random 的話<br>
<br>
0:25:33.000,0:25:42.200<br>
那 ReLU 跟 Sigmoid function 來比的話，<br>
Sigmoid 的 performance 會比較好<br>
<br>
0:25:42.200,0:25:46.820<br>
但是如果你今天用了 identity matrix 的話<br>
<br>
0:25:46.820,0:25:50.880<br>
如果你今天用了 identity matrix 來當作 initialization 的話<br>
<br>
0:25:50.880,0:25:53.880<br>
這時候用 ReLU 的 performance 就會比較好<br>
<br>
0:25:54.040,0:25:59.560<br>
這件事情真的非常的神奇<br>
<br>
0:25:59.880,0:26:03.740<br>
當你用了這一招以後，用一般的 RNN<br>
<br>
0:26:03.860,0:26:08.540<br>
不用 LSTM，他的 performance 就可以屌打 LSTM<br>
<br>
0:26:08.980,0:26:13.980<br>
你就覺得 LSTM 用的這麼複雜，都是白忙一場<br>
<br>
0:26:13.980,0:26:17.260<br>
這個是非常神奇的一篇文章<br>
<br>
0:26:19.220,0:26:21.840<br>
那其實 RNN 有很多的 applications<br>
<br>
0:26:21.840,0:26:24.820<br>
在我們前面舉的 slot filling 例子裡面<br>
<br>
0:26:24.820,0:26:29.320<br>
我們是假設 input 跟 output 的 element 數目是一樣多的<br>
<br>
0:26:30.040,0:26:37.080<br>
也就是說 input 有幾個 word 我們就給每一個 word，<br>
一個 slot 的 label<br>
<br>
0:26:37.840,0:26:42.040<br>
但事實上 RNN 他可以做到呢<br>
<br>
0:26:42.220,0:26:44.320<br>
更複雜的事情<br>
<br>
0:26:44.320,0:26:45.720<br>
可以做到更複雜的事情<br>
<br>
0:26:45.960,0:26:51.320<br>
比如說，他可以 input 是 一個 sequence<br>
 output 只是一個 vector<br>
<br>
0:26:51.620,0:26:56.180<br>
這有什麼應用呢，比如說你可以做 Sentiment Analysis<br>
<br>
0:26:56.180,0:27:02.060<br>
Sentiment Analysis 現在有很多的 applications<br>
比喻來說<br>
<br>
0:27:07.940,0:27:13.000<br>
某家公司想要知道說<br>
他們的產品在網路上評價呢<br>
<br>
0:27:13.260,0:27:15.440<br>
是 positive 還是 negative<br>
<br>
0:27:15.440,0:27:17.840<br>
他們可能就會寫一個爬蟲<br>
<br>
0:27:17.840,0:27:24.100<br>
把跟他們網路評價有關，<br>
或跟它們產品有關係的網路文章，都爬下來<br>
<br>
0:27:24.180,0:27:29.600<br>
但是一篇篇看太累了，<br>
所以你可以用一個 machine learning 的方法<br>
<br>
0:27:29.980,0:27:36.300<br>
自動 learn 一個 classify，<br>
去分類說那些 documents 是正向，那些是負向<br>
<br>
0:27:39.800,0:27:41.700<br>
或者是在電影版上呢<br>
<br>
0:27:41.700,0:27:45.540<br>
Sentiment Analysis 做的事情，<br>
就是給 machine 看很多文章<br>
<br>
0:27:45.680,0:27:52.240<br>
然後 machine 要自動知道說，<br>
那些文章是正雷，那些是負雷<br>
<br>
0:27:53.000,0:27:54.740<br>
怎麼讓 machine 做到這件事情呢？<br>
<br>
0:27:54.740,0:27:57.700<br>
你就是認一個 RNN<br>
<br>
0:27:57.700,0:28:02.220<br>
這個 input 呢是一個 character sequence<br>
<br>
0:28:02.220,0:28:04.740<br>
這個 input 呢是一個 character sequence<br>
<br>
0:28:05.020,0:28:08.800<br>
然後 RNN 呢，<br>
把這個 character sequence 讀過一遍<br>
<br>
0:28:13.140,0:28:17.340<br>
然後在最後一個時間點，把 hidden layer 拿出來<br>
<br>
0:28:20.900,0:28:24.460<br>
把 hidden layer 拿出來，可能再通過幾個 transform<br>
<br>
0:28:24.460,0:28:29.060<br>
然後呢，<br>
你就可以得到最後的 sentiment analysis 的 prediction<br>
<br>
0:28:29.280,0:28:30.740<br>
比如說 input 這個 document<br>
<br>
0:28:30.740,0:28:34.440<br>
他是 超好／好／普／負／超負 雷<br>
<br>
0:28:34.440,0:28:36.160<br>
他是一個分類的問題<br>
<br>
0:28:36.180,0:28:41.000<br>
但 input 是一個 sequence，<br>
所以你需要用 RNN 來處理這個 input<br>
<br>
0:28:42.200,0:28:47.120<br>
或是我們實驗室做過，用 RNN 來做 key term extraction<br>
<br>
0:28:47.120,0:28:49.660<br>
所謂 key term extraction 的意思是說<br>
<br>
0:28:51.680,0:28:59.680<br>
給 machine 看一篇文章，<br>
然後 machine 要 predict 這篇文章有那些關鍵詞彙<br>
<br>
0:28:59.840,0:29:05.020<br>
跟我們在 final project 裡面的第三個 task <br>
做的其實是非常類似的事<br>
<br>
0:29:10.560,0:29:13.580<br>
如果你今天能收集到一堆 training data<br>
<br>
0:29:13.580,0:29:15.360<br>
你能夠收集到一堆 document<br>
<br>
0:29:15.360,0:29:17.040<br>
然後這些 document 都有 label 說<br>
<br>
0:29:17.040,0:29:19.280<br>
哪些詞彙是對應它對應的 key word 的話<br>
<br>
0:29:19.380,0:29:21.920<br>
那你就可以直接 train 一個 RNN<br>
<br>
0:29:21.920,0:29:26.400<br>
這個 RNN 呢，把 document word sequences 當作 input<br>
<br>
0:29:28.440,0:29:32.260<br>
然後通過 embedding layer<br>
<br>
0:29:32.720,0:29:36.800<br>
然後用 RNN 把這個 document 讀過一次<br>
<br>
0:29:39.400,0:29:43.000<br>
然後呢，把出現在最後一個<br>
<br>
0:29:44.060,0:29:48.520<br>
把這出現在最後一個時間點的 output 拿過來做 attention<br>
<br>
0:29:48.800,0:29:53.620<br>
我發現我們沒有講過 attention 是什麼，<br>
這部分你就聽聽就好<br>
<br>
0:29:53.620,0:29:57.320<br>
用 attention 以後呢，<br>
你可以把重要的 information 抽出來<br>
<br>
0:29:57.400,0:30:00.440<br>
再丟到 feed forward network 裡面去<br>
<br>
0:30:00.580,0:30:03.080<br>
得到最後的 output<br>
<br>
0:30:05.280,0:30:08.360<br>
那它也可以是多對多的<br>
<br>
0:30:08.880,0:30:13.380<br>
比如說你的 input/output 都是 sequences<br>
<br>
0:30:13.480,0:30:17.540<br>
但 output sequence 比 input sequence 短的時候<br>
<br>
0:30:17.860,0:30:20.220<br>
RNN 可以處理這個問題<br>
<br>
0:30:20.660,0:30:25.500<br>
什麼樣的任務是 input sequence 長 <br>
output sequence 短呢？<br>
<br>
0:30:26.180,0:30:29.320<br>
比如說語音辨識就是這樣一個任務<br>
<br>
0:30:29.840,0:30:35.120<br>
在語音辨識這個任務，<br>
input 是一串 acoustic feature sequence<br>
<br>
0:30:36.220,0:30:38.380<br>
語音是一段聲訊號<br>
<br>
0:30:38.380,0:30:42.000<br>
要做語音辨識的時候，你就說一句話<br>
<br>
0:30:42.320,0:30:44.480<br>
我們一般處理聲音訊號的方式<br>
<br>
0:30:44.480,0:30:51.360<br>
就是在聲音訊號裡面，每隔一小段時間，<br>
就把它用一個 vector 來表示<br>
<br>
0:30:51.540,0:30:57.380<br>
那一個一小段時間，通常很短，比如說是 0.01 秒之類的<br>
<br>
0:30:59.160,0:31:03.980<br>
那它的 output 是 character 的 sequence<br>
<br>
0:31:05.440,0:31:11.020<br>
那如果你是用原來的 RNN，<br>
用我們在做 Slot Filling 那個 RNN<br>
<br>
0:31:11.020,0:31:14.040<br>
你把這一串 input 丟進去<br>
<br>
0:31:14.040,0:31:17.400<br>
它充其量，只能做到說，告訴你<br>
<br>
0:31:17.400,0:31:21.920<br>
每一個 vector，它對應到哪一個 character<br>
<br>
0:31:21.920,0:31:24.720<br>
假設說中文的語音辨識<br>
<br>
0:31:24.720,0:31:32.560<br>
那你 output 的 target，<br>
理論上就是這世界上所有可能的中文詞彙<br>
<br>
0:31:32.660,0:31:38.200<br>
所有中文的可能的 characters，常用的可能就有 8000 個<br>
<br>
0:31:39.820,0:31:43.620<br>
RNN output 的 class 數目，會有 8000<br>
<br>
0:31:43.620,0:31:45.900<br>
雖然很大，是有辦法做的<br>
<br>
0:31:46.380,0:31:49.520<br>
但充其量，你只能做到說<br>
<br>
0:31:49.520,0:31:52.880<br>
每一個 vector 屬於一個 character<br>
<br>
0:31:52.880,0:31:54.120<br>
但是<br>
<br>
0:31:55.040,0:31:57.560<br>
input 每一個 vector 對應到的時間是很短的<br>
<br>
0:31:57.560,0:31:59.460<br>
通常才 0.01 秒<br>
<br>
0:31:59.580,0:32:03.960<br>
所以通常是好多個 vector 才對應到同一個 character<br>
<br>
0:32:03.960,0:32:07.860<br>
所以你辨識的結果，就變成，好好好棒棒棒棒棒<br>
<br>
0:32:08.480,0:32:11.960<br>
可是這不是語音辨識的結果啊，怎麼辦？<br>
<br>
0:32:12.240,0:32:14.000<br>
有一招叫 trimming<br>
<br>
0:32:14.000,0:32:18.040<br>
trimming 就是把重複的東西拿掉，就變好棒<br>
<br>
0:32:18.220,0:32:22.340<br>
但這樣會有一個很嚴重問題，它就沒有辦法辨識 好棒棒<br>
<br>
0:32:22.620,0:32:26.660<br>
不知道的說一下，好棒跟好棒棒正好是相反的<br>
<br>
0:32:53.260,0:32:57.420<br>
所以不把好棒跟好棒棒分開來是不行的<br>
<br>
0:32:57.540,0:33:00.500<br>
所以需要把好棒跟好棒棒分開來<br>
<br>
0:33:00.700,0:33:05.980<br>
怎麼辦，我們要用一招，叫做 CTC<br>
<br>
0:33:07.840,0:33:10.460<br>
這一招也是那種說穿了不值錢的方法<br>
<br>
0:33:10.460,0:33:11.800<br>
但這一招很神妙<br>
<br>
0:33:11.800,0:33:18.660<br>
它說，我們在 output 的時候，<br>
不只是 output 所有中文的 character<br>
<br>
0:33:18.960,0:33:23.520<br>
我們還多 output 一個符號，叫做 Null<br>
<br>
0:33:23.840,0:33:26.880<br>
叫做沒有任何東西<br>
<br>
0:33:26.880,0:33:30.020<br>
所以今天如果我 input 一串 acoustic feature sequence<br>
<br>
0:33:30.020,0:33:35.160<br>
它的 output 是 好 null null 棒 null null null null<br>
<br>
0:33:35.160,0:33:39.840<br>
然後我就把 null 的部分拿掉，它就變好棒<br>
<br>
0:33:40.340,0:33:46.520<br>
如果我們輸入另外一個 sequence <br>
它的 output 是 好 null null 棒 null 棒 null null<br>
<br>
0:33:46.520,0:33:49.720<br>
它的 output 就是好 棒 棒<br>
<br>
0:33:49.960,0:33:54.180<br>
所以就可以解決疊字的問題了<br>
<br>
0:33:54.180,0:34:00.140<br>
那 CTC 怎麼做訓練呢？<br>
<br>
0:34:00.800,0:34:07.420<br>
CTC 在做訓練的時候，你手上的 training data<br>
<br>
0:34:07.420,0:34:10.780<br>
就會告訴你說，這一串 acoustic feature<br>
<br>
0:34:10.960,0:34:13.680<br>
對應到這一串 character sequence<br>
<br>
0:34:13.680,0:34:15.760<br>
這個 sequence 對應到這個 sequence<br>
<br>
0:34:16.260,0:34:20.345<br>
但他不會告訴你說，<br>
好 是對應第幾個 frame 到第幾個 frame<br>
<br>
0:34:20.345,0:34:23.380<br>
棒 <br>
是對應第幾個 frame 到第幾個 frame<br>
<br>
0:34:23.520,0:34:24.660<br>
那怎麼辦呢？<br>
<br>
0:34:24.660,0:34:28.080<br>
窮舉所有可能的 alignment<br>
<br>
0:34:28.360,0:34:34.020<br>
簡單來說，我們不知道 好 對應到哪幾個 frame<br>
棒 對應到哪幾個 frame<br>
<br>
0:34:34.120,0:34:35.940<br>
我們就假設所有的狀況都是可能的<br>
<br>
0:34:35.940,0:34:37.920<br>
可能第一個是 好，後面接 null，棒 後面接 3 個 null<br>
<br>
0:34:37.920,0:34:41.860<br>
可能 好，後面接 2 個 null，棒 後面接 2 個 null<br>
<br>
0:34:41.860,0:34:45.420<br>
可能 好，後面接 3 個 null，棒 後面接 1 個 null<br>
<br>
0:34:45.600,0:34:48.520<br>
我們不知道哪個是對的，就假設全部都是對的<br>
<br>
0:34:48.520,0:34:51.280<br>
在 training 的時候，全部都當作正確的一起去 train<br>
<br>
0:34:51.820,0:34:56.520<br>
可能會想說，窮舉所有的可能，那可能性感覺太多了<br>
<br>
0:34:56.520,0:35:00.040<br>
這個有巧妙的演算法，可以解決這個問題<br>
<br>
0:35:00.080,0:35:03.500<br>
那我們今天就不細講這個部分<br>
<br>
0:35:04.780,0:35:08.760<br>
以下是在文獻上 CTC 得到的一個結果<br>
<br>
0:35:08.760,0:35:10.020<br>
這是英文的<br>
<br>
0:35:10.080,0:35:14.120<br>
在做英文辨識的時候，你的 RNN 的 output target<br>
<br>
0:35:14.300,0:35:16.300<br>
就是 character<br>
<br>
0:35:16.300,0:35:18.180<br>
就英文的字母<br>
<br>
0:35:18.180,0:35:20.180<br>
加上空白，空白就是說<br>
<br>
0:35:20.220,0:35:23.360<br>
你也不需要給你的 RNN 10 點啊，什麼之類的<br>
<br>
0:35:24.180,0:35:26.180<br>
它就直接 output 字母<br>
<br>
0:35:26.440,0:35:31.440<br>
如果當那的字與字之間有 boundary，<br>
它就自動用空白區隔<br>
<br>
0:35:31.920,0:35:35.380<br>
以下是一個例子，第一個 frame 就 output H<br>
<br>
0:35:35.500,0:35:38.300<br>
第二個frame output null，第三個 frame output null<br>
<br>
0:35:38.340,0:35:41.800<br>
第四個frame output I，第五個 frame output S<br>
<br>
0:35:41.980,0:35:45.580<br>
接下來 output 底線，代表空白<br>
<br>
0:35:46.420,0:35:49.920<br>
然後一串 null 然後 F null null R I<br>
<br>
0:35:49.920,0:35:54.940<br>
null null... E N D null..  ' S _<br>
<br>
0:35:54.980,0:35:58.040<br>
如果你看到 output 是這樣子的話<br>
<br>
0:35:58.040,0:36:03.060<br>
你把 null 拿掉，這句話辨識結果就是 HIS FRIEND'S<br>
<br>
0:36:03.180,0:36:08.640<br>
你不需要告訴 machine 說 HIS 是一個詞彙，<br>
FRIEND'S 是一個詞彙<br>
<br>
0:36:09.020,0:36:12.940<br>
machine 透過 training data，它自己會學到這件事情<br>
<br>
0:36:13.680,0:36:21.240<br>
那傳說呢，google 的語音辨識系統，<br>
已經全面換成 CTC 了<br>
<br>
0:36:22.200,0:36:24.880<br>
如果你用 CTC 來做語音辨識的話<br>
<br>
0:36:24.880,0:36:29.100<br>
就算是有某一個詞彙，比如說英文的人名，地名<br>
<br>
0:36:29.100,0:36:31.860<br>
從來在 training data 沒有出現過<br>
<br>
0:36:31.860,0:36:33.860<br>
machine 從來不知道這詞彙<br>
<br>
0:36:33.960,0:36:37.420<br>
它其實有也機會把它正確的辨識出來<br>
<br>
0:36:40.160,0:36:42.540<br>
另外一個神奇的 RNN 應用呢<br>
<br>
0:36:42.640,0:36:45.460<br>
叫做 sequence to sequence learning<br>
<br>
0:36:45.760,0:36:51.380<br>
在 sequence to sequence learning 裡面，<br>
RNN 的 input and output 都是 sequence<br>
<br>
0:36:51.420,0:36:54.680<br>
這兩段 sequence 的長度是不一樣的<br>
<br>
0:36:54.800,0:36:58.920<br>
剛剛在講 CTC 的時候，input 比較長，output 比較短<br>
<br>
0:36:58.920,0:37:05.560<br>
在這邊我們要考慮的case是，<br>
不確定 input output 誰比較長，誰比較短<br>
<br>
0:37:08.720,0:37:11.720<br>
比如說我們現在要做的是 machine translation<br>
<br>
0:37:11.720,0:37:19.120<br>
input 英文的 word sequence <br>
要把它翻成中文的 character sequence<br>
<br>
0:37:24.620,0:37:31.620<br>
我們並不知道英文或中文，誰比較長，誰比較短<br>
<br>
0:37:31.760,0:37:35.140<br>
都有可能是 output 比較長，或 output 比較短<br>
<br>
0:37:35.260,0:37:40.100<br>
所以怎麼辦呢？<br>
現在假如 input 的是 machine learning<br>
<br>
0:37:40.100,0:37:45.140<br>
machine learning 用 RNN 讀過去<br>
<br>
0:37:45.340,0:38:04.000<br>
然後在最後一個時間點呢，memory 就<br>
存了所有 input 的整個 sequence 的 information<br>
<br>
0:38:05.060,0:38:09.260<br>
然後接下來，你就讓 machine 吐一個 character<br>
<br>
0:38:09.300,0:38:12.740<br>
比如說它吐的第一個 character 就是 機<br>
<br>
0:38:12.740,0:38:15.320<br>
你把 machine learning 讓 machine 讀過一遍<br>
<br>
0:38:15.320,0:38:18.700<br>
然後在讓它 output character，它可能就會 output 機<br>
<br>
0:38:18.840,0:38:21.940<br>
接下來再叫它 output 下一個 character<br>
<br>
0:38:21.940,0:38:25.340<br>
你把之前 output 出來的 character 當作 input<br>
<br>
0:38:25.340,0:38:28.080<br>
再把 memory 裡面存的值讀進來<br>
<br>
0:38:28.080,0:38:29.720<br>
它就會 output 器<br>
<br>
0:38:30.800,0:38:40.180<br>
這個 機 要如何接到這裡，這地方有很多枝枝節節的技巧<br>
<br>
0:38:42.560,0:38:45.500<br>
這個太多了，我們以後再講<br>
<br>
0:38:46.000,0:38:49.260<br>
這個以後或許下學期，在 MLTS 再講<br>
<br>
0:38:49.260,0:38:55.540<br>
這個其實有很多枝枝節節的地方，<br>
還有很多各種不同的變形<br>
<br>
0:38:56.640,0:39:01.560<br>
那它在下一個時間點，器 以後它就 output 學<br>
<br>
0:39:01.560,0:39:03.720<br>
然後學就 output 習<br>
<br>
0:39:03.720,0:39:06.520<br>
它就會一直 output 下去<br>
<br>
0:39:06.620,0:39:09.140<br>
習 後面接 慣，慣 後面接 性<br>
<br>
0:39:09.160,0:39:11.660<br>
永遠都不停止這樣<br>
<br>
0:39:11.660,0:39:16.840<br>
第一次看到這 model 根本不知道什麼時候該停止<br>
<br>
0:39:17.660,0:39:23.600<br>
那怎麼辦呢，這就讓我想到推文接龍<br>
<br>
0:39:50.060,0:39:52.060<br>
那你要怎麼讓他停下來呢<br>
<br>
0:39:53.520,0:39:56.600<br>
你要有一個冒險去推一個 ==斷==<br>
然後它就會停下來了<br>
<br>
0:40:03.400,0:40:06.120<br>
所以今天讓 machine 做的事情，也是一樣<br>
<br>
0:40:06.120,0:40:09.100<br>
要如何阻止它不斷的繼續產生詞彙呢？<br>
<br>
0:40:09.100,0:40:11.880<br>
你要多加一個 symbol 叫做 斷<br>
<br>
0:40:11.880,0:40:14.720<br>
所以 machine 不只 output 所有可能的 character<br>
<br>
0:40:14.720,0:40:17.860<br>
它還有一個可能的 output，就做斷<br>
<br>
0:40:17.920,0:40:21.760<br>
所以如果今天 習 後面呢，<br>
它的 output 是 斷 的話<br>
<br>
0:40:22.140,0:40:24.600<br>
就停下來<br>
<br>
0:40:24.600,0:40:26.860<br>
可能覺得說，這東西 train 得起來嗎？<br>
<br>
0:40:26.860,0:40:29.040<br>
恩，train 得起來<br>
<br>
0:40:29.040,0:40:32.560<br>
神奇的就是這一招，是有用的！<br>
<br>
0:40:32.560,0:40:34.580<br>
它也有被用在語音辨識上<br>
<br>
0:40:34.580,0:40:37.060<br>
你就直接 input acoustic feature sequence<br>
<br>
0:40:37.180,0:40:39.540<br>
直接就 output character sequence<br>
<br>
0:40:39.540,0:40:42.080<br>
只是這方法，還沒有 CTC 強<br>
<br>
0:40:42.080,0:40:44.520<br>
所以這方法，還不是 state of the art 的結果<br>
<br>
0:40:44.520,0:40:50.900<br>
但讓人真正 surprise 的地方，<br>
這麼做是行的通，然後它的結果是沒有爛掉<br>
<br>
0:40:52.760,0:40:58.420<br>
在翻譯上，據說用這個方法，<br>
已經可以達到 state of the art 的結果了<br>
<br>
0:41:00.380,0:41:06.160<br>
那最近呢，這應該是 google 在 12 月初發的 paper<br>
<br>
0:41:06.160,0:41:08.380<br>
所以是幾周前，放在 arxiv 上的 paper<br>
<br>
0:41:08.860,0:41:13.400<br>
他們做了一件事情，我相信這件事情很多人都想到，<br>
只是沒人去做而已<br>
<br>
0:41:13.400,0:41:15.220<br>
他的想法是這樣<br>
<br>
0:41:15.220,0:41:19.280<br>
sequence to sequence learning 假設是做翻譯的話<br>
<br>
0:41:19.280,0:41:21.700<br>
也就是 input 某種語言的文字<br>
<br>
0:41:21.720,0:41:24.440<br>
翻成另外一種語言的文字<br>
<br>
0:41:28.260,0:41:30.260<br>
我們有沒有可能，直接 input 某種語言的聲音訊號<br>
<br>
0:41:30.500,0:41:32.500<br>
output 另為一種語言的文字呢？<br>
<br>
0:41:32.640,0:41:36.080<br>
我們完全不做語音辨識<br>
<br>
0:41:38.100,0:41:42.000<br>
比如說你要把英文翻成中文<br>
<br>
0:41:42.260,0:41:46.440<br>
你就收集一大堆英文句子，和他對應的中文翻譯<br>
<br>
0:41:46.500,0:41:48.040<br>
你完全不要做語音辨識<br>
<br>
0:41:48.040,0:41:51.400<br>
直接把英文的聲音訊號，丟到這個 model 裡面去<br>
<br>
0:41:51.440,0:41:53.740<br>
看他能不能 output 正確的中文<br>
<br>
0:41:53.900,0:41:57.940<br>
結果這一招居然看起來是行得通的<br>
<br>
0:41:57.940,0:42:01.180<br>
我相信很多人想過，大概覺得做不起來，<br>
所以沒有人去試<br>
<br>
0:42:01.260,0:42:03.900<br>
這一招看起來是行得通的<br>
<br>
0:42:03.900,0:42:08.140<br>
你可以直接 input 一串法文的聲音訊號<br>
<br>
0:42:08.280,0:42:13.340<br>
然後 model 就得到辨識的結果<br>
<br>
0:42:17.500,0:42:24.120<br>
如果這個東西能夠成功的話，他可以帶給我們的好處是<br>
<br>
0:42:28.240,0:42:33.760<br>
如果我們在 collect translation 的 training data 的時候<br>
<br>
0:42:33.760,0:42:35.360<br>
會比較容易<br>
<br>
0:42:35.360,0:42:37.560<br>
假設你今天要把某種方言<br>
<br>
0:42:37.560,0:42:40.040<br>
比如說台語，轉成英文<br>
<br>
0:42:40.080,0:42:44.060<br>
但是台語的語音辨識系統比較不好做<br>
<br>
0:42:44.060,0:42:49.860<br>
因為台語根本就沒有一個 standard 的文字的系統<br>
<br>
0:42:50.160,0:42:54.240<br>
所以你要找人來 label 台語的文字，<br>
可能也有點麻煩<br>
<br>
0:42:54.240,0:42:56.760<br>
如果這樣子技術是可以成功的話<br>
<br>
0:42:56.760,0:42:59.680<br>
未來你在訓練台語轉英文的語音辨識系統的時候<br>
<br>
0:42:59.680,0:43:02.900<br>
你只需要收集台語的聲音訊號<br>
<br>
0:43:02.900,0:43:07.020<br>
跟他的英文翻譯就可以了<br>
<br>
0:43:07.020,0:43:10.840<br>
你就不需要台語的語音辨識結果<br>
<br>
0:43:10.840,0:43:15.180<br>
你就不需要知道台語的文字，你也可以做這種翻譯<br>
<br>
0:43:19.720,0:43:23.420<br>
那現在還可以用 sequence to sequence 的技術<br>
<br>
0:43:23.420,0:43:28.340<br>
甚至可以做到 Beyond Sequence<br>
<br>
0:43:34.680,0:43:40.100<br>
比如說這個技術呢，<br>
也被用在 Syntactic parsing tree 裡面<br>
<br>
0:43:40.100,0:43:43.220<br>
用在產生，Syntactic parsing tree 上面<br>
<br>
0:43:44.080,0:43:47.840<br>
這個 Syntactic parsing tree 是什麼呢？<br>
<br>
0:43:47.840,0:43:50.180<br>
意思就是，讓 machine 看一個句子<br>
<br>
0:43:50.180,0:43:54.800<br>
然後他得到這個句子的文法的結構樹<br>
<br>
0:44:04.800,0:44:08.860<br>
要怎麼讓 machine 得到這樣的樹狀的結構呢？<br>
<br>
0:44:09.640,0:44:12.680<br>
過去呢，<br>
你可能要用 structure learning 的技術<br>
<br>
0:44:12.680,0:44:16.260<br>
才能夠解這一個問題<br>
<br>
0:44:18.700,0:44:21.640<br>
但現在有了 sequence to sequence 的技術以後<br>
<br>
0:44:21.660,0:44:27.740<br>
你只要把這個樹狀圖，描述成一個 sequence<br>
<br>
0:44:27.740,0:44:31.100<br>
樹狀圖當然可以描述成一個 sequence<br>
<br>
0:44:32.500,0:44:34.440<br>
root 的地方是 S<br>
<br>
0:44:34.440,0:44:37.660<br>
S 的左括號，S 的右括號<br>
<br>
0:44:37.660,0:44:39.980<br>
他下面有 NP 跟 VP<br>
<br>
0:44:39.980,0:44:42.260<br>
所以有 NP 的左括號，NP 的右括號<br>
<br>
0:44:42.260,0:44:44.740<br>
VP 的左括號，VP 的右括號<br>
<br>
0:44:44.740,0:44:54.980<br>
NP 下面有 NNP，VP 下面有 VBZ，NP<br>
NP 下面有 DT/NN 等等<br>
<br>
0:44:56.180,0:44:59.980<br>
所以他有一個 sequence<br>
<br>
0:45:01.020,0:45:04.220<br>
所以如果今天是 sequence to sequence learning 的話<br>
<br>
0:45:04.420,0:45:06.940<br>
你就直接 learn 一個 sequence to sequence 的 model<br>
<br>
0:45:06.980,0:45:15.320<br>
他的 output 直接是這個 Syntactic 的 parsing tree<br>
<br>
0:45:16.220,0:45:19.900<br>
你可能覺得這樣真的 train 得起來嗎？<br>
<br>
0:45:20.000,0:45:24.140<br>
恩，可以 train 得起來，這很 surprise<br>
<br>
0:45:25.520,0:45:34.160<br>
當然你可能會想說 machine 今天長出來的 output sequence 他不符合文法結構呢？<br>
<br>
0:45:34.160,0:45:37.160<br>
如果他記得加左括號，卻忘了加右括號呢？<br>
<br>
0:45:37.160,0:45:44.000<br>
但神奇的地方是，LSTM 它有記憶力，<br>
他不會忘記加上右括號<br>
<br>
0:45:46.320,0:45:50.780<br>
好，那我們之前講過 word vector<br>
<br>
0:45:51.020,0:45:55.240<br>
那如果我們要把一個 document 表示成一個 vector 的話<br>
<br>
0:45:55.240,0:45:57.280<br>
往往會用 bag-of-word 的方法<br>
<br>
0:45:57.280,0:46:00.020<br>
但當我們用 bag-of-word 的方法<br>
<br>
0:46:00.060,0:46:05.200<br>
我們就會忽略到 word order 的 information<br>
<br>
0:46:05.500,0:46:08.700<br>
舉例來說，有一個 word sequence<br>
<br>
0:46:08.700,0:46:11.640<br>
是 white blood cells destroying an infection<br>
<br>
0:46:12.300,0:46:16.520<br>
另外一個 word sequence 是 an infection destroying white blood cells<br>
<br>
0:46:16.520,0:46:19.060<br>
這兩句話的意思，完全是相反的<br>
<br>
0:46:19.060,0:46:22.080<br>
但是如果你用 bag-of-word 來描述他的話<br>
<br>
0:46:22.160,0:46:26.740<br>
它們的 bag-or-word 完全是一樣的<br>
<br>
0:46:26.740,0:46:31.280<br>
它們裡面有一模一樣的 6 個詞彙<br>
<br>
0:46:31.280,0:46:34.540<br>
但是因為這個詞彙的 order 是不一樣的<br>
<br>
0:46:34.540,0:46:39.440<br>
對他們的意思，一個變成 positive，一個變成 negative<br>
<br>
0:46:39.440,0:46:42.620<br>
意思是很不一樣的<br>
<br>
0:46:43.280,0:46:46.960<br>
那我們可以用 sequence to sequence auto-encoder 這種做法<br>
<br>
0:46:46.960,0:46:52.200<br>
在有考慮 word sequence order 的情況下<br>
<br>
0:46:52.480,0:46:56.640<br>
把一個 document 變成一個 vector<br>
<br>
0:46:57.340,0:46:59.340<br>
怎麼做呢？<br>
<br>
0:46:59.360,0:47:02.660<br>
我們就 input 一個 word sequence<br>
<br>
0:47:05.140,0:47:07.580<br>
通過一個 RNN<br>
<br>
0:47:07.580,0:47:11.300<br>
把它變成一個 embedded 的 vector<br>
<br>
0:47:11.660,0:47:16.740<br>
然後再把這個 embedded vector 當成 decoder 的輸入<br>
<br>
0:47:17.440,0:47:22.900<br>
然後讓這個 decoder 長回一個一模一樣的句子<br>
<br>
0:47:22.900,0:47:27.040<br>
如果今天 RNN 可以做到這件事情的話<br>
<br>
0:47:27.100,0:47:29.260<br>
那 encoding 的這個 vector<br>
<br>
0:47:29.260,0:47:33.200<br>
就代表這個 input sequence 裡面，重要的資訊<br>
<br>
0:47:33.200,0:47:37.740<br>
所以這個 decoder 呢，才能根據 encoder 的 vector<br>
<br>
0:47:38.000,0:47:41.240<br>
把這個訊號 decode 回來<br>
<br>
0:47:41.280,0:47:45.940<br>
train 這個 sequence to sequence auto-encoder<br>
你是不需要 label data 的<br>
<br>
0:47:45.960,0:47:50.620<br>
你只需要收集到大量的文章<br>
<br>
0:47:50.620,0:47:54.120<br>
然後直接 train 下去就好了<br>
<br>
0:47:55.100,0:47:59.840<br>
那這個 sequence to sequence auto-encoder，<br>
還有另外一個版本叫做 ******<br>
<br>
0:47:59.840,0:48:04.480<br>
當你是用 ****，如果是用 Seq2Seq auto encoder<br>
<br>
0:48:04.480,0:48:06.560<br>
input 跟 output 都是同一個句子<br>
<br>
0:48:06.560,0:48:11.840<br>
如果你用 **** 的話，output target會是下一個句子<br>
<br>
0:48:11.840,0:48:17.160<br>
如果是用 Seq2Seq auto encoder，通常你得到的 code 比較容易表達文法的意思<br>
<br>
0:48:17.160,0:48:23.080<br>
如果你要得到語意的意思，<br>
用 **** 可能會得到比較好結果<br>
<br>
0:48:23.160,0:48:27.200<br>
這個結構，甚至可以是 Hierarchy 的<br>
<br>
0:48:27.200,0:48:31.840<br>
你可以每一個句子都先得到一個 vector<br>
<br>
0:48:39.340,0:48:44.800<br>
再把這些 vector 加起來，變成一個整個<br>
<br>
0:48:44.800,0:48:46.940<br>
document high level 的 vector<br>
<br>
0:48:46.940,0:48:49.320<br>
再用這個 document high level 的 vector<br>
<br>
0:48:49.320,0:48:51.700<br>
去產生一串 sentence 的 vector<br>
<br>
0:48:51.700,0:48:53.360<br>
再根據每一個 sentence vector<br>
<br>
0:48:53.360,0:48:56.860<br>
再去解回 word sequence<br>
<br>
0:48:57.140,0:49:01.540<br>
所以這是一個 4 層的 LSTM<br>
<br>
0:49:01.540,0:49:04.780<br>
你從 word 變成 sentence sequence<br>
<br>
0:49:04.780,0:49:06.820<br>
再變成 document level 的東西<br>
<br>
0:49:06.820,0:49:10.460<br>
再解回 sentence sequence，再解回 word sequence<br>
<br>
0:49:10.460,0:49:12.820<br>
這個東西也是可以 train 的<br>
<br>
0:49:16.260,0:49:19.720<br>
那剛才的東西，也可以被用在語音上<br>
<br>
0:49:19.720,0:49:24.460<br>
seq2seq auto encoder 除了被用在文字上，<br>
也可以被用在語音上<br>
<br>
0:49:24.460,0:49:27.940<br>
如果用在語音上，它可以做到的事情，就是<br>
<br>
0:49:28.040,0:49:35.920<br>
它可以把一段 audio segment <br>
變成一段 fixed length 的 vector<br>
<br>
0:49:37.580,0:49:42.120<br>
比如說它可以把 dog 變成<br>
<br>
0:49:42.420,0:49:46.660<br>
比如說這邊有一堆聲音訊號，<br>
它們長長短短的都不一樣<br>
<br>
0:49:46.660,0:49:52.200<br>
你把它們變成 vector 的話，<br>
可能 dog/dogs 的 vector 比較接近<br>
<br>
0:49:52.200,0:49:57.340<br>
可能 never/ever 的 vector 是比較接近的<br>
<br>
0:49:59.060,0:50:01.860<br>
這個我稱之為 audio 的 word to vector<br>
<br>
0:50:01.860,0:50:05.400<br>
就像一般的 word to vector，它是把一個 word 變成一個 vector<br>
<br>
0:50:05.400,0:50:10.520<br>
這邊是把一段聲音訊號，變成一個 vector<br>
<br>
0:50:11.700,0:50:18.100<br>
這個東西有什麼用呢？<br>
一開始在想這個我覺得應該沒有什麼用<br>
<br>
0:50:18.400,0:50:21.780<br>
但它其實可以拿來做很多事，比如說<br>
<br>
0:50:21.780,0:50:24.760<br>
我們可以拿來做語音的搜尋<br>
<br>
0:50:24.900,0:50:28.260<br>
什麼是語音的搜尋呢？<br>
你有一個聲音的 database<br>
<br>
0:50:28.260,0:50:29.800<br>
比如說上課的錄影錄音<br>
<br>
0:50:29.920,0:50:32.520<br>
然後你說一句話<br>
<br>
0:50:33.140,0:50:37.860<br>
比如說你今天要找美國白宮有關的東西<br>
<br>
0:50:37.880,0:50:39.720<br>
你就用說的，說美國白宮<br>
<br>
0:50:39.720,0:50:41.360<br>
然後不需要做語音辨識<br>
<br>
0:50:41.360,0:50:43.320<br>
直接比對聲音訊號的相似度<br>
<br>
0:50:43.400,0:50:47.700<br>
machine 就可以從 database 裡面，<br>
把有提到美國白宮的部分，找出來<br>
<br>
0:50:48.300,0:50:51.860<br>
那這個怎麼做呢？你有一個 audio 的 data base<br>
<br>
0:50:51.860,0:50:54.240<br>
把這個 database 做 segmentation<br>
<br>
0:50:54.240,0:50:56.560<br>
切成一段一段<br>
<br>
0:50:56.820,0:51:02.520<br>
然後每一段呢，<br>
用剛才講的 audio segment to vector 的技術呢<br>
<br>
0:51:02.520,0:51:06.060<br>
把他們通通變成 vector<br>
<br>
0:51:07.220,0:51:09.820<br>
然後現在使用者輸入一個 Query<br>
<br>
0:51:09.820,0:51:11.820<br>
Query 也是語音的<br>
<br>
0:51:11.820,0:51:14.660<br>
透過 audio segment to vector 的技術呢<br>
<br>
0:51:14.660,0:51:18.140<br>
可以把這一段聲音訊號呢，也變成 vector<br>
<br>
0:51:18.420,0:51:25.360<br>
然後接下來呢，計算它們的相似程度<br>
<br>
0:51:25.360,0:51:30.080<br>
然後就得到搜尋的結果<br>
<br>
0:51:41.760,0:51:43.400<br>
這件事情怎麼做呢？<br>
<br>
0:51:43.400,0:51:47.180<br>
怎麼把一個 audio segment 變成一個 vector 呢？<br>
<br>
0:51:48.540,0:51:51.800<br>
作法是這樣，先把 audio segment<br>
<br>
0:51:51.800,0:51:54.000<br>
抽成 acoustic feature sequence<br>
<br>
0:51:55.560,0:51:57.560<br>
然後呢，把它丟到 RNN 裡面去<br>
<br>
0:51:57.600,0:52:01.880<br>
這個 RNN 它的角色，就是一個 encoder<br>
<br>
0:52:01.880,0:52:05.680<br>
而這個 RNN 它讀過這個 acoustic feature sequence 以後<br>
<br>
0:52:05.760,0:52:09.240<br>
它存在 memory 裡面的值，就代表了<br>
<br>
0:52:09.240,0:52:12.440<br>
它在最後時間點存在這 memory 裡面的值<br>
<br>
0:52:12.440,0:52:17.720<br>
就代表了它的整個 input 的聲音訊號<br>
它的 information<br>
<br>
0:52:18.900,0:52:20.900<br>
它存在 memory 裡面的值，是一個 vector<br>
<br>
0:52:21.100,0:52:26.380<br>
這個東西，<br>
其實就是我們要拿來表示一整段聲音訊號的 vector<br>
<br>
0:52:26.560,0:52:29.260<br>
但是只有這個 RNN encoder 我們沒有辦法 train<br>
<br>
0:52:29.260,0:52:31.540<br>
你同時還要 train 一個 RNN 的 decoder<br>
<br>
0:52:31.760,0:52:34.920<br>
RNN decoder 它的作用呢，它把<br>
<br>
0:52:34.920,0:52:38.820<br>
encoder 存在 memory 裡面的值呢，拿進來當作做 input<br>
<br>
0:52:38.940,0:52:43.080<br>
然後產生一個 acoustic feature sequence<br>
<br>
0:52:43.080,0:52:47.160<br>
那你會希望這個 y1 跟 x1 越接近越好<br>
<br>
0:52:48.220,0:52:52.980<br>
然後根據 y1 再產生 y2 y3 y4<br>
<br>
0:52:53.160,0:52:54.800<br>
而今天訓練的 target<br>
<br>
0:52:54.800,0:53:02.340<br>
就是希望 y1 到 y4 跟 x1 到 x4 它們是越接近越好<br>
<br>
0:53:02.340,0:53:03.760<br>
那在訓練的時候<br>
<br>
0:53:03.760,0:53:09.000<br>
這個 RNN 的 encoder 和 RNN decoder 他們是 jointly learned<br>
<br>
0:53:09.000,0:53:14.740<br>
它們是一起 train的<br>
<br>
0:53:16.960,0:53:21.540<br>
如果 RNN encoder/decoder，它們只有一個人，<br>
是沒有辦法 train 的<br>
<br>
0:53:21.540,0:53:23.400<br>
但是把他們兩個人接起來<br>
<br>
0:53:23.500,0:53:27.920<br>
你就有一個 target 可以從這邊，<br>
一路 back propagate 回來<br>
<br>
0:53:27.920,0:53:31.040<br>
你就可以同時 train RNN encoder 跟 decoder<br>
<br>
0:53:31.040,0:53:36.380<br>
這邊呢是我們在實驗上得到的一些有趣結果<br>
<br>
0:53:36.380,0:53:39.320<br>
這個圖上的每一個點，都是一段聲音訊號<br>
<br>
0:53:39.380,0:53:43.160<br>
你把聲音訊號用剛才講的<br>
 sequence to sequence encoder 技術<br>
<br>
0:53:43.160,0:53:46.360<br>
把它變成平面上的一個 vector<br>
<br>
0:53:46.380,0:53:49.300<br>
會發現說 fear 的位置，在左上角<br>
<br>
0:53:49.900,0:53:51.640<br>
near 的位置在右下角<br>
<br>
0:53:51.640,0:53:53.440<br>
中間是這樣子的關係<br>
<br>
0:53:53.440,0:53:55.660<br>
fame 的位置在左上角，name 的位置在右下角<br>
<br>
0:53:55.660,0:53:59.140<br>
它們中間有一個這樣子的關係<br>
<br>
0:53:59.200,0:54:05.160<br>
哪你會發現說，把 fear 開頭的 f 換成 n<br>
跟 fame 開頭的 f 換成 n<br>
<br>
0:54:05.340,0:54:10.160<br>
它們的 word vector 的變化，方向是一樣的<br>
<br>
0:54:10.160,0:54:13.340<br>
就好像我們之前看到的這個 vector 一樣<br>
<br>
0:54:13.340,0:54:16.600<br>
跟我們好像之前看到文字的 word vector 一樣<br>
<br>
0:54:16.600,0:54:21.260<br>
不過這邊的 vector <br>
還沒有辦法考慮 semantic 語意的 information<br>
<br>
0:54:21.360,0:54:24.900<br>
那我們下一步要做的事情，就是把語意加進去<br>
<br>
0:54:24.900,0:54:27.380<br>
但這部分現在還沒有完成<br>
<br>
0:54:27.560,0:54:32.480<br>
那接下來我有一個 demo，這個 demo  是<br>
用 sequence to sequence auto encoder<br>
<br>
0:54:32.660,0:54:35.540<br>
來訓練一個 chat bot<br>
<br>
0:54:35.540,0:54:38.060<br>
chat bot 就是聊天機器人<br>
<br>
0:54:40.740,0:54:43.120<br>
那怎麼用 sequence to sequence，<br>
<br>
0:54:43.120,0:54:44.860<br>
喔，這不是 sequence to sequence auto encoder<br>
<br>
0:54:44.860,0:54:46.840<br>
這是 sequence to sequence learning<br>
<br>
0:54:46.880,0:54:49.360<br>
那怎麼用來<br>
來 train 一個 chat bot 呢？<br>
<br>
0:54:49.360,0:54:54.060<br>
你就收集很多對話，比如說電影的台詞<br>
<br>
0:54:54.240,0:54:57.440<br>
假設電影的台詞裡面，有一個人說 how are you<br>
<br>
0:54:57.440,0:54:59.520<br>
另外一個人就接 I am fine<br>
<br>
0:54:59.520,0:55:02.580<br>
那就告訴 machine 說，<br>
這個 sequence to sequence learning<br>
<br>
0:55:02.580,0:55:05.900<br>
它的 input 當它是 how are you 的時候<br>
<br>
0:55:06.040,0:55:09.400<br>
這個 model 的 output 就要是 I am fine<br>
<br>
0:55:09.400,0:55:13.840<br>
假如你可以收集到這種 data，然後就讓 machine 去 train<br>
<br>
0:55:15.520,0:55:18.500<br>
然後我們就收集了 40000 句的電視影集<br>
<br>
0:55:18.500,0:55:20.760<br>
和美國總統大選辯論的句子<br>
<br>
0:55:20.760,0:55:27.340<br>
然後就讓 machine 去學這個 <br>
sequence to sequence 的 model<br>
<br>
0:55:27.900,0:55:33.560<br>
這個是跟中央大學 蔡宗翰 老師的團隊一起開發的<br>
<br>
0:55:33.600,0:55:38.640<br>
然後作的同學呢，台大這邊呢，是有<br>
<br>
0:55:42.100,0:55:46.200<br>
那其實現在除了 RNN 以外呢<br>
<br>
0:55:46.200,0:55:50.200<br>
還有另外一種有用到 memory 的 network<br>
<br>
0:55:50.200,0:55:52.140<br>
叫做 attention-base model<br>
<br>
0:55:52.140,0:55:55.360<br>
它可以想成是 RNN 的一個進階版本<br>
<br>
0:55:55.760,0:56:04.840<br>
那我們知道人的大腦，有非常強的記憶力<br>
<br>
0:56:04.920,0:56:07.880<br>
所以你可以記得非常多的東西<br>
<br>
0:56:07.880,0:56:11.100<br>
比如說你現在可能同時記得，早餐吃了什麼<br>
<br>
0:56:11.100,0:56:15.440<br>
可能同時記得 10 年前中二的夏天發生了什麼<br>
<br>
0:56:15.620,0:56:19.980<br>
可能同時記得在這幾門課學到的東西<br>
<br>
0:56:20.280,0:56:23.460<br>
那當然有人問你什麼是 deep learning 的時候<br>
<br>
0:56:23.460,0:56:28.800<br>
那你的腦中會去提取重要的 information<br>
<br>
0:56:28.800,0:56:31.040<br>
然後再把這些 information 組織起來<br>
<br>
0:56:31.060,0:56:33.060<br>
產生答案<br>
<br>
0:56:33.740,0:56:39.080<br>
但你的腦會自動忽略掉那些無關的事情<br>
<br>
0:56:39.080,0:56:43.100<br>
比如說 10 年前中二的夏天發生的事情，等等<br>
<br>
0:56:43.800,0:56:47.620<br>
那其實 machine 也可以做到類似的事情<br>
<br>
0:56:47.820,0:56:52.100<br>
machine 也可以有很大的記憶容量<br>
<br>
0:56:52.180,0:56:54.340<br>
它也可以有一個很大的 data base<br>
<br>
0:56:54.340,0:56:58.740<br>
在這個 data base 裡面，每一個 vector 就代表某種 information<br>
<br>
0:56:58.740,0:57:00.740<br>
被存在 machine 的記憶裡面<br>
<br>
0:57:01.780,0:57:05.660<br>
當你輸入一個 input 的時候，<br>
這個 input 會被丟進一個中央處理器<br>
<br>
0:57:05.720,0:57:09.600<br>
這個中央處理器，可能是一個 DNN/RNN<br>
<br>
0:57:10.180,0:57:14.240<br>
那這個中央處理器，會操控一個讀寫頭<br>
<br>
0:57:14.240,0:57:17.520<br>
操恐一個 reading head controller<br>
<br>
0:57:17.520,0:57:22.160<br>
最後這個 reading head controller <br>
會決定這個 reading head<br>
<br>
0:57:22.160,0:57:23.400<br>
放的位置<br>
<br>
0:57:23.400,0:57:28.200<br>
然後 machine 再從這個 reading head 放的位置，<br>
去讀取 information 出來<br>
<br>
0:57:29.240,0:57:33.320<br>
然後產生最後的 output<br>
<br>
0:57:34.020,0:57:37.060<br>
那我們就不打算細講這樣的 model，<br>
如果你有興趣<br>
<br>
0:57:37.060,0:57:40.480<br>
可以參考我之前上課的錄影<br>
<br>
0:57:40.940,0:57:45.660<br>
這個 model 還有一個 2.0 的版本<br>
<br>
0:57:45.920,0:57:52.680<br>
這個 2.0 版本，它會去操控一個 <br>
writing head controller<br>
<br>
0:57:53.040,0:57:57.620<br>
這個 writing head controller <br>
會去決定 writing head 放的位置<br>
<br>
0:57:57.660,0:58:01.480<br>
然後 machine 會把它的 information <br>
透過這個 writing head 呢<br>
<br>
0:58:01.660,0:58:04.240<br>
寫進它的 database 裡面<br>
<br>
0:58:06.360,0:58:09.800<br>
所以他不只有讀的功能，還可以把資訊<br>
<br>
0:58:09.920,0:58:11.920<br>
它 discover 出來的東西<br>
<br>
0:58:12.260,0:58:14.700<br>
寫到它的 memory 裡面去<br>
<br>
0:58:14.820,0:58:18.540<br>
這個東西就是大名鼎鼎的 Neural Turing Machine<br>
<br>
0:58:18.940,0:58:21.100<br>
這些其實都是很新的東西<br>
<br>
0:58:21.100,0:58:33.700<br>
Neural Turing Machine 應該是在 14 年的年底提出來的<br>
<br>
0:58:33.700,0:58:35.180<br>
我也忘了<br>
<br>
0:58:35.580,0:58:40.980<br>
不知道是 15 年初，還是 14 年底的時候，提出來的<br>
<br>
0:58:40.980,0:58:42.940<br>
所以都是很新的東西<br>
<br>
0:58:44.540,0:58:49.000<br>
現在 attention-based model，<br>
常常被用在 reading comprehension<br>
<br>
0:58:49.240,0:58:54.200<br>
所謂 reading comprehension，就是，<br>
讓 machine 去讀一堆 document<br>
<br>
0:58:54.200,0:58:56.960<br>
然後這些 document 裡面的內容呢？<br>
<br>
0:58:56.960,0:59:00.340<br>
每一句話，變成一個 vector 存起來<br>
<br>
0:59:00.340,0:59:08.400<br>
每一個 vector 代表某一句話的語意<br>
<br>
0:59:08.580,0:59:11.020<br>
接下來呢，你問 machine 一個問題<br>
<br>
0:59:11.020,0:59:12.960<br>
比如說玉山有多高之類的<br>
<br>
0:59:13.500,0:59:17.440<br>
然後這個問題被丟進一個中央處理器裡面<br>
<br>
0:59:17.460,0:59:21.680<br>
那這個中央處理去去控制一個 reading head controller<br>
<br>
0:59:22.200,0:59:25.420<br>
去決定現在在這個 database 裡面<br>
<br>
0:59:25.420,0:59:30.280<br>
那些句子是跟中央處理器有關的<br>
<br>
0:59:30.880,0:59:35.040<br>
所以假設呢 machine 發現說這個句子，<br>
是跟現在這個問題有關的<br>
<br>
0:59:35.040,0:59:37.340<br>
它就把 reading head 放在這個地方<br>
<br>
0:59:37.340,0:59:39.820<br>
把 information 讀到中央處理器裡面<br>
<br>
0:59:40.260,0:59:44.100<br>
這個讀取 information 的過程，它可以是 iterative<br>
<br>
0:59:44.100,0:59:46.020<br>
它可以是重複數次的<br>
<br>
0:59:46.020,0:59:50.160<br>
也就是說 machine 並不會只從一個地方讀取 information<br>
<br>
0:59:50.300,0:59:52.420<br>
它先從這裡讀取 information 以後<br>
<br>
0:59:52.420,0:59:57.160<br>
它還可以換一個位置，從另外一個地方，<br>
再去讀取 information<br>
<br>
0:59:57.300,0:59:59.900<br>
然後它把所有讀到的 information collect 起來<br>
<br>
0:59:59.980,1:00:02.580<br>
它可以給你一個最終的答案<br>
<br>
1:00:03.060,1:00:05.380<br>
以下呢，是 facebook AI research<br>
<br>
1:00:05.380,1:00:11.660<br>
在 baby **** 上面的一個實驗結果<br>
<br>
1:00:13.140,1:00:16.540<br>
baby **** 是一個 QA question answer 的一個 test<br>
<br>
1:00:16.540,1:00:19.160<br>
它其實是一個比較簡單的 test<br>
<br>
1:00:19.160,1:00:22.360<br>
有很多用 template 產生的 document<br>
<br>
1:00:22.360,1:00:24.360<br>
和一些簡單的問題，我們需要回答這些問題<br>
<br>
1:00:25.340,1:00:28.160<br>
我們現在要做的事情就是，讀過這五個句子<br>
<br>
1:00:28.200,1:00:29.780<br>
來問它 what color is Greg<br>
<br>
1:00:29.780,1:00:32.180<br>
它要得到正確的答案，yes<br>
<br>
1:00:32.740,1:00:35.220<br>
你可以從 machine attention 的位置<br>
<br>
1:00:35.220,1:00:37.140<br>
也就是它 reading head 的位置<br>
<br>
1:00:37.360,1:00:38.940<br>
看出 machine 的思路<br>
<br>
1:00:38.940,1:00:43.840<br>
這邊的藍色代表 machine reading head 放置的位置<br>
<br>
1:00:44.080,1:00:49.860<br>
hop1/2/3 代表的是時間<br>
<br>
1:00:49.860,1:00:55.080<br>
也就是他第一個時間點 machine <br>
先把它的 reading head 放在 Greg is a frog<br>
<br>
1:00:55.080,1:00:56.900<br>
所以他把這個 information 把它提取出來<br>
<br>
1:00:56.900,1:00:58.960<br>
它提取 Greg is a frog 這個 information<br>
<br>
1:00:59.680,1:01:03.920<br>
接下來它再提取 Brian is a frog 這個 information<br>
<br>
1:01:04.000,1:01:07.300<br>
接下來它再提取 Brian is a yellow 的 information<br>
<br>
1:01:07.300,1:01:09.140<br>
最後呢，它就得到結論說<br>
<br>
1:01:09.140,1:01:12.520<br>
它按了 Greg 的顏色是 yellow<br>
<br>
1:01:13.180,1:01:16.420<br>
這些事情是 machine 自動 learn 出來的<br>
<br>
1:01:16.420,1:01:24.540<br>
也就是 machine 要 attend 在哪一個位置，是透過 neural network 自己去學到知道怎麼做的<br>
<br>
1:01:24.600,1:01:27.940<br>
也就是說，並不是去寫程式，告訴 machine 說<br>
<br>
1:01:27.940,1:01:30.640<br>
你要先看這個句子，再看這個句子...，不是<br>
<br>
1:01:30.640,1:01:33.880<br>
是 machine 自動去決定，它要看哪一個句子<br>
<br>
1:01:35.060,1:01:37.680<br>
那也可以做 Visual 的 Question Answering<br>
<br>
1:01:37.680,1:01:41.180<br>
Visual Question Answering 就是讓 machine 看一張圖<br>
<br>
1:01:41.280,1:01:42.320<br>
然後問它一個問題<br>
<br>
1:01:42.320,1:01:43.800<br>
比如說問他這是什麼<br>
<br>
1:01:43.800,1:01:48.600<br>
如果它可正確回答是香蕉的話，它就超越部分人類了<br>
<br>
1:01:48.600,1:01:51.500<br>
那這個 Visual Question Answering 怎麼做呢？<br>
<br>
1:01:51.500,1:01:54.440<br>
就讓 machine 看一張圖<br>
<br>
1:01:54.440,1:01:58.360<br>
透過 CNN 呢，你可以把這個圖<br>
<br>
1:01:58.360,1:02:03.320<br>
的每一小塊 region ，<br>
用一個 vector 來表示<br>
<br>
1:02:03.840,1:02:06.780<br>
那接下來呢？輸入一個 Query<br>
<br>
1:02:07.000,1:02:11.220<br>
然後這個 Query 被丟到中央處理器裡面<br>
<br>
1:02:11.280,1:02:16.120<br>
那這個中央處理器，去操控 reading head controller<br>
<br>
1:02:16.320,1:02:23.280<br>
然後這個 reading head controller <br>
決定了它要讀取資訊的位置<br>
<br>
1:02:23.380,1:02:28.740<br>
看看這圖片的什麼位置呢，<br>
是跟現在輸入的問題是有關的<br>
<br>
1:02:28.740,1:02:31.220<br>
那把 information 讀到中央處理器裡面<br>
<br>
1:02:31.380,1:02:34.380<br>
這個讀取的 process 可能有好幾個步驟<br>
<br>
1:02:34.380,1:02:37.400<br>
machine 會分好幾次把 information 讀到中央處理器裡面<br>
<br>
1:02:37.400,1:02:39.380<br>
最後得到答案<br>
<br>
1:02:41.300,1:02:45.380<br>
那也可以做語音的 Question Answering<br>
<br>
1:02:45.480,1:02:46.700<br>
比如說<br>
<br>
1:02:46.700,1:02:51.420<br>
在語音處理實驗室，<br>
我們讓 machine 做 TOFEL 的聽力測驗<br>
<br>
1:02:51.560,1:02:53.280<br>
所謂 TOFEL 的聽力測驗就是<br>
<br>
1:02:53.280,1:02:54.980<br>
讓 machine 聽一段聲音<br>
<br>
1:02:54.980,1:02:58.100<br>
然後問他問題<br>
<br>
1:02:58.100,1:03:03.480<br>
然後從四個正確選項裡面呢，machine 要選出正確選項<br>
<br>
1:03:03.640,1:03:07.480<br>
那 machine 做的事情，跟人類考生做的事情<br>
<br>
1:03:07.480,1:03:08.820<br>
是一模一樣的<br>
<br>
1:03:08.820,1:03:13.540<br>
我們用來訓練測試 machine 的資料<br>
就是 TOFEL 聽力測驗資料<br>
<br>
1:03:15.480,1:03:19.360<br>
用的 model architecture 跟我們剛才看到的<br>
<br>
1:03:19.400,1:03:21.760<br>
其實就是大同小異<br>
<br>
1:03:21.820,1:03:26.540<br>
你讓 machine 先讀一下 question<br>
<br>
1:03:26.840,1:03:29.940<br>
然後把這個 question 做語意的分析<br>
<br>
1:03:29.940,1:03:31.700<br>
得到這個 question 的語意<br>
<br>
1:03:31.720,1:03:35.480<br>
那聲音的部分，先用語音辨識把它轉成文字<br>
<br>
1:03:35.620,1:03:38.960<br>
那再把這些文字做語意的分析<br>
<br>
1:03:39.340,1:03:41.740<br>
得到這段文字的語意<br>
<br>
1:03:41.740,1:03:44.820<br>
那 machine 了解了問題的語意<br>
<br>
1:03:44.820,1:03:47.680<br>
和 audio story 的語意以後<br>
<br>
1:03:47.680,1:03:49.080<br>
它就可以做 attention<br>
<br>
1:03:49.080,1:03:53.780<br>
決定在這個 audio story 裡面，<br>
那些部分是和回答問題有關<br>
<br>
1:03:54.460,1:03:58.160<br>
這就好像是畫重點一樣，<br>
machine 根據它畫的重點呢<br>
<br>
1:03:58.160,1:04:00.040<br>
產生答案<br>
<br>
1:04:00.040,1:04:05.240<br>
那它甚至也可以回頭過去修正它產生出來的答案<br>
<br>
1:04:05.240,1:04:08.440<br>
經過幾個 process 以後呢，最後 machine 得到它的答案<br>
<br>
1:04:08.440,1:04:12.520<br>
那它把它答案呢，跟其他選項，計算相似度<br>
<br>
1:04:12.520,1:04:15.220<br>
然後看哪一個選項的相似度最高<br>
<br>
1:04:15.220,1:04:20.340<br>
它就選哪一個選項<br>
<br>
1:04:20.520,1:04:25.680<br>
那這整個 task 其實就是一個大的 neural network<br>
<br>
1:04:25.680,1:04:27.240<br>
除了語音辨識以外<br>
<br>
1:04:27.520,1:04:33.440<br>
Question semantic 的部分，<br>
還有 audio story semantic 的部分呢<br>
<br>
1:04:33.600,1:04:34.780<br>
都是 neural network<br>
<br>
1:04:34.780,1:04:37.360<br>
所以他們都是 jointly trained<br>
<br>
1:04:37.360,1:04:40.160<br>
你就只要給 machine TOFEL 聽一次考古題<br>
<br>
1:04:40.160,1:04:42.620<br>
machine 就自己會去學了<br>
<br>
1:04:43.020,1:04:48.580<br>
那底下是一些實驗結果啦<br>
<br>
1:04:48.800,1:04:50.500<br>
這個實驗結果是這樣子<br>
<br>
1:04:50.500,1:04:54.160<br>
random 猜阿，正確率是 25%<br>
<br>
1:04:55.020,1:04:57.560<br>
你會發現說有兩個方法<br>
<br>
1:04:57.560,1:04:59.560<br>
是遠比 25 % 強的<br>
<br>
1:04:59.780,1:05:01.580<br>
這是很重要的 information<br>
<br>
1:05:01.580,1:05:04.880<br>
這邊這五個方法，都是 naive 的方法<br>
<br>
1:05:04.940,1:05:09.960<br>
也就是完全不管文章的內容<br>
<br>
1:05:09.960,1:05:12.960<br>
就直接看問題跟選項，就猜答案<br>
<br>
1:05:13.120,1:05:16.560<br>
然後我們發現說，如果你選最短的那個選項<br>
<br>
1:05:16.560,1:05:18.800<br>
可以得到 35 % 的正確率<br>
<br>
1:05:21.440,1:05:23.440<br>
這是計中計，你可能會覺得應該要選最長的<br>
<br>
1:05:23.680,1:05:25.400<br>
其實要選最短的<br>
<br>
1:05:25.480,1:05:30.460<br>
另外一個是這樣，如果你分析四個選項的 semantic<br>
<br>
1:05:30.460,1:05:33.000<br>
你做那個 sequence to sequence auto encoder<br>
<br>
1:05:33.000,1:05:36.200<br>
去把每個選項的 semantic 找出來<br>
<br>
1:05:36.580,1:05:40.440<br>
然後你再去看說，某一個選項，跟另外三個選項<br>
<br>
1:05:40.440,1:05:42.240<br>
的語意上的相似度<br>
<br>
1:05:42.240,1:05:47.400<br>
你會發現說，如果某一個選項，<br>
和另外三個選項的語意相似度<br>
<br>
1:05:47.400,1:05:49.060<br>
比較高的話<br>
<br>
1:05:49.180,1:05:51.940<br>
然後你就把它選出來，那你有 35% 的正確率<br>
<br>
1:05:52.100,1:05:53.940<br>
這跟你的直覺是相反的<br>
<br>
1:05:53.940,1:05:57.040<br>
我們的直覺通常會覺得說，應該選一個選項<br>
<br>
1:05:57.040,1:05:59.920<br>
它的語意，與另外三個選項是不像的<br>
<br>
1:05:59.960,1:06:02.560<br>
但人家早就計算到你會這麼做了<br>
<br>
1:06:02.800,1:06:04.200<br>
所以這是一個計中計<br>
<br>
1:06:04.200,1:06:10.460<br>
如果你要選某一個選項的語意，<br>
與另外三個選項最像的話<br>
<br>
1:06:10.500,1:06:12.980<br>
你反而可以得到超過 random 的答案<br>
<br>
1:06:12.980,1:06:14.980<br>
如果你今天是選<br>
<br>
1:06:15.280,1:06:17.280<br>
最不像的，語意最不像的那個選項<br>
<br>
1:06:17.640,1:06:19.480<br>
你得到的答案就會接近 random<br>
<br>
1:06:19.480,1:06:20.840<br>
它都是設計好的<br>
<br>
1:06:22.320,1:06:25.140<br>
那這些都是一些 trivial 的方法<br>
<br>
1:06:25.200,1:06:26.820<br>
你可以用一些 machine learning 的方法<br>
<br>
1:06:26.880,1:06:30.360<br>
比如說用 memory network<br>
<br>
1:06:30.360,1:06:32.060<br>
可以得到 39% 的正確率<br>
<br>
1:06:32.060,1:06:34.860<br>
是比隨機弄一下的還好一些<br>
<br>
1:06:34.860,1:06:38.800<br>
如果用我們剛才講的那個 model 的話呢<br>
<br>
1:06:38.880,1:06:42.160<br>
我們現在在有語音辨識錯誤的情況之下<br>
<br>
1:06:42.160,1:06:45.680<br>
最好可以做到將近 50% 的正確率啦<br>
<br>
1:06:45.680,1:06:48.020<br>
所以其實 50% 正確率是沒有很高<br>
<br>
1:06:48.020,1:06:52.180<br>
我覺得這樣應該是去不了什麼美國學校的啦<br>
<br>
1:06:52.500,1:06:54.380<br>
但是就是兩題可以答對一題<br>
<br>
1:06:54.520,1:06:56.380<br>
所以如果你沒有辦法兩題答對一題，<br>
<br>
1:06:56.380,1:06:58.020<br>
你其實就是沒有 machine 強<br>
<br>
1:07:00.800,1:07:03.480<br>
以下是一些 reference 給大家參考<br>
<br>
1:07:03.480,1:07:04.520<br>
那最後<br>
<br>
1:07:05.060,1:07:08.640<br>
我這邊其實有一個問題<br>
<br>
1:07:08.980,1:07:13.620<br>
我們講了 Deep learning<br>
也講了 structured learning<br>
<br>
1:07:13.860,1:07:17.920<br>
它們中間有什麼樣的關係呢？你想想看<br>
<br>
1:07:20.760,1:07:26.220<br>
我們上周講了 HMM，<br>
講了 CRF/Structured Perceptron/SVM<br>
<br>
1:07:26.540,1:07:29.840<br>
它們可以做的事情，比如說做 pos taking<br>
<br>
1:07:29.840,1:07:32.440<br>
input 一個 sequence，output 一個 sequence<br>
<br>
1:07:32.980,1:07:36.900<br>
RNN/LSTM，也可以做到一樣的事情<br>
<br>
1:07:37.860,1:07:43.960<br>
當我們使用 deep learning 跟 structured learning 的技術<br>
<br>
1:07:44.080,1:07:46.080<br>
有什麼不同呢？<br>
<br>
1:08:06.040,1:08:08.040<br>
首先<br>
<br>
1:08:09.100,1:08:14.920<br>
假如我們現在用的是 uni-directional 的 RNN 或 LSTM<br>
<br>
1:08:15.080,1:08:20.180<br>
當你在 make decision 的時候，<br>
你只看了 sentence 的一半<br>
<br>
1:08:20.300,1:08:24.600<br>
而如果你是用 structured learning 的話<br>
<br>
1:08:24.600,1:08:26.520<br>
透過 Viterbi 的 algorithm<br>
<br>
1:08:26.520,1:08:29.060<br>
你考慮的是整個句子<br>
<br>
1:08:29.520,1:08:32.420<br>
如果你是用 Viterbi 的 algorithm 的話，<br>
machine 會讀過整個句子以後<br>
<br>
1:08:33.040,1:08:34.420<br>
才下決定<br>
<br>
1:08:34.740,1:08:38.320<br>
所以從這個角度來看，也許<br>
<br>
1:08:38.480,1:08:43.120<br>
HMM, CRF... 等等，還是有佔到一些優勢<br>
<br>
1:08:43.680,1:08:46.740<br>
但這個優勢並沒有很明顯，因為<br>
<br>
1:08:47.420,1:08:51.020<br>
RNN/LSTM 等等，它們可以做 Bi-directional<br>
<br>
1:08:51.020,1:08:55.860<br>
所以他們也有辦法考慮，一整個句子的 information<br>
<br>
1:09:03.100,1:09:06.680<br>
在 HMM/CRM 裡面啊<br>
<br>
1:09:06.680,1:09:14.400<br>
你可以很 explicitly 去考慮 label 和 label 之間的關係<br>
<br>
1:09:14.980,1:09:16.100<br>
什麼意思呢？<br>
<br>
1:09:16.480,1:09:17.420<br>
舉例來說<br>
<br>
1:09:17.440,1:09:20.080<br>
你今天在做 inference 的時候<br>
<br>
1:09:20.460,1:09:23.460<br>
你在用 Viterbi algorithm 求解的時候<br>
<br>
1:09:23.700,1:09:27.220<br>
假設你可以直接把你要的 constrain 下到<br>
<br>
1:09:27.220,1:09:30.160<br>
那個 Viterbi algorithm 裡面去<br>
<br>
1:09:30.160,1:09:31.320<br>
你了解我意思嗎？<br>
<br>
1:09:31.320,1:09:35.980<br>
你可以直接說，我希望每一個 label 出現的時候，<br>
都要連續出現五次<br>
<br>
1:09:36.080,1:09:39.120<br>
這件事情你可以輕易地用 Viterbi algorithm 做到<br>
<br>
1:09:39.120,1:09:42.660<br>
因為你可以修改 Viterbi algorithm，讓 machine 在選擇<br>
<br>
1:09:42.700,1:09:44.700<br>
分數最高的句子的時候<br>
<br>
1:09:44.700,1:09:48.940<br>
排除掉不符合你要的 constrain 的那些結果<br>
<br>
1:09:48.940,1:09:52.500<br>
但如果是 RNN 或 LSTM 的話<br>
<br>
1:09:52.660,1:09:56.760<br>
你要直接下一個 constrain 進去，是比較難的<br>
<br>
1:09:56.760,1:10:03.300<br>
你沒有辦法要求 RNN 一定要連續吐出<br>
某一個 level 5 次才是正確的<br>
<br>
1:10:04.420,1:10:06.880<br>
你可以在 training data 裡面，<br>
給他看這種 training data<br>
<br>
1:10:07.240,1:10:08.480<br>
但是<br>
<br>
1:10:08.480,1:10:11.440<br>
但是你叫他去學，然後再這樣，是比較麻煩的<br>
<br>
1:10:11.440,1:10:15.640<br>
Viterbi 可以直接告訴你的 machine 要它做什麼事<br>
<br>
1:10:15.740,1:10:21.820<br>
所以在這點上，<br>
structured learning 似乎是有一些優勢的<br>
<br>
1:10:22.780,1:10:26.360<br>
如果是 RNN 和 LSTM 你的 cost function<br>
<br>
1:10:26.360,1:10:31.600<br>
跟你實際上最後要考慮的 error 往往是沒有關係的<br>
<br>
1:10:31.640,1:10:34.760<br>
你想想看，當你在做 RNN/LSTM 的時候<br>
<br>
1:10:34.860,1:10:38.020<br>
你在考慮的 cost 是，比如說<br>
<br>
1:10:38.020,1:10:41.040<br>
每一個時間點的 cross entropy<br>
<br>
1:10:41.040,1:10:45.300<br>
每一個時間點，你的 RNN output <br>
跟 reference 的 cross entropy<br>
<br>
1:10:45.340,1:10:49.320<br>
它跟你的 error 往往不見得是直接相關的<br>
<br>
1:10:49.320,1:10:56.280<br>
因為你的 error 可能是比如說，<br>
兩個 sequence 之間的 ***<br>
<br>
1:10:56.800,1:11:00.720<br>
但如果你是用 structured learning 的話，它的 cost<br>
<br>
1:11:00.720,1:11:02.980<br>
會是你 error 的 upper bound<br>
<br>
1:11:02.980,1:11:08.260<br>
所以從這個角度來看，<br>
structured learning 也是有一些優勢的<br>
<br>
1:11:08.560,1:11:11.620<br>
但是最後最困難最重要的<br>
<br>
1:11:11.620,1:11:14.720<br>
RNN/LSTM 可以是 deep<br>
<br>
1:11:15.040,1:11:19.520<br>
而 HMM, CRF, ... 他們其實也可以是 deep<br>
<br>
1:11:19.520,1:11:23.220<br>
但是他們拿來做 deep 的 learning 其實是比較困難的<br>
<br>
1:11:23.400,1:11:26.900<br>
在我們下一堂課講的內容裡面<br>
<br>
1:11:26.900,1:11:30.440<br>
他們都是 linear<br>
<br>
1:11:30.480,1:11:34.520<br>
為什麼他們是 linear，<br>
因為我們定的 evaluation function 是 linear<br>
<br>
1:11:35.440,1:11:39.360<br>
如果它不是 linear，你在 training 的時候會有很多麻煩<br>
<br>
1:11:39.440,1:11:43.240<br>
所以他們是 linear，我們才能套用上一堂課教的那些方法<br>
<br>
1:11:43.260,1:11:46.560<br>
來做 inference 跟 training<br>
<br>
1:11:46.940,1:11:52.200<br>
那在這個比較上，deep learning 會佔到很大的優勢<br>
<br>
1:11:52.740,1:11:57.380<br>
最後整體說起來呢，<br>
其實如果你要得到一些 state of the art 的結果<br>
<br>
1:11:57.620,1:12:00.720<br>
在這種 sequence labeling task 上，<br>
你要得到 state of the art 的結果<br>
<br>
1:12:00.860,1:12:03.620<br>
RNN/LSTM 是不可或缺的<br>
<br>
1:12:03.620,1:12:09.240<br>
所以整體說起來 RNN/LSTM <br>
在這種 sequence labeling task 上面表現<br>
<br>
1:12:09.240,1:12:11.020<br>
其實會是比較好的<br>
<br>
1:12:11.180,1:12:13.880<br>
deep 這件事是比較強的<br>
<br>
1:12:15.160,1:12:17.160<br>
它非常的重要<br>
<br>
1:12:17.260,1:12:20.980<br>
如果你今天用的只是 linear model<br>
<br>
1:12:20.980,1:12:24.080<br>
如果你的 model 是 linear，<br>
你的 function space 就這麼大<br>
<br>
1:12:24.100,1:12:28.540<br>
就算你可以直接 minimize 一個 error 的 upper bound<br>
<br>
1:12:28.540,1:12:29.560<br>
那又怎麼樣？<br>
<br>
1:12:29.560,1:12:32.480<br>
因為你所有的 function 都是壞的啊<br>
<br>
1:12:32.700,1:12:37.460<br>
所以相比之下 deep learning 可以佔到很大的優勢<br>
<br>
1:12:38.240,1:12:43.820<br>
但是其實 deep learning 和 structured learning，<br>
它們是可以被結合起來的<br>
<br>
1:12:43.960,1:12:50.860<br>
而且有非常多成功結合的先例<br>
<br>
1:12:51.140,1:12:58.780<br>
你可以說我底部呢，就是我 input 的 feature<br>
<br>
1:12:58.940,1:13:02.780<br>
先通過 RNN 跟 LSTM<br>
<br>
1:13:03.420,1:13:04.820<br>
然後先通過 RNN 跟 LSTM<br>
<br>
1:13:04.820,1:13:11.800<br>
RNN/LSTM 的 output 再做為 HMM, CRF... 的 input<br>
<br>
1:13:11.800,1:13:20.280<br>
你用 RNN/LSTM 的 output 來定義 HMM, CRF... 的 evaluation function<br>
<br>
1:13:20.560,1:13:28.240<br>
如此，你就可以同時又享有 deep 的好處，<br>
同時又享有 structured learning 的好處<br>
<br>
1:13:29.360,1:13:32.840<br>
那這個再過去已經有很多先例，比如說呢<br>
<br>
1:13:33.780,1:13:37.260<br>
到最後你現在這邊有 deep，這邊有 structured<br>
<br>
1:13:37.260,1:13:39.820<br>
這兩個是可以 jointly 一起 learned<br>
<br>
1:13:40.060,1:13:43.760<br>
你可以想想看，HMM/CRF 可以用 Gradient decent train<br>
<br>
1:13:43.760,1:13:46.100<br>
其實 structured/SVM，我們好像沒有講<br>
<br>
1:13:46.100,1:13:48.140<br>
但它也可以用 Gradient decent train<br>
<br>
1:13:48.700,1:13:53.560<br>
所以你可以把 deep learning 部分跟 structured learning 部分 jointly 合起來<br>
<br>
1:13:53.820,1:13:57.280<br>
一起用 Gradient decent 來做 training<br>
<br>
1:14:01.200,1:14:02.480<br>
那在語音上呢<br>
<br>
1:14:02.480,1:14:05.800<br>
我們常常會把 <br>
deep learning 跟 structured learning 合起來<br>
<br>
1:14:05.840,1:14:07.840<br>
你可以常常見到的組合是<br>
<br>
1:14:08.080,1:14:13.520<br>
deep learning 的 model: CNN/LSTM/DNN <br>
加上 HMM 的組合<br>
<br>
1:14:13.620,1:14:19.640<br>
所以做語音的人常常說，<br>
我們把過去所做的東西丟掉了，其實不是<br>
<br>
1:14:19.660,1:14:23.340<br>
HMM 往往都還在<br>
<br>
1:14:23.440,1:14:26.820<br>
如果你要得到最 state of the art 的結果<br>
<br>
1:14:26.820,1:14:31.920<br>
現在還是用這樣 hybrid 的 system 得到的結果往往是最好<br>
<br>
1:14:33.020,1:14:35.160<br>
那這 hybrid system 怎麼 work 呢？<br>
<br>
1:14:35.280,1:14:37.280<br>
我們說在 HMM 裡面<br>
<br>
1:14:37.320,1:14:42.420<br>
我們必須要去計算 x 跟 y 的 joint probability<br>
<br>
1:14:42.420,1:14:47.400<br>
或是在 structured learning 裡面，我們要計算 x 跟 y 的 evaluation function<br>
<br>
1:14:47.580,1:14:52.700<br>
在語音辨識裡面，<br>
x 是聲音訊號，y 是語音辨識的結果<br>
<br>
1:14:53.080,1:14:57.720<br>
在 HMM 裡面，我們有 transition 的部分<br>
<br>
1:14:57.880,1:15:00.620<br>
我們有 emission 的部分<br>
<br>
1:15:02.480,1:15:07.900<br>
DNN 做的事情，其實就是去取代 Emission 的部分<br>
<br>
1:15:07.980,1:15:09.380<br>
原來在 HMM 裡面<br>
<br>
1:15:09.380,1:15:12.360<br>
這個 emission 就是簡單的統計<br>
<br>
1:15:12.360,1:15:14.440<br>
你就是統計一個 Gaussian mixture model<br>
<br>
1:15:14.440,1:15:19.140<br>
但是把它換成 DNN 以後，<br>
你會得到很好的 performance<br>
<br>
1:15:19.240,1:15:21.160<br>
怎麼換呢？<br>
<br>
1:15:21.160,1:15:24.720<br>
一般 RNN 它可以給我們的 output 是<br>
<br>
1:15:24.720,1:15:26.160<br>
input 一個 acoustic feature<br>
<br>
1:15:26.160,1:15:28.120<br>
它告訴你說這個 acoustic feature<br>
<br>
1:15:28.120,1:15:30.740<br>
屬於每一個 state 的機率<br>
<br>
1:15:30.740,1:15:32.880<br>
但你可能想說這跟我們要的東西不一樣啊<br>
<br>
1:15:32.880,1:15:36.020<br>
我們要的是 p of x given y<br>
<br>
1:15:36.020,1:15:39.000<br>
這邊給我們的是 p of y given x<br>
<br>
1:15:39.000,1:15:41.500<br>
怎麼辦呢？做一下轉換<br>
<br>
1:15:41.800,1:15:44.160<br>
RNN 可以給我們 p of x given y<br>
<br>
1:15:44.160,1:15:47.920<br>
然後你可以把它分解成 p of x, y 除以 p of y<br>
<br>
1:15:47.940,1:15:54.120<br>
再把它分解成 p of y given x 乘以 p of x 除以 p of y<br>
<br>
1:15:54.220,1:15:57.660<br>
那前面這個 p of y given x，它可以從 RNN 來<br>
<br>
1:15:57.660,1:15:59.420<br>
那 p of y 呢？<br>
<br>
1:16:00.180,1:16:02.180<br>
可以從，你就直接 count<br>
<br>
1:16:02.220,1:16:04.700<br>
你就可以直接從你的 **** 統計<br>
<br>
1:16:04.700,1:16:06.480<br>
p of y 出現的機率<br>
<br>
1:16:06.480,1:16:09.260<br>
這個 p of x 呢，你可以直接無視它<br>
<br>
1:16:09.380,1:16:11.340<br>
為什麼 p of x 可以直接無視它呢？<br>
<br>
1:16:11.340,1:16:14.420<br>
你想想看，最後你得到這個機率的時候<br>
<br>
1:16:14.420,1:16:18.840<br>
在 inference 的時候，x 是 input 是聲音訊號，是已知<br>
<br>
1:16:19.000,1:16:20.100<br>
你是窮舉所有的 y<br>
<br>
1:16:20.100,1:16:23.040<br>
看哪一個 y 可以讓 p of x,y 最大<br>
<br>
1:16:23.040,1:16:26.180<br>
所以跟 x 有關的項，最後不會影響<br>
<br>
1:16:26.980,1:16:28.840<br>
第一個 inference 的結果<br>
<br>
1:16:28.840,1:16:32.080<br>
所以我們不需要把 x 考慮進來<br>
<br>
1:16:33.160,1:16:36.540<br>
那其實加上 HMM，在語音辨識裡面<br>
<br>
1:16:36.540,1:16:38.540<br>
是蠻有幫助的<br>
<br>
1:16:38.820,1:16:42.500<br>
就算是你用 RNN，你在做辨識的時候啊<br>
<br>
1:16:42.500,1:16:44.620<br>
常常會遇到一個問題<br>
<br>
1:16:44.620,1:16:46.620<br>
假設我們是一個 frame<br>
<br>
1:16:46.940,1:16:48.100<br>
每一個一個 frame 丟到 RNN<br>
<br>
1:16:48.100,1:16:49.920<br>
然後問他說這一個 frame，這一個一個 frame<br>
<br>
1:16:49.920,1:16:51.580<br>
屬於哪一個 form<br>
<br>
1:16:51.660,1:16:54.440<br>
它往往會產生一些怪怪的結果<br>
<br>
1:16:54.520,1:16:58.460<br>
比如說因為一個 form 往往是 ****<br>
<br>
1:16:59.020,1:17:01.400<br>
所以本來理論上你應該會看到說<br>
<br>
1:17:01.600,1:17:06.120<br>
比如說第一個 frame 是 a，第二，第三，第四，第五個 frame 也是 a<br>
<br>
1:17:06.120,1:17:08.300<br>
然後接下來換成 b, b, b<br>
<br>
1:17:08.300,1:17:11.560<br>
但是如果你用 RNN 在做的時候<br>
<br>
1:17:11.860,1:17:14.520<br>
你知道 RNN 它每一個產生的<br>
<br>
1:17:14.940,1:17:17.780<br>
label 都是 independent 的<br>
<br>
1:17:17.980,1:17:20.760<br>
所以他可能會突然發狂<br>
<br>
1:17:20.840,1:17:24.000<br>
在這個地方突然若無其事地改成 b<br>
<br>
1:17:24.000,1:17:26.180<br>
然後又改回來這樣子<br>
<br>
1:17:27.260,1:17:30.320<br>
你會發現它很容易出現這個現象<br>
<br>
1:17:30.360,1:17:32.900<br>
然後如果今天這是一個比賽的話<br>
<br>
1:17:32.900,1:17:35.780<br>
你就會有人發現，嗯，RNN 有點弱<br>
<br>
1:17:35.780,1:17:36.800<br>
它就會發生這種現象<br>
<br>
1:17:36.800,1:17:41.380<br>
如果手動，只要比如說，某一個 output 跟前後不一樣<br>
<br>
1:17:41.380,1:17:46.600<br>
我就手動把它改掉，然後你就可以得到大概 2% 的進步<br>
<br>
1:17:46.600,1:17:48.640<br>
你就可以屌打其他同學<br>
<br>
1:17:48.640,1:17:51.680<br>
那如果你加上 HMM 的話<br>
<br>
1:17:51.680,1:17:53.120<br>
就不會有這種情形<br>
<br>
1:17:53.120,1:17:59.600<br>
HMM 會幫你把這種狀況自動修掉<br>
<br>
1:17:59.600,1:18:04.560<br>
所以加上 HMM 其實是還蠻有幫助的<br>
<br>
1:18:04.780,1:18:07.620<br>
對 RNN 來說，因為它在 training 的時候<br>
<br>
1:18:07.620,1:18:10.340<br>
它是一個一個 frame，分開考慮的<br>
<br>
1:18:10.620,1:18:14.780<br>
所以其實今天假如不同的錯誤<br>
<br>
1:18:14.900,1:18:18.760<br>
對語音辨識結果影響很大，但 RNN 不知道<br>
<br>
1:18:18.820,1:18:24.020<br>
如果我們今天把 b 改成錯在這個地方<br>
<br>
1:18:24.540,1:18:27.400<br>
對最後語音辨識的錯的影響就很小<br>
<br>
1:18:27.480,1:18:29.480<br>
但是 RNN 不知道這件事情<br>
<br>
1:18:29.760,1:18:33.760<br>
所以對它來說，<br>
在這邊放一個錯誤跟這邊放一個錯誤是一樣的<br>
<br>
1:18:33.760,1:18:36.300<br>
但是 RNN 認不出這一件事情來<br>
<br>
1:18:36.300,1:18:39.620<br>
你要讓 RNN 可以認出這件事情來<br>
<br>
1:18:39.620,1:18:42.660<br>
你需要加上一些 structural learning 的概念<br>
<br>
1:18:42.660,1:18:44.960<br>
才能夠做到<br>
<br>
1:18:45.840,1:18:48.780<br>
那在做 slot filling 的時候呢？<br>
<br>
1:18:48.780,1:18:53.280<br>
現在也很流行用 Bi-directional LSTM<br>
<br>
1:18:53.280,1:18:58.100<br>
再加上 CRF 或是 structured SVM<br>
<br>
1:18:58.140,1:19:01.980<br>
也就是說先用 Bi-directional LSTM 抽出 feature<br>
<br>
1:19:01.980,1:19:05.680<br>
再拿這些 feature 來定義<br>
<br>
1:19:05.680,1:19:09.880<br>
CRF 或者是 structured SVM 裡面，<br>
我們需要用到的 feature<br>
<br>
1:19:10.020,1:19:12.340<br>
CRF 跟 structured SVM 都是 linear 的 model<br>
<br>
1:19:12.340,1:19:14.900<br>
你都要先抽 feature phi of x,y<br>
<br>
1:19:14.900,1:19:17.380<br>
然後 learn 一個 weight w<br>
<br>
1:19:17.380,1:19:20.920<br>
這個 phi of x,y 的 feature，<br>
你不要直接從 raw 的 feature 來<br>
<br>
1:19:21.060,1:19:23.920<br>
你直接從 bi-directional RNN 的 output<br>
<br>
1:19:23.920,1:19:25.800<br>
可以得到比較好的結果<br>
<br>
1:19:29.900,1:19:36.220<br>
有人問說 structural learning 到底是否 practical<br>
<br>
1:19:36.220,1:19:37.420<br>
我們知道 structural learning<br>
<br>
1:19:37.420,1:19:39.500<br>
你需要解三個問題<br>
<br>
1:19:39.560,1:19:43.660<br>
那其中 inference 那個問題，往往是很困難的<br>
<br>
1:19:43.960,1:19:46.820<br>
你想想看 inference 那個問題，你需要 arg<br>
<br>
1:19:46.920,1:19:50.360<br>
你要窮舉所有的 y 看哪一個 y 可以讓你的值最大<br>
<br>
1:19:50.360,1:19:53.040<br>
你要解一個 optimization 的 problem<br>
<br>
1:19:53.040,1:19:54.400<br>
那這個 optimization 的 problem<br>
<br>
1:19:54.400,1:19:55.260<br>
很多時候<br>
<br>
1:19:55.260,1:19:58.000<br>
並不是所有的狀況都有好的解<br>
<br>
1:19:58.000,1:20:00.560<br>
應該說大部分的狀況都沒有好的 solution<br>
<br>
1:20:00.560,1:20:03.820<br>
sequence labeling 是少數有好的 solution 的狀況<br>
<br>
1:20:04.100,1:20:07.160<br>
但其他狀況，都沒有什麼好的 solution<br>
<br>
1:20:07.180,1:20:13.480<br>
所以好像會讓人覺得 structural learning，<br>
它的用途沒那麼廣泛<br>
<br>
1:20:13.480,1:20:17.020<br>
但未來還未必是這樣子<br>
<br>
1:20:23.840,1:20:27.200<br>
事實上你想想看，我們之前講過的 GAN<br>
<br>
1:20:27.200,1:20:30.680<br>
我認為 GAN 就是一種 structural learning<br>
<br>
1:20:31.060,1:20:37.420<br>
如果你把 Discriminator 看作是 evaluation function<br>
<br>
1:20:37.420,1:20:41.200<br>
就是我們之前講的，在 structural learning 裡面<br>
<br>
1:20:41.200,1:20:44.540<br>
你有一個 problem 1，你要找出一個 evaluation function<br>
<br>
1:20:44.640,1:20:50.000<br>
這個 Discriminator，<br>
我們就可以把它看作是 evaluation function<br>
<br>
1:20:51.240,1:20:54.460<br>
所以我們就知道 problem 1 要怎麼做<br>
<br>
1:20:54.460,1:20:57.960<br>
那最困難的 problem 2，<br>
要解一個 inference 的問題<br>
<br>
1:20:57.960,1:21:00.460<br>
我們要窮舉所有我們未知的東西<br>
<br>
1:21:00.460,1:21:04.600<br>
看看誰可以讓我們的 evaluation function 最大<br>
<br>
1:21:05.200,1:21:08.640<br>
這一步往往很困難，因為 x 的可能性太多了<br>
<br>
1:21:08.640,1:21:10.920<br>
未知的東西可能性太多<br>
<br>
1:21:11.400,1:21:15.960<br>
但事實上這個東西它可以就是 Generator<br>
<br>
1:21:16.220,1:21:20.200<br>
我們可以想成 generator 它不是就是<br>
<br>
1:21:20.200,1:21:23.900<br>
給一個 noise，給一個從 Gaussian sample 出來的 noise<br>
<br>
1:21:23.900,1:21:25.680<br>
它就 output 一個 x 嗎？<br>
<br>
1:21:25.680,1:21:28.240<br>
output 一個 object 出來嗎？<br>
<br>
1:21:28.240,1:21:30.120<br>
它 output 的這個 object<br>
<br>
1:21:30.120,1:21:33.980<br>
不是就是可以讓 Discriminator <br>
分辨不出來那個 object 嗎？<br>
<br>
1:21:34.480,1:21:38.060<br>
如果 Discriminator 就是 evaluation function 的話<br>
<br>
1:21:38.120,1:21:39.420<br>
它 output 的那個 object<br>
<br>
1:21:39.420,1:21:43.240<br>
就是可以讓 evaluation function 的值很大的那個 object<br>
<br>
1:21:43.240,1:21:47.460<br>
所以這個 Generator 它其實就是在解這個問題<br>
<br>
1:21:47.920,1:21:53.700<br>
這個 generator 的 output，<br>
其實就是這個 arg max 的 output<br>
<br>
1:21:53.980,1:21:57.860<br>
所以你可以把 Generator <br>
當作是在解 inference 的這個問題<br>
<br>
1:21:58.000,1:21:59.720<br>
那 problem 3 你已經知道了<br>
<br>
1:21:59.760,1:22:02.800<br>
我們怎麼 train GAN 就是 problem 3 的 solution<br>
<br>
1:22:03.420,1:22:04.880<br>
事實上 GAN 的 training<br>
<br>
1:22:04.880,1:22:10.420<br>
它跟 structured SVM 那些方法<br>
的 training 你不覺得其實也有異曲同工之妙嗎？<br>
<br>
1:22:10.640,1:22:13.840<br>
大家還記得 structured SVM 是怎麼 train 的嗎？<br>
<br>
1:22:13.840,1:22:20.820<br>
在 structured SVM 的 training 裡面，<br>
我們每次找出最 competitive 的那些 example<br>
<br>
1:22:21.320,1:22:27.960<br>
然後我們希望正確的 example，<br>
它的 evaluation function 的分數<br>
<br>
1:22:27.960,1:22:30.260<br>
大過 competitive 的 example<br>
<br>
1:22:30.260,1:22:34.480<br>
然後 update 我們的 model，<br>
然後再重新選 competitive 的 example<br>
<br>
1:22:34.480,1:22:36.480<br>
然後在讓正確的，大過 competitive<br>
<br>
1:22:36.540,1:22:38.760<br>
就這樣 iterative 去做<br>
<br>
1:22:39.080,1:22:42.060<br>
你不覺得 GAN 也是在做一樣的事情嗎？<br>
<br>
1:22:42.440,1:22:48.260<br>
GAN 的 training 是我們有正確的 example<br>
<br>
1:22:48.260,1:22:49.320<br>
就是這邊的 x<br>
<br>
1:22:49.380,1:22:53.040<br>
它應該要讓 evaluation function 的值，<br>
比 Discriminator 的值大<br>
<br>
1:22:53.400,1:22:58.960<br>
然後我們每次用這個 Generator，Generate 出，<br>
最competitive 的那個 x<br>
<br>
1:22:58.960,1:23:02.260<br>
也就是可以讓 Discriminator 的值最大的那個 x<br>
<br>
1:23:02.260,1:23:04.040<br>
然後再去 train Discriminator<br>
<br>
1:23:04.040,1:23:08.280<br>
Discriminator 要分辨正確的，real 的跟 Generated 的<br>
<br>
1:23:08.280,1:23:11.580<br>
也就是 Discriminator 要給 real 的 example 比較大的值<br>
<br>
1:23:11.580,1:23:15.240<br>
給那些 most competitive 的 x 比較小的值<br>
<br>
1:23:15.380,1:23:19.800<br>
然後這個 process 就不斷的 iterative 的進行下去<br>
<br>
1:23:19.900,1:23:24.140<br>
你會 update 你的 Discriminator <br>
然後 update 你的 Generator<br>
<br>
1:23:24.140,1:23:25.760<br>
然後再 update 你的 Discriminator<br>
<br>
1:23:25.800,1:23:34.660<br>
其實這個跟 Structured SVM 的<br>
 training 是有異曲同工之妙的<br>
<br>
1:23:35.160,1:23:37.380<br>
那你可能會想說在 GAN 裡面<br>
<br>
1:23:37.380,1:23:39.300<br>
我們之前在講 structured SVM 的時候<br>
<br>
1:23:39.300,1:23:42.960<br>
都是有一個 input/output，有一個 x 有一個 y<br>
<br>
1:23:42.960,1:23:46.380<br>
那我們之前講的 GAN 只有 x<br>
<br>
1:23:46.380,1:23:49.280<br>
聽起來好像不太像<br>
<br>
1:23:49.280,1:23:52.420<br>
那我們就另外講一個像的，給你聽看看<br>
<br>
1:23:52.700,1:23:57.100<br>
其實 GAN 也可以是 conditional 的 GAN<br>
<br>
1:23:57.200,1:23:59.100<br>
什麼是 conditional 的 GAN 呢？<br>
<br>
1:23:59.100,1:24:03.000<br>
我今天的 example 都是 x,y 的 pair<br>
<br>
1:24:03.200,1:24:07.980<br>
我要解的任務是，是 given x 找出最有可能的 y<br>
<br>
1:24:08.760,1:24:10.420<br>
你就想成是做語音辨識<br>
<br>
1:24:10.420,1:24:14.800<br>
x 是聲音訊號，y 是辨識出來的文字<br>
<br>
1:24:15.560,1:24:19.460<br>
如果是用 conditional GAN 的概念，怎麼做呢？<br>
<br>
1:24:19.620,1:24:25.280<br>
你的 Generator input 一個 x，它就會 output 一個 y<br>
<br>
1:24:25.700,1:24:30.020<br>
Discriminator 它是去 check 一個 x,y 的 pair，<br>
是不是對的<br>
<br>
1:24:30.040,1:24:34.780<br>
如果我們給它一個真正的 x,y pair，<br>
它會給它一個比較高的分數<br>
<br>
1:24:34.900,1:24:41.680<br>
你給它一個 Generator output 出來的 y，<br>
配上它的 input x，所產生一個假的 x,y pair<br>
<br>
1:24:41.680,1:24:43.440<br>
它會給它比較低的分數<br>
<br>
1:24:43.520,1:24:47.440<br>
training 的 process 就跟原來的 GAN 是一樣的<br>
<br>
1:24:47.600,1:24:52.680<br>
這個東西已經被成功應用在，<br>
用文字產生 image 的 task 上<br>
<br>
1:24:53.040,1:24:55.040<br>
在用文字產生 image 的 task<br>
<br>
1:24:55.600,1:25:01.580<br>
比如說你跟 machine 說一句話說，<br>
有一隻藍色的鳥，它就畫一張藍色的鳥的圖<br>
<br>
1:25:02.180,1:25:10.560<br>
這個 task 你的 input x 就是一句話，<br>
output y 就是一張 image<br>
<br>
1:25:10.560,1:25:14.260<br>
那 Generator 做的事情，就是給它一句話，在圖上<br>
<br>
1:25:22.260,1:25:26.120<br>
給它一句話，它就產生一張 image<br>
<br>
1:25:26.880,1:25:31.040<br>
Discriminator 做的事情就是，<br>
Discriminator 給它看一張 image<br>
<br>
1:25:31.140,1:25:36.320<br>
扔一句話，那它判斷說這個 x,y 的 pair<br>
這個 image/sentence pair<br>
<br>
1:25:36.320,1:25:39.640<br>
他們是真的，還是不是真的<br>
<br>
1:25:40.280,1:25:44.900<br>
那如果你把 Discriminator 換成就是 evaluation function<br>
<br>
1:25:45.020,1:25:48.920<br>
把 Generator 換成就是解 inference 的那些 problems<br>
<br>
1:25:48.980,1:25:53.120<br>
其實 conditional GAN 跟 structured learning，<br>
它們是可以類比的<br>
<br>
1:25:53.280,1:25:59.780<br>
或者你可以說 GAN 就是<br>
 train structured learning 的 model 的一種方法<br>
<br>
1:26:00.880,1:26:01.920<br>
你可能覺得<br>
<br>
1:26:02.020,1:26:05.260<br>
這聽起來，或許你沒有聽得很懂，就算了<br>
<br>
1:26:05.260,1:26:07.880<br>
你可能覺得這只是我隨便講講的<br>
<br>
1:26:08.020,1:26:11.540<br>
但是我就想說，其他人也一定就想到了<br>
<br>
1:26:11.540,1:26:15.160<br>
所以，我就 google 一下其他人的 publication<br>
<br>
1:26:15.160,1:26:19.720<br>
果然，很多人都有類似的想法<br>
<br>
1:26:19.720,1:26:23.440<br>
GAN 可以跟 energy based model 做 connection<br>
<br>
1:26:23.440,1:26:26.580<br>
GAN 可以視為 train energy based model 的一種方法<br>
<br>
1:26:26.700,1:26:29.220<br>
所謂 energy based model，其實我們之前有講<br>
<br>
1:26:29.220,1:26:33.540<br>
它就是 structured learning 的另外一種稱呼<br>
<br>
1:26:34.820,1:26:38.040<br>
這邊有一系列的 paper 在講這件事<br>
<br>
1:26:38.680,1:26:43.260<br>
那你可能覺得說把 Generator <br>
視做是在做 inference 這件事情<br>
<br>
1:26:43.260,1:26:47.120<br>
是在解 arg max 這個問題，聽起來感覺很荒謬<br>
<br>
1:26:47.120,1:26:50.100<br>
其實也有人就是這麼想的<br>
<br>
1:26:50.100,1:26:53.600<br>
也有人想說，這邊也列一些 reference 給大家參考<br>
<br>
1:26:53.640,1:27:01.400<br>
也有人覺得說，一個 neural network ，<br>
它有可能就是在解 arg max 這個 problem<br>
<br>
1:27:01.700,1:27:09.820<br>
所以也許 deep and structured <br>
就是未來一個研究的重點的方向<br>
<br>
1:27:11.920,1:27:13.900<br>
以下為其它課程資訊<br>
<br>
1:27:14.220,1:30:05.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
