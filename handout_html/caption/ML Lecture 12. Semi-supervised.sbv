0:00:00.040,0:00:01.960
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:01.960,0:00:02.980
我們來講 Semi-supervised learning

0:00:02.980,0:00:04.000
有一件事情是這樣子的

0:00:04.000,0:00:06.380
本來預計今天要公告作業四

0:00:06.380,0:00:08.980
但我想說，我們改到下周再公告好了

0:00:09.980,0:00:12.540
那你就先專心把作業三

0:00:12.540,0:00:15.660
做完，然後下周再開始做作業四

0:00:15.660,0:00:18.980
如果沒有意外的話，下周也會同時公告

0:00:18.980,0:00:20.620
final project

0:00:23.120,0:00:26.480
那我們先來講一下 Semi-supervised learning

0:00:26.480,0:00:30.300
就是我們作業三，要請大家稍微做一下的東西

0:00:30.300,0:00:33.580
因為如果只做 CIFAR-10 的辨識太簡單

0:00:33.580,0:00:38.560
去網路上 call 個 script，按個 enter，應該就可以得到結果

0:00:38.560,0:00:41.400
所以，我們增加一些挑戰性

0:00:41.400,0:00:43.740
那甚麼是 Semi-supervised learning

0:00:43.740,0:00:45.800
Supervised learning 大家都知道

0:00:45.800,0:00:47.240
在 Supervised learning 裡面

0:00:47.240,0:00:49.620
你就是有一大堆的 training data

0:00:49.620,0:00:51.900
這些 training data 的組成

0:00:51.900,0:00:57.160
是一個 function 的 input 跟 function 的 output 的 pair

0:00:57.160,0:00:58.880
假設你有 R 筆 training data

0:00:58.880,0:01:00.240
每一筆 training data 裡面

0:01:00.240,0:01:02.660
都有一個 x^r 代表 function 的 input

0:01:02.660,0:01:05.540
都有一個 y^r\head 代表 function 的 output

0:01:05.540,0:01:08.300
這個 x^r 代表是一張

0:01:08.300,0:01:10.460
舉例來說，在 homework 3 裡面

0:01:10.460,0:01:12.240
x^r 是一張 image

0:01:12.240,0:01:15.820
y^r\head 是 class 的 label

0:01:15.820,0:01:18.620
那所謂的 Semi-supervised learning 是甚麼呢

0:01:18.620,0:01:20.380
Semi-supervised learning 是說

0:01:20.380,0:01:23.820
在 labeled data 上面，我們有另外一組

0:01:23.820,0:01:25.720
unlabeled 的 data

0:01:26.060,0:01:28.360
那這一組 unlabeled 的 data，我們這邊

0:01:28.360,0:01:30.580
寫成 x^u

0:01:30.580,0:01:34.000
那在這些 unlabeled 的 data，它就只有 function 的 input

0:01:34.000,0:01:36.680
它沒有 output，在這邊呢

0:01:36.680,0:01:40.240
有 U 筆 unlabeled 的 data

0:01:40.240,0:01:43.520
通常我們在做 Semi-supervised learning 的時候

0:01:43.520,0:01:46.620
我們期待常見的 scenario 是

0:01:46.620,0:01:50.600
unlabeled 的數量遠大於 labeled 的數量

0:01:50.600,0:01:52.740
也就這邊的 U

0:01:52.740,0:01:55.420
是遠大於 R 的

0:01:56.100,0:02:00.040
那其實 Semi-supervised learning 可以分成兩種

0:02:00.040,0:02:01.920
一種叫做 Transductive learning

0:02:01.920,0:02:03.660
一種叫做 Inductive learning

0:02:04.080,0:02:06.680
那 Transductive learning 跟 Inductive learning

0:02:06.680,0:02:08.560
我認為最簡單的分法就是

0:02:08.560,0:02:10.900
在做 Transductive learning 的時候

0:02:10.900,0:02:13.740
你的 unlabeled data 就是你的 testing set

0:02:13.740,0:02:15.420
這樣大家懂我意思嗎

0:02:15.420,0:02:19.120
有人會說，這不是用了 testing set，這不是 cheating 嗎

0:02:19.120,0:02:21.900
其實不是，你用了 testing set 的

0:02:21.900,0:02:23.720
label 才是 cheating

0:02:23.720,0:02:27.420
你用了 testing set 的 feature

0:02:27.420,0:02:30.020
不是 cheating，這樣大家懂我意思嗎？

0:02:30.020,0:02:33.500
因為，那筆 testing set 的 feature

0:02:33.500,0:02:35.380
本來就在那邊了，所以

0:02:35.380,0:02:37.400
你是可以用它的

0:02:37.400,0:02:41.480
所以，如果你用了 testing set 的 feature 的話

0:02:41.480,0:02:44.680
這個叫做 Transductive learning

0:02:44.680,0:02:47.320
我已經跟助教確認過了

0:02:47.320,0:02:50.640
其實，只要在 Kaggle 上面載下來的 data 都是可以用的

0:02:50.640,0:02:53.160
所以，你其實可以用 testing set 的 image

0:02:53.160,0:02:55.940
你只是不能夠去找它的 label 出來而已

0:02:55.940,0:02:58.940
這樣大家了解我的意思嗎

0:02:58.940,0:03:02.160
那 Inductive learning 呢，Inductive learning 是說

0:03:02.160,0:03:05.260
我們不把 testing set 考慮進來

0:03:05.260,0:03:09.120
假設我們在 training 的時候

0:03:09.120,0:03:13.520
我們還不知道 testing set 會長什麼樣子

0:03:13.520,0:03:16.460
所以我沒有辦法事先跟據 testing set 去做任何事

0:03:16.460,0:03:18.200
那我們必須要先 learn 好一個 model

0:03:18.200,0:03:20.540
在  testing set 進來的時候

0:03:20.540,0:03:21.880
再去 classify 它

0:03:21.880,0:03:24.860
至於要用 Transductive learning 還是 Inductive learning

0:03:24.860,0:03:28.060
現在 testing set 是不是已經有給你了

0:03:28.060,0:03:30.220
有些比賽裡面，testing set 已經有給你了

0:03:30.220,0:03:33.300
你就確實可能可以用它了

0:03:33.300,0:03:35.800
不過還是跟主辦單位確認一下比較好

0:03:35.800,0:03:39.040
但是。在很多時候你是

0:03:39.040,0:03:40.880
你手上沒有那個 testing set 的

0:03:40.880,0:03:42.520
你要先 learn 好 model 以後

0:03:42.520,0:03:44.300
尤其是在真正你要用

0:03:44.300,0:03:46.260
machine learning 的 application 的時候

0:03:46.260,0:03:48.380
你並沒有 testing set 在你手上阿

0:03:48.380,0:03:50.660
你要先 learn 好 model 以後，再等 testing set 進來

0:03:50.660,0:03:53.240
這個時候你就只能做 Inductive learning

0:03:53.240,0:03:56.740
有人會說 Transductive learning 
不算是 Semi-supervised learning

0:03:56.740,0:03:59.300
不過，我覺得這個也算是一種 Semi-supervised learning

0:03:59.300,0:04:02.060
只是跟 Inductive learning 很不一樣就是了

0:04:02.060,0:04:05.560
為什麼做 Semi-supervised learning 呢

0:04:05.560,0:04:09.300
因為有人常會說我們沒有 data

0:04:09.300,0:04:11.460
其實，我們不會沒有 data

0:04:11.460,0:04:14.900
我們從來都不缺 data，我們只是

0:04:14.900,0:04:17.100
缺有 labeled 的 data

0:04:17.100,0:04:20.440
比如說，你要收集 image

0:04:20.440,0:04:21.520
其實是很容易的

0:04:21.520,0:04:24.660
我就放一個機器人每天在路上走來走去

0:04:24.660,0:04:27.280
一直拍照，它就收集到一大堆的image

0:04:27.280,0:04:30.100
只是這些 image 是沒有 label的

0:04:30.100,0:04:31.420
只有非常少量的 image

0:04:31.420,0:04:33.160
你才有可能僱人去 label

0:04:33.160,0:04:36.580
所以，labeled data 很少，unlabeled data 會很多

0:04:36.580,0:04:39.640
所以，Semi-supervised learning 如果你可以利用這些

0:04:39.640,0:04:41.620
unlabeled data 來做某些事的話

0:04:41.620,0:04:43.300
會是很有價值的

0:04:43.300,0:04:45.320
但事實上，對人類來說

0:04:45.320,0:04:49.380
我們人類可能也是一直在做 Semi-supervised learning

0:04:49.380,0:04:53.660
我們會從，比如說，小孩子會從父母那邊

0:04:53.660,0:04:55.680
得到一點點的 supervised

0:04:55.680,0:04:56.940
小孩在路上看到一條狗

0:04:56.940,0:04:58.500
他問他爸那是什麼，然後

0:04:58.500,0:05:01.400
他爸說是狗，他就認得說這個東西是狗

0:05:01.400,0:05:04.800
之後，他會再看到其他的東西

0:05:04.800,0:05:08.720
有狗啊、有貓啊，但是沒有人會告訴他其他的動物是什麼

0:05:08.720,0:05:12.260
他在他往後的人生裡面，看過很多其他奇奇怪怪的動物

0:05:12.260,0:05:13.940
那沒有人會去 label 那些動物

0:05:13.940,0:05:17.520
他必須要自己把它學出來

0:05:17.520,0:05:19.280
所以，對人類來說

0:05:19.280,0:05:22.580
我們也是在做 Semi-supervised learning

0:05:22.580,0:05:26.600
那為甚麼 Semi-supervised learning 有可能

0:05:26.600,0:05:27.920
會帶來幫助呢

0:05:27.920,0:05:30.920
假設我們現在要做一個分類的 task

0:05:30.920,0:05:34.100
我們要建一個貓跟狗的 classifier

0:05:34.100,0:05:37.080
我們同時，有一大堆有關貓跟狗的圖片

0:05:37.080,0:05:38.420
那這些圖片呢

0:05:38.420,0:05:41.500
是沒有 label 的，並不知道哪些是貓，哪些是狗

0:05:42.000,0:05:46.360
今天，假設我們只考慮這個

0:05:46.360,0:05:48.740
貓跟狗有 label 的 data 的話

0:05:49.960,0:05:53.480
假設你今天要畫一個 boundary

0:05:53.480,0:05:56.780
把貓跟狗的 training data 分開的話

0:05:56.780,0:05:59.880
你可能會想說，就畫在這邊

0:05:59.880,0:06:04.060
但是，假如那些 unlabeled data 的分布

0:06:04.060,0:06:06.620
是像灰色的點，這個樣子的話

0:06:06.620,0:06:10.200
這可能就會影響你的決定

0:06:10.200,0:06:13.820
unlabeled data，雖然它只告訴我們 function 的 input

0:06:13.820,0:06:17.760
但，unlabeled data 它的分布

0:06:17.760,0:06:21.300
可以告訴我們某一些事

0:06:21.300,0:06:24.260
比如說，在這個 example 裡面，你可能很直覺的

0:06:24.260,0:06:27.320
就會覺得說 boundary 應該切成這樣

0:06:27.320,0:06:30.380
但是，Semi-supervised learning

0:06:30.380,0:06:34.040
使用 unlabeled 的方式，往往伴隨著一些假設

0:06:34.040,0:06:37.980
所以，Semi-supervised learning 有沒有用就取決於

0:06:38.360,0:06:41.560
你這個假設符不符合實際

0:06:41.560,0:06:43.980
你這個假設精不精確，因為

0:06:43.980,0:06:46.960
你可能覺得說這個是貓吧

0:06:46.960,0:06:49.040
誰知道呢，搞不好這個是狗

0:06:49.040,0:06:51.740
它們看起來很像，是因為背景都是綠的

0:06:51.740,0:06:54.740
所以 Semi-supervised learning 有沒有用

0:06:54.740,0:06:55.940
不見得永遠都是有用

0:06:55.940,0:06:59.400
那 depend on 你現在的假設是不是合理的

0:06:59.400,0:07:03.140
我們這邊要講 4 件事，第一個我們會講說

0:07:03.140,0:07:05.180
在 Generative model 的時候

0:07:05.180,0:07:07.260
我們要怎麼用 Semi-supervised learning

0:07:07.260,0:07:11.320
然後，我們會講兩個還蠻通用的假設

0:07:11.320,0:07:14.160
一個是 Low-density 的 Separation Assumption

0:07:14.160,0:07:16.120
一個是 Smoothness Assumption

0:07:16.120,0:07:19.580
最後，我們會說 Semi-supervised learning 還有一招就是

0:07:19.580,0:07:21.860
找一個比較好的 Representation

0:07:21.860,0:07:25.840
這個我們會等到講 Supervised learning 的時候再講

0:07:25.840,0:07:29.440
我們來講一下在 Generative model 裡面

0:07:29.440,0:07:31.740
你怎麼做 Semi-supervised learning

0:07:31.740,0:07:36.020
我們都已經看過 
Supervised learning 的 Generative model

0:07:36.020,0:07:37.880
在 Supervised learning 裡面呢

0:07:37.880,0:07:40.320
你有一堆 training 的 example

0:07:40.320,0:07:42.960
你知道它們分別屬於 class 1

0:07:42.960,0:07:44.720
還是屬於 class 2

0:07:44.720,0:07:50.080
那你會去估測 class 1 和 class 2 的 prior probability

0:07:50.080,0:07:52.420
你會去估測 P(C1)、P(C2)

0:07:52.420,0:07:57.440
然後，你會去估測 P(x|C1)、P(x|C2)

0:07:57.440,0:07:59.760
比如說，我假設你假設每一個 class

0:07:59.760,0:08:02.740
它的分佈都是一個 Gaussian distribution 的話

0:08:02.740,0:08:05.600
那你會估測說，這個 class 1

0:08:05.600,0:08:08.260
是從這個 mean 是 μ^1

0:08:08.260,0:08:11.420
covariance 是 Σ 的 Gaussian 估測出來的

0:08:11.420,0:08:13.780
那 class 2 是從 mean 是 μ^2

0:08:13.780,0:08:20.800
covariance matrix 也是 Σ 的 Gaussian 所估測出來的

0:08:20.800,0:08:24.120
之前講過說，如果你 share Gaussian

0:08:24.120,0:08:26.600
你的 performance 可能會是比較好的

0:08:27.140,0:08:30.000
那現在有了這些 prior probability

0:08:30.000,0:08:33.160
有了這些 mean，有了這些 covariance matrix

0:08:33.160,0:08:36.980
你就可以估測 given 一個新的 data

0:08:36.980,0:08:39.820
它是屬於 C1 的 posterior probability

0:08:39.820,0:08:42.140
然後，你就可以

0:08:42.140,0:08:46.260
看一筆 data 就做一些 classification

0:08:46.260,0:08:50.000
那你會決定一個 boundary 的位置在哪裡

0:08:50.000,0:08:53.140
但是，如果今天給了我們一些 unlabeled data

0:08:53.140,0:08:55.140
它就會影響你的決定

0:08:55.140,0:08:58.320
舉例來說，如果我們看這一筆 data

0:08:58.320,0:09:01.680
假設這些綠色的其實是 unlabeled data 的話

0:09:01.680,0:09:03.700
那如果你的 mean 跟 variance

0:09:03.700,0:09:06.560
是 μ^1, μ^2 跟 Σ，顯然就是不合理

0:09:07.400,0:09:10.180
今天這個 Σ，顯然可能

0:09:10.180,0:09:11.620
應該要比較接近圓圈

0:09:11.620,0:09:13.500
或許你在 sample 的時候有一些問題

0:09:13.500,0:09:16.320
所以，你 sample 到比較奇怪的 distribution

0:09:16.320,0:09:19.220
或許它應該比較接近圓形

0:09:19.220,0:09:23.740
而這個 class 2 的 μ 呢，或許不應該在這邊

0:09:23.740,0:09:27.800
它或許應該在其他的地方，或許應該在更下面，等等

0:09:28.140,0:09:31.540
如果你看這個 prior 的話

0:09:31.540,0:09:34.900
那 prior 可能也會受到影響，比如說，我們本來覺得說

0:09:34.900,0:09:37.960
positive，這兩個的 labeled data 是一樣多的

0:09:37.960,0:09:39.660
但是，看了這些 unlabeled data 以後

0:09:39.660,0:09:43.700
你或許會覺得 class 2 的 data 其實是比較多的

0:09:43.700,0:09:46.100
它的 prior probability 應該是比較大的

0:09:46.100,0:09:48.740
總之，看了這些 unlabeled data 以後

0:09:48.740,0:09:51.200
會影響你對 prior probability

0:09:51.200,0:09:53.860
對 mean，還有對 covariance 的估測

0:09:53.860,0:09:54.940
影響了這些估測

0:09:54.940,0:09:57.100
就影響了你 posterior probability 的式子

0:09:57.100,0:10:00.280
然後，就影響了你的 decision boundary

0:10:00.900,0:10:04.480
這個是，在直覺上是這麼做的

0:10:04.480,0:10:07.040
但是，實際上在 formulation 上怎麼做呢

0:10:07.040,0:10:10.740
我們先講操作的方式

0:10:10.740,0:10:14.020
然後再稍微講它的原理

0:10:14.020,0:10:16.800
這邊會講稍微比較快帶過去，因為

0:10:16.800,0:10:19.700
我猜在這個作業，你大概用不上

0:10:19.700,0:10:22.200
因為你也不是用 Generative model 做

0:10:22.740,0:10:25.820
那 step 1，step 1 是怎麼樣呢

0:10:25.820,0:10:30.140
我們先計算每一筆 unlabeled data 的

0:10:30.140,0:10:32.380
posterior probability

0:10:32.820,0:10:36.820
對每一筆 unlabeled data,  x^u

0:10:36.820,0:10:42.780
我們都去計算，我們要先初始化一組參數

0:10:42.780,0:10:46.240
先初始化兩個，假設我們做 binary classification 的話

0:10:46.240,0:10:50.060
先初始化 class 1 和 class 2 的 prior 的機率

0:10:50.060,0:10:53.160
先初始化 μ^1、μ^2 跟  Σ

0:10:53.160,0:10:54.700
那你說初始化這個值怎麼來

0:10:54.700,0:10:58.000
你可以 random 來，你可以用已經有 labeled 的 data

0:10:58.000,0:10:59.620
先估測一個值

0:10:59.620,0:11:02.140
總之，你就得到一組初始化的參數

0:11:02.140,0:11:06.080
我們把這些 prior probability、class dependent 的

0:11:06.080,0:11:10.020
μ^1、μ^2、Σ 統稱為參數 θ

0:11:10.020,0:11:12.980
那根據我們現在有的 θ

0:11:12.980,0:11:17.920
你可以估算每一筆 unlabeled data 屬於 class 1 的機率

0:11:17.920,0:11:22.100
當然這個機率算出來怎樣，是跟你的 model 的值有關的

0:11:22.100,0:11:25.120
算出這個機率以後，你就可以去

0:11:25.120,0:11:26.700
update 你的 model

0:11:26.700,0:11:29.540
這個是 update 的式子非常的直覺

0:11:29.540,0:11:32.720
怎麼個直覺法呢

0:11:32.720,0:11:37.620
現在 C1 的 prior probability 怎麼算呢？

0:11:37.620,0:11:39.960
原來如果沒有 unlabeled data 的時候

0:11:39.960,0:11:41.580
你的計算方法可能是

0:11:41.580,0:11:44.560
這個 N 是所有的 example

0:11:45.040,0:11:49.080
N1 是被標註為 C1 的 example

0:11:49.080,0:11:53.140
如果你要算 C1 的 prior probability，這些事情太直覺了

0:11:53.140,0:11:57.460
如果不考慮 unlabeled data 的話，感覺就是 N1/N

0:11:57.460,0:12:01.080
但是，我們現在需要考慮 unlabeled data

0:12:01.080,0:12:03.320
我們需要考慮 unlabeled data

0:12:03.320,0:12:06.760
根據 unlabeled data 告訴我們的資訊

0:12:06.760,0:12:08.660
C1 出現的次數是多少呢

0:12:08.660,0:12:11.140
C1 出現的次數就是

0:12:11.140,0:12:13.520
所有 unlabeled data

0:12:13.520,0:12:17.160
它是 C1 的 posterior probability 的和

0:12:17.160,0:12:19.100
所以，unlabeled data 並不是

0:12:19.100,0:12:22.120
hard design，它一定要屬於 C1 或 C2

0:12:22.120,0:12:24.780
根據它的 posterior probability 決定

0:12:24.780,0:12:30.040
它有百分之多少是屬於 C1，它有百分之多少是屬於 C2

0:12:31.700,0:12:34.940
那你就得到 C1 的 prior probability

0:12:34.940,0:12:38.800
根據 unsupervised data 影響你對 C1 的估測

0:12:38.800,0:12:40.260
那 μ^1 怎麼算呢

0:12:40.260,0:12:43.480
如果不考慮 unlabeled data 的時候，所謂的 μ^1

0:12:43.480,0:12:46.600
就是把所有屬於 C1 的 labeled data

0:12:46.600,0:12:49.940
都平均起來，就結束了，這個很直覺

0:12:49.940,0:12:54.060
如果今天要加上 unlabeled data 怎麼做呢

0:12:54.060,0:12:56.180
其實就只是把

0:12:56.180,0:12:59.020
unlabeled data 的那每一筆 data, x^u

0:12:59.020,0:13:01.100
根據它的 posterior probability

0:13:01.100,0:13:02.880
做 weighted sum

0:13:02.880,0:13:06.720
如果這個 x^u，它比較偏向 class1、C1 的話

0:13:06.720,0:13:10.320
它對 class 1 的影響就大一點，反之，就小一點

0:13:10.320,0:13:13.220
你就把所有 unlabeled data 根據它是

0:13:13.220,0:13:16.200
這個 C1 的 posterior probability 做 weighted sum

0:13:16.200,0:13:20.540
然後，再除掉所有 weight 的 和

0:13:20.540,0:13:24.040
做一個 normalization，就結束了

0:13:24.040,0:13:27.340
這件事情你幾乎不用解釋 ，因為太直覺了

0:13:27.340,0:13:29.100
直覺就是這麼做的

0:13:29.100,0:13:31.960
跟這個 C2 的 prior probability 阿

0:13:31.960,0:13:33.560
μ^1、μ^2、Σ

0:13:33.560,0:13:35.180
也都用同樣的方式算出來

0:13:35.180,0:13:37.800
接下來，你有了新的 model

0:13:37.800,0:13:39.880
你就會 Back to step 1

0:13:39.880,0:13:41.420
有了新的 model 以後

0:13:41.420,0:13:43.160
你的這個機率就不一樣

0:13:43.160,0:13:45.600
你這個機率就不一樣，在 step 2

0:13:45.600,0:13:47.180
你的 model 算出來就不一樣

0:13:47.180,0:13:49.680
接下來，你又可以去 update 你的機率

0:13:49.680,0:13:52.960
所以，就反覆地繼續下去

0:13:53.540,0:13:56.360
在理論上這個方法會收斂

0:13:56.360,0:13:57.760
可以保證它會收斂

0:13:57.760,0:14:01.640
但是，它的初始值

0:14:01.640,0:14:02.980
它就跟 Gradient Descent 一樣

0:14:02.980,0:14:05.980
初始值會影響你最後收斂的結果

0:14:05.980,0:14:07.940
事實上，這個 step 1

0:14:07.940,0:14:10.440
如果你聽過 EM algorithm 的話

0:14:10.440,0:14:13.300
這個 step 1 就是 E step

0:14:13.300,0:14:16.960
這個 step 2 就是 M step

0:14:16.960,0:14:21.280
我們來解釋一下

0:14:21.280,0:14:26.040
為什麼這個 algorithm 是這樣子做的

0:14:26.040,0:14:28.380
雖然這件事情實在是很直覺

0:14:28.380,0:14:31.760
但是它背後的理論，它為什麼要這樣做呢

0:14:31.760,0:14:33.520
這個想法是這樣子的

0:14:34.400,0:14:37.200
原來假設我們只有 labeled data 的時候

0:14:37.200,0:14:38.840
我們只有 labeled data 的時候

0:14:38.840,0:14:41.500
我們要做的事情

0:14:41.500,0:14:44.520
是要去 maximize 一個 likelihood 對不對

0:14:44.520,0:14:47.000
或者是 maximize Log 的 likelihood

0:14:47.000,0:14:49.980
這個意思是一樣的

0:14:49.980,0:14:53.600
那每一筆 training data

0:14:53.600,0:14:56.960
它的 likelihood，我們是可以算出來的

0:14:56.960,0:14:58.800
如果你給一個 θ

0:14:58.800,0:15:02.200
每一筆 training data、每一筆 labeled data 的 likelihood

0:15:02.200,0:15:03.360
我們是可以算出來的

0:15:03.360,0:15:05.040
每一筆 data 的 likelihood

0:15:05.040,0:15:07.460
就是 P(y^r\head)

0:15:07.460,0:15:09.820
那個 label、那個 class 出現的 prior

0:15:09.820,0:15:12.960
跟根據那個 class，generate 那筆 data 的機率

0:15:12.960,0:15:15.980
所以，給一個 θ，你可以把那個 likelihood 算出來

0:15:15.980,0:15:17.860
把所有的 labeled data

0:15:17.860,0:15:19.860
的這個 log likelihood 加起來

0:15:19.860,0:15:22.780
就是你的 total log likelihood

0:15:22.780,0:15:25.060
然後，你要去找一個 θ 去 maximize 它

0:15:25.060,0:15:27.200
那個 solution，是很直覺的

0:15:27.200,0:15:28.740
它有 Closed-form solution

0:15:28.740,0:15:31.340
代個式子，你就可以把它解出來

0:15:31.780,0:15:34.060
現在如果有 unlabeled data 的時候

0:15:34.060,0:15:35.840
式子有什麼不一樣呢

0:15:38.800,0:15:41.340
我有一個地方寫錯了，就是這邊

0:15:41.340,0:15:44.720
應該要有 y^r\head，就是這一項

0:15:44.720,0:15:48.080
是要考慮 labeled data，所以這一項

0:15:48.080,0:15:51.420
跟前面這個部分是一樣的

0:15:51.420,0:15:54.220
但是，unlabeled data 怎麼辦呢

0:15:54.220,0:15:56.360
unlabeled data 我們並不知道

0:15:56.360,0:15:59.220
它是來自於哪一個 class 阿

0:15:59.220,0:16:02.760
我們怎麼估測它的機率呢

0:16:02.760,0:16:06.320
那我們說一筆 unlabeled data, x^u

0:16:06.320,0:16:08.340
它出現的機率

0:16:08.340,0:16:11.340
因為我不知道它是從 C1 還是從 C2 來的

0:16:11.340,0:16:14.460
所以，它就是 C1、C2 都有可能

0:16:14.460,0:16:17.460
所以，一筆 unlabeled data 出現的機率

0:16:17.460,0:16:20.540
就是它在 C1 的 prior probability

0:16:20.540,0:16:24.620
跟 C1 這個 class 產生這筆 unlabeled data 的機率

0:16:24.620,0:16:27.300
加上 C2 的 prior probability

0:16:27.300,0:16:33.800
乘上C2 這個 class 產生這筆 unlabeled data 的機率

0:16:33.800,0:16:35.340
把他們通通合起來

0:16:35.340,0:16:38.560
就是這筆 unlabeled data 出現的機率

0:16:38.560,0:16:40.900
你問 x^u，它可以從 C1 來，它可以從 C2 來

0:16:40.900,0:16:42.240
我不知道它從哪裡來

0:16:42.240,0:16:44.640
所以，你就說它兩個都有可能

0:16:44.640,0:16:46.240
接下來，你要做的事情

0:16:46.240,0:16:49.120
就是要去 maximize 這個式子

0:16:49.120,0:16:52.140
不幸的是，這個式子它不是 convex

0:16:52.140,0:16:54.060
所以，你解它的時候呢

0:16:54.060,0:16:56.940
你變成要用 EM algorithm 解

0:16:56.940,0:17:01.020
其實，你就是要用，要 iterative 的去 solve 它

0:17:01.020,0:17:04.020
所以，我們剛才做的那個步驟

0:17:04.020,0:17:06.540
我在前一頁投影片裡面的那個 algorithm

0:17:06.540,0:17:09.380
它做的事情就是，在每一次循環的時候

0:17:09.380,0:17:11.060
你做完 step1，你做完 step 2

0:17:11.060,0:17:15.180
你就可以讓這個 Log likelihood

0:17:15.180,0:17:17.140
增加一點，然後跑到最後呢

0:17:17.140,0:17:19.820
它會收斂在一個 local minimum 的地方

0:17:22.360,0:17:26.060
那這個是 Generative 的 model

0:17:26.060,0:17:28.720
那我們等一下會講

0:17:28.720,0:17:31.620
我們接下來要講一個，比較 general 的方式

0:17:31.620,0:17:34.180
這邊基於的假設

0:17:34.180,0:17:36.220
是 Low-density 的 Separation

0:17:36.220,0:17:40.560
也就是說，這個世界是非黑即白的

0:17:40.560,0:17:42.680
什麼是非黑即白呢

0:17:42.680,0:17:44.500
非黑即白，意思就是說

0:17:44.500,0:17:47.480
假設我們現在，有一大堆的 data

0:17:47.480,0:17:49.420
有 labeled data、有 unlabeled data

0:17:49.420,0:17:54.020
在兩個 class 之間呢

0:17:54.020,0:17:57.900
它們會有一個非常明顯的鴻溝

0:17:57.900,0:18:01.800
就是說，如果現在給你這些 labeled data

0:18:01.800,0:18:04.680
給你這些 labeled data

0:18:04.680,0:18:08.220
你可以說，我的 boundary 要切在這邊也可以

0:18:08.220,0:18:10.720
我的 boundary 要切在這邊也可以

0:18:10.720,0:18:13.980
你就可以把這兩個 class 分開

0:18:13.980,0:18:17.660
它們在 training data 上的正確率都是100%

0:18:17.660,0:18:20.100
但是，如果你考慮 unlabeled data 的話

0:18:20.100,0:18:22.940
或許這一個 boundary 是比較好的

0:18:22.940,0:18:24.620
這個 boundary 是比較不好的

0:18:24.620,0:18:26.800
為什麼呢？因為今天基於的假設就是

0:18:26.800,0:18:28.920
這是一個非黑即白的世界

0:18:28.920,0:18:30.680
在這兩個 class 之間呢

0:18:30.680,0:18:35.120
會有一個很明顯的楚河漢界，會有一個鴻溝

0:18:35.120,0:18:36.500
會有一個地方

0:18:36.500,0:18:38.560
它之所以叫 Low-density Separation

0:18:38.560,0:18:41.380
意思就是說，在這兩個 class 的交界處

0:18:41.380,0:18:42.900
它的 density 是低的

0:18:42.900,0:18:44.040
這兩個 class 的交界處

0:18:44.040,0:18:47.400
data 量是很少，不會出現 data 的

0:18:47.400,0:18:50.280
所以，這個 boundary 可能就是比較合理的

0:18:50.880,0:18:54.240
那 Low-density Separation 最具代表性、最簡單的方法

0:18:54.240,0:18:56.780
就是 Self-training，但是 Self-training 太直覺了

0:18:56.780,0:18:58.940
我覺得這個沒什麼好講的

0:18:58.940,0:19:01.720
我相信大家都是秒 implement 這樣

0:19:01.720,0:19:04.960
我們就很快地講過去，Self-training 就是說

0:19:04.960,0:19:07.600
我們有一些 labeled data

0:19:07.600,0:19:10.380
有一些 unlabeled data

0:19:10.380,0:19:14.860
接下來，先從 labeled data 去

0:19:14.860,0:19:20.200
train 一個 model，這個 model 叫做 f*

0:19:20.200,0:19:23.600
那這邊其實，你的這個 training 的方法

0:19:23.600,0:19:25.860
Self-training 其實是一個很 general 的方法

0:19:25.860,0:19:27.740
你用什麼方法得到你的 f*

0:19:27.780,0:19:29.280
你用 neural network 是

0:19:29.280,0:19:31.380
用 Deep 的方法、是用 Shallow 的方法

0:19:31.380,0:19:33.120
還是用其他 machine learning 的方法

0:19:33.120,0:19:36.300
都可以，反正你就是 train 出一個 model, f*

0:19:36.300,0:19:41.880
根據這個 f*，你去 label 你的 unlabeled data

0:19:41.880,0:19:44.380
你就把 x^u 丟進 f*

0:19:44.380,0:19:47.860
看它吐出來的 y^u 是什麼

0:19:47.860,0:19:49.540
那就是你的 labeled data

0:19:49.540,0:19:53.660
這個東西，叫做 Pseudo-label

0:19:54.320,0:19:55.940
接下來呢

0:19:55.940,0:20:00.940
你要從你的 unlabeled data set 裡面拿出一些 data

0:20:00.940,0:20:02.920
把它加到 labeled data set 裡面

0:20:02.920,0:20:04.880
至於哪些 data 會被加進去

0:20:04.880,0:20:06.700
這就是 open question，你要自己

0:20:06.700,0:20:11.540
design 一些 heuristic 的 rule，自己想個辦法來解決

0:20:11.540,0:20:14.620
你甚至可以給每一筆 unlabeled data provide weight

0:20:14.620,0:20:15.740
那有一些比較 confidence

0:20:15.740,0:20:17.260
有一些 Pseudo-label 比較 confident

0:20:17.260,0:20:19.580
有一些 Pseudo-label 比較不 confident

0:20:19.580,0:20:21.960
那有了更多的 labeled data 以後

0:20:21.960,0:20:23.600
現在 labeled data 從 unlabeled data 那邊

0:20:23.600,0:20:24.780
得到額外的 data

0:20:24.780,0:20:28.080
你就可以回頭再去 train 你的 model, f*

0:20:28.080,0:20:30.420
這件事情，非常的直覺

0:20:30.900,0:20:32.200
那 Self-training 這麼簡單

0:20:32.200,0:20:34.380
你可能覺得自己非常的懂

0:20:34.380,0:20:36.180
那我來問大家一個問題

0:20:36.640,0:20:38.140
以下這個 process

0:20:38.140,0:20:41.580
如果我們用在 Regression 上面

0:20:41.660,0:20:43.260
會怎樣呢？

0:20:44.120,0:20:48.080
當然你永遠可以把 Regression 用在這邊，沒有什麼問題

0:20:48.080,0:20:49.500
程式也不會 segmentation fault

0:20:49.500,0:20:51.420
那問題就是

0:20:51.420,0:20:54.720
這一招在 Regression 上面

0:20:54.720,0:20:57.140
你覺得有可能會有用嗎

0:20:59.360,0:21:01.740
我們給大家5秒鐘想一下

0:21:01.740,0:21:03.500
你覺得這一招在 Regression 上

0:21:03.500,0:21:05.760
有可能會有用的，舉手一下

0:21:06.700,0:21:08.880
你覺得這一招在 Regression 上

0:21:08.880,0:21:10.620
一定沒有用的，舉手一下

0:21:11.220,0:21:14.600
那都沒有人舉手

0:21:15.680,0:21:19.080
你仔細想想看

0:21:19.080,0:21:21.540
你覺得這一招在 Regression 上會有用嗎？

0:21:22.440,0:21:28.680
Regression 大家知道，就是 output 一個數字

0:21:28.680,0:21:32.520
就是 output 一個 real number，那你有一個

0:21:32.520,0:21:36.040
x^u，然後，你 output 一個 real number

0:21:36.040,0:21:38.560
你把這筆 data 加到你的 data 裡面再 train

0:21:38.560,0:21:40.840
你會影響 f* 嗎？

0:21:43.940,0:21:46.340
其實不會影響 f*，對不對

0:21:46.340,0:21:50.680
所以，Regression 其實不能用這一招的

0:21:54.000,0:21:56.160
這樣大家有問題嗎？

0:22:01.380,0:22:03.620
那其實是這樣子的

0:22:03.620,0:22:07.700
你可能會覺得剛才這個 Self-training

0:22:07.700,0:22:11.800
它很像是，我們剛才在 Generative model 裡面

0:22:11.800,0:22:13.040
用的那個方法

0:22:13.040,0:22:14.680
它們唯一的差別是在

0:22:14.680,0:22:16.300
做 Self-training 的時候

0:22:16.300,0:22:18.680
你用的是Hard label

0:22:18.680,0:22:21.880
在做 Generative model 的時候，你用的是Soft label

0:22:21.880,0:22:24.900
在做 Self-training 的時候，我們會強制 assign

0:22:24.900,0:22:27.520
一筆 training data，它一定是屬於某一個 class

0:22:27.520,0:22:29.840
但是，在 Generative model 的時候

0:22:29.840,0:22:33.560
我們是說，根據它的 posterior probability

0:22:33.560,0:22:36.080
你可能有部分屬於 class 1，有部分屬於 class 2

0:22:36.080,0:22:38.360
所以，Self-training 是 Hard label

0:22:38.360,0:22:41.980
Generative model 的時候，我們用的是Soft label

0:22:41.980,0:22:45.580
那到底哪一個比較好呢

0:22:45.580,0:22:48.220
如果我們今天考慮的是

0:22:48.220,0:22:49.900
neural network 的話

0:22:49.900,0:22:52.560
你可以比較看看，到底哪一個方法比較好

0:22:52.560,0:22:56.380
假設我們用的是 neural network

0:22:56.380,0:23:01.320
那你從你的 labeled data，得到一組 network 的參數、θ*

0:23:01.320,0:23:03.980
那現在有一筆 unlabeled data，x^u

0:23:03.980,0:23:06.840
然後呢，你說

0:23:06.840,0:23:09.980
根據我們現在手上的參數，θ*

0:23:09.980,0:23:11.240
我把它分成兩類

0:23:11.240,0:23:13.480
它有 0.7 的機率屬於 class a

0:23:13.480,0:23:16.260
有 0.3 的機率屬於 class b

0:23:16.580,0:23:17.900
屬於 class 2

0:23:17.900,0:23:22.060
如果是 Hard label 的話，你就把它直接 label 成 class 1

0:23:22.060,0:23:24.920
然後你就說，因為它變成 class 1 了

0:23:24.920,0:23:27.200
所以，x^u 的新的 target

0:23:27.200,0:23:28.940
就是你拿 x^u 在 train neural network 的時候

0:23:28.940,0:23:30.940
它的 target 就是第一維是 1

0:23:30.940,0:23:33.180
第二維是 0

0:23:33.180,0:23:35.120
或是，你就把這個東西

0:23:35.120,0:23:38.180
跟你 neural network 的 output 去算 cross entropy

0:23:38.180,0:23:41.160
如果是做 soft 的話

0:23:41.160,0:23:44.360
那你就是說 70% 屬於 class 1

0:23:44.360,0:23:47.100
30% 屬於 class 2

0:23:47.100,0:23:49.860
然後你就說，新的 target 就是

0:23:49.860,0:23:53.100
0.7 跟 0.3

0:23:55.600,0:24:00.640
你覺得，如果我們今天用的是 neural network 的話

0:24:00.640,0:24:04.580
上面跟下面哪一個方法，有可能是有用的呢

0:24:04.580,0:24:07.160
你覺得下面這個方法

0:24:07.160,0:24:09.760
有可能有用的同學舉手一下

0:24:10.720,0:24:12.060
手放下

0:24:12.060,0:24:15.840
如果你覺得，下面這個方法完全不可能有用的舉手

0:24:17.200,0:24:18.860
手放下

0:24:18.860,0:24:21.780
比較多人覺得它完全不會有用

0:24:21.780,0:24:23.580
為什麼它完全不會有用呢

0:24:23.580,0:24:25.060
你仔細想想看

0:24:25.060,0:24:27.400
你現在 model 的 output 在這些 unlabeled data 上

0:24:27.400,0:24:29.660
用這個值，參數 output 是 0.7, 0.3

0:24:29.660,0:24:31.960
你說你把它的 target 又設成 0.7, 0.3

0:24:31.960,0:24:34.400
那不就是同一組參數可以做到一樣的事情嗎

0:24:35.380,0:24:38.220
所以如果你是做 neural network 的時候

0:24:38.220,0:24:39.720
你用一個 Soft label

0:24:39.720,0:24:41.980
結果是沒有用的

0:24:41.980,0:24:46.080
所以，這邊你一定要用 Hard label

0:24:46.080,0:24:49.060
那我們用 Hard label 是什麼意思呢

0:24:49.060,0:24:50.860
我們用  Hard label 的時候

0:24:50.860,0:24:53.440
我們用的就是 Low-density Separation 的概念

0:24:53.440,0:24:56.420
也就是說，今天我們看 x^u

0:24:56.420,0:24:59.640
它屬於 class 1 的機率只是比較高而已

0:24:59.640,0:25:02.960
我們沒有很確定，它一定是屬於 class 1

0:25:02.960,0:25:05.000
但是這是一個非黑即白的世界

0:25:05.000,0:25:07.340
所以，如果你看起來有點像 class 1

0:25:07.340,0:25:09.240
那你就一定是 class 1

0:25:09.240,0:25:11.900
所以，本來根據我的 model 是說

0:25:11.900,0:25:13.620
機率是 0.7 是 class 1

0:25:13.620,0:25:15.000
0.3 是 class 2

0:25:15.000,0:25:18.120
那用 Hard label

0:25:18.120,0:25:21.040
用 Low-density Separation Assumption

0:25:21.040,0:25:23.520
就改成說，它這邊是 class 1 的機率是 1

0:25:23.520,0:25:26.800
你就把它往 class 1 那邊推過去

0:25:26.800,0:25:29.680
它就完全不可能是屬於 class 2

0:25:29.680,0:25:32.880
那下面這個方法不會 work

0:25:33.260,0:25:35.800
我之前還有看過有 paper propose 就是

0:25:35.800,0:25:38.520
propose 在做 neural network 的時候

0:25:38.520,0:25:39.740
用一個甚麼 soft 的方法

0:25:39.740,0:25:41.660
那果然 performance 不 work

0:25:41.660,0:25:43.400
不用做，我就知道結果會怎樣

0:25:47.280,0:25:50.260
那剛才這一招

0:25:50.260,0:25:52.080
有一個進階版

0:25:52.080,0:25:55.920
叫做 Entropy-based Regularization

0:25:55.920,0:25:57.460
你可能會覺得說

0:25:57.460,0:25:59.860
直接看它有點像 class 1 就變 1

0:25:59.860,0:26:02.420
直接看它有點像 class 2 就變 2

0:26:02.420,0:26:03.560
有點太武斷了

0:26:03.560,0:26:06.380
那你可以用 Entropy-based 的這個方法

0:26:06.380,0:26:08.040
Entropy-based 這個方法是說

0:26:08.040,0:26:10.900
如果你用 neural network 的時候，你的 output

0:26:10.900,0:26:13.620
是一個 distribution

0:26:13.620,0:26:16.900
那我們不要限制說這個 output 一定要是

0:26:16.900,0:26:18.560
class 1，一定要是 class 2

0:26:18.560,0:26:20.660
但是，我們做的假設是這樣

0:26:20.660,0:26:22.280
這個 output 的 distribution

0:26:22.280,0:26:24.120
它一定要很集中

0:26:24.120,0:26:26.200
因為這是一個非黑即白的世界

0:26:26.200,0:26:28.380
所以，output 的 distribution 一定要很集中

0:26:28.380,0:26:32.160
也就是說你 output，假設我們現在做 5 個 class 的分類

0:26:32.160,0:26:34.420
那如果你的 output 都是

0:26:34.420,0:26:37.640
在 class 1 的機率很大，在其他 class 的機率很小

0:26:37.640,0:26:39.220
這個是好的

0:26:39.220,0:26:41.740
因為是 unlabeled data，所以我不知道它的 label 是什麼

0:26:41.740,0:26:43.280
但是，如果你的 model

0:26:43.280,0:26:46.180
可以讓這筆 data，在 class 1 的機率很大

0:26:46.180,0:26:48.280
在其他的機率很小，那是好的

0:26:48.280,0:26:50.940
如果它在 class 5 的機率很大

0:26:50.940,0:26:52.140
其他的機率都很小

0:26:52.140,0:26:53.120
這個也是好的

0:26:53.120,0:26:54.900
因為我也不知道它是 class 1 還是 class 5

0:26:54.900,0:26:56.600
所以，這樣是好的

0:26:56.600,0:26:58.000
甚麼狀況不好呢

0:26:58.000,0:27:01.380
如果今天分布是很平均的話

0:27:01.380,0:27:04.600
這樣是不好的，因為這是一個非黑即白的世界

0:27:04.600,0:27:07.960
這樣子不符合 Low-density Separation 的假設

0:27:10.280,0:27:13.300
但是，現在的問題就是我們要怎麼

0:27:13.300,0:27:16.700
用數值的方法來evaluate

0:27:16.700,0:27:19.400
這個 distribution 到底是好的還是不好

0:27:19.400,0:27:22.520
這個 distribution是集中的還是不集中的呢

0:27:22.520,0:27:25.400
這邊要用的東西 ，叫做Entropy

0:27:25.400,0:27:28.380
你就去算一個 distribution 的 Entropy

0:27:28.380,0:27:30.120
這個 distribution 的 Entropy 呢

0:27:30.120,0:27:34.000
告訴你說，這個 distribution 它到底是集中還是不集中

0:27:34.000,0:27:38.760
我們用一個值來表示，這個 distribution 是集中還是分散

0:27:38.760,0:27:40.860
這個怎麼算呢

0:27:40.860,0:27:44.020
其實就算你沒有修過 information theory 之類的

0:27:44.020,0:27:46.080
我相信你也是聽得懂的

0:27:46.080,0:27:47.360
就記一下它的式子

0:27:47.360,0:27:50.280
它的式子是這樣

0:27:50.280,0:27:53.500
某一個 distribution，它的 entropy 呢

0:27:53.500,0:27:56.100
就是負的

0:27:56.100,0:27:59.980
它對每一個 class 的機率

0:27:59.980,0:28:02.760
有 5 個 class，就 summation 1到 5

0:28:02.760,0:28:04.500
它對每一個 class 的機率

0:28:04.500,0:28:07.440
乘上 log(那一個 class 的機率)

0:28:08.160,0:28:10.420
如果我們今天把

0:28:10.420,0:28:13.940
這一個、第一個 distribution 的機率

0:28:13.940,0:28:17.200
代到這裡面去，它只有一個是 1

0:28:17.200,0:28:18.560
其他都是 0

0:28:18.560,0:28:20.680
那你得到的 entropy 是多少呢

0:28:20.680,0:28:22.040
你得到的 entropy

0:28:22.040,0:28:24.920
算出來會是 0

0:28:24.920,0:28:29.680
因為 1*ln1 是 0

0:28:29.680,0:28:31.360
0*ln0 也是 0，所以

0:28:31.360,0:28:35.180
這個就是 0，這沒有甚麼特別好講的

0:28:35.180,0:28:39.560
這個也是 0

0:28:39.560,0:28:43.060
那下面這個呢

0:28:43.060,0:28:44.340
這邊每一個機率

0:28:44.340,0:28:48.240
也就是這一邊每一個 (y 上標 u, 下標 m) 都是 1/5

0:28:48.240,0:28:50.160
所以，你就把這些值代進去

0:28:50.160,0:28:52.620
你就把這些 1/5 的值都代進去

0:28:52.620,0:28:56.620
你算出來就是 1- ln(1/5)

0:28:56.620,0:28:58.700
也就是 ln5，所以

0:28:58.700,0:29:02.100
它的 entropy 比較大，它是散佈比較開的

0:29:02.100,0:29:03.400
所以，它 entropy 比較大

0:29:03.400,0:29:04.780
它是散佈比較窄的

0:29:04.780,0:29:06.480
所以，它的 entropy 比較小

0:29:06.480,0:29:08.500
所以，我們需要做的事情是

0:29:08.500,0:29:10.260
我們希望這個 model 的 output

0:29:10.260,0:29:12.520
當然在 labeled data 上，它的分類要正確

0:29:12.520,0:29:13.720
但是在 unlabeled data 上，

0:29:13.720,0:29:17.260
它的 output、entropy 要越小越好

0:29:17.260,0:29:19.960
所以，根據這個假設

0:29:19.960,0:29:24.640
你就可以去重新設計你的 loss function

0:29:24.640,0:29:26.560
我們原來的 loss function 是說

0:29:26.560,0:29:28.880
我希望，找一組參數

0:29:28.880,0:29:31.960
讓我現在在 labeled data 上的 model 的 output

0:29:31.960,0:29:33.160
跟正確的 model 的 output

0:29:33.160,0:29:34.880
它的距離越近越好

0:29:34.880,0:29:38.080
你用 cross entropy 來 evaluate 它們之間的距離

0:29:38.080,0:29:40.360
這個是 labeled data 的部分

0:29:40.360,0:29:42.200
在 unlabeled data 的部分呢

0:29:42.200,0:29:45.380
你會加上每一筆 unlabeled data

0:29:45.380,0:29:48.020
它的 output distribution 的 entropy

0:29:48.020,0:29:51.200
那你會希望這些 unlabeled data 的 entropy

0:29:51.200,0:29:53.520
越小越好，那在這兩項中間呢

0:29:53.520,0:29:56.000
你其實可以乘一個 weight 來考慮說

0:29:56.000,0:29:58.940
你要偏向 unlabeled data 多一點

0:29:58.940,0:30:00.300
還是少一點

0:30:00.300,0:30:02.080
那在 training 的時候怎麼辦呢？

0:30:02.080,0:30:03.800
在 training 的時候就是

0:30:03.800,0:30:06.640
一句話就 train 下去這樣，懂嗎？

0:30:06.640,0:30:08.280
這個可以算微分

0:30:08.280,0:30:10.740
可以算微分就

0:30:10.740,0:30:13.680
就沒有甚麼問題，就用 Gradient Descent

0:30:13.680,0:30:16.680
來 minimize 這個式子而已

0:30:16.680,0:30:18.020
那這一件事情

0:30:18.020,0:30:21.940
它的角色就很像我們之前講的 Regularization

0:30:21.940,0:30:24.580
所以它稱之為 Entropy-based 的 Regularization

0:30:24.580,0:30:26.100
之前我們說 Regularization 的時候

0:30:26.100,0:30:28.340
我們說，我們在原來的 loss function 後面

0:30:28.340,0:30:32.160
加一個 parameter 的 1-norm 或 2-norm

0:30:32.160,0:30:34.000
讓它比較不會 overfitting

0:30:34.000,0:30:35.540
那現在加上一個

0:30:35.540,0:30:38.200
根據 unlabeled data 得到的 entropy

0:30:38.200,0:30:40.400
來讓它比較不會 overfitting

0:30:41.760,0:30:44.420
那還有別的 Semi-supervised learning 的方式

0:30:44.420,0:30:46.600
有一個很著名的叫做

0:30:46.600,0:30:50.080
Semi-supervised SVM，不過我們還沒有講 SVM

0:30:50.080,0:30:53.100
所以，這邊就是當作一個 outlook

0:30:53.100,0:30:57.100
這個 Semi-supervised SVM 它的精神是這樣

0:30:57.100,0:30:59.060
我們知道 SVM 做的事情就是

0:30:59.060,0:31:01.680
給你兩個 class 的 data，然後找一個 boundary

0:31:01.680,0:31:05.720
那這個 boundary，一方面它要有最大的 margin

0:31:05.720,0:31:09.800
所謂最大的 margin 就是讓這兩個 class 分的越開越好

0:31:09.800,0:31:13.740
同時，它也要有最小的

0:31:13.740,0:31:15.700
分類的錯誤

0:31:15.700,0:31:18.800
現在，假設有一些 unlabeled data

0:31:18.800,0:31:21.560
這個 Semi-supervised SVM 會怎麼處理這個問題呢

0:31:21.560,0:31:24.660
它會窮舉所有可能的 label

0:31:24.660,0:31:26.820
這邊有 4 筆 unlabeled data

0:31:26.820,0:31:30.500
每一筆它都可以是屬於 class 1，也可以是屬於 class 2

0:31:30.500,0:31:32.840
我們就窮舉它所有可能的 label

0:31:32.840,0:31:34.740
它可以是長這樣的

0:31:34.740,0:31:38.120
就是說這三筆是屬於藍色 class

0:31:38.120,0:31:40.500
這一筆是屬於橙色 class，它可以是長這樣

0:31:40.500,0:31:41.920
它可以是長這樣

0:31:41.920,0:31:44.540
這兩個是藍色 class，這兩個是橙色 class

0:31:44.540,0:31:47.960
它可以長這樣，這個是橙的，這個是藍的

0:31:47.960,0:31:50.820
這個是藍的，這個是橙的，有各種可能，有很多的可能

0:31:50.820,0:31:53.520
有很多可能

0:31:53.520,0:31:56.840
然後，對每一個可能的結果，你都去算一個

0:31:56.840,0:31:59.180
你都去做一個 SVM

0:31:59.180,0:32:01.140
如果是在這個可能，這個情況下

0:32:01.140,0:32:02.660
你的 SVM 的 boundary 切在這邊

0:32:02.660,0:32:05.400
然後，這個可能你的 SVM 的 boundary 切在這邊

0:32:05.400,0:32:08.160
這個可能你的 SVM 的 boundary 不得不切在這邊

0:32:08.160,0:32:11.060
因為找不到一個方法可以把兩個 class 分開

0:32:11.060,0:32:13.920
然後，你再去看說，哪一個

0:32:13.920,0:32:17.260
unlabeled data 的可能性

0:32:17.260,0:32:21.460
在窮舉所有的可能的 label 裡面，哪一個可能性

0:32:21.460,0:32:24.700
可以讓你的 margin 最大，同時又 minimize error

0:32:24.700,0:32:26.520
今天在這個 example 裡面呢

0:32:26.520,0:32:29.720
可能是這一個方法

0:32:29.720,0:32:30.980
可以讓你的 margin 最大

0:32:30.980,0:32:32.520
它的 margin 是小的

0:32:32.520,0:32:35.080
它的 margin 不只小，而且還有分類錯誤

0:32:35.080,0:32:37.260
它的 margin 大而且都分類對，所以

0:32:37.260,0:32:40.900
你可能最後就選擇這一個 boundary

0:32:40.900,0:32:44.920
那這個 SVM，我把它的 reference 放在下面

0:32:44.920,0:32:46.300
你可能會有一個問題說

0:32:46.300,0:32:49.800
窮舉所有 unlabeled data 的 label

0:32:49.800,0:32:52.480
這聽起來不 make sense 阿，我有一萬筆 unlabeled data

0:32:52.820,0:32:57.080
2 的一萬次方，可能沒辦法做啊

0:32:57.080,0:32:58.440
所以，這個 paper 裡面

0:32:58.440,0:33:01.040
它就提出了一個很 approximate 的方法

0:33:01.040,0:33:02.980
基本精神是

0:33:02.980,0:33:06.480
我們今天，先很快帶過

0:33:06.480,0:33:08.840
它的基本精神是你一開始先得到一些 label

0:33:08.840,0:33:11.300
然後，你每次改一筆 unlabeled data 的 label

0:33:11.300,0:33:14.080
看看可不可以讓你的 objective function 變大

0:33:14.080,0:33:15.520
變大的話就改這樣子

0:33:19.080,0:33:23.400
接下來我們要講的方法呢

0:33:23.400,0:33:26.760
叫做 Smoothness Assumption

0:33:26.760,0:33:30.100
它的精神就是：近朱者赤；近墨者黑

0:33:30.100,0:33:33.900
或者是，像勸學篇說的那個

0:33:33.900,0:33:38.500
蓬生麻中，不扶而直；白沙在涅，與之俱黑

0:33:40.280,0:33:42.940
它的假設是這樣子

0:33:42.940,0:33:50.400
如果 x 是像的，那它們的 label y 也就像

0:33:50.400,0:33:52.540
這個假設聽起來沒有甚麼，而且

0:33:52.540,0:33:53.860
光講這個假設

0:33:53.860,0:33:56.620
其實是不精確的，因為

0:33:56.620,0:33:59.360
你知道一個正常的 model，你給它一個像的 input

0:33:59.360,0:34:01.880
如果它不是很 deep 的話，output 就會很像阿

0:34:01.880,0:34:05.140
所以，這個這樣講其實是不夠精確的

0:34:05.780,0:34:08.880
真正精確的假設，應該是下面這個講法

0:34:08.880,0:34:13.300
x 的分布是不平均的

0:34:13.300,0:34:16.140
它在某些地方是很集中

0:34:16.140,0:34:18.920
某些地方又很分散

0:34:19.700,0:34:22.700
如果今天 x1 和 x2

0:34:22.700,0:34:26.580
它們在一個 high density 的 region

0:34:26.580,0:34:28.680
很 close 的話

0:34:28.680,0:34:31.300
x^1 的 label、y^1\head

0:34:31.300,0:34:34.600
跟 x^2 的 label、y^2\head，它們才會很像

0:34:35.080,0:34:37.160
這句話有點讓人不知道在說甚麼

0:34:37.160,0:34:40.820
甚麼叫做在 high density 的 region 下呢

0:34:40.820,0:34:42.880
這句話的意思就是說

0:34:42.880,0:34:46.960
它們可以用 high density 的 path 做 connection

0:34:46.960,0:34:49.620
這樣講你還是不知道我在說什麼，所以我直接

0:34:49.620,0:34:52.280
舉一個例子，假設這個是

0:34:52.280,0:34:55.280
我們 data 的分布，假設這個是 data 的分布

0:34:55.280,0:34:57.960
它分布就像是一個血輪眼的樣子

0:35:00.640,0:35:03.920
假設我們現在有 3 筆 data

0:35:03.920,0:35:05.260
有 3 筆 data

0:35:05.260,0:35:07.560
x^1、x^2 跟 x^3

0:35:07.560,0:35:11.560
如果我們只是考慮這個比較粗的假設

0:35:11.560,0:35:14.460
像的 x，它的 output 像

0:35:14.460,0:35:16.180
那它的 label 像，所以

0:35:16.180,0:35:19.320
感覺好像應該是，x^2 跟 x^3 的 label 應該比較像

0:35:19.320,0:35:22.420
x^1 跟 x^2 的 label 比較不像

0:35:22.420,0:35:25.620
但是，其實 Smoothness Assumption 的假設不是這樣

0:35:25.620,0:35:27.320
它更精確的假設是說

0:35:27.320,0:35:29.680
你的像要透過一個

0:35:29.680,0:35:32.240
high density 的 region 像

0:35:32.240,0:35:34.020
懂嗎？就是說

0:35:34.020,0:35:37.220
x^1 跟 x^2，它們中間有一個

0:35:37.220,0:35:38.700
high density 的 region

0:35:38.700,0:35:42.880
它們中間有很多很多很多的 data

0:35:42.880,0:35:45.240
所以，它們兩個相連的地方是

0:35:45.240,0:35:47.820
通過一個 high density 的 path 相連的

0:35:47.820,0:35:51.740
從 x^1 走到 x^2 中間都是點，都是人煙

0:35:51.740,0:35:55.940
然後 x^2 跟 x^3 中間沒有點，所以可以走過去

0:35:55.940,0:35:57.660
這樣懂我意思嗎？就是

0:35:57.660,0:36:01.120
假設藍色點是聚落的分布的話

0:36:01.120,0:36:03.480
這中間是平原，所以人很多

0:36:03.480,0:36:05.480
所以，從 x^1 走到 x^2 比較容易

0:36:05.480,0:36:07.240
x^2 跟 x^3 中間是個山

0:36:07.240,0:36:09.200
所以這邊沒有住人，所以你走不過去

0:36:09.200,0:36:10.100
所以

0:36:10.100,0:36:13.580
根據這個真正的 Smoothness Assumption 的假設

0:36:13.580,0:36:15.940
它要告訴我們的意思是說

0:36:15.940,0:36:18.420
x^1 跟 x^2 是會有

0:36:18.420,0:36:19.960
比較可能有一樣的 label

0:36:19.960,0:36:24.560
x^2 跟 x^3 比較可能有不一樣的 label

0:36:24.560,0:36:28.820
因為，它們中間沒有 high density 的 path

0:36:28.820,0:36:33.980
那為甚麼會有 Smoothness Assumption 這樣的假設？

0:36:33.980,0:36:37.860
因為在真實的情況下，這個假設很有可能是成立的

0:36:37.860,0:36:40.100
比如說，我們考慮這個例子

0:36:40.100,0:36:43.040
我們考慮手寫數字辨識的例子

0:36:43.040,0:36:46.940
我們現在看到這邊有兩個 2，這邊有一個 3

0:36:46.940,0:36:51.200
對人來說，你當然知道這兩個都是 2

0:36:51.200,0:36:55.440
但是，如果你是單純算他們 pixel 的相似度的話

0:36:55.440,0:36:58.500
搞不好，這兩個其實是比較不像的

0:36:58.500,0:37:01.260
這兩個搞不好還比較像，因為它這邊有一個圈圈

0:37:01.260,0:37:03.600
它沒有圈圈，它這邊有一個勾勾

0:37:03.600,0:37:05.400
它有一個這樣的勾勾

0:37:05.400,0:37:07.220
我看它們兩個搞不好還比較像

0:37:07.220,0:37:09.800
對不對，你這邊再稍微彎曲一點

0:37:09.800,0:37:12.220
就變成 3 了，所以它們搞不好還比較像

0:37:12.220,0:37:15.940
但是，如果你把你的 data 通通倒出來的話

0:37:15.940,0:37:17.900
你會發現，這個 2

0:37:17.900,0:37:21.640
和這個 2 中間，它們有很多連續的型態

0:37:21.640,0:37:23.400
就是這個 2 稍微變一下變它

0:37:23.400,0:37:25.320
再變一下變它、變一下變它這樣

0:37:25.320,0:37:28.580
它和它中間有很多連續的變化

0:37:28.580,0:37:30.680
所以，可以從這種生物演化成這種生物

0:37:30.680,0:37:32.160
但是沒有辦法演化成這種生物

0:37:32.160,0:37:33.840
中間沒有過渡的型態

0:37:33.840,0:37:38.200
所以說，它們中間有很多

0:37:38.200,0:37:41.460
不直接相連的相似

0:37:41.460,0:37:45.760
它們中間有很多 stepping stone，可以讓它這樣跳過去

0:37:45.760,0:37:48.500
所以，如果根據 Smoothness Assumption 的話

0:37:48.500,0:37:50.560
你就可以得到說，這個東西

0:37:50.560,0:37:52.160
和這個東西是比較像的

0:37:52.160,0:37:55.160
這個東西和這個東西，它們中間沒有

0:37:55.160,0:37:57.600
過渡的型態，所以它們其實是比較不像的

0:37:57.600,0:37:59.540
它們其實不應該是屬於同一個 class

0:37:59.540,0:38:01.720
它們其實是屬於同一個 class

0:38:01.720,0:38:05.200
如果你看人臉辨識的話呢

0:38:05.200,0:38:07.560
其實也是一樣，一個人的

0:38:07.560,0:38:10.760
一個人，如果從他的左臉照一張相

0:38:10.760,0:38:13.120
跟右臉照一張相，那差很多

0:38:13.120,0:38:16.140
你拿這一張相片，跟另外一個人的

0:38:16.140,0:38:21.000
這邊是甚麼，正側面，這也是正側面

0:38:21.000,0:38:25.480
你拿另外一張一樣是眼睛朝左的相片來比較的話

0:38:25.480,0:38:27.680
我看還比這個像

0:38:27.680,0:38:30.460
還比較像這個眼睛朝左的相片

0:38:30.460,0:38:33.220
跟這個眼睛朝右的相片相比的話

0:38:33.220,0:38:36.760
但是，假設你收集到夠多的 unlabeled data 的話

0:38:36.760,0:38:41.000
你會找到說，這一張臉和這一張臉中間呢

0:38:41.000,0:38:43.680
有很多過渡的型態

0:38:43.680,0:38:47.440
所以，這一張臉跟這一張臉可能是同一個人的臉

0:38:48.140,0:38:50.660
或者是，在這個

0:38:50.660,0:38:52.920
這招在文件分類上面呢

0:38:52.920,0:38:55.320
可能是會非常有用的

0:38:55.320,0:38:57.560
為甚麼呢？假設你現在

0:38:57.560,0:39:02.280
要分天文學跟旅遊的文章

0:39:02.280,0:39:05.640
那天文學的文章有一個它固定的 word distribution

0:39:05.640,0:39:08.940
比如說，它會出現這個

0:39:08.940,0:39:13.400
asteroid, bright，那如果旅遊的文章，它會出現

0:39:13.400,0:39:15.600
yellow stone 等等

0:39:15.600,0:39:20.780
如果今天，你的 unlabeled data

0:39:20.780,0:39:24.440
如果今天你的 unlabeled data 跟你的 labeled data

0:39:24.440,0:39:26.800
是有 overlapped 的

0:39:26.800,0:39:31.000
那你就很容易可以處理這個問題

0:39:31.000,0:39:32.580
但是，在真實的情況下

0:39:32.580,0:39:36.360
你的 unlabeled data 跟 labeled data，它們中間可能

0:39:36.360,0:39:38.480
沒有任何 overlapped 的 word

0:39:38.480,0:39:41.980
為甚麼呢？因為世界上的 word 很多

0:39:41.980,0:39:43.540
一篇文章裡面，你往往

0:39:43.540,0:39:45.220
你的詞彙不會太多

0:39:45.220,0:39:46.620
但是，世界上可能 word 很多

0:39:46.620,0:39:49.980
所以，每一篇文章它裡面的詞彙，其實是非常 sparse 的

0:39:49.980,0:39:51.660
它只提到非常少量的 word

0:39:51.660,0:39:54.860
所以，你拿兩篇文章出來，它們中間

0:39:54.860,0:39:58.140
有重複的 word 的比例，是沒有那麼多的

0:39:58.140,0:40:00.740
所以，很有可能你的 data

0:40:00.740,0:40:05.680
你的 unlabeled data 跟你的 labeled data 中間

0:40:05.680,0:40:07.300
是沒有任何 overlap 的

0:40:07.300,0:40:11.220
但是，如果你 collect 到夠多的 unlabeled data 的話

0:40:11.220,0:40:14.340
如果你 collect 到夠多的 unlabeled data 的話

0:40:14.340,0:40:18.580
你就可以說，這個是 d1 跟 d5 像

0:40:18.580,0:40:23.060
d5 又跟 d6 像，這個像就可以一路 propagate 過去

0:40:23.060,0:40:26.160
知道說 d1 跟 d3 一類，那 d2 跟 d9 像

0:40:26.160,0:40:29.600
d9 跟 d8 像，那你就會得到 d2 跟 d4 像

0:40:29.600,0:40:32.680
這個像也可以一路 propagate 過去

0:40:33.380,0:40:36.500
那如何實踐這個 Smoothness Assumption  呢

0:40:36.500,0:40:37.860
最簡單的方法這個

0:40:37.860,0:40:42.920
呃，電腦卡住了，沒有卡住

0:40:42.920,0:40:45.520
又回來了

0:40:45.520,0:40:47.400
是 Cluster and then Label

0:40:47.400,0:40:50.260
這個方法太簡單了，沒什麼可以講的

0:40:50.260,0:40:53.740
我們現在 data distribution 長這個樣子

0:40:53.740,0:40:56.960
橙色是 class 1 ，綠色是 class 2

0:40:56.960,0:40:59.600
藍色是 unlabeled data

0:40:59.600,0:41:02.620
接下來，你就做一下 clustering

0:41:02.620,0:41:04.980
你把這些所有的 data 拿來做 clustering

0:41:04.980,0:41:07.120
你可能就分成 3 個 class

0:41:07.120,0:41:09.940
3 個 class，3 個 cluster

0:41:09.940,0:41:12.040
然後，你就看出 cluster 1 裡面呢

0:41:12.040,0:41:14.920
橙色 class 1 的 label data 最多

0:41:14.920,0:41:18.440
所以，cluster 1 裡面所有的 data 都算 class 1

0:41:18.440,0:41:21.100
那 cluster 2 跟 cluster 3 都算 class 2

0:41:21.160,0:41:23.820
就結束了

0:41:23.820,0:41:25.480
那你把這些 data 拿去 learn 就結束了

0:41:25.480,0:41:27.800
那這個方法不一定有用，尤其是

0:41:27.800,0:41:30.860
在你的作業三裡面，你可以 implement 這個方法

0:41:30.860,0:41:33.880
因為我們只說，助教只說要實踐兩種方法

0:41:33.880,0:41:36.060
沒有說做完以後一定要進步嘛，所以

0:41:39.480,0:41:41.860
真的是這樣的阿，如果你今天在

0:41:41.860,0:41:43.480
就是說助教只提供兩個方法

0:41:43.480,0:41:45.100
一個是 self-learning

0:41:45.100,0:41:46.840
我們自己試過啦，是一定會進步的

0:41:46.840,0:41:48.760
如果你今天要做 Cluster and then Label

0:41:48.760,0:41:50.520
你這個 cluster 要很強

0:41:51.120,0:41:53.160
因為只有這一招 work 的假設就是

0:41:53.160,0:41:55.460
你可以把同一個 class 的東西 cluster 在一起

0:41:55.460,0:41:56.940
可是在 image 裡面

0:41:56.940,0:41:58.680
你要把同一個 class 的東西 cluster 在一起

0:41:58.680,0:41:59.820
其實是沒那麼容易的

0:41:59.820,0:42:00.820
我們之前有講過說

0:42:00.820,0:42:03.380
我們在前面的投影片講為甚麼要用 deep learning 的時候

0:42:03.380,0:42:05.980
不同 class，可能會長得很像

0:42:05.980,0:42:07.400
同一個 class，可能會長得很不像

0:42:07.400,0:42:10.100
你單純只用 pixel 來做 clustering

0:42:10.100,0:42:11.460
你結果八成是會壞掉

0:42:11.460,0:42:14.020
你沒有辦法把同一個 class 的 data cluster 在一起

0:42:14.020,0:42:16.260
那 unlabeled data 沒有什麼幫助

0:42:16.260,0:42:17.360
做出來就是會壞掉

0:42:17.360,0:42:20.020
所以，如果你要讓 Cluster and then Label 這個方法

0:42:20.020,0:42:21.540
有用，你的 cluster 要很強

0:42:21.540,0:42:24.620
你要有很好的方法，來描述你的一張 image

0:42:24.620,0:42:29.240
在我們自己試的時候，我們會用 Deep Autoencoder

0:42:29.240,0:42:31.380
我們還沒有講 Deep Autoencoder，所以

0:42:31.380,0:42:34.240
如果你覺得沒有辦法這實作，這個也是正常的

0:42:34.240,0:42:35.900
我們是用 Deep Autoencoder call feature

0:42:35.900,0:42:38.260
然後再 call clustering，這樣才會 work

0:42:38.260,0:42:40.840
如果你不這樣做的話，我覺得應該是不會 work 的

0:42:41.500,0:42:44.500
但是，你還是可以直接用 pixel 做 cluster

0:42:45.300,0:42:48.880
剛才講這個比較直覺的做法

0:42:48.880,0:42:53.180
另外一個方法是引入 Graph structure

0:42:53.180,0:42:56.600
我們用 Graph structure 呢

0:42:56.600,0:43:01.500
來表達 connected by a high density path 這件事情

0:43:01.500,0:43:03.520
就是說，我們現在呢

0:43:03.520,0:43:10.720
把所有的 data points，都建成一個 graph

0:43:10.720,0:43:13.960
每一個 data point, x，就是這個圖上的一個點

0:43:13.960,0:43:18.000
你要想辦法算它們之間的 singularity

0:43:18.000,0:43:20.840
你要想辦法它們之間的 edge 建出來

0:43:20.840,0:43:22.600
有了這個 graph 以後

0:43:22.600,0:43:26.560
你就可以說所謂的 high density path 的意思就是說

0:43:26.560,0:43:29.180
如果今天有兩個點

0:43:29.180,0:43:31.820
它們在這個 graph 上面是相連的

0:43:31.820,0:43:34.180
是走得到的

0:43:34.180,0:43:36.820
它們就是同一個 class，如果沒有相連

0:43:36.820,0:43:38.400
就算是實際上的距離呢

0:43:38.400,0:43:40.460
也不算太遠，那你也走不到

0:43:41.340,0:43:43.420
那怎麼建一個 graph

0:43:43.420,0:43:44.840
有些時候

0:43:44.840,0:43:48.040
這個 graph 的 representation 是很自然就可以得到

0:43:48.040,0:43:50.520
舉例來說，假設你現在要做的是

0:43:50.520,0:43:52.300
這個網頁的分類

0:43:52.300,0:43:55.820
而你有記錄網頁有網頁之間的 hyperlink

0:43:55.820,0:44:00.220
hyperlink 自然地就告訴你說，這些網頁先是如何連結的

0:44:00.220,0:44:02.720
或者是，你現在要做的是論文的分類

0:44:02.720,0:44:05.460
而論文和論文之間有引用的關係

0:44:05.460,0:44:08.400
這個引用的關係式也是另外一種 graph 的 edge

0:44:08.400,0:44:12.300
它也可以很自然地把這種圖畫出來給你

0:44:12.300,0:44:17.980
當然有時候，你需要自己想辦法建這個 graph

0:44:17.980,0:44:20.100
怎麼自己想辦法建這個 graph？

0:44:20.100,0:44:22.040
其實這邊

0:44:22.040,0:44:24.300
你的 graph 的好壞對你的結果

0:44:24.300,0:44:26.260
影響是非常的 critical 的

0:44:26.260,0:44:28.580
不過這個地方就非常的 heuristic

0:44:28.580,0:44:30.240
就是憑著經驗和直覺

0:44:30.240,0:44:31.380
你就覺得你怎麼做比較好

0:44:31.380,0:44:34.280
就選擇你覺得爽的方法做就是了

0:44:34.280,0:44:37.420
那這邊通常的做法是這個樣子

0:44:37.420,0:44:39.660
你要先定義兩個 object 之間

0:44:39.660,0:44:42.080
你怎麼算它們的相似度

0:44:42.080,0:44:44.500
影像的話，你可以 base on pixel 算相似度

0:44:44.500,0:44:45.860
performance 不太好

0:44:45.860,0:44:49.240
因為 base on autoencoder 抽出來的 feature 算相似度

0:44:49.240,0:44:51.380
可能 performance 就會比較好

0:44:51.380,0:44:53.840
算完相似度以後

0:44:53.840,0:44:55.940
你就可以建 graph 了

0:44:55.940,0:44:58.440
那 graph 有很多種，比如說你可以建

0:44:58.440,0:45:00.460
K nearest Neighbor 的 graph

0:45:00.460,0:45:02.500
所謂 K nearest Neighbor 的 graph，意思是說

0:45:02.500,0:45:06.540
我現在有一大堆的 data

0:45:06.540,0:45:11.140
那 data 和 data 之間，我都可以算出它的相似度

0:45:11.140,0:45:13.500
我就說

0:45:13.500,0:45:15.720
我 K nearest Neighbor 設 k = 3

0:45:15.720,0:45:19.520
那每一個 point 都跟它最近的、相似度最像的

0:45:19.520,0:45:21.660
三個點做相連

0:45:21.660,0:45:23.220
或者，你可以做

0:45:23.220,0:45:26.880
e-Neighborhood，所謂 e-Neighborhood 是甚麼意思呢

0:45:26.880,0:45:29.820
是說，每一個點

0:45:29.820,0:45:32.780
只有跟它相似度超過某一個 threshold

0:45:32.780,0:45:34.900
跟它相似大於 1 的那些點

0:45:34.900,0:45:36.360
才不會被黏起來

0:45:36.360,0:45:38.140
這都是很直覺的

0:45:38.140,0:45:40.940
那所謂的相連也不是只有

0:45:40.940,0:45:43.620
所謂的 edge 也不是只有相連和不相連

0:45:43.620,0:45:45.540
這樣 binary 的選擇而已

0:45:45.540,0:45:48.660
你可以給 edge 一些 weight

0:45:48.660,0:45:51.600
你可以讓你的 edge 跟你的

0:45:51.600,0:45:54.920
要被連接起來的兩個 data point 之間

0:45:54.920,0:45:57.180
相似度是成正比的

0:45:57.180,0:46:01.220
怎麼定義這個相似度呢

0:46:01.460,0:46:04.200
我會建議比較好的選擇

0:46:04.200,0:46:09.780
其實是用 RBM function 來訂這個相似度

0:46:09.780,0:46:13.120
然後，我就發現說我這邊寫錯了

0:46:13.120,0:46:16.600
這邊這個應該是 x 才對啊

0:46:16.600,0:46:21.400
這個其實應該是 x

0:46:21.400,0:46:25.100
怎麼算這個 function

0:46:25.100,0:46:26.380
你可以先算說

0:46:26.380,0:46:30.160
x^i 跟 x^j 如果你都把它們用 vector 來表示的話

0:46:30.160,0:46:32.140
算它們的 Euclidean distance

0:46:32.140,0:46:35.420
前面乘一個參數

0:46:35.420,0:46:38.640
然後，前面乘一個負號，再取 exponential

0:46:38.640,0:46:41.640
那其實取 exponential 這件事情呢

0:46:41.640,0:46:45.800
是我覺得還滿必要的，在經驗上

0:46:45.800,0:46:50.300
用這樣的 function，可以給你比較好的 performance

0:46:50.300,0:46:52.480
為甚麼用這樣子的 function

0:46:52.480,0:46:54.220
會給你比較好的 performance 呢？

0:46:54.220,0:46:58.480
因為你想想看這個 function，它下降的速度是非常快的

0:46:58.480,0:47:00.820
因為它有取 exponential

0:47:00.820,0:47:04.740
所以，只有當 x^i 跟 x^j 非常靠近的時候

0:47:04.740,0:47:09.180
它的 singularity 才會大

0:47:09.180,0:47:10.980
只要距離稍微遠一點

0:47:10.980,0:47:14.180
singularity 就會下降很快，變得很小

0:47:14.180,0:47:17.780
也就是說，如果你用這種 RBM function 的話

0:47:17.780,0:47:20.740
你才能夠製造，比如說，像這個圖上

0:47:20.740,0:47:23.500
這邊有兩個橙色的點，是距離很近的

0:47:23.500,0:47:26.140
這個綠色的點，其實它跟橙色的點的距離

0:47:26.140,0:47:29.540
也蠻近，只是稍微遠一點

0:47:29.540,0:47:31.240
但是，你用這種 exponential 的話

0:47:31.240,0:47:33.240
每一個點都只跟非常近的點連

0:47:33.240,0:47:35.200
跟它稍微遠一點，它就不連了

0:47:35.200,0:47:37.520
你要有這樣子的機制

0:47:37.520,0:47:39.300
才可以避免你連到這種

0:47:39.300,0:47:45.600
跨海溝的這種 link 這樣

0:47:45.600,0:47:49.360
所以如果你有 exponential，通常效果是會比較好的

0:47:51.880,0:47:55.680
所以，graph-based 的方法，它的精神

0:47:55.680,0:47:56.980
是這樣子的

0:47:56.980,0:47:59.700
如果我們現在，在這個 graph 上面

0:47:59.700,0:48:02.060
我有一些 labeled data

0:48:02.060,0:48:03.600
比如說，在這個 graph 上面

0:48:03.600,0:48:07.680
我們已經知道說，這筆 data 是屬於 class 1

0:48:07.680,0:48:09.480
這筆 data 是屬於 class 1

0:48:09.480,0:48:12.200
那跟它們有相連的

0:48:12.200,0:48:15.980
那些 data point，它是屬於 class 1 的機率也就會上升

0:48:15.980,0:48:19.080
比如說，這筆 data 它屬於 class 1 的機率也會上升

0:48:19.080,0:48:22.700
這筆 data 它屬於 class 1 的機率也會上升

0:48:22.700,0:48:26.540
所以，每一筆 data 它會去影響它的鄰居

0:48:26.540,0:48:29.780
光是會影響它的鄰居是不夠的

0:48:29.780,0:48:32.880
如果你只考慮光會影響它的鄰居的話，其實可能

0:48:32.880,0:48:34.340
幫助不會太大

0:48:34.340,0:48:37.460
為甚麼呢？因為如果說它們相連

0:48:37.460,0:48:39.860
本來就很像

0:48:39.860,0:48:41.120
你 train 一個 model

0:48:41.120,0:48:43.700
input 很像的東西，output 本來就很像的東西

0:48:44.140,0:48:46.320
所以，幫助不會太大

0:48:46.320,0:48:49.180
那 Graph-based 的 approach 真正會有幫助它的

0:48:49.180,0:48:50.980
這個醍醐味就是

0:48:50.980,0:48:55.800
它的這個 class 是會傳遞的

0:48:55.800,0:48:58.620
也就是說，本來 class 1

0:48:58.620,0:49:00.780
只有這個點有跟 class 1 相連

0:49:00.780,0:49:02.180
所以，它會變得比較像 class 1

0:49:02.180,0:49:06.360
但是，這件事情會像傳染病一樣傳遞過去

0:49:06.360,0:49:08.020
所以，這個點雖然它沒有

0:49:08.020,0:49:10.320
這個 data point，它雖然沒有真的跟任何

0:49:10.320,0:49:12.260
真的是 class 1 的點相連

0:49:12.260,0:49:14.860
但是，因為這件事情

0:49:14.860,0:49:16.880
像 class 1 這件事情是會感染的

0:49:16.880,0:49:20.920
所以，這件事情也會透過這個 graph 的 link 傳遞過來

0:49:20.920,0:49:24.260
所以，舉例來說，我們如果看這個例子

0:49:24.260,0:49:27.280
你把你所有的 data point 都建成一個 graph

0:49:27.280,0:49:30.440
當然這個是比較理想的例子

0:49:30.440,0:49:33.140
然後，你有一個藍色的點

0:49:33.140,0:49:35.320
是你 label 一筆 data 是屬於 class 1

0:49:35.320,0:49:38.120
你 label 一筆 data 是屬於 class 2

0:49:38.120,0:49:39.620
經過  Graph-based approach

0:49:39.620,0:49:42.760
如果你的 graph 是建得這麼漂亮的話

0:49:42.760,0:49:45.260
這邊就會通通是藍色

0:49:45.260,0:49:47.700
這邊就會通通是紅色

0:49:47.700,0:49:50.480
雖然說，這一點其實跟它的尾巴其實沒有接在一起

0:49:50.480,0:49:55.320
但是紅色這個 class 這件事情會一路 propagate 過去

0:49:55.320,0:49:56.960
會一路 propagate 過去

0:49:56.960,0:49:59.120
那如果你要讓 graph-based 這種

0:49:59.120,0:50:01.480
這種 Semi-supervised 的方法有用

0:50:01.480,0:50:03.460
你的一個 critical 的方法是你的 data 要夠多

0:50:03.460,0:50:05.600
如果你 data 不夠多

0:50:05.600,0:50:08.620
這個地方你沒收集到 data，這個點斷掉了

0:50:08.620,0:50:11.500
那你這 information就傳不過去

0:50:13.540,0:50:15.020
那剛才是

0:50:15.020,0:50:19.060
定性的說一下說，怎麼把

0:50:19.060,0:50:22.460
怎麼使用這個 graph，接下來是要說怎麼

0:50:22.460,0:50:24.920
定量的使用這個 graph

0:50:24.920,0:50:27.040
這個定量的使用方式

0:50:27.040,0:50:30.440
是我們在這個 graph 的 structure 上面

0:50:30.440,0:50:32.340
定一個東西，叫做

0:50:32.340,0:50:35.060
label 的 smoothness

0:50:35.060,0:50:37.540
我們會定義說，今天這個 label

0:50:37.540,0:50:42.840
有多符合我們剛才說的 Smoothness Assumption 的假設

0:50:43.960,0:50:45.860
怎麼定這個東西呢？

0:50:45.860,0:50:48.480
如果我們看這兩個例子

0:50:48.480,0:50:50.700
在這兩個例子裡面都有

0:50:50.700,0:50:52.080
4 個 data point

0:50:52.080,0:50:55.960
那 data point 和 data point 之間連接的數字

0:50:55.960,0:50:58.920
代表了這個 edge 的 weight

0:50:58.920,0:51:02.320
我們說，假設在左邊這個例子

0:51:02.320,0:51:04.900
左邊和右邊這兩個 graph 是一樣的，但是

0:51:04.900,0:51:08.440
我們現在給每一個 data 不同的 label

0:51:08.440,0:51:10.080
假設在這個 class 裡面

0:51:10.080,0:51:13.080
你給它的 label 是 1、1、1、0

0:51:13.080,0:51:15.800
再這個 example 裡面

0:51:15.800,0:51:18.100
給它的例子是 0、1、1、0

0:51:18.100,0:51:22.920
那誰比較 smooth 呢

0:51:22.920,0:51:25.160
給大家一秒鐘的時間考慮一下

0:51:25.980,0:51:28.760
你覺得左邊比較 smooth 的同學舉手

0:51:29.440,0:51:31.560
手放下

0:51:31.560,0:51:34.460
你覺得右邊比較 smooth 的同學舉手

0:51:34.460,0:51:36.740
沒有人，所以你覺得

0:51:36.740,0:51:38.460
多數人都覺得左邊比較 smooth

0:51:38.460,0:51:40.760
所以，大家的看法是非常一致的

0:51:40.760,0:51:43.500
左邊、這個三角形的地方都是 1

0:51:43.500,0:51:44.540
這邊是 0

0:51:44.540,0:51:46.960
這邊三角的地方有 0、有 1

0:51:46.960,0:51:48.740
這邊是 0，感覺這個比較

0:51:48.740,0:51:50.840
不符合 Smoothness Assumption 的假設

0:51:50.840,0:51:52.000
這個比較符合

0:51:52.000,0:51:54.560
但是，我們需要用一個數字來

0:51:54.560,0:51:57.140
定量的描述它說

0:51:57.140,0:51:59.220
它有多 smooth

0:51:59.220,0:52:00.960
那常見的作法是這個樣子

0:52:00.960,0:52:02.980
常見的作法是，你寫一個式子

0:52:02.980,0:52:06.060
這個式子你可以這樣寫，我們考慮

0:52:06.060,0:52:09.700
兩兩、有相連的這個

0:52:09.700,0:52:13.700
point，兩兩拿出來

0:52:13.700,0:52:17.680
summation over 所有的 data pair (i, j)

0:52:17.680,0:52:21.220
然後，我們計算 i, j 之間的 weight

0:52:21.220,0:52:24.480
跟 i 的 label

0:52:24.480,0:52:28.240
減掉 j 的 label 的平方

0:52:28.240,0:52:32.320
這邊是 summation over 所有的 data

0:52:32.320,0:52:36.480
不管它現在是有 label，還是沒有 label

0:52:36.480,0:52:39.260
所以你看左邊這個 case

0:52:39.260,0:52:41.540
(1 - 1) 的平方是 0

0:52:41.540,0:52:43.940
(1 - 1) 的平方是 0

0:52:43.940,0:52:46.180
(1 - 1) 的平方是 0

0:52:46.180,0:52:49.580
只有這邊是 (1 - 0) 的平方是 1

0:52:49.580,0:52:53.300
所以，你在 summation over 所有的 data pair 的時候

0:52:53.300,0:52:55.860
你只需要考慮 x^3 跟 x^4 這邊

0:52:56.260,0:52:58.900
那 (y^i - y^j) 的平方是 1

0:52:58.900,0:53:03.560
那 w (下標i, j) 是 1，再除 0.5，除 0.5 這件事情只是

0:53:03.560,0:53:06.560
為了等一下做某一個計算的時候

0:53:06.560,0:53:10.440
比較方便，它其實沒有甚麼真正的效用

0:53:10.440,0:53:13.140
這邊乘一個 0.5

0:53:13.140,0:53:15.280
最後得到的這個 smoothness

0:53:15.280,0:53:16.360
有多 smooth 呢？

0:53:16.360,0:53:19.580
evaluation 就是 0.5

0:53:21.660,0:53:24.060
那如果是右邊這一個 case

0:53:24.060,0:53:26.180
你自己回去算一下，到底有沒有算錯

0:53:26.180,0:53:31.580
根據這個定義，它算出來的 smoothness = 3

0:53:31.580,0:53:34.940
所以，這個值越小

0:53:34.940,0:53:37.540
它越 smooth

0:53:37.540,0:53:40.120
所以，你會希望你得出來的 label

0:53:40.120,0:53:43.320
根據這個 smoothness 的定義

0:53:43.320,0:53:47.680
它算出來，越小越好

0:53:47.680,0:53:51.100
其實，這邊可以很快告訴大家一件事情，這一項

0:53:51.100,0:53:54.020
可以稍微整理一下

0:53:54.020,0:53:59.940
寫成一個比較簡潔的式子，怎麼寫呢？

0:53:59.940,0:54:03.940
我們把 y 串成一個 vector

0:54:03.940,0:54:07.640
現在，y 是包括 labeled data，也包括 unlabeled data

0:54:07.640,0:54:10.240
每一筆  labeled data 跟 unlabeled data

0:54:10.240,0:54:11.460
都出一個值給你

0:54:11.460,0:54:14.580
所以，你現在有 (R + U) 個 dimension

0:54:14.580,0:54:19.620
串成一個 vector，寫成 y，我們粗體字來表示一個 vector

0:54:19.620,0:54:21.360
如果你這樣寫的話

0:54:21.360,0:54:23.980
這一個式子，可以寫成

0:54:23.980,0:54:28.080
y 這個 vector 的 transpose，乘上一個 matrix，叫做 L

0:54:28.080,0:54:31.020
再乘上 y

0:54:31.020,0:54:33.520
那這個 L，它是一個

0:54:33.520,0:54:38.340
因為 y 的 dimension 是 (R + U)

0:54:38.340,0:54:42.320
所以，這個 L 是一個 (R +U) * (R + U) 的 matrix

0:54:42.320,0:54:45.300
那這個 L，它是有名字的

0:54:45.300,0:54:48.300
它叫 Graph Laplacian，你可能有聽過這個名字

0:54:48.300,0:54:50.700
Graph Laplacian 就是指這個 L

0:54:50.700,0:54:52.740
這個是它的名字

0:54:52.740,0:54:54.340
那這個 L 的定義是甚麼呢？

0:54:54.340,0:54:59.340
它寫成兩個 matrix 的相減，就是 D - W

0:54:59.340,0:55:03.260
我們現在看 W 是什麼，W 就是

0:55:04.100,0:55:08.260
你把這些 data point 阿

0:55:08.260,0:55:12.760
兩兩之間 weight 的 connection 的關係建成一個 matrix

0:55:12.760,0:55:15.480
就是 W，這邊的

0:55:15.480,0:55:19.460
4 個 row 跟 4 個 column，分別就代表了

0:55:19.460,0:55:21.500
data x^1 到 data x^4

0:55:21.500,0:55:23.800
也就是說，你看現在 x^1 跟 x^2 之間的

0:55:23.800,0:55:26.360
connection 的 weight 是 2

0:55:26.360,0:55:29.640
這個 (1, 2) 這邊就是 2

0:55:29.640,0:55:31.700
那 x^1 跟 x^3 的 connection 是 3

0:55:31.700,0:55:34.420
(1, 3) 這邊就是 3，以此類推

0:55:34.420,0:55:37.380
就建出一個 matrix, W

0:55:37.380,0:55:39.880
D 是甚麼呢？ D 是這樣的

0:55:39.880,0:55:42.640
你把 W 的每一個 row

0:55:42.640,0:55:46.280
每一個 row 合起來，你把第一個 row：2 + 3 合起來

0:55:46.280,0:55:48.640
放在 diagonal 的地方，變成 5

0:55:48.640,0:55:53.380
2 + 1 變成 3，3 + 1 + 1 變成 5，1 變成 1

0:55:53.380,0:55:56.200
然後，把這些合起來的值放在 diagonal 的地方就是 D

0:55:56.200,0:55:58.900
然後，你把 D - W 就得到 Laplacian

0:55:58.900,0:56:01.920
你再把它放在這邊

0:56:01.920,0:56:04.560
這個左邊的式子，就會等於右邊的式子

0:56:04.560,0:56:07.360
你可能沒有辦法一下子看出來說

0:56:07.360,0:56:09.960
為什麼左邊的式子等於右邊的式子

0:56:09.960,0:56:13.080
這個證明其實很無聊

0:56:13.080,0:56:14.800
正確講你也不會覺得特別有趣

0:56:14.800,0:56:16.560
你就回去，把這個東西展開

0:56:16.560,0:56:19.000
你就知道左邊其實是等於右邊的

0:56:20.780,0:56:24.200
所以，現在我們知道這件事情了

0:56:24.200,0:56:26.720
我們可以用這一個式子

0:56:26.720,0:56:33.220
來 evaluate 說，我們現在得到的 label有多 smooth

0:56:33.220,0:56:34.720
那在這個式子裡面

0:56:34.720,0:56:41.360
我們會看到有 y，那這個 y 是 label

0:56:41.360,0:56:45.580
這個 label 的值，你的 neural network output 的值是

0:56:45.580,0:56:49.080
取決於你的 network 的 parameter，所以這一項

0:56:49.080,0:56:51.720
其實是 network 的 dependent

0:56:51.720,0:56:55.640
所以，你要把 graph 的 information

0:56:55.640,0:56:57.600
考慮到 neural network 的 training 的時候

0:56:57.600,0:56:59.940
你要做的事情，其實就是在

0:56:59.940,0:57:02.300
原來的 loss function 裡面，加一項

0:57:02.300,0:57:05.420
你原來的 loss function 是考慮 cross entropy 之類的

0:57:05.420,0:57:09.960
你就加另外一項，你加這一項是 smoothness 的值

0:57:09.960,0:57:13.860
乘上某一個你想要調的參數，λ

0:57:13.860,0:57:17.940
那這後面這一項，它其實就象徵了一個

0:57:17.940,0:57:22.160
Regularization，它就像是一個 Regularization 的 term

0:57:22.160,0:57:26.860
你不只要調你的參數讓你的那些 labeled data 的

0:57:26.860,0:57:29.860
你的 neural network 在那些 labeled data 的 output

0:57:29.860,0:57:31.840
跟真正的 label，越接近越好

0:57:31.840,0:57:34.160
你同時還要做到說

0:57:34.160,0:57:36.420
你 output 的這些 label

0:57:36.420,0:57:39.040
不管是在 labeled data 或 unlabeled data 上面

0:57:39.040,0:57:41.400
它都符合 Smoothness Assumption 的假設

0:57:41.400,0:57:44.300
Smoothness Assumption 的假設是由這個 x

0:57:44.300,0:57:47.600
所衡量出來的，所以你同時 minimize 這項

0:57:47.600,0:57:50.840
也要同時 minimize 這項，你可能會說，這件事怎麼做？

0:57:50.840,0:57:53.320
這件事沒有甚麼好講的

0:57:53.320,0:57:55.480
你就算一下它的 gradient

0:57:55.480,0:58:00.300
然後，做 gradient descent 就可以了

0:58:00.300,0:58:03.680
那其實，你也要算 smoothness 時候，不一定要算在

0:58:03.680,0:58:06.840
不一定要算在 output 的地方

0:58:06.840,0:58:09.660
不一定要算在 output 的地方，如果你今天是一個

0:58:09.660,0:58:11.080
deep neural network 的話

0:58:11.080,0:58:14.520
你可以把你的 smoothness 放在 network 的任何地方

0:58:14.520,0:58:16.720
你可以假設你的 output 是 smooth

0:58:16.720,0:58:18.480
你也可以同時說

0:58:18.480,0:58:20.780
我把某一個 hidden layer 接出來

0:58:20.780,0:58:23.880
再乘上一些別的 transform

0:58:23.880,0:58:25.480
它也要 smooth

0:58:25.480,0:58:28.320
你也可以說，每一個 hidden layer 的 output

0:58:28.320,0:58:30.960
都要是 smooth 的，都可以

0:58:30.960,0:58:34.040
你可以同時把這些 smooth 
通通都加到 neural network 上面去

0:58:34.040,0:58:37.460
最後呢，這個

0:58:37.460,0:58:40.620
最後的方法是 Better Representation

0:58:40.620,0:58:44.040
這個方法的精神是：是去蕪存菁、化繁為簡

0:58:44.040,0:58:47.080
這一部分我們會等到 unsupervised learning 的時候再講

0:58:47.080,0:58:50.320
那它的精神是這樣子的

0:58:50.320,0:58:53.680
我們觀察到的世界其實是比較複雜的

0:58:53.680,0:58:55.660
我們在我們觀察到的世界背後

0:58:55.660,0:58:58.400
其實有一些比較簡單的 vector

0:58:58.400,0:59:03.600
比較簡單的東西，在操控我們這個複雜的世界

0:59:03.600,0:59:05.880
所以，你只要能夠看透這個世界的假象

0:59:05.880,0:59:09.620
支持它的核心的話，你就可以讓 training 變得比較容易

0:59:09.620,0:59:14.600
舉例來說，這個圖是出自神鵰俠侶

0:59:14.600,0:59:16.380
這個大家知道什麼意思嗎

0:59:16.380,0:59:19.320
這個是楊過，他手上拿了一個剪刀

0:59:19.320,0:59:21.900
這個是樊一翁，這個是他的鬍子

0:59:21.900,0:59:26.140
然後，楊過跟樊一翁打的時候

0:59:26.140,0:59:28.380
他說我可以在三招之內，就剪掉你的鬍子

0:59:28.380,0:59:30.980
大家都不相信，楊過後來就真的在三招之內

0:59:30.980,0:59:33.500
剪掉他的鬍子，為甚麼呢？因為

0:59:33.500,0:59:36.400
楊過觀察到說，鬍子只是一個假相

0:59:36.400,0:59:39.440
雖然，鬍子的變化是比較複雜的

0:59:39.440,0:59:41.200
但是，鬍子是受到頭所操控

0:59:41.200,0:59:44.020
頭的變化是有限的，所以他看透這一件事情

0:59:44.020,0:59:46.080
以後，他就可以把鬍子剪掉

0:59:46.080,0:59:48.420
所以說，樊一翁他的鬍子就是 observation

0:59:48.420,0:59:52.800
而他的頭，就是你要找的 Better Representation

0:59:52.800,0:59:54.980
那就是我們下一堂課要講的東西

0:59:54.980,0:59:56.600
我們在這邊休息10分鐘

0:59:56.600,0:59:59.200
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
