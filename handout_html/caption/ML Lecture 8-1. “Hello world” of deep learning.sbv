0:00:00.000,0:00:02.800
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.860,0:00:06.100
如果實作的話，要怎麼做呢？

0:00:06.720,0:00:11.900
那我今天要教大家的 toolkit 阿，是 Keras

0:00:11.900,0:00:14.880
那你可能想說，怎麼不教 TensorFlow

0:00:14.880,0:00:16.420
TensorFlow 的星星數

0:00:16.420,0:00:20.160
不是 deep learning 的 toolkit 裡面最多的嗎？

0:00:20.160,0:00:22.440
應該要教 TensorFlow

0:00:22.440,0:00:24.100
其實是這樣子的

0:00:24.200,0:00:26.160
在另外一門 deep learning 的課裡面，

0:00:26.160,0:00:27.400
我們教的是 TensorFlow

0:00:27.400,0:00:30.980
其實，TensorFlow 沒有那麼好用

0:00:31.420,0:00:34.080
該怎麼說呢，應該是這樣

0:00:34.240,0:00:37.980
TensorFlow 跟另外一個跟他比較相近的 toolkit，Theano

0:00:37.985,0:00:40.795
它們是非常的 flexible

0:00:40.845,0:00:44.235
也就是說，你甚至可以把它想成是一座微分器

0:00:44.235,0:00:47.315
它甚至

0:00:47.320,0:00:49.580
它完全可以做 deep leaning 以外的事情

0:00:49.580,0:00:53.140
它做的事情就是幫你算微分

0:00:53.360,0:00:55.540
那它把微分的值算給你以後

0:00:55.540,0:00:58.240
就可以拿去做 Gradient Descent

0:00:59.080,0:01:01.360
所以，它非常的 flexible

0:01:01.360,0:01:05.320
那這麼 flexible 的 toolkit 學起來，是有一些難度的

0:01:05.320,0:01:07.820
至少我覺得，比如我們在接下來的課裡面

0:01:07.820,0:01:09.500
大概有半個小時的時間

0:01:09.500,0:01:12.900
那你沒有辦法在半個小時以內，精通

0:01:12.900,0:01:14.980
熟悉這個 toolkit

0:01:15.080,0:01:18.240
但是，另外一個 toolkit，Keras 呢

0:01:18.240,0:01:19.295
我覺得

0:01:19.300,0:01:22.040
你是可以在數十分鐘內

0:01:22.040,0:01:23.980
就精通，就可以非常熟悉它

0:01:23.980,0:01:26.980
然後，用它來 implement 一個自己的 deep learning

0:01:27.220,0:01:30.700
那這個 Keras，其實是 TensorFlow 跟 Theano

0:01:31.015,0:01:32.125
interface

0:01:32.720,0:01:35.960
所以，就有人覺得說，TensorFlow 其實也沒那麼好用

0:01:35.960,0:01:39.940
所以，它搭了 TensorFlow 的 interface

0:01:39.960,0:01:41.920
就叫做 Keras

0:01:41.920,0:01:45.920
所以，你用 Keras 其實就等於你是在用 TensorFlow

0:01:45.920,0:01:47.240
只是有人幫你把

0:01:47.360,0:01:49.940
操縱 TensorFlow 那件事情，先幫你寫好

0:01:49.940,0:01:52.780
那其實它比較好學

0:01:52.780,0:01:56.940
而且，其實它也有足夠的彈性

0:01:56.940,0:01:58.140
如果，你想要做的事情

0:01:58.140,0:02:01.120
多數你想要做的，除非你想要做 deep learning 的研究

0:02:01.120,0:02:03.060
你想要自己兜奇奇怪怪的 network，不然

0:02:03.120,0:02:08.040
多數你可以想到的 network，這裡面都有現成

0:02:08.120,0:02:11.160
現成的 function 可以 call 了

0:02:11.160,0:02:14.580
而且它背後就是 TensorFlow 或是 Theano

0:02:14.580,0:02:16.660
所以，你永遠可以去

0:02:17.060,0:02:19.860
就是有一天，如果你想更精進自己的

0:02:20.040,0:02:21.340
能力的話

0:02:21.340,0:02:23.820
你永遠可以去改 Keras 背後的 TensorFlow 的 code

0:02:23.820,0:02:27.480
然後，做更厲害的事情

0:02:27.480,0:02:31.740
其實，這邊有一個，我知道一個小道消息是

0:02:31.800,0:02:35.575
因為 Keras 的作者，他其實是在 Google 工作

0:02:35.580,0:02:40.240
所以，據說 Keras 即將要變成，這個

0:02:40.360,0:02:44.380
TensorFlow 官方的 API

0:02:44.600,0:02:50.440
所以，當時我選擇教 Keras 真的是站對邊了

0:02:53.480,0:02:58.000
這邊稍微講一下說 Keras

0:02:58.000,0:03:00.320
Keras 其實是牛角的意思

0:03:00.320,0:03:02.520
在希臘文裡面，是牛角的意思

0:03:02.520,0:03:04.255
這個我有特別查過

0:03:04.260,0:03:07.040
這個來源就是，因為 Keras 就是一個在

0:03:07.040,0:03:10.900
叫做夢精靈的 project 裡面被 develop 出來的

0:03:10.900,0:03:15.540
然後，根據伊利亞德，我忘記還是奧德賽裡面的傳說

0:03:15.540,0:03:19.080
就是夢精靈在來到人世間的時候

0:03:19.080,0:03:21.320
他會通過兩種門

0:03:21.320,0:03:24.580
一種門是象牙做的，另一種是牛角做的

0:03:24.580,0:03:27.900
那如果是通過象牙做的門的夢精靈呢

0:03:27.900,0:03:31.300
那個人夢的內容就會被實現

0:03:31.300,0:03:34.760
那如果是通過牛角門，也就是 Keras 來那個夢精靈呢

0:03:34.765,0:03:37.295
那你的夢就會被實現

0:03:37.295,0:03:38.540
那個 project 叫做夢精靈

0:03:38.540,0:03:42.060
所以，在那個 project 開發出來的 toolkit，就叫做 Keras

0:03:42.360,0:03:46.660
底下是一些它的 documentation 還有一些 example

0:03:46.660,0:03:52.520
這個，助教會在助教時間跟大家講一下怎麼安裝 Keras

0:03:52.520,0:03:53.780
如果你不會安裝的話

0:03:54.760,0:03:58.020
它的安裝的這個 process

0:03:58.020,0:04:00.540
它其實是寫得很清楚啦，那如果你不會安裝的話

0:04:00.575,0:04:02.940
助教會跟你講，所以我們等一下就只是

0:04:02.940,0:04:03.665
看一下說

0:04:03.665,0:04:05.300
Keras 如果運作起來

0:04:05.300,0:04:06.400
如果你真的在操控

0:04:06.400,0:04:09.440
真的在實作的時候，大概看起來像是甚麼樣子

0:04:09.740,0:04:13.660
以下是某位同學使用 Keras 的心得

0:04:13.660,0:04:17.140
那他把他的心得做成 6 格的圖

0:04:17.140,0:04:18.820
放在 Facebook 上面

0:04:20.000,0:04:23.460
他說，一個 deep learning 的研究生，都在做甚麼事？

0:04:23.460,0:04:25.980
朋友覺得他在做 AlphaGo

0:04:26.020,0:04:27.200
他媽覺得他坐在電腦前面

0:04:27.200,0:04:29.120
大家都覺得他在做很潮的東西

0:04:29.120,0:04:31.660
指導教授知道是在解一個 Optimization problem

0:04:31.660,0:04:34.300
他以為自己很潮這樣子

0:04:34.300,0:04:36.420
我覺得應該是這個意思啦

0:04:36.500,0:04:38.540
然後，事實上你在做的事情

0:04:38.540,0:04:40.020
就是疊積木

0:04:40.020,0:04:42.445
那這個並不是他玩得很開心的意思

0:04:42.445,0:04:45.940
疊積木的意思就是說，當你在使用 Keras 的時候

0:04:45.940,0:04:48.405
它是非常的容易的

0:04:48.405,0:04:51.635
你在做的事情就是把現成的

0:04:51.640,0:04:54.020
module 呢，把現成的 function 呢

0:04:54.020,0:04:56.120
疊來疊去而已，所以

0:04:56.120,0:04:59.220
用 Keras 就好像是在疊積木一樣

0:05:00.060,0:05:03.360
那我們等一下就是用，我在前一堂課講的

0:05:03.500,0:05:06.100
Handwriting 的 Digit Recognition

0:05:06.100,0:05:08.980
來當作例子，來示範 Keras 怎麼使用

0:05:08.980,0:05:12.480
如果，你這輩子都還沒有寫過 deep learning 的程式的話

0:05:12.480,0:05:16.600
那今天這個，就是 deep learning 的 "Hello world"

0:05:16.600,0:05:19.980
我用的 data，就是 MNIST 的 data

0:05:19.980,0:05:21.940
MNIST 的 data 就好像是

0:05:21.940,0:05:24.120
machine learning 的果蠅一樣

0:05:24.280,0:05:26.480
果蠅知道嗎？

0:05:27.060,0:05:30.360
果蠅就是，如果你想要很快做一個實驗

0:05:30.360,0:05:32.400
你就用 MNIST

0:05:32.400,0:05:35.120
因為它的 data 量少

0:05:35.120,0:05:38.420
做起來又強

0:05:39.800,0:05:42.615
然後，這個在 MNIST 裡面阿

0:05:42.620,0:05:45.380
你要 machine 做的事情就是 input 一個 image

0:05:45.380,0:05:49.860
然後，output 就是這個 image 是 0~9 的哪一個數值

0:05:49.860,0:05:53.280
那 input image 的 size 是 28*28

0:05:53.280,0:05:54.845
它是一個 matrix

0:05:54.845,0:05:56.380
28*28 的 matrix

0:05:56.380,0:05:57.575
那其實 Keras

0:05:57.580,0:06:01.940
有 provide 自動下載 MNIST 的 data 的 function

0:06:03.740,0:06:06.240
那如果用 Keras 的話

0:06:06.240,0:06:10.540
你要怎麼做呢？

0:06:10.660,0:06:12.920
那我們之前有講過說

0:06:12.920,0:06:14.960
deep learning 就是 3 個步驟

0:06:14.960,0:06:18.020
第一個步驟就是決定一下你的 function set

0:06:18.020,0:06:19.815
決定一下你的 neural network

0:06:19.820,0:06:21.320
要長甚麼樣子

0:06:21.660,0:06:24.520
那在 Keras 裡面，你就是先宣告

0:06:24.520,0:06:27.280
model = Sequential( )，你就先宣告一個 model

0:06:27.625,0:06:30.655
接下來，你就看你的 network 想要長甚麼樣子

0:06:30.660,0:06:33.420
你就自己決定它要長甚麼樣子

0:06:33.600,0:06:36.615
舉例來說，我這邊想要疊一個 network

0:06:36.615,0:06:39.635
它有，我們把滑鼠叫出來

0:06:39.900,0:06:43.720
我們這邊想要疊一個 network，它有兩個 hidden layer

0:06:43.720,0:06:47.280
然後，每一個 layer 都有 500 個 neuron

0:06:47.280,0:06:48.960
我想要做這件事情的話

0:06:48.960,0:06:50.500
我應該要怎麼做呢？

0:06:50.500,0:06:51.920
其實很容易

0:06:51.920,0:06:55.860
你先宣告一個 model，接下來就說 model.add

0:06:56.060,0:06:58.480
加一個東西，加甚麼？

0:06:58.480,0:07:01.640
這邊我們要加一個 Fully connected 的 layer

0:07:01.645,0:07:04.595
Fully connected 的 layer 就是用 Dense 來表示

0:07:04.595,0:07:07.935
還可以加別的 layer，比如說 convolution 的 layer

0:07:08.140,0:07:10.260
所以，這邊宣告 Dense

0:07:10.260,0:07:13.220
( input dimension 是 28*28)

0:07:13.220,0:07:16.580
output dimension 是 500

0:07:16.580,0:07:19.560
那你就是 input 一個 28*28 的 vector

0:07:19.820,0:07:22.700
這個 vector 代表一個 image，然後呢

0:07:22.780,0:07:26.215
output = 500，就是說你今天要有

0:07:26.215,0:07:27.715
500 個 neuron

0:07:28.100,0:07:30.680
接下來呢，你要告訴 network 說你的

0:07:30.680,0:07:33.675
activation function 要用甚麼，這邊就直接寫說

0:07:33.680,0:07:37.980
model.add ( Activation ("sigmoid") )

0:07:37.980,0:07:41.600
那你就是用 sigmoid 當你的 activation function

0:07:41.600,0:07:43.005
那你可以選別的

0:07:43.005,0:07:45.880
Keras 裡面你可以選別的，比如說，softplus, softsign 阿

0:07:45.960,0:07:48.400
relu, tanh 等等一大堆別的

0:07:48.400,0:07:51.400
而且你要改，加上自己新的 activation function 呢

0:07:51.400,0:07:53.780
其實也滿容易的

0:07:53.780,0:07:56.460
你只要找到 Keras 裡面寫 activation function 的地方

0:07:56.460,0:07:59.940
然後，自己再加一個自己的 function 進去就好了

0:08:00.560,0:08:01.560
然後

0:08:01.920,0:08:04.360
如果要再加一個新的 layer 怎麼辦呢？

0:08:04.360,0:08:07.920
你就一樣下 model.add 一個 dense layer

0:08:07.920,0:08:09.000
它的 output 是 500

0:08:09.000,0:08:11.080
這邊你就不需要再給它 input 了

0:08:11.080,0:08:13.875
因為，下一個 layer 的 input

0:08:13.875,0:08:15.220
就等於前一個 layer 的 output

0:08:15.220,0:08:19.380
所以，你不需要再 redefine input 的 dimension 是多少

0:08:19.380,0:08:23.480
你只要直接告訴它說，output 你要 500 個 neuron 就好

0:08:23.480,0:08:26.120
不需要告訴它說，input 也是 500 個 neuron

0:08:26.120,0:08:28.900
這個 Keras 自己知道

0:08:29.300,0:08:32.280
activation function 這邊一樣是用 sigmoid

0:08:32.280,0:08:35.060
最後，output 是

0:08:35.060,0:08:37.880
要做數字分類嘛，要 10 個數字

0:08:37.880,0:08:40.440
所以你 output 一定要 10 維，不可以設別的數字

0:08:40.440,0:08:42.440
設 11, 12，那都不 work

0:08:42.440,0:08:46.100
所以，你這個 output 就是設 10 維

0:08:46.100,0:08:47.820
output dimension 就是 10 維

0:08:47.820,0:08:50.020
那 activation function，你這邊呢

0:08:50.020,0:08:51.500
我們說在 output 的 layer

0:08:51.500,0:08:53.920
如果把它當成一個 multi-class classifier 來看的話

0:08:53.920,0:08:56.380
我們就用 softmax，但你也可以用別的

0:08:56.380,0:08:58.815
我們完全可以用 sigmoid，甚麼都可以

0:08:58.820,0:09:01.540
那這邊呢，選擇用 softmax

0:09:02.420,0:09:07.800
那接下來我們要決定一個 function，
要 evaluate 一個 function 的好壞

0:09:07.820,0:09:10.160
怎麼 evaluate 一個 function 的好壞呢？

0:09:10.160,0:09:12.360
你要做的事情就是

0:09:13.140,0:09:14.800
model.compile

0:09:14.800,0:09:17.340
然後，定義你的 loss 是什麼

0:09:17.340,0:09:20.640
比如說，如果你要用 cross entropy 的話

0:09:20.640,0:09:22.700
那你的 loss 就是

0:09:23.040,0:09:26.220
但在 Keras 裡面，cross entropy 它是寫成

0:09:26.220,0:09:29.800
categorical cross entropy，就是 cross entropy，OK？

0:09:29.940,0:09:34.720
寫說你的 loss = cross entropy 就行了

0:09:34.920,0:09:37.640
其實，它也有支援很多其他的 loss function

0:09:37.640,0:09:40.920
在不同的場合，你會需要用到不同的 loss function

0:09:40.920,0:09:45.360
這就留給大家自己去看 Keras 的 documentation 了

0:09:47.020,0:09:49.580
那再來呢，是 training 的部分

0:09:49.580,0:09:52.215
在 training 之前，你要先下一些

0:09:52.220,0:09:55.420
Configuration 告訴它你 training 的時候

0:09:55.420,0:09:58.580
你打算要怎麼做

0:09:58.580,0:10:00.180
所以，現在呢

0:10:00.920,0:10:02.560
所以，現在呢

0:10:02.560,0:10:06.000
你要下的第一個東西是 optimizer

0:10:06.000,0:10:08.645
也就是說，你要找最好的 function 的時候

0:10:08.645,0:10:12.720
你要用甚麼樣的方式，來找最好的 function

0:10:12.720,0:10:17.120
雖然說，這邊 optimizer 的後面可以接不同的方式

0:10:17.120,0:10:21.380
但是，這些不同的方式，其實都是 Gradient Descent

0:10:21.380,0:10:22.580
類似的方法

0:10:22.800,0:10:25.980
只是他們用的 learning rate

0:10:25.980,0:10:27.795
learning rate 你可以

0:10:27.800,0:10:30.120
就是我們在做一般的 Gradient Descent 的時候

0:10:30.120,0:10:31.840
你要自己設一個 learning rate，對不對？

0:10:31.840,0:10:35.160
但是，有一些方法是 machine 會

0:10:35.660,0:10:36.900
自動的

0:10:37.220,0:10:40.660
跟 empirically 決定 learning rate 的值應該是多少

0:10:40.660,0:10:43.415
所以，有一些方法是不需要

0:10:43.420,0:10:44.800
給它 learning rate

0:10:44.800,0:10:47.740
machine 自己會決定 learning rate 應該要多少

0:10:47.740,0:10:51.380
那這邊呢，有支援各式各樣的方法

0:10:52.500,0:10:56.700
那最後就是，決定好

0:10:56.705,0:10:58.200
要怎麼做 Gradient Descent 以後

0:10:58.200,0:10:59.915
再來就是真的去跑 Gradient Descent

0:10:59.980,0:11:02.720
再來就是真的給它做下去

0:11:02.840,0:11:05.980
那這一步，很簡單

0:11:05.980,0:11:08.860
你就給它 4 個 input，第一個 input 是

0:11:08.860,0:11:11.820
training data，在我們的 case 裡面

0:11:11.820,0:11:15.300
training data 就是一張一張的 image

0:11:16.180,0:11:18.980
那你要給每一張 training data label，在這邊我們就是

0:11:18.980,0:11:20.880
要告訴 machine 說，現在 training data 裡面

0:11:20.880,0:11:23.880
每一張 image 它對應到 0~9 的哪一個數字

0:11:23.880,0:11:26.680
後面那兩個 flag 我們等一下再解釋

0:11:27.140,0:11:29.680
我們來看一下，實際上

0:11:29.680,0:11:32.960
這個 x_train 跟 y_train 應該是長甚麼樣子

0:11:33.340,0:11:35.840
在這 case 裡面，你 training 的 image 呢

0:11:35.840,0:11:38.260
就是要存成一個 numpy array 啦

0:11:38.260,0:11:41.940
那你就不要問我說，怎麼把一個 image
存到 numpy array 裡面去

0:11:41.940,0:11:45.920
這個，應該作業裡有做過了，對不對？

0:11:46.720,0:11:50.420
那你要把你的 image 都存到 numpy array 裡面去

0:11:50.740,0:11:52.800
那它的存法是這樣子

0:11:52.800,0:11:56.080
這個 numpy array 是 two-dimension 的

0:11:56.080,0:11:59.180
它是一個 two-dimension 的 matrix

0:11:59.180,0:12:03.400
它的第一個 dimension 代表你有多少個 example

0:12:03.400,0:12:06.620
假設你有一萬個 example，
第一個 dimension 就是一萬維

0:12:07.340,0:12:10.560
第二個 dimension 就是看你的 image 有多大

0:12:10.560,0:12:12.620
要幾個 pixel，那第二個 dimension 就有多大

0:12:12.620,0:12:13.560
其實在這個 case 裡面

0:12:13.560,0:12:16.960
在這個 case 裡面，有 28*28 個 pixel

0:12:16.960,0:12:20.340
所以，有 784 個 pixel

0:12:20.340,0:12:23.880
所以，每一個 dimension 就是 784 維

0:12:24.220,0:12:26.920
那我們來看 y_train

0:12:26.920,0:12:29.740
y_train 這每一個 image 的 label 怎麼表示呢？

0:12:29.740,0:12:33.720
一樣第一個 dimension 代表
你有幾個 training 的 example

0:12:33.720,0:12:37.680
你有幾個 training 的 example，第
一個 dimension 就有多維

0:12:37.680,0:12:40.520
那第二個 dimension 呢，第二個 dimension 就是 10

0:12:40.520,0:12:42.920
因為我們現在的 output 就是 10 維嘛

0:12:42.920,0:12:45.700
我們 label 只有 10 個可能，0~9 而已

0:12:45.700,0:12:48.720
所以，output dimension 就是 10 維

0:12:49.000,0:12:53.160
然後，今天這個

0:12:53.160,0:12:57.100
這個 image 所對應的數字，它就會是 1

0:12:57.100,0:12:59.540
也就是在那邊是塗黑的，其他是 0

0:12:59.540,0:13:01.380
舉例來說，第一個 image

0:13:01.380,0:13:03.360
這個是 5

0:13:03.360,0:13:05.780
所以，在它的 label 裡面

0:13:05.780,0:13:09.060
這個數字是 0, 1, 2, 3, 4

0:13:09.435,0:13:11.580
它從 0 開始算，所以就是

0:13:11.580,0:13:14.720
對應到 5 的那維是 1，其他是 0

0:13:14.720,0:13:18.960
比如說，第 4 個數字

0:13:18.960,0:13:22.260
就是對應到 1 的那一維是 1，其他是 0

0:13:22.760,0:13:26.015
那前面還有兩個我沒有解釋過的東西，一個是

0:13:26.020,0:13:29.280
batch_size，另一個是 nb_epoch

0:13:29.280,0:13:30.700
他們是甚麼意思呢？

0:13:31.845,0:13:34.865
所謂的 batch_size 是這樣，首先呢

0:13:34.865,0:13:38.480
這邊有一個秘密，就是

0:13:38.560,0:13:42.260
我們其實在做 Gradient Descent 的時候，
在做 deep learning 的時候

0:13:42.340,0:13:45.400
我們並不會真的去

0:13:45.405,0:13:47.425
minimize total loss

0:13:47.975,0:13:50.515
我們做的是甚麼呢？我們會把

0:13:50.520,0:13:55.140
training data 分成一個一個的 batch

0:13:55.220,0:13:57.200
也就是說你把你的 training data

0:13:57.200,0:13:58.920
比如說，一萬張 image 拿出來

0:13:59.060,0:14:01.460
然後，每一次 random 的選

0:14:01.460,0:14:03.820
100 張進來，作為一個 batch

0:14:04.500,0:14:08.080
那這個 batch 你要隨機的分

0:14:08.080,0:14:09.995
如果你 batch 沒有隨機的分

0:14:10.000,0:14:12.120
比如說，你某一個 batch 裡面通通都是 1

0:14:12.120,0:14:14.260
那另外一個 batch 裡面通通都是數字 2

0:14:14.260,0:14:15.680
這個你可以自己試試看

0:14:15.680,0:14:17.360
你 train 起來，會有問題的

0:14:17.360,0:14:20.880
對你的 performance 會有不小的影響

0:14:24.840,0:14:27.000
接下來，怎麼做呢？

0:14:27.000,0:14:29.820
首先，你先 randomly initialize network 的參數

0:14:29.820,0:14:31.300
就跟一般的 Gradient Descent 一樣

0:14:31.300,0:14:33.960
接下來，隨機的選一個 batch 出來

0:14:33.960,0:14:35.820
比如說，我們選了第一個 batch 出來

0:14:35.840,0:14:38.380
然後，接下來我們計算

0:14:38.700,0:14:42.180
對第一個 batch 裡面的 element 的 total loss

0:14:42.180,0:14:44.160
不是全部 training data 的 total loss 哦

0:14:44.160,0:14:47.100
是第一個 batch 裡面的 element 的 total loss

0:14:47.340,0:14:49.360
我們計算說 L'

0:14:49.520,0:14:51.740
等於 l1 + l31

0:14:51.740,0:14:54.280
加上別的 batch 的 loss

0:14:54.280,0:14:58.240
不是別的 batch，
同一個 batch 裡面其他 example 的 loss

0:14:58.620,0:15:02.540
然後，接下來根據 L' update 參數

0:15:02.540,0:15:05.800
也就是去計算參數對 L' 的偏微分

0:15:05.800,0:15:07.660
然後，update 參數

0:15:07.800,0:15:09.580
接下來，再隨機選一個 batch

0:15:09.580,0:15:11.960
比如說，這邊選的是第二個 batch

0:15:12.080,0:15:14.580
那你就計算說，現在你的 total loss

0:15:14.580,0:15:15.880
變成 L"

0:15:15.880,0:15:20.740
它不是 total L，它是 l2 + l16

0:15:20.840,0:15:24.860
再加同一個 batch 裡面其他的 example

0:15:24.860,0:15:27.960
接下來，計算你的參數對 L" 的偏微分

0:15:27.960,0:15:30.880
然後，去 update 你的參數

0:15:30.880,0:15:32.940
你就反覆做這個 process

0:15:32.940,0:15:36.120
直到把所有的 batch

0:15:36.120,0:15:38.080
通通選過一次

0:15:38.680,0:15:43.080
所以，今天假設你有 100 個 batch 的話

0:15:43.080,0:15:46.780
你就把這個參數 update 100 次

0:15:46.780,0:15:50.040
那把每一個 batch 都看過以後

0:15:50.040,0:15:52.360
那你就等於

0:15:52.360,0:15:56.740
把所有的 batch 都看過一次，叫做一個 epoch

0:15:56.940,0:15:57.940
叫做一個 epoch

0:15:58.060,0:15:59.600
那我們要做的事情就是

0:15:59.600,0:16:01.360
重複以上的 process

0:16:01.480,0:16:03.160
所以，你在 train  一個 network 的時候

0:16:03.160,0:16:06.600
你會需要好幾十個 epoch

0:16:06.600,0:16:08.180
不是只有一個 epoch

0:16:08.600,0:16:11.895
所以，這邊這兩個 flag，一個是 batch_size

0:16:11.900,0:16:15.580
就是告訴 Keras 說，我們的一個 batch

0:16:15.580,0:16:17.600
要有多大

0:16:17.880,0:16:20.660
舉例來說，這邊 batch_size = 100

0:16:20.660,0:16:23.700
就是說，我們要把 100 個 example

0:16:23.945,0:16:26.625
放在一個 batch 裡面

0:16:26.835,0:16:29.735
那 Keras 會幫你隨機的放

0:16:29.740,0:16:32.360
所以這個部分你就不需要自己寫 code

0:16:32.360,0:16:34.360
那 number of epoch 就是說呢

0:16:34.360,0:16:37.300
每一個 batch 看過一次，叫做一個 epoch

0:16:37.300,0:16:39.065
那我們到底要用幾個 epoch 呢？

0:16:39.065,0:16:41.535
這邊 epoch  = 20 就是以上這個 process

0:16:42.060,0:16:44.680
重複 20 次，也就是每一個 batch

0:16:44.680,0:16:47.040
被看過 20 次，那你要注意

0:16:47.040,0:16:50.000
我們並不會在一個

0:16:50.745,0:16:53.755
在一個 batch 裡面阿

0:16:53.755,0:16:56.725
我們已經 update 很多次參數了

0:16:56.725,0:16:58.720
我們每看一個 batch 就 update 一次參數

0:16:58.720,0:17:00.195
假設我們現在有 100 個 batch

0:17:00.715,0:17:03.725
那一個 epoch 裡面，我們就已經 update 100 次參數了

0:17:03.725,0:17:06.900
20 個 epoch 就是 20*100，就是 2000 次參數

0:17:06.900,0:17:09.685
所以，並不是說這邊設 20 個 epoch

0:17:09.685,0:17:11.460
我們就只 update 20次參數的意思

0:17:11.460,0:17:12.895
在一個 epoch 裡面

0:17:12.895,0:17:14.765
我們會 update 很多次參數

0:17:15.435,0:17:17.240
那我們記得我們之前

0:17:17.240,0:17:20.940
林宗男老師應該有講過 Stochastic Gradient Descent

0:17:20.940,0:17:24.140
今天如果我們的 batch_size 設為 1 的話

0:17:24.320,0:17:30.320
那就是 equivalent to Stochastic Gradient Descent

0:17:30.540,0:17:31.540
對不對？

0:17:31.800,0:17:36.340
那我們之前有講過 Stochastic Gradient Descent 的好處

0:17:36.340,0:17:39.460
它的好處就是，它的速度比較快

0:17:39.460,0:17:42.040
對不對，相較於 full 的

0:17:42.040,0:17:44.240
原來的 Gradient Descent

0:17:44.260,0:17:45.820
它的速度是比較快的

0:17:45.820,0:17:48.900
因為原來的 Gradient Descent，你 update 參數的時候

0:17:48.900,0:17:50.460
你用 Stochastic Gradient Descent

0:17:50.460,0:17:52.120
假設你有 100 個 training data 的話

0:17:52.120,0:17:54.420
那你已經 update 100 次參數了

0:17:54.420,0:17:57.160
雖然說，每一次 update 參數的方向

0:17:57.160,0:18:01.060
是不穩定的，但是就是天下武功，唯快不破

0:18:01.160,0:18:04.220
雖然它的出拳可能會落空

0:18:04.220,0:18:07.660
但是，它可以在別人出一拳的時候，它已經出 100 拳了

0:18:07.660,0:18:09.080
所以，它是比較強的

0:18:11.585,0:18:14.705
那你可能會想說，既然是這樣子的話

0:18:14.705,0:18:18.005
為甚麼我們不用 Stochastic Gradient Descent 就好

0:18:18.105,0:18:21.160
還要用 Mini-batch 呢？

0:18:21.160,0:18:24.860
接下來，就是一些實作上的問題

0:18:24.860,0:18:27.140
讓我們必須要用 Mini-batch

0:18:27.140,0:18:28.760
Mini-batch 這件事很重要

0:18:28.760,0:18:32.700
但是，最主要要用 Mini-batch 的理由

0:18:32.700,0:18:35.380
其實是一個實作上的 issue

0:18:37.905,0:18:40.245
我們之前有講說

0:18:40.500,0:18:43.260
你在一個 epoch

0:18:43.260,0:18:45.120
在一個 epoch

0:18:45.680,0:18:49.380
舉例來說，我們這邊有 50000 個 example

0:18:49.660,0:18:52.380
那我們的 batch_size，如果你設 1

0:18:52.380,0:18:54.620
也就是 Stochastic Gradient Descent 的話

0:18:54.620,0:18:59.020
那在一個 epoch 裡面，你會 update 50000 次參數

0:18:59.640,0:19:03.180
如果今天你的 batch_size 設為 10 的話

0:19:03.340,0:19:06.180
在一個 epoch 裡面

0:19:06.180,0:19:09.220
你會 update 5000 次參數

0:19:09.560,0:19:11.600
這樣看起來，好像是

0:19:12.360,0:19:15.100
Stochastic Gradient Descent 比較快

0:19:15.100,0:19:16.700
Mini-batch 設為 10

0:19:16.700,0:19:17.880
你在一個 epoch 裡面

0:19:17.880,0:19:19.920
才 update 5000 次參數

0:19:20.140,0:19:21.740
那 Stochastic Gradient Descent

0:19:21.740,0:19:23.600
它可以 update 50000 次參數

0:19:24.085,0:19:27.215
它的速度好像應該是它的 10 倍

0:19:27.335,0:19:28.335
但是

0:19:28.985,0:19:29.985
實際上阿

0:19:30.175,0:19:31.455
實際上

0:19:32.245,0:19:34.755
當你 batch_size 設不一樣的時候

0:19:35.105,0:19:37.865
一個 epoch 需要的時間

0:19:38.135,0:19:39.135
是不一樣的

0:19:39.360,0:19:41.620
這樣大家了解我的意思嗎？

0:19:41.620,0:19:44.360
就是你想說，這個 training data 不是都是五萬筆嗎？

0:19:44.360,0:19:46.060
就是這個 training data 不一定都是五萬筆

0:19:46.060,0:19:48.980
你設 batch_size = 1，你設 batch_size = 10

0:19:48.980,0:19:51.760
運算量是一樣多的嗎？

0:19:52.400,0:19:55.535
就是說，雖然運算量不是一樣多

0:19:55.540,0:19:58.400
你要把五萬筆 example，每一筆 example 都

0:19:58.400,0:19:59.900
都看過一遍

0:19:59.980,0:20:04.140
那同一個 epoch 裡面，它要做的運算量不是一樣多的嗎

0:20:04.240,0:20:06.000
一樣多的運算跟一樣多的時間

0:20:06.000,0:20:07.500
但是 batch_size 設成 1

0:20:07.500,0:20:10.220
你可以 update 五萬次，聽起來好像是比較厲害

0:20:10.220,0:20:12.980
但是，實際上，在實作上

0:20:13.200,0:20:15.480
當你 batch_size 設不一樣的時候

0:20:15.480,0:20:17.740
運算的時間是不一樣的

0:20:17.740,0:20:21.660
就算是同樣多的 example，但它的運算時間是不一樣的

0:20:21.660,0:20:23.560
那等一下會解釋為甚麼

0:20:23.560,0:20:25.280
我們先來看實際上的例子

0:20:25.280,0:20:27.880
這個就是在 GTX 980

0:20:27.880,0:20:31.100
然後，跑在 MNIST 的五萬個 training example 上面

0:20:31.100,0:20:35.120
一個 batch 需要的時間，當我
設不同的 batch_size 的時候

0:20:35.120,0:20:38.800
如果今天 batch_size 設 1，
也就是 Stochastic Gradient Descent

0:20:38.800,0:20:41.740
一個 epoch 要 166 秒

0:20:41.740,0:20:45.140
也就是接近 3 分鐘

0:20:45.660,0:20:48.040
如果，今天 batch_size 設 10 的話

0:20:48.040,0:20:50.960
那一個 epoch 是 17 秒

0:20:51.040,0:20:52.720
你會發現說

0:20:52.720,0:20:55.500
如果你今天設 100, 1000, 10000

0:20:55.500,0:20:59.580
那它就是越來越快，每一個 epoch 都是越來越快

0:20:59.580,0:21:02.200
所以，你會發現說今天過了 166 秒

0:21:02.460,0:21:05.120
它才算一個 epoch

0:21:05.120,0:21:06.600
166 秒

0:21:07.275,0:21:10.295
在下面這個 batch_size 設為 10 這個 case

0:21:10.300,0:21:12.740
它已經算 10 個 epoch 了

0:21:12.900,0:21:15.860
幾乎已經算 10 個 epoch 了

0:21:15.860,0:21:18.480
所以，這樣比較起來，因為

0:21:18.480,0:21:21.660
它算一個 epoch 要 166 秒

0:21:21.660,0:21:23.560
同樣時間，它已經算 10 個 epoch 了

0:21:23.600,0:21:25.080
它一個 epoch update 五萬次

0:21:25.180,0:21:26.780
它一個 epoch update 五千次

0:21:26.780,0:21:30.180
但是，在同樣時間，它已經跑 10 個 epoch

0:21:30.180,0:21:32.020
所以，會變成說

0:21:32.020,0:21:33.840
參數 update 的數目

0:21:33.965,0:21:36.615
batch_size 設 1 跟 batch_size 設 10

0:21:36.715,0:21:38.105
幾乎是一樣的

0:21:39.175,0:21:40.175
然後，再來呢

0:21:40.220,0:21:45.640
如果今天幾乎，這兩件事情

0:21:45.640,0:21:46.700
在同樣時間內

0:21:46.700,0:21:49.635
參數 update 的數目幾乎是一樣的

0:21:49.635,0:21:52.615
那你其實會想要選 batch_size  = 10

0:21:52.620,0:21:55.740
為甚麼？因為如果你選 batch_size = 10 的時候

0:21:55.920,0:21:58.180
是會比較穩定阿

0:21:58.180,0:22:02.000
我們之前之所以從 Gradient 換成 Stochastic Gradient

0:22:02.000,0:22:03.840
目標就是因為這樣比較快

0:22:04.020,0:22:06.020
你 update 次數比較多

0:22:06.020,0:22:08.500
可是，如果你現在用  Stochastic Gradient

0:22:08.500,0:22:10.600
其實也不會比較快

0:22:10.660,0:22:15.420
那你為甚麼不選一個比較穩定，update 次數比較多的呢

0:22:15.420,0:22:17.140
所以，這邊你就會選擇

0:22:17.360,0:22:20.440
batch_size = 10 的 case

0:22:21.980,0:22:27.300
那接下來有人就會想說

0:22:28.740,0:22:31.640
接下來我們的下一個問題就是，為甚麼

0:22:31.965,0:22:34.715
為甚麼 batch_size 設比較大的時候

0:22:35.355,0:22:36.385
速度會比較快

0:22:37.285,0:22:40.235
這個就是因為我們使用了平行運算

0:22:40.235,0:22:43.495
用了 GPU，這個我在下一頁會再更仔細的解釋啦

0:22:43.500,0:22:46.040
你就先知道說，因為我們用了平行運算

0:22:46.040,0:22:48.795
所以，這 10 個 example 它是同時運算

0:22:48.795,0:22:50.840
所以，你算 10 個 example 的時間

0:22:50.840,0:22:53.700
一個 batch 裡面 10 個 example 的時間
跟算一個 example 的時間

0:22:53.700,0:22:57.340
其實，是可以幾乎一樣的

0:22:58.740,0:23:02.100
那你會想說，既然 batch_size 越大

0:23:02.100,0:23:04.700
那既然 batch_size 越大

0:23:06.925,0:23:09.565
既然 batch_size 越大

0:23:09.565,0:23:10.920
它會越穩定

0:23:10.920,0:23:12.740
batch_size 變大的話

0:23:12.740,0:23:15.420
你還是可以平行運算，那你為甚麼不把 batch_size

0:23:15.520,0:23:17.840
開超級大就好了呢？

0:23:18.140,0:23:20.675
這邊有兩個 claim，一個 claim 就是

0:23:20.680,0:23:23.880
如果你把 batch_size 開到很大

0:23:23.880,0:23:26.100
最終，GPU 會沒有辦法平行運算

0:23:26.100,0:23:28.700
它終究是有它的極限，也就是說

0:23:28.700,0:23:32.700
它同時考慮 10 個 example 跟 1 個 example 的時間

0:23:32.700,0:23:36.040
是一樣的，但它同時考慮一萬個 example 的時候

0:23:36.040,0:23:38.740
它的時間就不會跟 1 個 example 一樣

0:23:38.740,0:23:42.740
所以，batch_size 考慮到硬體真正的限制的話

0:23:42.860,0:23:46.260
你也沒有辦法無窮盡的增長

0:23:46.540,0:23:49.280
那撇開硬體的限制不談

0:23:49.280,0:23:52.940
另外一個，batch_size 不應該設太大的理由是

0:23:52.940,0:23:54.580
你其實可以自己試試看

0:23:54.580,0:23:56.320
如果你把 batch_size 設很大

0:23:56.320,0:24:00.140
在 train Gradient Descent，在做 deep learning 的時候

0:24:00.140,0:24:02.295
你跑兩下，你的 network 就卡住了

0:24:02.300,0:24:05.660
你跑兩下，你就陷到 saddle point

0:24:05.660,0:24:07.120
或是 local minimum 裡面去了

0:24:07.120,0:24:11.260
如果那個 neural network 的 error surface 上面

0:24:11.440,0:24:13.420
它不是一個 convex optimization problem

0:24:13.620,0:24:15.660
它有很多的坑坑洞洞

0:24:15.660,0:24:16.940
它有很多坑坑洞洞

0:24:16.960,0:24:20.200
如果，你今天是 full 的 batch

0:24:20.200,0:24:22.685
原來的 Gradient Descent 裡面有一些 mini batch

0:24:22.685,0:24:26.680
那你就是完全順著 total loss 的方向走

0:24:26.680,0:24:29.320
你可以發現說，你每走兩步，就卡住了

0:24:29.320,0:24:31.080
這件事情，你可以回去自己試試看

0:24:31.080,0:24:33.060
你把 batch_size 設成 full batch

0:24:33.060,0:24:34.860
那如果你的 GPU 跑得動的話呢

0:24:35.060,0:24:37.280
你還是可以得到它的結果

0:24:37.280,0:24:39.760
但是你的 performance 就會很差，因為

0:24:39.760,0:24:41.300
你會發現說你的那個

0:24:41.520,0:24:43.240
在 training set 上的 loss

0:24:43.240,0:24:45.560
它跑兩下，整個就卡了

0:24:45.560,0:24:48.720
你就沒有辦法再 train，它就卡到一個 local minimum

0:24:48.720,0:24:52.700
local minimum 或是 saddle point 的地方，你就無法再 train

0:24:52.700,0:24:54.980
但是，如果你用 Stochastic 的好處就是

0:24:54.980,0:24:56.720
如果你有隨機性

0:24:56.720,0:25:00.120
每一次你走的方向，會是隨機的

0:25:00.160,0:25:05.020
所以，如果你今天從某一步陷到 local minimum 裡面去

0:25:05.020,0:25:10.000
如果那個 local minimum 不是一個很深的 local minimum

0:25:10.000,0:25:11.880
或是那個 saddle point

0:25:11.880,0:25:14.460
是一個特別麻煩的 saddle point

0:25:14.460,0:25:17.940
你只要下一步再加一點 random

0:25:17.940,0:25:21.220
你就可以跳出那個

0:25:21.220,0:25:22.835
Gradient 是 0 的區域了

0:25:22.835,0:25:25.345
所以，如果你沒有這個隨機性的話

0:25:25.345,0:25:27.740
你 train Neural network 其實是會有問題的

0:25:27.740,0:25:29.900
你如果沒有這個 mini batch 的隨機性的話

0:25:29.900,0:25:34.460
你每 update 兩次參數

0:25:34.460,0:25:38.120
你就會卡住，所以，這個 mini batch 是需要的

0:25:39.040,0:25:40.720
接下來，我們要解釋說

0:25:40.720,0:25:44.560
為甚麼當有 batch 的時候

0:25:44.560,0:25:47.580
GPU 是如何平行地加速

0:25:47.600,0:25:49.560
那我們剛才有講過說

0:25:49.560,0:25:53.680
整個 network 你就可以把它看成是一連串的

0:25:53.720,0:25:56.380
矩陣運算的結果

0:25:56.380,0:25:59.120
不管是 Forward pass 或是 Backward pass

0:25:59.120,0:26:01.600
都可以看成是一連串矩陣運算的結果

0:26:01.600,0:26:03.380
Forward pass 就是我們圖上看到這樣

0:26:03.380,0:26:06.580
Backward pass 我們前一份投影片有解釋過，就是

0:26:06.580,0:26:09.320
把整個 network 逆轉

0:26:09.340,0:26:12.800
然後，把 neuron 變成 op-amp

0:26:14.440,0:26:17.840
那我們今天就可以比較  Stochastic Gradient Descent

0:26:17.840,0:26:22.080
也就是 batch_size = 1，還有 batch_size = 10 的差別

0:26:22.460,0:26:24.400
如果，batch_size = 1

0:26:24.400,0:26:26.940
我們看第一個 layer，你 input 一個 x

0:26:27.320,0:26:28.820
然後，你乘上 W^1

0:26:29.140,0:26:30.400
你就得到 z^1

0:26:30.400,0:26:32.420
在 Forward pass 的時候，你要做這個計算嘛

0:26:32.420,0:26:35.520
在 Backward pass 你也會做一個類似的計算

0:26:35.520,0:26:38.760
在 Forward pass，你就會做這樣一個 matrix operation

0:26:38.760,0:26:42.700
這是第一筆 data，那你做完這些 matrix 的計算以後

0:26:42.700,0:26:44.260
你會 update 你的參數

0:26:44.260,0:26:46.380
接下來，你第二筆預測的 x 進來

0:26:46.520,0:26:47.960
再乘 W^1

0:26:47.960,0:26:50.600
再得到另外一個 z^1，再 update 參數

0:26:51.000,0:26:53.200
但是，在 Mini-batch 的時候

0:26:53.200,0:26:57.635
你會把同一個 batch 裡面的 input 通通集合起來

0:26:57.640,0:26:59.420
每一個 input 都是一個 vector 嘛

0:26:59.420,0:27:01.140
假設我們現在 batch_size 就是 2

0:27:01.160,0:27:04.120
那你裡面有黃色這個 vector，也有綠色這個 vector

0:27:04.140,0:27:06.900
那你就把黃色的 vector 和綠色的 vector 拼起來

0:27:06.900,0:27:08.140
變成一個 matrix

0:27:08.520,0:27:11.780
再把這個 matrix 乘上 W^1

0:27:11.780,0:27:15.180
你就可以直接得到 z^1 跟 z^2

0:27:15.180,0:27:17.895
我們可以把 x 乘上 W^1 得到 z^1

0:27:17.895,0:27:20.005
把 x 乘上 W^1 得到 z^1

0:27:20.005,0:27:21.800
這兩件事情分開來做

0:27:21.800,0:27:24.020
也可以把這兩個東西

0:27:24.500,0:27:25.620
併在一起

0:27:26.180,0:27:29.620
再乘上 W^1，直接得到 z^1

0:27:29.795,0:27:30.680
或者 z^2

0:27:30.680,0:27:32.765
這兩件事情理論上

0:27:32.875,0:27:35.835
運算量是一模一樣多的

0:27:35.840,0:27:38.080
對不對？上面這件事

0:27:38.600,0:27:40.880
跟下面這件事

0:27:40.880,0:27:44.120
它在理論上的運算量是一樣多的

0:27:44.120,0:27:45.920
但是，就實作上

0:27:45.920,0:27:49.640
你覺得哪一件事情是比較快的呢？

0:27:50.340,0:27:53.900
你覺得上面比較快的同學舉手一下

0:27:55.080,0:27:59.060
你覺得下面比較快的同學舉手一下

0:27:59.200,0:28:01.680
所有同學都覺得下面比較好，手放下

0:28:01.680,0:28:03.820
沒錯，下面就是比較快的

0:28:03.820,0:28:07.760
因為，如果你今天讓 GPU 做這個運算

0:28:07.760,0:28:10.100
和讓 GPU 做這個運算

0:28:10.100,0:28:12.420
它的時間，其實是一樣的

0:28:12.420,0:28:18.820
對 GPU 來說，你在矩陣運算裡面相乘的每一個 element

0:28:18.820,0:28:20.725
都是可以平行運算的

0:28:20.725,0:28:23.300
所以，今天上面這個運算的時間

0:28:23.300,0:28:26.680
反而會變成下面這個 GPU 運算的時間的兩倍

0:28:26.680,0:28:31.400
所以，這就是為甚麼我們用 Mini-batch

0:28:31.460,0:28:34.920
再加上 GPU 的時候，你是可以加速的

0:28:35.000,0:28:39.080
但是，如果你今天用 GPU

0:28:39.080,0:28:42.020
但是，你沒用 Mini-batch 的話

0:28:42.020,0:28:43.900
你其實就加速不了太多

0:28:44.040,0:28:47.065
所以，這就是有人買了 GPU

0:28:47.065,0:28:49.640
有人凹了他的老師買了 GPU 來

0:28:49.640,0:28:51.260
但他不知道要設 Mini-batch

0:28:51.260,0:28:53.940
所以，裝了 GPU 以後，也沒變快

0:28:57.680,0:29:00.260
那 Keras 當然可以 save 和 load model

0:29:00.260,0:29:01.940
你可以把 train 好的 model 存起來

0:29:01.940,0:29:04.840
然後以後再用另外一個程式讀出來

0:29:04.840,0:29:07.340
那它也可以幫你做 testing

0:29:07.340,0:29:10.320
有兩個 testing 的 case，第一個 case 是這個

0:29:10.320,0:29:11.715
evaluation

0:29:11.715,0:29:13.700
也就 evaluation case 是

0:29:13.700,0:29:15.680
我今天有一組 testing set

0:29:15.700,0:29:17.960
testing set 的答案我也知道

0:29:17.960,0:29:20.680
那 Keras 就幫你算說你現在的正確率

0:29:20.680,0:29:24.375
有多少，那這個 evaluation 有兩個 input

0:29:24.375,0:29:27.395
就是 testing 的 image 和 testing 的 label

0:29:27.580,0:29:30.460
那在這 case 2 呢，case 2 要做 predict

0:29:30.460,0:29:33.820
這個時候，你沒有任何的 label data

0:29:33.820,0:29:35.840
你只有 image，就是你真的 train 好這個 model

0:29:35.840,0:29:37.440
你要放到網路上讓人家用

0:29:37.440,0:29:40.600
別人會輸入一個 image，然後你就告訴他你的

0:29:40.600,0:29:42.400
分類好的數字是多少

0:29:42.540,0:29:45.695
這個時候，你 input 就只有 x，就只有 image

0:29:45.700,0:29:48.340
output 就直接是分類的結果

0:29:48.340,0:29:51.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
