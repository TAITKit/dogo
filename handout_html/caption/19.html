<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:01.400<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:01.460,0:00:02.800<br>
告訴我們說，network 越 deep<br>
<br>
0:00:02.800,0:00:06.040<br>
從 1 層到 7 層，我的 error rate 不斷地下降<br>
<br>
0:00:06.040,0:00:06.662<br>
那問題是<br>
<br>
0:00:06.662,0:00:08.840<br>
如果你仔細地思考一下這個問題<br>
<br>
0:00:08.840,0:00:10.840<br>
你的 network 越深<br>
<br>
0:00:10.840,0:00:13.220<br>
如果你 imply 的是你的參數越多的話<br>
<br>
0:00:13.220,0:00:16.020<br>
那這個有甚麼好說的<br>
<br>
0:00:16.020,0:00:18.580<br>
你的 model 比較複雜<br>
<br>
0:00:18.580,0:00:20.680<br>
你的 data 如果比較多的話<br>
<br>
0:00:20.685,0:00:22.480<br>
本來你的 performance 就會比較好<br>
<br>
0:00:22.500,0:00:25.320<br>
所以你真正要比較一個<br>
<br>
0:00:25.320,0:00:28.900<br>
你真正要比較 Deep 和 Shallow 的 model 的時候<br>
<br>
0:00:28.900,0:00:32.600<br>
你應該做的事情是，你要調整 Deep 和 Shallow 的 model<br>
<br>
0:00:32.620,0:00:35.720<br>
讓他們的參數是一樣多的<br>
<br>
0:00:35.740,0:00:37.360<br>
那很多人在比較 Deep 和 Shallow 的 model 的時候<br>
<br>
0:00:37.360,0:00:38.760<br>
是沒有注意到這一件事情的<br>
<br>
0:00:38.760,0:00:40.720<br>
所以那一個評比是不公平的<br>
<br>
0:00:40.720,0:00:42.760<br>
如果你要給 Deep 和 Shallow 的 model 公平的評比<br>
<br>
0:00:42.760,0:00:44.800<br>
你要故意調整他的參數<br>
<br>
0:00:44.800,0:00:47.840<br>
讓他們的參數是一樣多的，在這個情況下<br>
<br>
0:00:47.840,0:00:50.280<br>
Shallow 的 model 就會是一個<br>
<br>
0:00:50.280,0:00:55.060<br>
矮胖的 model，Deep 的 model就會是一個瘦高的 model<br>
<br>
0:00:55.060,0:00:56.960<br>
接下的問題就是<br>
<br>
0:00:56.960,0:00:58.580<br>
在這個公平的評比之下<br>
<br>
0:00:58.580,0:01:01.020<br>
是 Shallow 比較強還是 Deep 比較強 ?<br>
<br>
0:01:01.020,0:01:03.700<br>
所以剛才那一個實驗結果裡面呢<br>
<br>
0:01:03.700,0:01:05.260<br>
是有後半段<br>
<br>
0:01:05.260,0:01:07.120<br>
後半段的實驗結果是這樣說的<br>
<br>
0:01:07.120,0:01:10.180<br>
它說我們用 5 層<br>
<br>
0:01:10.180,0:01:15.120<br>
每層兩千個 neuron， 得到 error rate 是17.2<br>
<br>
0:01:15.120,0:01:18.440<br>
error rate 是越小越好 ，那這一邊用一層<br>
<br>
0:01:18.440,0:01:21.980<br>
3772 的 neuron，得到 error rate 是 22.5<br>
<br>
0:01:21.980,0:01:23.660<br>
為什麼是 37722 個 neuron 呢？<br>
<br>
0:01:23.660,0:01:25.500<br>
這並不是什麼 lucky number<br>
<br>
0:01:25.500,0:01:28.300<br>
這是為了讓一個 hidden layer 的 network 跟<br>
<br>
0:01:28.300,0:01:34.280<br>
有 5 層的 hidden layer 的 network，它們的參數是接近的<br>
<br>
0:01:34.280,0:01:37.300<br>
那如果我們只有一層的 network<br>
<br>
0:01:37.300,0:01:40.740<br>
這個時候它的 error rate 是 22.5<br>
<br>
0:01:42.520,0:01:44.440<br>
這個 error rate 遠比它大<br>
<br>
0:01:44.440,0:01:46.340<br>
如果我們看另外一個 case，7 層<br>
<br>
0:01:46.340,0:01:50.040<br>
每層 2000 個 neuron 跟一層 4634 個 neuron<br>
<br>
0:01:50.040,0:01:51.980<br>
它們的參數數目是接近的<br>
<br>
0:01:51.980,0:01:52.980<br>
這個時候你會發現說<br>
<br>
0:01:52.980,0:01:56.700<br>
只有一層它的 performance 是比較差的<br>
<br>
0:01:56.700,0:02:02.880<br>
甚至如果你今天再增加 network 的參數變成一層<br>
<br>
0:02:02.880,0:02:08.000<br>
但是有 16k 個 neuron，有好多好多的 neuron<br>
<br>
0:02:08.000,0:02:10.560<br>
有原來這個 case 的4倍<br>
<br>
0:02:10.560,0:02:16.020<br>
那你的 error rate 也只有從 22.6 降到 22.1 而已<br>
<br>
0:02:16.020,0:02:21.980<br>
當你用一個只有一層，但非常非常寬的  network<br>
<br>
0:02:21.980,0:02:25.060<br>
跟也是只有一層，但是沒有那麼寬的 network 來比<br>
<br>
0:02:25.060,0:02:26.040<br>
因為他的參數比較多<br>
<br>
0:02:26.040,0:02:28.080<br>
所以它的 performance 還比它強<br>
<br>
0:02:28.080,0:02:29.840<br>
它的 performance 還是比它強<br>
<br>
0:02:29.840,0:02:31.000<br>
但如果你比較<br>
<br>
0:02:31.000,0:02:33.640<br>
這個有兩層的 network 跟這個只有一層的 network<br>
<br>
0:02:33.640,0:02:35.300<br>
這個有兩層的 network 的參數遠比它少<br>
<br>
0:02:35.300,0:02:37.360<br>
這個參數是少的<br>
<br>
0:02:37.360,0:02:38.420<br>
這個參數是多的<br>
<br>
0:02:38.420,0:02:41.720<br>
但是結果，這個 case 只有兩層<br>
<br>
0:02:41.720,0:02:43.320<br>
它的 performance 還是比<br>
<br>
0:02:43.320,0:02:47.480<br>
只有一層的 network 的 performance 還要好<br>
<br>
0:02:47.480,0:02:49.360<br>
現在，接下來的問題就是<br>
<br>
0:02:50.860,0:02:52.960<br>
為什麼會這樣<br>
<br>
0:02:52.960,0:02:55.460<br>
因為在很多人的的想像裡面<br>
<br>
0:02:55.460,0:02:57.260<br>
deep learning 會 work 就是因為<br>
<br>
0:02:57.260,0:03:01.420<br>
這是一個，有人覺得說 deep learning 就是一個暴力輾壓的方法<br>
<br>
0:03:01.420,0:03:03.340<br>
我弄一個大很大的 model<br>
<br>
0:03:03.340,0:03:05.240<br>
然後我 collect 一大堆的 data<br>
<br>
0:03:05.240,0:03:08.080<br>
所以就得到比較好的 performance<br>
<br>
0:03:08.080,0:03:09.540<br>
它就是一個暴力的方法<br>
<br>
0:03:09.540,0:03:11.540<br>
你有沒有發現實際不是這個樣子<br>
<br>
0:03:11.540,0:03:14.060<br>
如果你今天只是單純的增加 parameter<br>
<br>
0:03:14.060,0:03:16.120<br>
但是你是讓 network 長寬<br>
<br>
0:03:16.120,0:03:18.420<br>
而不是長高的話<br>
<br>
0:03:18.420,0:03:21.460<br>
其實你對 performance 的幫助，是比較小的<br>
<br>
0:03:21.460,0:03:25.040<br>
把 network 長高，對 performance 的影響很有幫助<br>
<br>
0:03:25.040,0:03:27.420<br>
把 network 長寬 ，幫助沒有那麼好<br>
<br>
0:03:27.420,0:03:29.300<br>
為什麼會這樣呢？<br>
<br>
0:03:29.300,0:03:33.200<br>
我們可以想像說，當我們在做 deep learning 的時後<br>
<br>
0:03:33.200,0:03:38.120<br>
其實我們就是在做這個模組化的這件事情<br>
<br>
0:03:38.120,0:03:39.400<br>
甚麼意思呢 ?<br>
<br>
0:03:39.400,0:03:41.020<br>
大家都應該會寫程式<br>
<br>
0:03:41.020,0:03:43.640<br>
所以你就知道說，當你在寫程式的時候<br>
<br>
0:03:43.640,0:03:47.160<br>
你不能把所有的程式都寫在 main function  裡面<br>
<br>
0:03:47.160,0:03:50.520<br>
你不能把 5000 行的程式都寫在 main function 裡面<br>
<br>
0:03:50.520,0:03:53.740<br>
你會寫一些 subfunction<br>
<br>
0:03:53.740,0:03:56.300<br>
對不對，你會在你的 main function 裡面<br>
<br>
0:03:56.300,0:03:59.660<br>
去 call subfunction 1、function 2 和 function 3<br>
<br>
0:03:59.660,0:04:02.340<br>
然後 function 2 裡面可能還會去 call<br>
<br>
0:04:02.340,0:04:04.320<br>
subsub 1、subsub 2 和 subsub 3<br>
<br>
0:04:04.320,0:04:08.020<br>
然後 subsub 2 還會再 call  subsubsub2 這個樣子<br>
<br>
0:04:08.020,0:04:09.940<br>
它是一層一層<br>
<br>
0:04:09.940,0:04:12.980<br>
它是有這個結構化的結構<br>
<br>
0:04:12.980,0:04:16.440<br>
但是這個層次，你要有結構化的架構<br>
<br>
0:04:16.440,0:04:18.040<br>
這樣做的好處是<br>
<br>
0:04:18.040,0:04:21.440<br>
有一些 function 是可以共用的<br>
<br>
0:04:21.500,0:04:25.100<br>
比如說，搞不好這個  function 是  sorting<br>
<br>
0:04:25.100,0:04:27.300<br>
然後，你只要在其他的<br>
<br>
0:04:27.300,0:04:29.600<br>
更 high level 的 function 裡面 call sorting<br>
<br>
0:04:29.600,0:04:31.000<br>
就會 call 到這個 function<br>
<br>
0:04:31.000,0:04:35.360<br>
你就不用每一次在每一個 subfunction 裡面<br>
<br>
0:04:35.360,0:04:36.700<br>
需要做 sorting 的時候<br>
<br>
0:04:36.700,0:04:38.460<br>
都 implement 一個完整的 sorting function<br>
<br>
0:04:38.460,0:04:39.980<br>
你可以把它變成模組<br>
<br>
0:04:39.980,0:04:41.260<br>
需要的時候再去 call 它<br>
<br>
0:04:41.260,0:04:44.660<br>
這樣你就可以減少你程式的複雜度<br>
<br>
0:04:44.660,0:04:46.200<br>
可以讓你的程式比較簡潔<br>
<br>
0:04:47.300,0:04:49.640<br>
那如果用在 machine learning 上面呢<br>
<br>
0:04:49.640,0:04:51.740<br>
你可以想像我們現在有架一個 task<br>
<br>
0:04:51.740,0:04:53.880<br>
假設我們要做影像分類<br>
<br>
0:04:53.880,0:04:57.220<br>
那我們要把 image 分成，比如說 4 類<br>
<br>
0:04:57.220,0:04:59.900<br>
比如說，長頭髮女生、長頭髮男生<br>
<br>
0:04:59.900,0:05:01.980<br>
和短頭髮女生、短頭髮男生<br>
<br>
0:05:01.980,0:05:05.000<br>
那你可能說，我們對這四類我們要分類的影像<br>
<br>
0:05:05.000,0:05:06.560<br>
通通去 collect data<br>
<br>
0:05:06.560,0:05:09.640<br>
比如說，長頭髮女生可以 collect 到一堆 data<br>
<br>
0:05:09.640,0:05:11.500<br>
長頭髮男生也有一些 data<br>
<br>
0:05:11.500,0:05:13.460<br>
短頭髮女生，短頭髮男生都有一些 data<br>
<br>
0:05:13.460,0:05:15.400<br>
你就去 train 4 個 classifier<br>
<br>
0:05:15.400,0:05:17.220<br>
你就可以 solve 這個 problem<br>
<br>
0:05:17.340,0:05:19.620<br>
但是問題就是，長頭髮的男生<br>
<br>
0:05:19.620,0:05:21.160<br>
你的 data 可能是比較少的<br>
<br>
0:05:21.160,0:05:22.360<br>
比如說，在立法院裡面<br>
<br>
0:05:22.360,0:05:23.600<br>
只有林昶佐是長頭髮<br>
<br>
0:05:23.600,0:05:26.440<br>
那他現在也不是長頭髮，所以這個 data 是比較少的<br>
<br>
0:05:26.440,0:05:28.740<br>
就是你沒有太多的 training data<br>
<br>
0:05:28.740,0:05:30.340<br>
所以，你 train 出來的<br>
<br>
0:05:30.340,0:05:32.440<br>
detect 長頭髮男生的 classifier<br>
<br>
0:05:32.440,0:05:33.880<br>
就比較 weak<br>
<br>
0:05:33.880,0:05:36.980<br>
所以，你 detect 長頭髮男生的 performance 就比較差<br>
<br>
0:05:37.120,0:05:38.340<br>
那怎麼辦呢？<br>
<br>
0:05:38.340,0:05:41.320<br>
你可以用模組化的概念<br>
<br>
0:05:41.340,0:05:45.020<br>
假設我們現在不是先直接去解那一個問題<br>
<br>
0:05:45.020,0:05:49.080<br>
而是把原來的問題切成比較小的問題<br>
<br>
0:05:49.080,0:05:51.520<br>
比如說，我們 learn 一些 classifier<br>
<br>
0:05:51.520,0:05:53.300<br>
這一些 classifier 它的工作<br>
<br>
0:05:53.300,0:05:57.360<br>
是去 detect 有沒有某一種 attribute 出現<br>
<br>
0:05:57.360,0:06:00.520<br>
比如說，它不是直接去 detect 說<br>
<br>
0:06:00.520,0:06:02.840<br>
是長頭髮男生，還是長頭髮女生<br>
<br>
0:06:02.840,0:06:05.520<br>
它是把這個問題切成比較小的問題<br>
<br>
0:06:05.520,0:06:07.020<br>
它把這個問題切成<br>
<br>
0:06:07.020,0:06:10.600<br>
我們先決定說，input 一張 image，它是男生還是女生<br>
<br>
0:06:10.600,0:06:13.580<br>
input 一張 image，它是長頭髮還是短頭髮<br>
<br>
0:06:13.580,0:06:16.580<br>
雖然說，長頭髮的男生 data 很少<br>
<br>
0:06:16.580,0:06:20.040<br>
雖然說，長頭髮的男生 data 很少<br>
<br>
0:06:20.040,0:06:22.140<br>
但女生的 data 和男生的 data 都可以<br>
<br>
0:06:22.140,0:06:24.180<br>
分別 collect 到足夠的 data<br>
<br>
0:06:24.200,0:06:26.320<br>
雖然，長頭髮男生的 data 很少<br>
<br>
0:06:26.320,0:06:29.180<br>
但是長髮的人跟短髮的人的 data<br>
<br>
0:06:29.180,0:06:31.060<br>
你都可以 collect 到夠多<br>
<br>
0:06:31.060,0:06:34.580<br>
所以你 train 這些 basic 的 classifier 的時候<br>
<br>
0:06:34.580,0:06:36.640<br>
你就不會 train 的太差<br>
<br>
0:06:36.640,0:06:39.420<br>
你這些 basic 的 classifier 都是有足夠 data<br>
<br>
0:06:39.420,0:06:41.500<br>
把它 train 好<br>
<br>
0:06:42.180,0:06:45.160<br>
所以，接下來<br>
<br>
0:06:45.160,0:06:49.460<br>
如果你要解，最後我們要真正處理的問題的時候<br>
<br>
0:06:49.460,0:06:52.900<br>
你的每一個 classifier 就去參考這一些<br>
<br>
0:06:52.900,0:06:54.940<br>
basic 的 attribute 的 output<br>
<br>
0:06:54.940,0:06:57.520<br>
你就最後要下決定的那一個 classifier<br>
<br>
0:06:57.520,0:07:01.100<br>
它是把前面的 basic 的 classifier 當中的 module<br>
<br>
0:07:01.100,0:07:03.340<br>
去 call 它的 output<br>
<br>
0:07:03.340,0:07:06.300<br>
而每一個 classifier 都共用同樣的 module<br>
<br>
0:07:06.300,0:07:07.920<br>
都共用同樣的 module<br>
<br>
0:07:07.920,0:07:10.000<br>
只是可能用不同的方式來使用它而已<br>
<br>
0:07:10.000,0:07:11.340<br>
對 classifier 來說<br>
<br>
0:07:11.340,0:07:14.220<br>
它看到前面的 basic 的 classifier 告訴它說<br>
<br>
0:07:14.220,0:07:16.060<br>
是女生、是長頭髮<br>
<br>
0:07:16.060,0:07:19.320<br>
那這個 classifier output 就是 yes，反之就是 no<br>
<br>
0:07:19.320,0:07:22.440<br>
所以，對後面這些 classifier 來說<br>
<br>
0:07:22.440,0:07:24.300<br>
它可以利用前面這些 classifier<br>
<br>
0:07:24.300,0:07:28.060<br>
所以它只要用比較少的 training data<br>
<br>
0:07:28.060,0:07:31.940<br>
就可以把結果 train 好<br>
<br>
0:07:31.940,0:07:34.980<br>
雖然說 classifier 2 的 data 很少<br>
<br>
0:07:34.980,0:07:37.460<br>
但是，現在要做的事情是比較簡單的<br>
<br>
0:07:37.460,0:07:40.460<br>
真正複雜的事都被 basic classifier 做掉了<br>
<br>
0:07:40.460,0:07:42.600<br>
所以，classifier 需要做的事情比較簡單<br>
<br>
0:07:42.600,0:07:45.000<br>
所以，比較少的 data，就可以把它 train 好<br>
<br>
0:07:45.000,0:07:49.860<br>
那 deep learning 怎麼跟模組化的概念，扯上關係呢？<br>
<br>
0:07:49.860,0:07:51.080<br>
你想想看<br>
<br>
0:07:51.080,0:07:54.940<br>
每一個 neuron 其實就是一個 basic 的 classifier<br>
<br>
0:07:54.940,0:07:58.320<br>
第一層 neuron，它是一個最 basic 的 classifier<br>
<br>
0:07:58.320,0:08:01.640<br>
第二層 neuron 是比較複雜的 classifier<br>
<br>
0:08:01.640,0:08:03.720<br>
它用第一層 basic 的 classifier 的 output<br>
<br>
0:08:03.720,0:08:05.720<br>
當作它的 input<br>
<br>
0:08:05.720,0:08:08.660<br>
它把第一層的 classifier 當作 module<br>
<br>
0:08:08.660,0:08:11.420<br>
而第三層的 neuron 又把第二層的 neuron<br>
<br>
0:08:11.420,0:08:15.880<br>
當作它  module，以此類推<br>
<br>
0:08:15.880,0:08:18.280<br>
當然這邊要強調的是說<br>
<br>
0:08:18.280,0:08:19.560<br>
在做 deep learning 的時候<br>
<br>
0:08:19.560,0:08:22.300<br>
怎麼做模組化這件事情是 machine<br>
<br>
0:08:22.300,0:08:24.120<br>
自動學到的<br>
<br>
0:08:24.120,0:08:26.000<br>
那我覺得呢，這件事情還頗神奇<br>
<br>
0:08:26.000,0:08:27.460<br>
那 machine 就自動學到說<br>
<br>
0:08:27.460,0:08:29.380<br>
比如說，在 image 裡面<br>
<br>
0:08:29.380,0:08:33.080<br>
第一層 classifier 就是 detect 最單純的 attribute 等等<br>
<br>
0:08:33.080,0:08:34.740<br>
那我們剛才說<br>
<br>
0:08:34.740,0:08:36.560<br>
做 modularization 的好處是甚麼<br>
<br>
0:08:36.560,0:08:38.280<br>
做 modularization 的好處是<br>
<br>
0:08:39.000,0:08:42.720<br>
讓我們的模型變簡單了，對不對<br>
<br>
0:08:42.720,0:08:45.680<br>
我們是把本來的比較複雜的問題，變得比較簡單<br>
<br>
0:08:45.680,0:08:48.920<br>
所以，當我們把問題變簡單的時候<br>
<br>
0:08:48.920,0:08:50.740<br>
就算 training data 沒有那麼多<br>
<br>
0:08:50.740,0:08:54.020<br>
我們也可以把這個 task 做好<br>
<br>
0:08:54.020,0:08:56.980<br>
這個是 modularization、這是模組化的精神<br>
<br>
0:08:56.980,0:08:59.240<br>
如果 deep learning 做得是做模組化的話<br>
<br>
0:08:59.240,0:09:02.220<br>
其實，神奇的事就是<br>
<br>
0:09:02.220,0:09:05.300<br>
deep learning 需要的 training data 是比較少的<br>
<br>
0:09:05.980,0:09:08.780<br>
這個，有沒有跟你的認知是相反的呢<br>
<br>
0:09:08.780,0:09:11.900<br>
我知道現在因為 deep learning 很紅<br>
<br>
0:09:11.900,0:09:14.880<br>
新聞上都有各種各式樣的說法<br>
<br>
0:09:14.880,0:09:16.080<br>
常常聽有人說<br>
<br>
0:09:16.080,0:09:21.760<br>
AI 就等於 big data 加 deep learning<br>
<br>
0:09:21.760,0:09:23.220<br>
那很多人就會覺得說<br>
<br>
0:09:23.220,0:09:25.300<br>
所以，這個 deep learning 會 work<br>
<br>
0:09:25.300,0:09:28.300<br>
是因為 big data 的關係<br>
<br>
0:09:28.300,0:09:31.420<br>
沒有 big data，deep learning 就不會 work<br>
<br>
0:09:31.420,0:09:35.720<br>
其實，我認為並不是這個樣子<br>
<br>
0:09:35.720,0:09:39.100<br>
你知道嗎 ， 如果你仔細想想看<br>
<br>
0:09:39.100,0:09:42.960<br>
假設我有真正很大的 big data<br>
<br>
0:09:42.960,0:09:45.700<br>
假設我們今天要做 image 的 classification<br>
<br>
0:09:45.700,0:09:49.020<br>
然後，我們的 data base 實在是太大了<br>
<br>
0:09:49.020,0:09:52.320<br>
大到我可以把全世界的 image 通通收集進來<br>
<br>
0:09:52.320,0:09:54.100<br>
testing 的每一張 image<br>
<br>
0:09:54.100,0:09:55.420<br>
都在我的 data base 裡面有一張<br>
<br>
0:09:55.420,0:09:57.020<br>
那我何必做 machine learning<br>
<br>
0:09:57.020,0:09:58.740<br>
我直接 table lookup 就好了<br>
<br>
0:09:58.740,0:10:01.860<br>
所以其實 machine learning 跟 big data<br>
<br>
0:10:01.860,0:10:03.640<br>
在某種程度上它們其實是相反的<br>
<br>
0:10:03.640,0:10:05.000<br>
你有真正的 big data 的時候<br>
<br>
0:10:05.000,0:10:05.940<br>
你就不用 learn 它，你就 table lookup<br>
<br>
0:10:05.940,0:10:08.220<br>
我們之所以不能 table lookup<br>
<br>
0:10:08.220,0:10:09.540<br>
就是因為沒有足夠 data<br>
<br>
0:10:09.540,0:10:13.160<br>
所以，我們才需要 machine 去做舉一反三這件事情<br>
<br>
0:10:13.160,0:10:16.620<br>
我們才需要  machine 去做學習這一件事情<br>
<br>
0:10:16.620,0:10:21.540<br>
所以這一邊有沒有跟你的認知是不太一樣的呢<br>
<br>
0:10:21.540,0:10:25.000<br>
其實當我們做 deep learning 的時候<br>
<br>
0:10:25.000,0:10:27.340<br>
就是因為我們沒有足夠的 training data<br>
<br>
0:10:27.340,0:10:29.960<br>
所以，我們需要 deep learning<br>
<br>
0:10:29.960,0:10:33.620<br>
那剩下的我們就留待下一次再說，謝謝<br>
<br>
0:10:46.180,0:10:48.600<br>
好，各位同學大家好<br>
<br>
0:10:48.600,0:10:50.120<br>
我們開始上課吧<br>
<br>
0:10:50.120,0:10:52.600<br>
上周我我們講到說<br>
<br>
0:10:52.600,0:10:55.860<br>
為什麼我們需要用到 deep learning<br>
<br>
0:10:55.860,0:10:59.900<br>
然後 ，這是我們已經講過的<br>
<br>
0:10:59.900,0:11:02.460<br>
如果你們在用 deep learning 的話<br>
<br>
0:11:02.460,0:11:06.560<br>
其實你們是在做模組化這一件事情<br>
<br>
0:11:06.560,0:11:10.420<br>
所以，如果從模組化的觀點來看的話<br>
<br>
0:11:10.420,0:11:14.360<br>
deep learning 所給我們帶來的優勢並不是<br>
<br>
0:11:14.360,0:11:18.220<br>
就像有人說的，我就用一個比較大的 model<br>
<br>
0:11:18.220,0:11:22.600<br>
然後有比較多的參數，collect 比較多的 training data<br>
<br>
0:11:22.600,0:11:23.920<br>
然後，硬 train 下去，所以<br>
<br>
0:11:23.920,0:11:25.860<br>
performance 比 Shallow 的 model 好<br>
<br>
0:11:25.860,0:11:27.220<br>
可能不是這樣<br>
<br>
0:11:27.220,0:11:31.280<br>
因為我們說 deep learning 的好處是來自於模組化<br>
<br>
0:11:31.280,0:11:34.620<br>
那模組化的好處是，我現在是用<br>
<br>
0:11:34.620,0:11:38.840<br>
比較 efficient 的方式來使用我的參數<br>
<br>
0:11:39.660,0:11:44.180<br>
在影像上面<br>
<br>
0:11:44.180,0:11:47.140<br>
你可以觀察到類似模組化的現象<br>
<br>
0:11:47.140,0:11:50.200<br>
我今天要講的是，接下來我要講的是<br>
<br>
0:11:50.200,0:11:51.720<br>
語音的部分<br>
<br>
0:11:51.720,0:11:57.340<br>
那我們知道 deep learning 在影像和語音上表現的特別好<br>
<br>
0:11:57.340,0:12:00.920<br>
我們在來一下在語音上<br>
<br>
0:12:00.920,0:12:04.120<br>
為什麼我們會需要用到模組化的概念<br>
<br>
0:12:04.120,0:12:08.000<br>
那我們先非常非常簡短的介紹一下<br>
<br>
0:12:08.000,0:12:12.340<br>
語音的、人類語言的架構<br>
<br>
0:12:12.340,0:12:14.420<br>
當你說一句話的時候<br>
<br>
0:12:14.420,0:12:17.420<br>
比如說 ，你說 what do you think<br>
<br>
0:12:17.420,0:12:21.440<br>
那這句話其實是由一串 phoneme 所組成的<br>
<br>
0:12:21.440,0:12:25.140<br>
所謂的 phoneme，它的中文翻成音素<br>
<br>
0:12:25.140,0:12:31.820<br>
它是語言學家訂出來的，人類發音的基本單位<br>
<br>
0:12:31.820,0:12:33.460<br>
如果你不知道 phoneme 是甚麼的話<br>
<br>
0:12:33.460,0:12:36.080<br>
就把它想成是音標<br>
<br>
0:12:36.080,0:12:38.460<br>
所以，what 由 4 個 phoneme 組成<br>
<br>
0:12:38.460,0:12:41.340<br>
do 由兩個 phoneme 組成，you 由兩個 phoneme 組成<br>
<br>
0:12:41.340,0:12:46.540<br>
等等，然後接下來，同樣的 phoneme<br>
<br>
0:12:46.540,0:12:49.860<br>
它可能會有不太一樣的發音<br>
<br>
0:12:49.860,0:12:53.180<br>
為甚麼呢？當你發 d uw 的時候<br>
<br>
0:12:53.180,0:12:55.080<br>
和你發 y uw 的時候<br>
<br>
0:12:55.080,0:12:57.860<br>
你心裡想的是同一個 phoneme<br>
<br>
0:12:57.860,0:13:00.760<br>
你心裡想要發的都是 uw<br>
<br>
0:13:00.760,0:13:04.260<br>
但是，因為人類口腔器官的限制<br>
<br>
0:13:04.260,0:13:08.920<br>
所以，你沒辦法每一次發的 uw 都是一樣的<br>
<br>
0:13:08.920,0:13:12.900<br>
為甚麼呢？因為這個 uw 前面跟後面<br>
<br>
0:13:12.900,0:13:15.000<br>
有接了其他的 phoneme<br>
<br>
0:13:15.000,0:13:18.120<br>
因為人類發音器官的限制<br>
<br>
0:13:18.120,0:13:22.400<br>
所以，你的 phoneme 的發音會受到前後的 phone 所影響<br>
<br>
0:13:22.400,0:13:24.820<br>
所以，為了表達這一件事情<br>
<br>
0:13:24.820,0:13:27.880<br>
我們會給同樣的 phoneme<br>
<br>
0:13:27.880,0:13:29.720<br>
不同的 model<br>
<br>
0:13:29.720,0:13:32.280<br>
這個東西叫做 tri-phone<br>
<br>
0:13:32.280,0:13:34.440<br>
那 tri-phone 表達的方式是這樣<br>
<br>
0:13:34.440,0:13:37.540<br>
你把這個 uw<br>
<br>
0:13:37.540,0:13:40.460<br>
加上前面的 phoneme d 跟後面的 phoneme y<br>
<br>
0:13:40.460,0:13:44.620<br>
跟這個 uw 加上前面的 phoneme y 跟後面的 phoneme th<br>
<br>
0:13:44.620,0:13:45.940<br>
就是 tri-phone<br>
<br>
0:13:45.940,0:13:48.160<br>
那有人看到這種表示方式就覺得說<br>
<br>
0:13:48.160,0:13:50.220<br>
tri-phone 就是 3 個 phone<br>
<br>
0:13:50.220,0:13:51.680<br>
看起來像是<br>
<br>
0:13:51.680,0:13:54.940<br>
本來只考慮一個 phone，現在考慮 3 個<br>
<br>
0:13:54.940,0:13:57.460<br>
不是這個意思，不是考慮 3 個 phone 的意思<br>
<br>
0:13:57.460,0:13:59.680<br>
這個意思是說，現在一個 phone<br>
<br>
0:13:59.680,0:14:02.580<br>
我們用不同的 model 來表示它<br>
<br>
0:14:02.580,0:14:05.680<br>
如果一個 phoneme ，它的 contest 不一樣<br>
<br>
0:14:05.680,0:14:07.620<br>
這兩個 uw 的 contest 不一樣<br>
<br>
0:14:07.620,0:14:09.380<br>
我們就用不同的 model<br>
<br>
0:14:09.380,0:14:12.320<br>
來模擬、來描述這樣子的 phoneme<br>
<br>
0:14:12.320,0:14:15.580<br>
那一個 phoneme，它可以拆成幾個 state<br>
<br>
0:14:15.580,0:14:19.780<br>
state 有幾個，其實是你要自己訂的<br>
<br>
0:14:19.780,0:14:22.340<br>
是 engineer 自己訂的，比如說，我們通常就訂成<br>
<br>
0:14:22.340,0:14:24.840<br>
3 個 state<br>
<br>
0:14:24.840,0:14:30.140<br>
那這個是人類語言的基本架構<br>
<br>
0:14:30.140,0:14:32.880<br>
怎麼做語音辨識呢？<br>
<br>
0:14:32.880,0:14:34.300<br>
怎麼做語音辨識呢？<br>
<br>
0:14:34.300,0:14:37.500<br>
語音辨識其實非常的複雜<br>
<br>
0:14:37.500,0:14:40.480<br>
我們現在只是講語音辨識的第一步<br>
<br>
0:14:40.480,0:14:42.520<br>
第一步這是你要做的事情是把<br>
<br>
0:14:42.520,0:14:46.400<br>
acoustic feature 轉成 state<br>
<br>
0:14:46.400,0:14:49.200<br>
這是一個單純的 classification 的 problem<br>
<br>
0:14:49.200,0:14:51.360<br>
這個 classification 的 problem 就跟<br>
<br>
0:14:51.360,0:14:53.060<br>
比如說，你在作業三<br>
<br>
0:14:53.060,0:14:55.760<br>
把 input 一張 image 分成 10 類是一樣的<br>
<br>
0:14:55.760,0:14:57.900<br>
現在只是要 input 一個 acoustic feature<br>
<br>
0:14:57.900,0:15:00.520<br>
然後，把它分說它是哪一個 state<br>
<br>
0:15:00.520,0:15:02.900<br>
所以，acoustic feature 是甚麼呢？<br>
<br>
0:15:02.900,0:15:05.460<br>
這邊我們不會細談，所謂的 acoustic feature<br>
<br>
0:15:05.460,0:15:07.180<br>
簡單講起來就是<br>
<br>
0:15:07.180,0:15:11.120<br>
input 聲音訊號，它是一串 wave form<br>
<br>
0:15:11.120,0:15:16.220<br>
那你把這個，在這個 weight phone 上面取一個 window<br>
<br>
0:15:16.220,0:15:18.940<br>
這個通常不會取太大<br>
<br>
0:15:18.940,0:15:20.500<br>
比如說，250 個 mini second<br>
<br>
0:15:20.500,0:15:24.680<br>
你把一個 window 當作<br>
<br>
0:15:24.680,0:15:25.940<br>
你把一個 window<br>
<br>
0:15:25.940,0:15:27.620<br>
裡面就把它用一個 feature<br>
<br>
0:15:27.620,0:15:30.820<br>
來描述這個 window 裡面的特性<br>
<br>
0:15:30.820,0:15:33.540<br>
那這個東西，就是一個 acoustic feature<br>
<br>
0:15:33.540,0:15:38.220<br>
那你在這個聲音訊號上面呢<br>
<br>
0:15:38.220,0:15:41.040<br>
你會每隔一個時間點，每隔一小段<br>
<br>
0:15:41.040,0:15:44.020<br>
時間，就取一個 window<br>
<br>
0:15:44.020,0:15:47.460<br>
所以，一段聲音訊號就會變成一串<br>
<br>
0:15:47.460,0:15:51.120<br>
vector sequence，這個叫做 acoustic feature sequence<br>
<br>
0:15:51.120,0:15:53.980<br>
那在做語音辨識的第一階段<br>
<br>
0:15:53.980,0:15:55.440<br>
你需要做的事情就是<br>
<br>
0:15:55.440,0:15:58.240<br>
決定每一個 acoustic feature<br>
<br>
0:15:58.240,0:16:00.900<br>
它屬於哪一個 state<br>
<br>
0:16:00.900,0:16:03.840<br>
你要建一個 classifier，這個 classifier 告訴我們說<br>
<br>
0:16:03.840,0:16:07.600<br>
第一個 acoustic feature 它屬於 state a, state a<br>
<br>
0:16:07.600,0:16:09.280<br>
第三個也屬於 state a<br>
<br>
0:16:09.280,0:16:12.000<br>
接下來屬於 state b，接下來屬於 state c 等等<br>
<br>
0:16:12.000,0:16:16.640<br>
光只有做這樣子，是沒有辦法做一個語音辨識系統的<br>
<br>
0:16:16.640,0:16:18.820<br>
這個東西只是 state 而已<br>
<br>
0:16:18.820,0:16:22.220<br>
你要把 state 轉成 phoneme<br>
<br>
0:16:22.220,0:16:24.980<br>
然後再把 phoneme 轉成文字<br>
<br>
0:16:24.980,0:16:28.760<br>
接下來，你還要考慮同音異字的問題<br>
<br>
0:16:28.760,0:16:31.120<br>
用 language model 考慮同音異字的問題，等等<br>
<br>
0:16:31.120,0:16:33.300<br>
這個就不是我們今天所要講的<br>
<br>
0:16:33.300,0:16:36.660<br>
我想要比較一下<br>
<br>
0:16:36.660,0:16:39.800<br>
過去在用 deep learning 之前<br>
<br>
0:16:39.800,0:16:42.540<br>
和用 deep learning 之後，在語音辨識上的模型<br>
<br>
0:16:42.540,0:16:43.480<br>
有什麼不同<br>
<br>
0:16:43.480,0:16:46.180<br>
這個時候，你就更能夠體會說為甚麼 deep learning<br>
<br>
0:16:46.180,0:16:51.160<br>
在語音上，會有非常顯著的成果<br>
<br>
0:16:51.160,0:16:54.180<br>
那我們說<br>
<br>
0:16:54.180,0:16:57.020<br>
我們要繼續做的事情<br>
<br>
0:16:57.060,0:16:59.800<br>
在語音辨識的第一個階段，就是要做分類這個問題<br>
<br>
0:16:59.800,0:17:01.940<br>
也就是決定一個 acoustic feature<br>
<br>
0:17:01.940,0:17:05.160<br>
它屬於哪一個 state<br>
<br>
0:17:05.160,0:17:06.440<br>
那傳統的方法呢<br>
<br>
0:17:06.440,0:17:09.200<br>
叫做這個 HMM-GMM<br>
<br>
0:17:09.200,0:17:11.780<br>
這個 GMM 的方法是怎麼做的呢<br>
<br>
0:17:11.780,0:17:12.900<br>
這個方法是說<br>
<br>
0:17:12.900,0:17:14.600<br>
我們假設每一個 state<br>
<br>
0:17:14.600,0:17:16.920<br>
它就是一個 stationary 的<br>
<br>
0:17:16.920,0:17:21.840<br>
它裡面訊號的分佈<br>
<br>
0:17:21.840,0:17:25.020<br>
每一個屬於某一個 state 的 acoustic feature 的分佈呢<br>
<br>
0:17:25.020,0:17:28.320<br>
是 stationary 的，所以你可以用一個model 來描述他<br>
<br>
0:17:28.320,0:17:30.500<br>
比如說，這一個 state<br>
<br>
0:17:30.500,0:17:34.780<br>
這個第一當作中心的這個 tri-phone 的第一個 state<br>
<br>
0:17:34.780,0:17:39.440<br>
它可以用一個 GMM 來描述它<br>
<br>
0:17:39.440,0:17:41.380<br>
那另外一個 state<br>
<br>
0:17:41.380,0:17:44.500<br>
可以用另外一個 GMM 來描述它<br>
<br>
0:17:44.500,0:17:46.160<br>
這時候給你一個 feature<br>
<br>
0:17:46.160,0:17:48.500<br>
你就可以算說，這一個 acoustic feature<br>
<br>
0:17:48.500,0:17:51.440<br>
從每一個 state 產生出來的機率<br>
<br>
0:17:51.440,0:17:54.680<br>
這個東西叫做 Gaussian Mixture Model，叫做 GMM<br>
<br>
0:17:54.680,0:17:57.880<br>
但是如果說你仔細想一想，發現這一招其實<br>
<br>
0:17:57.880,0:18:00.240<br>
根本不太 work，為甚麼呢？<br>
<br>
0:18:00.240,0:18:03.720<br>
因為 tri-phone 的數目太多了<br>
<br>
0:18:03.720,0:18:06.260<br>
一般語言，中文英文<br>
<br>
0:18:06.260,0:18:10.160<br>
都有 30 幾、將近 40 個 phoneme<br>
<br>
0:18:10.160,0:18:11.680<br>
我們就算 30 個好了<br>
<br>
0:18:11.680,0:18:13.420<br>
那在 tri-phone 裡面<br>
<br>
0:18:13.420,0:18:16.120<br>
每一個 phoneme，隨著它 contest 的不同<br>
<br>
0:18:16.120,0:18:17.660<br>
也要用不同的 model<br>
<br>
0:18:17.660,0:18:19.560<br>
所以，到底有多少個 tri-phone<br>
<br>
0:18:19.560,0:18:23.660<br>
你有 30 的 3 次方個 tri-phone<br>
<br>
0:18:23.660,0:18:28.320<br>
你有 9000 個，不是 9000，是<br>
<br>
0:18:28.320,0:18:31.760<br>
是 27000 個 tri-phone<br>
<br>
0:18:31.760,0:18:33.160<br>
每一個 tri-phone 又有三個 state<br>
<br>
0:18:33.160,0:18:35.760<br>
所以，你有數萬個 state<br>
<br>
0:18:35.760,0:18:39.040<br>
你每一個 state 都要很用一個 GMM 來描述<br>
<br>
0:18:39.040,0:18:40.520<br>
那參數太多了<br>
<br>
0:18:40.520,0:18:41.980<br>
你的 training data 根本不夠<br>
<br>
0:18:41.980,0:18:46.180<br>
所以傳統上，在有 deep learning 之前怎麼處理這件事呢<br>
<br>
0:18:46.180,0:18:48.960<br>
我們說有一些 state<br>
<br>
0:18:48.960,0:18:52.760<br>
其實它們會共用同樣的 model distribution<br>
<br>
0:18:52.760,0:18:56.080<br>
這件事情叫做 Tied-state<br>
<br>
0:18:56.080,0:18:58.180<br>
這件事情叫做 Tied-state<br>
<br>
0:18:58.180,0:19:00.120<br>
那你可能會覺得有點抽象<br>
<br>
0:19:00.120,0:19:04.880<br>
甚麼叫做不同的 state 共用同樣的 distribution 呢<br>
<br>
0:19:04.880,0:19:05.940<br>
意思就是說<br>
<br>
0:19:05.940,0:19:08.900<br>
假如你在寫程式的時候<br>
<br>
0:19:08.900,0:19:13.520<br>
不同的 state 的名稱，就好像是 pointer 一樣<br>
<br>
0:19:13.520,0:19:16.200<br>
所以，實際上你真的在寫程式的時候，你就這麼寫<br>
<br>
0:19:16.200,0:19:18.700<br>
state 的名稱是 pointer<br>
<br>
0:19:18.700,0:19:20.640<br>
那不同的 pointer<br>
<br>
0:19:20.640,0:19:23.880<br>
它們可能會指向同樣的<br>
<br>
0:19:26.440,0:19:30.200<br>
它們可能會指向同樣的 distribution<br>
<br>
0:19:30.200,0:19:31.420<br>
所以，有一些 state<br>
<br>
0:19:31.420,0:19:33.380<br>
它的 distribution 是共用的<br>
<br>
0:19:33.380,0:19:34.660<br>
有一些沒有共用<br>
<br>
0:19:34.660,0:19:36.520<br>
到底哪一些要共有，哪一些不要共用<br>
<br>
0:19:36.520,0:19:39.220<br>
就變成說，你要憑著經驗<br>
<br>
0:19:39.220,0:19:42.300<br>
還有一些語言學的知識阿<br>
<br>
0:19:42.300,0:19:45.660<br>
來決定說哪些 state 它們的聲音是需要共用的<br>
<br>
0:19:45.660,0:19:47.420<br>
可是，這樣是不夠的<br>
<br>
0:19:47.420,0:19:51.040<br>
如果只分 state 的 distribution<br>
<br>
0:19:51.040,0:19:53.360<br>
要共用或不共用，這樣太粗了<br>
<br>
0:19:53.360,0:19:55.740<br>
所以，有的人就會開始提出一些想法說<br>
<br>
0:19:55.740,0:19:59.060<br>
如何讓它部分共用，等等<br>
<br>
0:19:59.060,0:20:01.320<br>
那在 deep learning 火紅之前<br>
<br>
0:20:01.320,0:20:05.660<br>
再前一個提出來比較有創新的方法，叫做 subspace GMM<br>
<br>
0:20:05.660,0:20:10.200<br>
那其實它裡面有這個 modularization、有模組化的影子<br>
<br>
0:20:10.200,0:20:12.380<br>
在這個 subspace GMM 裡面呢<br>
<br>
0:20:12.380,0:20:15.580<br>
這個方法是說，我們原來是每一個<br>
<br>
0:20:15.580,0:20:17.680<br>
state 它就有一個 distribution<br>
<br>
0:20:17.680,0:20:19.860<br>
在 subspace GMM 裡面，它說<br>
<br>
0:20:19.860,0:20:23.320<br>
我們先把很多很多的 Gaussian<br>
<br>
0:20:23.320,0:20:26.700<br>
先找出來，我們先找一個 Gaussian pool<br>
<br>
0:20:26.700,0:20:30.240<br>
那每一個 state，它的 information 就是一個 key<br>
<br>
0:20:30.240,0:20:33.220<br>
那一個  key 告訴我們說，這個 state<br>
<br>
0:20:33.220,0:20:36.420<br>
要從這個 Gaussian 的 pool 裡面<br>
<br>
0:20:36.420,0:20:40.920<br>
挑那些 Gaussian 出來，比如說<br>
<br>
0:20:40.920,0:20:42.540<br>
可能有某一個 state 1<br>
<br>
0:20:42.540,0:20:45.240<br>
它挑第一、第三、第五個 Gaussian<br>
<br>
0:20:45.240,0:20:48.400<br>
某一個 state 2，它挑第一、第四、第六個 Gaussian<br>
<br>
0:20:48.400,0:20:50.260<br>
如果你這樣做的話<br>
<br>
0:20:50.260,0:20:53.620<br>
這些 state 有些時候就可以 share 部分的 Gaussian<br>
<br>
0:20:53.620,0:20:56.020<br>
那有些時候就可以完全不 share Gaussian<br>
<br>
0:20:56.020,0:20:57.780<br>
那至於要 share 多少的 Gaussian<br>
<br>
0:20:57.780,0:21:02.120<br>
這個東西，是可以從 training data 去把它學出來的<br>
<br>
0:21:02.120,0:21:06.320<br>
這個是在 DNN 火紅之前的做法<br>
<br>
0:21:06.320,0:21:09.580<br>
但是，如果你仔細想想<br>
<br>
0:21:09.580,0:21:12.340<br>
剛才講的這個，HMM-GMM 的方式<br>
<br>
0:21:12.340,0:21:15.020<br>
所有的 phone 或者是 state<br>
<br>
0:21:15.020,0:21:16.960<br>
是 independent model 的<br>
<br>
0:21:16.960,0:21:20.180<br>
這件事情是不 efficient 的<br>
<br>
0:21:20.180,0:21:22.260<br>
對 model 人類的聲音來說<br>
<br>
0:21:22.260,0:21:24.900<br>
那如果你想想看人類的聲音<br>
<br>
0:21:24.900,0:21:27.700<br>
不同的 phoneme<br>
<br>
0:21:27.700,0:21:32.080<br>
雖然說我們把它歸類為不同的音素<br>
<br>
0:21:32.080,0:21:35.220<br>
我們在分類的時候把他歸類為不同的 class<br>
<br>
0:21:35.220,0:21:37.240<br>
但這些 phoneme 之間並不是<br>
<br>
0:21:37.240,0:21:38.800<br>
完全無關的<br>
<br>
0:21:38.800,0:21:44.040<br>
它們都是由人類的發音器官所 generate 出來的<br>
<br>
0:21:44.040,0:21:48.140<br>
它們中間是有根據人類發音器官發音的方式<br>
<br>
0:21:48.140,0:21:50.120<br>
它們是有某些關係的<br>
<br>
0:21:50.120,0:21:52.640<br>
舉例來說，在這個圖上<br>
<br>
0:21:52.640,0:21:55.480<br>
這個圖呢，在這個圖上呢<br>
<br>
0:21:55.480,0:22:01.000<br>
人類語言裡面所有的母音<br>
<br>
0:22:01.000,0:22:03.680<br>
這個母音的發音呢<br>
<br>
0:22:03.680,0:22:06.940<br>
其實，就只受到三件事情的影響而已<br>
<br>
0:22:06.940,0:22:10.560<br>
一個是你舌頭的前後的位置<br>
<br>
0:22:10.560,0:22:13.340<br>
一個是你舌頭上下的位置<br>
<br>
0:22:13.340,0:22:16.700<br>
還有一個，就是你的嘴型<br>
<br>
0:22:16.700,0:22:18.240<br>
所以，一個母音的發音<br>
<br>
0:22:18.240,0:22:20.480<br>
其實就只受到這三件事的影響而已<br>
<br>
0:22:20.480,0:22:22.100<br>
比如說，在這個圖上呢<br>
<br>
0:22:22.100,0:22:25.580<br>
你可以找到英文的五個<br>
<br>
0:22:25.580,0:22:30.840<br>
常見的、英文的 5 個母音 a, e, i, o, u<br>
<br>
0:22:30.840,0:22:33.400<br>
這個  a, e, i, o, u 啊<br>
<br>
0:22:33.400,0:22:35.380<br>
它們之間的差別就是<br>
<br>
0:22:35.380,0:22:39.840<br>
當你發 a 到 e 到 i 的時候<br>
<br>
0:22:39.840,0:22:42.280<br>
你的舌頭是由下往上<br>
<br>
0:22:42.280,0:22:45.000<br>
那個 i 跟 o 的差別呢<br>
<br>
0:22:45.000,0:22:48.380<br>
是你的舌頭放在前面或放在後面的差別<br>
<br>
0:22:48.380,0:22:53.740<br>
所以，如果你發 a, e, i, o, u 的話<br>
<br>
0:22:53.740,0:22:55.540<br>
你的舌頭變化方式呢<br>
<br>
0:22:55.540,0:22:57.100<br>
就會這張圖一樣<br>
<br>
0:22:57.100,0:22:59.560<br>
相信這個時候，你心理一定是在想<br>
<br>
0:22:59.560,0:23:01.920<br>
一定是在默念 a, e, i, o, u 這樣<br>
<br>
0:23:01.920,0:23:04.500<br>
然後，你會想說<br>
<br>
0:23:04.500,0:23:06.240<br>
怎麼感覺不太出來<br>
<br>
0:23:06.240,0:23:11.260<br>
我發現你自己唸，不太會感覺你的舌頭位置在哪裡<br>
<br>
0:23:11.260,0:23:15.220<br>
你要知道說，你的舌頭位置是不是真的跟這個圖上一樣<br>
<br>
0:23:15.220,0:23:19.540<br>
你就回去張大嘴巴，對著鏡子唸 a, e, i, o, u<br>
<br>
0:23:19.540,0:23:21.780<br>
你會發現說，你舌頭的位置呢<br>
<br>
0:23:21.780,0:23:23.820<br>
就跟這個圖上是一模一樣的<br>
<br>
0:23:24.320,0:23:27.640<br>
在這個圖上，同一個位置<br>
<br>
0:23:27.640,0:23:29.680<br>
的母音呢<br>
<br>
0:23:29.680,0:23:33.040<br>
代表說，舌頭的位置是一樣的，但是<br>
<br>
0:23:33.040,0:23:34.840<br>
嘴型是不一樣的<br>
<br>
0:23:34.840,0:23:36.780<br>
比如說，我們看左上角<br>
<br>
0:23:36.780,0:23:40.320<br>
在最左上角的位置有兩個母音<br>
<br>
0:23:40.320,0:23:45.320<br>
一個是 i，一個是 u 這樣<br>
<br>
0:23:45.320,0:23:47.520<br>
那 i 跟 u 的差別<br>
<br>
0:23:47.520,0:23:51.120<br>
它們舌頭位置是一樣的，只是嘴型是不一樣的<br>
<br>
0:23:51.120,0:23:54.420<br>
如果是 i 的話，嘴是比較扁的<br>
<br>
0:23:54.420,0:23:57.520<br>
u 的話，嘴是比較圓的<br>
<br>
0:23:57.520,0:24:00.520<br>
所以，你只要改變嘴型的位置，就可以從 i 變成 u<br>
<br>
0:24:00.520,0:24:03.860<br>
你本來發 i~~~u~~~ 這樣子<br>
<br>
0:24:03.860,0:24:06.380<br>
你改變一下嘴型，它的發音就不一樣<br>
<br>
0:24:10.760,0:24:15.620<br>
所以說，因為不同的 phoneme 之間是有關係的<br>
<br>
0:24:15.620,0:24:19.180<br>
所以，你說每一個 phoneme 都搞一個自己的 model<br>
<br>
0:24:19.180,0:24:22.420<br>
這件事情其實是沒有效率的<br>
<br>
0:24:22.420,0:24:24.820<br>
那如果今天是用<br>
<br>
0:24:24.820,0:24:26.740<br>
deep learning 是怎麼做的呢<br>
<br>
0:24:26.740,0:24:28.880<br>
如果是 deep learning 的話<br>
<br>
0:24:28.880,0:24:33.160<br>
你就是去 learn 一個 deep neural network<br>
<br>
0:24:33.160,0:24:35.100<br>
這個 deep neural network 的 input 呢<br>
<br>
0:24:35.100,0:24:38.000<br>
就是一個 acoustic feature<br>
<br>
0:24:38.000,0:24:43.080<br>
它的 output 就是每一個 feature 屬於哪一個 state 的機率<br>
<br>
0:24:43.080,0:24:45.420<br>
這是一個很單純的 classification 的 problem<br>
<br>
0:24:45.420,0:24:48.200<br>
跟你作業三做在影像上是沒有甚麼差別的<br>
<br>
0:24:48.200,0:24:51.760<br>
learn 一個 DNN，input 是一個 acoustic feature<br>
<br>
0:24:51.760,0:24:55.100<br>
output 就是告訴你說，這個 acoustic feature<br>
<br>
0:24:55.100,0:24:56.240<br>
屬於每一個 state<br>
<br>
0:24:56.240,0:24:59.980<br>
它屬於 state a, state b, state c 的機率<br>
<br>
0:24:59.980,0:25:02.800<br>
那這邊最關鍵的一點是<br>
<br>
0:25:02.800,0:25:04.720<br>
所有的 state<br>
<br>
0:25:04.720,0:25:07.500<br>
都共用同一個 DNN<br>
<br>
0:25:07.500,0:25:10.260<br>
在整個辨識裡面，你就只有一個 DNN 而已<br>
<br>
0:25:10.260,0:25:14.840<br>
你沒有每一個 state 都有一個 DNN<br>
<br>
0:25:14.840,0:25:16.240<br>
所以，有人覺得說<br>
<br>
0:25:16.240,0:25:18.700<br>
所以，有些人他沒有想清楚<br>
<br>
0:25:18.700,0:25:21.480<br>
這個 deep learning 到底 powerful 在哪裡，他會說<br>
<br>
0:25:21.480,0:25:24.160<br>
從 GMM 變到 deep learning 厲害的地方就是<br>
<br>
0:25:24.160,0:25:30.780<br>
本來 GMM 通常你最多也就做 64 個 Gaussian mixture 而已<br>
<br>
0:25:30.780,0:25:35.880<br>
那 DNN 有 10 層，每層 1000 個 neuron<br>
<br>
0:25:35.880,0:25:38.780<br>
果然參數很多，參數變多了，所以 performance 變好了<br>
<br>
0:25:38.780,0:25:41.400<br>
這是一個暴力輾壓的方法，其實也沒什麼<br>
<br>
0:25:41.400,0:25:43.580<br>
其實 DNN 不是一個暴力輾壓的方法<br>
<br>
0:25:43.580,0:25:48.100<br>
你仔細想想看，在做 HMM-GMM 的時候<br>
<br>
0:25:48.100,0:25:49.860<br>
你說 GMM 只有 64 個 mixture<br>
<br>
0:25:49.860,0:25:52.460<br>
好像覺得很簡單，但是其實你是<br>
<br>
0:25:52.460,0:25:55.040<br>
每一個 state 都有一個 Gaussian mixture<br>
<br>
0:25:55.040,0:25:57.640<br>
所以真正合起來，它的參數是多得不得了的<br>
<br>
0:25:57.640,0:25:59.920<br>
如果你仔細去算一下<br>
<br>
0:25:59.920,0:26:02.800<br>
GMM 用的參數跟 DNN 用的參數<br>
<br>
0:26:02.800,0:26:05.420<br>
我曾經在不同的 task 上估測過這一件事情<br>
<br>
0:26:05.420,0:26:08.280<br>
它們用的參數，你會發現其實是差不多多的<br>
<br>
0:26:08.280,0:26:10.780<br>
所以，DNN 它只是用一個很大的 model<br>
<br>
0:26:10.780,0:26:13.280<br>
GMM 是用很多很小的 model<br>
<br>
0:26:13.280,0:26:15.700<br>
但是，當我們把這兩個東西拿來比較的時候<br>
<br>
0:26:15.700,0:26:18.280<br>
其實它們用的參數量，是差不多多的<br>
<br>
0:26:18.280,0:26:21.320<br>
但是，DNN 把所有的<br>
<br>
0:26:21.320,0:26:25.580<br>
它把所有的 state<br>
<br>
0:26:25.580,0:26:29.640<br>
通通用同一個 model 來做分類<br>
<br>
0:26:29.640,0:26:32.060<br>
會是比較有效率的做法<br>
<br>
0:26:32.060,0:26:35.400<br>
為甚麼這樣是比較有效率的做法呢<br>
<br>
0:26:35.400,0:26:39.160<br>
舉例來說，如果你今天把<br>
<br>
0:26:39.160,0:26:40.700<br>
一個 DNN<br>
<br>
0:26:40.700,0:26:44.140<br>
它的某一個 hidden layer 拿出來<br>
<br>
0:26:44.140,0:26:47.480<br>
然後，因為一個 hidden layer<br>
<br>
0:26:47.480,0:26:49.560<br>
比如說，它其實有 1000 個 neuron<br>
<br>
0:26:49.560,0:26:51.280<br>
你沒有辦法分析它<br>
<br>
0:26:51.280,0:26:54.060<br>
但是，你可以把那 1000 個 layer 的 output 降維<br>
<br>
0:26:54.060,0:26:56.060<br>
降到 2 維<br>
<br>
0:26:56.060,0:26:58.080<br>
所以，在這個圖上<br>
<br>
0:26:58.080,0:27:03.480<br>
每一個點代表了一個 acoustic feature<br>
<br>
0:27:03.480,0:27:06.320<br>
它通過 DNN 以後<br>
<br>
0:27:06.320,0:27:09.960<br>
它通過 DNN 以後<br>
<br>
0:27:09.960,0:27:12.860<br>
它把它這個 output layer 的 output 降到二維<br>
<br>
0:27:12.860,0:27:15.280<br>
可以發現說它的分布是長這個樣子的<br>
<br>
0:27:15.280,0:27:18.460<br>
在這個圖上的顏色代表甚麼意思呢<br>
<br>
0:27:18.460,0:27:21.340<br>
這邊的顏色，其實就是<br>
<br>
0:27:21.340,0:27:23.340<br>
a, e, i<br>
<br>
0:27:23.340,0:27:31.100<br>
a, e, i, o, u 這樣，特別把這 5 個母音<br>
<br>
0:27:31.100,0:27:34.680<br>
用跟這邊圖的顏色一樣的框框<br>
<br>
0:27:34.680,0:27:36.020<br>
把它框起來<br>
<br>
0:27:36.020,0:27:38.800<br>
那你會發現神奇的事就是<br>
<br>
0:27:38.800,0:27:41.820<br>
這邊，這 5 個母音的分布<br>
<br>
0:27:41.820,0:27:46.380<br>
跟這一個圖的分布，其實幾乎是一樣的<br>
<br>
0:27:46.380,0:27:49.840<br>
這邊是 a, e, i, o, u<br>
<br>
0:27:49.840,0:27:51.920<br>
這邊是 a, e, i, o, u<br>
<br>
0:27:51.920,0:27:54.420<br>
所以，你可以發現說<br>
<br>
0:27:54.420,0:27:56.180<br>
DNN 在做的事情<br>
<br>
0:27:56.180,0:27:58.820<br>
它的比較 lower 的 layer 做的事情<br>
<br>
0:27:58.820,0:27:59.720<br>
它其實是在<br>
<br>
0:27:59.720,0:28:02.420<br>
它並不是真的要要馬上去偵測說<br>
<br>
0:28:02.420,0:28:04.560<br>
現在 input 這個發音，它是屬於<br>
<br>
0:28:04.560,0:28:07.400<br>
哪一個 phone 或哪一個 state，它做的事情是<br>
<br>
0:28:07.400,0:28:10.480<br>
它先觀察說，當你聽到這個發音的時候<br>
<br>
0:28:10.480,0:28:12.520<br>
人是用甚麼樣的方式<br>
<br>
0:28:12.520,0:28:13.880<br>
在發這個聲音的<br>
<br>
0:28:13.880,0:28:15.860<br>
它的舌頭的位置在哪裡<br>
<br>
0:28:15.860,0:28:18.500<br>
它的舌頭位置是高還是低呢<br>
<br>
0:28:18.500,0:28:21.900<br>
它的舌頭位置是在前還是後呢，等等<br>
<br>
0:28:21.900,0:28:25.860<br>
然後，lower 的 layer，比較靠近 input 的 layer<br>
<br>
0:28:25.860,0:28:28.180<br>
我們今天知道了發音的方式以後<br>
<br>
0:28:28.180,0:28:31.060<br>
接下來的 layer，再根據<br>
<br>
0:28:31.060,0:28:33.680<br>
這個結果，去決定說<br>
<br>
0:28:33.680,0:28:37.400<br>
現在的發音是屬於哪一個 state 或哪一個 phone<br>
<br>
0:28:37.400,0:28:39.420<br>
所以，所有的 phone 呢<br>
<br>
0:28:39.420,0:28:42.380<br>
會用同一組 detector<br>
<br>
0:28:42.380,0:28:44.100<br>
也就是這些 lower 的 layer<br>
<br>
0:28:44.100,0:28:46.140<br>
是一個人類發音方式的 detector<br>
<br>
0:28:46.140,0:28:48.600<br>
而所有的 phone 的偵測都是用<br>
<br>
0:28:48.600,0:28:50.580<br>
同一組 detector 完成的<br>
<br>
0:28:50.580,0:28:54.220<br>
所有 phone 的偵測，都 share 同一組的參數<br>
<br>
0:28:54.220,0:28:58.460<br>
所以，它這邊就有做到模組化這件事情<br>
<br>
0:28:58.460,0:29:00.320<br>
當你做模組化的時候<br>
<br>
0:29:00.320,0:29:02.960<br>
你是用比較少的參數<br>
<br>
0:29:02.960,0:29:05.880<br>
你是用比較有效率的方式<br>
<br>
0:29:05.880,0:29:07.720<br>
來使用你的參數<br>
<br>
0:29:07.720,0:29:10.120<br>
所以，我們回到<br>
<br>
0:29:10.120,0:29:11.980<br>
我們很久以前就提過的<br>
<br>
0:29:11.980,0:29:14.400<br>
Universality 的 Theorem<br>
<br>
0:29:14.400,0:29:18.160<br>
過去有一個理論告訴我們說<br>
<br>
0:29:18.160,0:29:20.660<br>
任何的 continuous 的 function<br>
<br>
0:29:20.660,0:29:24.860<br>
它都可以用一層 neural network 來完成<br>
<br>
0:29:24.860,0:29:27.840<br>
只要那層 neural network 夠寬的話<br>
<br>
0:29:27.840,0:29:30.160<br>
在 90 年代<br>
<br>
0:29:30.160,0:29:33.740<br>
這是很多人放棄做 deep learning 的一個原因<br>
<br>
0:29:33.740,0:29:35.800<br>
你想想看，只要一層 hidden layer<br>
<br>
0:29:35.800,0:29:39.120<br>
就可以完成所有的 function<br>
<br>
0:29:39.120,0:29:41.380<br>
一層 hidden layer 就可以表示所有的 function<br>
<br>
0:29:41.380,0:29:44.160<br>
那做 deep learning 的意義何在呢<br>
<br>
0:29:44.160,0:29:47.620<br>
所以很多人覺得說，deep 是沒有必要的<br>
<br>
0:29:47.620,0:29:50.400<br>
我們就只要一個 hidden layer 就好<br>
<br>
0:29:51.380,0:29:55.540<br>
但是，這個理論有一件事情沒有告訴我們的是<br>
<br>
0:29:55.540,0:29:58.560<br>
它只告訴我們可能性<br>
<br>
0:29:58.560,0:30:02.380<br>
但是它沒有告訴我們說，要做到這件事情<br>
<br>
0:30:02.380,0:30:04.780<br>
有多有效率<br>
<br>
0:30:04.780,0:30:08.640<br>
就是，沒錯，你只要有夠多的<br>
<br>
0:30:08.640,0:30:11.380<br>
參數，你只要這個 hidden layer 夠寬<br>
<br>
0:30:11.380,0:30:15.520<br>
你就可以描述任何的 function<br>
<br>
0:30:15.520,0:30:18.440<br>
但是，這個理論沒有告訴我們的事情是<br>
<br>
0:30:18.440,0:30:21.660<br>
當我們用這一件事情<br>
<br>
0:30:21.660,0:30:23.560<br>
我們只用一個 hidden layer<br>
<br>
0:30:23.560,0:30:25.660<br>
來描述 function 的時候<br>
<br>
0:30:25.660,0:30:27.700<br>
它其實是沒有效率的<br>
<br>
0:30:27.700,0:30:31.740<br>
當你有 multi-layer，當你有 hierarchy 的 structure<br>
<br>
0:30:31.740,0:30:34.860<br>
你用這個方式來描述你的 function 的時候<br>
<br>
0:30:34.860,0:30:36.960<br>
它是比較有效率的<br>
<br>
0:30:37.720,0:30:41.440<br>
如果剛才模組化的概念，你沒有聽得很明白的話呢<br>
<br>
0:30:41.440,0:30:43.840<br>
我們這邊舉另外一個例子<br>
<br>
0:30:43.840,0:30:46.880<br>
如果，你是 EE 的 background<br>
<br>
0:30:46.880,0:30:49.200<br>
然後，你修過交換電路的話<br>
<br>
0:30:49.200,0:30:52.440<br>
我相信你聽過這個例子以後，就會對<br>
<br>
0:30:52.440,0:30:56.500<br>
deep 為甚麼 powerful 沒有太多的懷疑<br>
<br>
0:30:56.500,0:31:00.740<br>
我想 EE background 的人，都修過<br>
<br>
0:31:00.740,0:31:02.700<br>
邏輯電路<br>
<br>
0:31:02.700,0:31:07.160<br>
其實邏輯電路可以跟 neural network 類比<br>
<br>
0:31:07.160,0:31:09.580<br>
我們知道在邏輯電路裡面<br>
<br>
0:31:09.580,0:31:12.840<br>
我們的電路是由一堆邏輯閘<br>
<br>
0:31:12.840,0:31:15.300<br>
AND gate, NOR gate 所構成的<br>
<br>
0:31:15.300,0:31:18.400<br>
對不對，在 neural network 裡面<br>
<br>
0:31:18.400,0:31:22.680<br>
整個 network 是由一堆 neuron、神經元所構成的<br>
<br>
0:31:22.680,0:31:26.660<br>
如果你有修過邏輯電路的話，你會知道說<br>
<br>
0:31:26.660,0:31:29.580<br>
其實只要兩層邏輯閘<br>
<br>
0:31:29.580,0:31:33.100<br>
你就可以表示任何的 boolean function<br>
<br>
0:31:33.100,0:31:36.700<br>
如果你修過邏輯電路的話，你應該知道這件事<br>
<br>
0:31:36.700,0:31:39.940<br>
這件事情應該不會讓你特別的驚訝<br>
<br>
0:31:39.940,0:31:44.920<br>
所以，既然兩層邏輯閘可以表示任何的 boolean function<br>
<br>
0:31:44.920,0:31:47.580<br>
那有一個 hidden layer 的 neural network<br>
<br>
0:31:47.580,0:31:49.620<br>
有一個 hidden layer 的 neural network，其實也是兩層<br>
<br>
0:31:49.620,0:31:52.740<br>
它一個 input layer、一個 output layer，所以它也是兩層<br>
<br>
0:31:52.740,0:31:54.800<br>
有一個 hidden layer 的 neural network<br>
<br>
0:31:54.800,0:31:57.060<br>
它可以表示任何的 continuous function<br>
<br>
0:31:57.060,0:32:00.100<br>
其實，也不會讓人特別的驚訝<br>
<br>
0:32:00.100,0:32:03.860<br>
但是，雖然我們可以用兩層邏輯閘<br>
<br>
0:32:03.860,0:32:07.200<br>
就描述任何的 boolean function<br>
<br>
0:32:07.200,0:32:10.280<br>
但是，實際上你在做電路設計的時候，你根本<br>
<br>
0:32:10.280,0:32:12.180<br>
不可能會這樣做，對不對<br>
<br>
0:32:12.180,0:32:15.180<br>
你可以用兩層邏輯閘就做一台電腦，但是<br>
<br>
0:32:15.180,0:32:17.300<br>
沒有人會這麼做<br>
<br>
0:32:17.300,0:32:22.620<br>
為甚麼呢？因為當你用 hierarchy 的架構的時候<br>
<br>
0:32:22.620,0:32:24.700<br>
當你不是用兩層邏輯閘，而是用<br>
<br>
0:32:24.700,0:32:26.380<br>
很多層的時候<br>
<br>
0:32:26.380,0:32:29.240<br>
這個時候，你拿來設計一個電路是<br>
<br>
0:32:29.240,0:32:30.500<br>
比較有效率的<br>
<br>
0:32:30.500,0:32:32.720<br>
雖然，兩層邏輯閘可以做到同樣的事情<br>
<br>
0:32:32.720,0:32:34.820<br>
但是，這麼做是沒有效率的<br>
<br>
0:32:34.820,0:32:37.620<br>
如果類比到 neural network 的話<br>
<br>
0:32:37.620,0:32:39.920<br>
其實，意思是一樣的<br>
<br>
0:32:39.920,0:32:44.080<br>
你用一層 hidden layer 可以做到任何事情<br>
<br>
0:32:44.080,0:32:48.740<br>
但是，用比較多的 hidden layer 是比較有效率的<br>
<br>
0:32:49.220,0:32:53.360<br>
所以，從邏輯閘這邊來看，你用多個邏輯閘<br>
<br>
0:32:53.360,0:32:57.680<br>
你用多層的架構，可以用比較少的邏輯閘就完成一個電路<br>
<br>
0:32:57.680,0:33:00.620<br>
那你用比較多層的<br>
<br>
0:33:00.620,0:33:03.400<br>
你用比較多層的 neural network<br>
<br>
0:33:03.400,0:33:07.020<br>
你就可以用比較少的 neuron 就完成同樣的 function<br>
<br>
0:33:07.020,0:33:09.660<br>
所以，你會需要比較少的參數<br>
<br>
0:33:09.660,0:33:11.660<br>
比較少的參數意謂著甚麼<br>
<br>
0:33:11.660,0:33:14.840<br>
比較少的參數意謂著，你比較不容易 overfitting<br>
<br>
0:33:14.840,0:33:19.060<br>
或者是，你其實只需要比較少的 data<br>
<br>
0:33:19.060,0:33:23.500<br>
你就可以完成你現在要 train 的任務<br>
<br>
0:33:23.500,0:33:27.940<br>
所以，這件事情有沒有跟你平常的認知是相反的呢<br>
<br>
0:33:27.940,0:33:29.780<br>
很多人的認知是<br>
<br>
0:33:29.780,0:33:32.800<br>
deep learning 就是很多 data 硬輾壓過去<br>
<br>
0:33:32.800,0:33:35.720<br>
其實不是，當我們用 deep learning 的時候<br>
<br>
0:33:35.720,0:33:37.780<br>
我們可以用比較少的 data<br>
<br>
0:33:37.780,0:33:39.700<br>
就達到同樣的任務<br>
<br>
0:33:39.700,0:33:45.180<br>
我們從邏輯閘這邊，再舉一個實際的例子<br>
<br>
0:33:45.180,0:33:48.240<br>
假設我們現在要做 parity check<br>
<br>
0:33:48.240,0:33:50.320<br>
假設你要設計一個電路做 parity check<br>
<br>
0:33:50.320,0:33:52.780<br>
那甚麼是 parity check，就是<br>
<br>
0:33:52.780,0:33:56.180<br>
你希望 input 一串數字<br>
<br>
0:33:56.180,0:33:58.920<br>
input 一串 binary 的數字<br>
<br>
0:33:58.920,0:34:03.300<br>
如果裡面出現的 1 的數目是偶數的話<br>
<br>
0:34:03.300,0:34:06.480<br>
它的 output 就是 1，如果出現的是奇數的話呢<br>
<br>
0:34:06.480,0:34:08.680<br>
它的 output 就是 0<br>
<br>
0:34:08.680,0:34:13.580<br>
假設你 input 的 sequence 的長度<br>
<br>
0:34:13.580,0:34:16.220<br>
總共有 1 個 bit 的話<br>
<br>
0:34:16.220,0:34:19.440<br>
那用兩層邏輯閘，理論上可以保證你<br>
<br>
0:34:19.440,0:34:23.660<br>
你要 2^d 個 gate<br>
<br>
0:34:23.660,0:34:28.200<br>
你要 2^d 個 gate，才能夠描述<br>
<br>
0:34:28.200,0:34:33.320<br>
才能描述這樣子的一個電路<br>
<br>
0:34:33.320,0:34:36.340<br>
但是，如果你用多層次的架構的話<br>
<br>
0:34:36.340,0:34:37.960<br>
你就可以用比較少的邏輯閘<br>
<br>
0:34:37.960,0:34:40.060<br>
就做到 parity check 這件事情<br>
<br>
0:34:40.060,0:34:42.380<br>
舉例來說，你可以把<br>
<br>
0:34:42.380,0:34:45.900<br>
好幾個 XNOR gate 接在一起<br>
<br>
0:34:45.900,0:34:50.820<br>
如果你把邏輯閘用這種方式接的話<br>
<br>
0:34:50.820,0:34:52.720<br>
現在 input 1 跟 0<br>
<br>
0:34:52.720,0:34:56.880<br>
我把 XNOR gate 的真值表放在右上角<br>
<br>
0:34:56.880,0:34:59.380<br>
input 1 跟 0，它的 output 就是 0<br>
<br>
0:34:59.380,0:35:02.960<br>
i然後，input 0 跟 1，它的 output 就是 0<br>
<br>
0:35:02.960,0:35:04.940<br>
input 0 跟 0，它的 output 是 1<br>
<br>
0:35:04.940,0:35:07.180<br>
你就做完 parity check 這件事情了<br>
<br>
0:35:07.180,0:35:11.340<br>
這邊用的就是一個 hierarchical 的架構<br>
<br>
0:35:11.340,0:35:13.820<br>
當你用這樣子的架構的時候<br>
<br>
0:35:13.820,0:35:17.560<br>
當你用這種比較多層次的架構的時候<br>
<br>
0:35:17.560,0:35:20.720<br>
你其實只需要 O(d) gates<br>
<br>
0:35:20.720,0:35:23.520<br>
你就可以完成你現在要做的任務了<br>
<br>
0:35:23.520,0:35:27.300<br>
所以，當你用比較多層次的架構來設計電路的時候<br>
<br>
0:35:27.300,0:35:30.480<br>
你可以用比較少的邏輯閘，就達到同樣的事情<br>
<br>
0:35:30.480,0:35:32.700<br>
這對 neural network 來說也是一樣的<br>
<br>
0:35:32.700,0:35:35.640<br>
用比較少的 neuron，去描述同樣的 function<br>
<br>
0:35:35.640,0:35:39.300<br>
如果剛才舉的例子，你沒有聽懂的話<br>
<br>
0:35:39.300,0:35:41.620<br>
如果你沒有修過邏輯電路，你沒有聽懂的話<br>
<br>
0:35:41.620,0:35:44.940<br>
以下是一個日常生活中就會碰到的例子<br>
<br>
0:35:44.940,0:35:46.920<br>
這個例子是剪窗花的<br>
<br>
0:35:46.920,0:35:48.860<br>
剪窗花大家知道嗎？<br>
<br>
0:35:48.860,0:35:50.500<br>
剪窗花就是說<br>
<br>
0:35:50.500,0:35:52.120<br>
這個應該不用解釋啦<br>
<br>
0:35:52.120,0:35:54.620<br>
就是一個色紙，然後把它摺起來<br>
<br>
0:35:54.620,0:35:56.960<br>
然後再剪一剪，就可以變成這個樣子<br>
<br>
0:35:56.960,0:36:00.320<br>
你並不是真的去把這個形狀的花樣剪出來<br>
<br>
0:36:00.320,0:36:02.280<br>
這樣太麻煩了，你先把紙摺起來<br>
<br>
0:36:02.280,0:36:03.820<br>
然後才剪這樣子<br>
<br>
0:36:03.820,0:36:06.600<br>
這個跟 deep learning 有什麼關係呢<br>
<br>
0:36:06.600,0:36:12.100<br>
你想想看我們用之前講的<br>
<br>
0:36:12.100,0:36:15.460<br>
我們用之前講的這個例子來做比喻<br>
<br>
0:36:15.460,0:36:19.540<br>
假設我們現在 input 的點有四個<br>
<br>
0:36:19.540,0:36:23.180<br>
有四個，那這個紅色的點是一類<br>
<br>
0:36:23.180,0:36:25.420<br>
藍色的點是一類<br>
<br>
0:36:25.420,0:36:28.280<br>
我們之前講說如果你沒有 hidden layer 的話<br>
<br>
0:36:28.280,0:36:30.580<br>
如果你是一個 linear 的 model，你要怎麼做<br>
<br>
0:36:30.580,0:36:32.960<br>
都沒有辦法把藍色分在一邊<br>
<br>
0:36:32.960,0:36:34.500<br>
把紅色分在一邊<br>
<br>
0:36:34.500,0:36:37.720<br>
但是，當你加了 hidden layer 的時候會發生什麼事呢<br>
<br>
0:36:37.720,0:36:40.060<br>
當你加了 hidden layer 的時候<br>
<br>
0:36:40.060,0:36:43.200<br>
就做了一個 feature 的 transformation<br>
<br>
0:36:43.200,0:36:45.700<br>
你把原來的 x1, x2<br>
<br>
0:36:45.700,0:36:47.260<br>
你把原來的 x1, x2<br>
<br>
0:36:47.260,0:36:50.620<br>
ship 到另外一個平面，轉換到另外一個平面<br>
<br>
0:36:50.620,0:36:53.440<br>
變成 x1'、x2'<br>
<br>
0:36:53.440,0:36:55.800<br>
變成 x1'、x2'<br>
<br>
0:36:55.800,0:36:59.220<br>
所以，原來的紅色這個點跑到這裡<br>
<br>
0:36:59.220,0:37:02.000<br>
原來的紅色這個點跑到這裡<br>
<br>
0:37:02.000,0:37:04.820<br>
原來這兩個藍色的點都跑到這裡<br>
<br>
0:37:04.820,0:37:07.200<br>
所以，你發現這兩個藍色的點呢<br>
<br>
0:37:07.200,0:37:08.940<br>
是重合在一起<br>
<br>
0:37:08.940,0:37:12.140<br>
所以，當你從這裡<br>
<br>
0:37:12.140,0:37:16.920<br>
通過一個 hidden layer 變到這裡的時候<br>
<br>
0:37:16.920,0:37:20.480<br>
其實你就好像是把原來的這個平面<br>
<br>
0:37:20.480,0:37:21.720<br>
對折了一樣<br>
<br>
0:37:21.720,0:37:24.200<br>
你把這個平面對折，所以這個藍色的點<br>
<br>
0:37:24.200,0:37:27.940<br>
跟這個藍色的點，這兩個藍色的點重和在一起<br>
<br>
0:37:27.940,0:37:30.620<br>
這就好像是說我們在做<br>
<br>
0:37:30.620,0:37:34.000<br>
剪窗花的時候，先把色紙對折一樣<br>
<br>
0:37:34.000,0:37:37.800<br>
你把這兩個平面對折，就好像是把色紙對折一樣<br>
<br>
0:37:37.800,0:37:40.040<br>
當你把這個色紙對折的時候<br>
<br>
0:37:40.040,0:37:43.540<br>
如果你在這個地方戳一個洞<br>
<br>
0:37:43.540,0:37:46.600<br>
到時候，你把色紙打開的時候<br>
<br>
0:37:46.600,0:37:50.280<br>
它只要看你摺幾折，它在這些地方<br>
<br>
0:37:50.280,0:37:52.560<br>
都會有一個洞<br>
<br>
0:37:52.560,0:37:56.520<br>
所以，如果你把剪窗花這件事情<br>
<br>
0:37:56.520,0:37:58.660<br>
想成是 training<br>
<br>
0:37:58.660,0:38:01.740<br>
你把剪色紙這件事情<br>
<br>
0:38:01.740,0:38:04.820<br>
想成是根據我們的 training data<br>
<br>
0:38:04.820,0:38:07.200<br>
training data 告訴我們說<br>
<br>
0:38:07.200,0:38:09.700<br>
有畫斜線的部分是 positive<br>
<br>
0:38:09.700,0:38:13.400<br>
沒畫斜線的部分是屬於 negative example<br>
<br>
0:38:13.400,0:38:15.800<br>
假設我們已經把這個平面 <br>
<br>
0:38:15.800,0:38:17.920<br>
像色紙一樣折起來的時候 <br>
<br>
0:38:17.920,0:38:20.780<br>
這個時候 training data 只要告訴我們說<br>
<br>
0:38:20.780,0:38:24.260<br>
在這個範圍之內、在這個範圍之內、在這個範圍之內<br>
<br>
0:38:24.260,0:38:26.540<br>
是屬於 positive 的<br>
<br>
0:38:26.540,0:38:29.620<br>
它只要告訴我們這個小的區間裡面的 data<br>
<br>
0:38:29.620,0:38:33.100<br>
展開以後我們就可以做出複雜的圖樣<br>
<br>
0:38:33.100,0:38:36.280<br>
那本來 training data 只告訴我們比較簡單的事情 <br>
<br>
0:38:36.280,0:38:41.100<br>
但是因為，現在有把空間對折的關係<br>
<br>
0:38:41.100,0:38:43.940<br>
你現在要把空間做各種各樣對折的關係<br>
<br>
0:38:43.940,0:38:48.140<br>
所以展開以後，你就可以有非常複雜的圖案<br>
<br>
0:38:48.140,0:38:51.360<br>
或者是說，你只要在這個地方戳一個洞<br>
<br>
0:38:51.360,0:38:55.480<br>
在其他地方，也就都等於戳一個洞<br>
<br>
0:38:55.480,0:38:58.680<br>
所以，一筆 data，如果只用這個例子來看的話<br>
<br>
0:38:58.680,0:39:01.420<br>
一筆 data 它就可以發揮五筆 data 的效用<br>
<br>
0:39:01.420,0:39:03.360<br>
所以，當你做 deep learning 的時候<br>
<br>
0:39:03.360,0:39:05.840<br>
你其實是用比較有效率的方式<br>
<br>
0:39:05.840,0:39:08.120<br>
來使用你的 data<br>
<br>
0:39:08.120,0:39:11.000<br>
你可能會想說，真的是這個樣子嗎<br>
<br>
0:39:11.000,0:39:13.860<br>
我在文獻上沒有看到太好的例子<br>
<br>
0:39:13.860,0:39:16.140<br>
這個比較像是我臆測<br>
<br>
0:39:16.140,0:39:17.840<br>
但是我做了一個 toy example<br>
<br>
0:39:17.840,0:39:19.600<br>
來展示這件事情<br>
<br>
0:39:19.600,0:39:21.640<br>
這個 toy example 是這樣子的<br>
<br>
0:39:21.640,0:39:23.740<br>
我們有一個 function<br>
<br>
0:39:23.740,0:39:26.760<br>
它的 input 是<br>
<br>
0:39:26.760,0:39:30.700<br>
二維 R^2，它的 output 是 0 跟 1<br>
<br>
0:39:30.700,0:39:33.760<br>
那這個 function 是一個地毯形狀的 function<br>
<br>
0:39:33.760,0:39:35.600<br>
地毯形狀的 function<br>
<br>
0:39:35.600,0:39:37.540<br>
在這個紅色的、菱形的範圍內<br>
<br>
0:39:37.540,0:39:41.740<br>
它的 input 這個 R^2，就是座標<br>
<br>
0:39:41.740,0:39:44.160<br>
那紅色的、這個菱形的範圍內<br>
<br>
0:39:44.160,0:39:46.160<br>
它的 output 就要是 1<br>
<br>
0:39:46.160,0:39:50.060<br>
藍色的、菱形的範圍內，它的 output 就要是 0<br>
<br>
0:39:50.060,0:39:52.720<br>
那現在，我們來考慮<br>
<br>
0:39:52.720,0:39:56.980<br>
如果我們用了不同的 training example<br>
<br>
0:39:56.980,0:39:58.640<br>
不同量的 training example<br>
<br>
0:39:58.640,0:40:02.480<br>
在一個 hidden layer 跟三個 hidden layer 的時候<br>
<br>
0:40:02.480,0:40:04.700<br>
我們看到甚麼樣的情境<br>
<br>
0:40:04.700,0:40:09.080<br>
這邊要注意一下就是，我們有特別調整一個 hidden layer<br>
<br>
0:40:09.080,0:40:11.740<br>
和三個 hidden layer 的參數<br>
<br>
0:40:11.740,0:40:14.580<br>
所以並不是說，當有三個 hidden layer 的時候<br>
<br>
0:40:14.580,0:40:17.300<br>
它的參數是比一個 hidden layer 多的<br>
<br>
0:40:17.300,0:40:18.860<br>
所以，一個 hidden layer 的 neural network<br>
<br>
0:40:18.860,0:40:20.340<br>
是一個很胖的 neural network<br>
<br>
0:40:20.340,0:40:22.120<br>
三個 hidden layer 的 neural network<br>
<br>
0:40:22.120,0:40:23.520<br>
它是一個很瘦的 neural network<br>
<br>
0:40:23.520,0:40:25.180<br>
所以，它們的參數<br>
<br>
0:40:25.180,0:40:26.960<br>
是調整到接近<br>
<br>
0:40:26.960,0:40:30.040<br>
所以你要注意一下，當你在比<br>
<br>
0:40:30.040,0:40:31.380<br>
一個 shallow 的 network 跟<br>
<br>
0:40:31.380,0:40:32.880<br>
一個比較 deep 的 network 的時候<br>
<br>
0:40:32.880,0:40:35.680<br>
一個公平的評比，應該要讓它們有一樣的參數<br>
<br>
0:40:35.680,0:40:37.940<br>
應該要讓它們有一樣的參數量<br>
<br>
0:40:37.940,0:40:41.260<br>
那如果你現在給它看這個<br>
<br>
0:40:41.260,0:40:43.380<br>
這邊是十萬筆 data 的話<br>
<br>
0:40:43.380,0:40:46.460<br>
那這兩個 network<br>
<br>
0:40:46.460,0:40:49.300<br>
都可以 learn 出這樣子的 training data<br>
<br>
0:40:49.300,0:40:53.300<br>
你從這個 function 裡面 sample 十萬筆 data<br>
<br>
0:40:53.300,0:40:55.480<br>
然後，給它去學，給它去學<br>
<br>
0:40:55.480,0:40:58.620<br>
然後，它學出來就是長這樣<br>
<br>
0:40:58.620,0:41:00.180<br>
長的就是這樣<br>
<br>
0:41:00.180,0:41:02.100<br>
那對一個 hidden layer 來說<br>
<br>
0:41:02.100,0:41:04.280<br>
反正，它可以模擬任何 function<br>
<br>
0:41:04.280,0:41:06.160<br>
只要它夠寬，它就可以模擬任何 function<br>
<br>
0:41:06.160,0:41:07.160<br>
這種菱形的 function<br>
<br>
0:41:07.160,0:41:10.380<br>
這種地毯的 function 應該也不是甚麼問題<br>
<br>
0:41:10.380,0:41:13.920<br>
現在，如果我們減少參數的量<br>
<br>
0:41:13.920,0:41:15.940<br>
減少到只用兩萬筆<br>
<br>
0:41:15.940,0:41:18.620<br>
我們只從這裡 sample 出兩萬筆來做 training <br>
<br>
0:41:18.620,0:41:20.000<br>
這個時候你會發現說<br>
<br>
0:41:20.000,0:41:21.700<br>
如果只有 1 個 hidden layer 的時候<br>
<br>
0:41:21.700,0:41:24.680<br>
你的結果就崩掉了<br>
<br>
0:41:24.680,0:41:26.620<br>
但是，如果是 3 個 hidden layer 的時候<br>
<br>
0:41:26.620,0:41:28.800<br>
你結果也是變得比較差<br>
<br>
0:41:28.800,0:41:30.980<br>
比 training data 多的時候還要差<br>
<br>
0:41:30.980,0:41:34.460<br>
但是你會發現說，你用 3 個 hidden layer 的時候<br>
<br>
0:41:34.460,0:41:37.900<br>
它的崩壞是有次序的崩壞<br>
<br>
0:41:37.900,0:41:40.160<br>
你看這個結果<br>
<br>
0:41:40.160,0:41:42.160<br>
它這個結果就像是<br>
<br>
0:41:42.160,0:41:44.200<br>
你今天要剪窗花的時候<br>
<br>
0:41:44.200,0:41:46.720<br>
你把色紙折起來，但是最後剪壞了<br>
<br>
0:41:46.720,0:41:50.340<br>
然後，展開以後長成這個樣子<br>
<br>
0:41:51.420,0:41:54.320<br>
所以說<br>
<br>
0:41:54.320,0:41:56.260<br>
而且你會發現說<br>
<br>
0:41:56.260,0:41:58.660<br>
在比較少的 training data 的時候<br>
<br>
0:41:58.660,0:42:01.800<br>
你有比較多的 hidden layer 最後得到的結果呢<br>
<br>
0:42:01.800,0:42:04.760<br>
其實是比較好的<br>
<br>
0:42:08.160,0:42:10.440<br>
當我們用 deep learning 的時候呢<br>
<br>
0:42:10.440,0:42:14.540<br>
另外一個好處是我們可以做 End-to-end learning<br>
<br>
0:42:14.540,0:42:17.320<br>
所謂的  End-to-end learning 的意思是這樣<br>
<br>
0:42:17.320,0:42:19.620<br>
比如說我們要處理的問題呢<br>
<br>
0:42:19.620,0:42:21.140<br>
非常的複雜<br>
<br>
0:42:21.140,0:42:25.600<br>
比如說，語音辨識就是一個非常複雜的問題<br>
<br>
0:42:25.600,0:42:28.500<br>
那我們說，我們要解一個 machine learning 的 problem 的時候<br>
<br>
0:42:28.500,0:42:29.800<br>
我們要做的事情就是<br>
<br>
0:42:29.800,0:42:33.120<br>
先找一個 hypothesis 的 function set<br>
<br>
0:42:33.120,0:42:35.880<br>
也就是找一個 model<br>
<br>
0:42:35.880,0:42:38.720<br>
當你要處理的問題是很複雜的時候<br>
<br>
0:42:38.720,0:42:41.300<br>
你這個 model 裡面，它會變成一個<br>
<br>
0:42:41.300,0:42:44.140<br>
它會需要是一個生產線<br>
<br>
0:42:44.140,0:42:45.920<br>
它會需要是一個生產線<br>
<br>
0:42:45.920,0:42:48.800<br>
那你這個 model 裡面，它表示一個很複雜的 function<br>
<br>
0:42:48.800,0:42:52.220<br>
一個很複雜的 function 是由很多比較簡單的 function<br>
<br>
0:42:52.220,0:42:53.880<br>
串接在一起<br>
<br>
0:42:53.880,0:42:55.760<br>
比如說，你要做語音辨識的話<br>
<br>
0:42:55.760,0:42:57.240<br>
你先把聲音訊號送進來<br>
<br>
0:42:57.240,0:42:59.840<br>
再透過很多層 function，一層一層的轉換<br>
<br>
0:42:59.840,0:43:02.580<br>
最後，變成文字<br>
<br>
0:43:02.580,0:43:05.040<br>
當你做 End-to-end learning 的時候<br>
<br>
0:43:05.040,0:43:07.820<br>
意思就是說，你只給你的 model<br>
<br>
0:43:07.820,0:43:09.460<br>
input 跟 output<br>
<br>
0:43:09.460,0:43:12.340<br>
不告訴它說，中間每一個 function<br>
<br>
0:43:12.340,0:43:14.440<br>
要怎麼分工<br>
<br>
0:43:14.440,0:43:16.200<br>
你就只給它 input 跟 output<br>
<br>
0:43:16.200,0:43:18.400<br>
然後，讓它自己去學<br>
<br>
0:43:18.400,0:43:19.740<br>
讓它去學會說<br>
<br>
0:43:19.740,0:43:23.460<br>
 中間每一個 function，生產線的每一個點<br>
<br>
0:43:23.460,0:43:27.240<br>
每一站，它應該要做什麼事情<br>
<br>
0:43:29.580,0:43:33.360<br>
那這件事情，如果<br>
你在 deep learning 裡面要做這件事情的時候<br>
<br>
0:43:33.360,0:43:35.740<br>
你就是疊一個很深的 neural network<br>
<br>
0:43:35.740,0:43:39.260<br>
每一層就是生產線上的一個點<br>
<br>
0:43:39.260,0:43:41.300<br>
每一層都是一個 simple 的 function<br>
<br>
0:43:41.300,0:43:43.120<br>
每一層會自己學到說<br>
<br>
0:43:43.120,0:43:44.800<br>
它應該要做什麼樣的事情<br>
<br>
0:43:45.680,0:43:47.720<br>
比如說，在語音辨識裡面<br>
<br>
0:43:47.720,0:43:50.940<br>
在還沒有用 deep learning 的時候<br>
<br>
0:43:50.940,0:43:53.460<br>
在還是 shallow learning 的時代<br>
<br>
0:43:53.460,0:43:56.300<br>
我們怎麼做語音辨識呢，們可能是這樣做的<br>
<br>
0:43:56.300,0:43:58.940<br>
你先有一段聲音訊號<br>
<br>
0:43:58.940,0:44:03.000<br>
然後，要怎麼把聲音訊號對應成文字呢<br>
<br>
0:44:03.000,0:44:07.060<br>
你要先做 DFT，你不知道這是甚麼也沒有關係<br>
<br>
0:44:07.060,0:44:09.280<br>
反正就是一個 function<br>
<br>
0:44:09.280,0:44:11.300<br>
生產線上的某一個站<br>
<br>
0:44:11.300,0:44:13.380<br>
然後，它變成 spectrogram<br>
<br>
0:44:13.380,0:44:16.340<br>
然後，這個 spectrogram 通過 filter bank<br>
<br>
0:44:16.340,0:44:19.120<br>
不知道 filter bank 是甚麼東西沒有關係<br>
反正就生產線上另外一站<br>
<br>
0:44:19.120,0:44:22.020<br>
再得到 output，再取 log<br>
<br>
0:44:22.020,0:44:24.480<br>
取 log 我想大家應該都知道吧，但是<br>
<br>
0:44:24.480,0:44:28.120<br>
它的原理其實是，這個取 log 其實是非常有道理的<br>
<br>
0:44:28.120,0:44:30.280<br>
不過，我們這邊不講就是了<br>
<br>
0:44:30.280,0:44:33.740<br>
然後再做 DCT，最後得到 MFCC<br>
<br>
0:44:33.740,0:44:35.880<br>
然後再把 MFCC 丟到 GMM 裡面<br>
<br>
0:44:35.880,0:44:39.560<br>
最後，你可以得到語音辨識的結果<br>
<br>
0:44:39.560,0:44:42.740<br>
這個 GMM 其實你把它換成 DNN<br>
<br>
0:44:42.740,0:44:45.700<br>
也是會有非常顯著的 improvement<br>
<br>
0:44:45.700,0:44:48.940<br>
那在這整個生產線上面呢<br>
<br>
0:44:48.940,0:44:50.960<br>
只有最後一個 block<br>
<br>
0:44:50.960,0:44:54.040<br>
只有最後這個 GMM 這個部分<br>
<br>
0:44:54.040,0:44:57.040<br>
藍色這個 block，是由 training data 學出來的<br>
<br>
0:44:57.040,0:45:00.700<br>
前面這個綠色的部分，這個都是人手訂的<br>
<br>
0:45:00.700,0:45:04.440<br>
就是過去有五聖先賢<br>
<br>
0:45:04.440,0:45:09.800<br>
他們研究了各種人類生理的知識以後<br>
<br>
0:45:09.800,0:45:13.600<br>
訂出了這些 function，那它非常非常的強<br>
<br>
0:45:13.600,0:45:16.400<br>
增一分則太肥，減一分則太瘦這樣<br>
<br>
0:45:16.400,0:45:18.220<br>
你就不要想在這上面再去改什麼東西<br>
<br>
0:45:18.220,0:45:19.720<br>
你改了之後會比較差<br>
<br>
0:45:19.720,0:45:22.960<br>
就這樣子大概卡了 20 年<br>
<br>
0:45:22.960,0:45:25.840<br>
五聖先賢實在太厲害了<br>
<br>
0:45:25.840,0:45:29.220<br>
但是，後來有了 deep learning 以後<br>
<br>
0:45:29.220,0:45:31.680<br>
我們可以把這些東西<br>
<br>
0:45:31.680,0:45:35.820<br>
用 neural network 把它取代掉<br>
<br>
0:45:35.820,0:45:37.800<br>
就是說，你可以把<br>
<br>
0:45:37.800,0:45:41.560<br>
你就把你的 deep neural network 多加幾層<br>
<br>
0:45:41.560,0:45:43.540<br>
然後，你就把 DCT 拿掉<br>
<br>
0:45:43.540,0:45:45.640<br>
這件事情現在已經是<br>
<br>
0:45:45.640,0:45:47.060<br>
typical 的做法了<br>
<br>
0:45:47.060,0:45:52.040<br>
過去 MFCC 這種 feature，如果你上語音課的話呢<br>
<br>
0:45:52.040,0:45:53.300<br>
你上李琳山老師的語音課<br>
<br>
0:45:53.300,0:45:55.580<br>
你很有可能知道 MFCC 是甚麼<br>
<br>
0:45:55.580,0:45:58.280<br>
過去，可能 20 年，這個 feature<br>
<br>
0:45:58.280,0:46:00.460<br>
是 dominate 語音辨識這件事情<br>
<br>
0:46:00.460,0:46:03.540<br>
但是現在已經不再是這樣子了 <br>
<br>
0:46:03.540,0:46:06.640<br>
你可以直接從 log 的 output 開始做<br>
<br>
0:46:06.640,0:46:09.520<br>
甚至，比較多人從 log 的 output 開始做<br>
<br>
0:46:09.520,0:46:12.720<br>
你把你的 neural network 疊深一點，<br>
直接從 log 的 output 開始做<br>
<br>
0:46:12.720,0:46:16.160<br>
你會得到比較好的結果<br>
<br>
0:46:16.160,0:46:20.720<br>
所以我記得在14年的、在語音的會議上的時候<br>
<br>
0:46:22.960,0:46:24.840<br>
MSR 的 research team (MSR = Microsoft Research)<br>
<br>
0:46:24.840,0:46:28.360<br>
語音 research team 的 head, Deng Li 進來說，掰掰 MFCC<br>
<br>
0:46:28.360,0:46:31.340<br>
大家也沒有甚麼特別的意見這樣<br>
<br>
0:46:31.340,0:46:35.100<br>
現在，甚至你可以從 spectrogram 開始做<br>
<br>
0:46:35.100,0:46:38.160<br>
你把這些都拿掉<br>
<br>
0:46:38.160,0:46:41.160<br>
通通都拿 deep neural network 來取代掉<br>
<br>
0:46:41.160,0:46:43.100<br>
也可以得到更好的結果<br>
<br>
0:46:43.100,0:46:47.600<br>
那 deep neural network 學到的，它要做的事情<br>
<br>
0:46:47.600,0:46:50.660<br>
你會發現，如果你分析<br>
那個 deep neural network 的 weight 的話 <br>
<br>
0:46:50.660,0:46:53.720<br>
它可以自動學到要做 filter bank 這件事情<br>
<br>
0:46:53.720,0:46:57.120<br>
filter bank 是模擬人類的聽覺器官 <br>
<br>
0:46:57.120,0:46:58.400<br>
所製定出來的 filter<br>
<br>
0:46:58.400,0:47:00.700<br>
但是 deep learning 可以自動學到這件事情<br>
<br>
0:47:00.700,0:47:03.120<br>
接下來，有人就會想要挑戰說<br>
<br>
0:47:03.120,0:47:06.680<br>
我們能不能夠疊一個很深很深的 neural network<br>
<br>
0:47:06.680,0:47:09.580<br>
我們能不能夠疊一個很深很深的 neural network<br>
<br>
0:47:09.580,0:47:11.960<br>
直接 input 就是 time domain 上的聲音訊號<br>
<br>
0:47:11.960,0:47:13.940<br>
然後，output 直接就是<br>
<br>
0:47:13.940,0:47:17.500<br>
文字，中間完全不要做 feature transform 之類的<br>
<br>
0:47:17.500,0:47:19.760<br>
如果連 feature transform 都不用做的話<br>
<br>
0:47:19.760,0:47:21.780<br>
那你就不需要學訊號與系統<br>
<br>
0:47:24.940,0:47:28.580<br>
但是還好這件事情，後來結局是這個樣子<br>
<br>
0:47:28.580,0:47:30.160<br>
這件事情的結局是這個樣子<br>
<br>
0:47:30.160,0:47:32.580<br>
有好多好多人前仆後繼的<br>
<br>
0:47:32.580,0:47:33.920<br>
在做這一件事情<br>
<br>
0:47:33.920,0:47:35.860<br>
這個甚至曾經一度<br>
<br>
0:47:35.860,0:47:39.240<br>
在 conference 裡面有兩個 session 都在做這件事情<br>
<br>
0:47:39.240,0:47:41.980<br>
最後，Google 有一篇 paper 是這樣<br>
<br>
0:47:41.980,0:47:43.380<br>
它最後的結果是<br>
<br>
0:47:43.380,0:47:46.480<br>
它拼死去 learn 了一個很大的 neural network<br>
<br>
0:47:46.480,0:47:48.040<br>
input 就是聲音訊號<br>
<br>
0:47:48.040,0:47:49.680<br>
完全不做其他任何的事情 <br>
<br>
0:47:49.680,0:47:52.880<br>
input 就是 row 的 wave phone<br>
<br>
0:47:52.880,0:47:56.600<br>
最後可以做到跟有做 feature transform 的結果打平<br>
<br>
0:47:56.600,0:47:59.100<br>
但也僅止於打平而已<br>
<br>
0:47:59.100,0:48:02.320<br>
我目前還沒有看到有一個結果是，可以 input<br>
<br>
0:48:02.320,0:48:03.780<br>
聲音訊號<br>
<br>
0:48:03.780,0:48:06.600<br>
input time domain 的聲音，不做 feature transform<br>
<br>
0:48:06.600,0:48:08.780<br>
結果比 feature transform 好<br>
<br>
0:48:08.780,0:48:11.480<br>
可見 feature transform 很強，或許它已經是<br>
<br>
0:48:11.480,0:48:13.340<br>
訊號處理的極限了<br>
<br>
0:48:13.340,0:48:14.700<br>
就跟 machine learning learn 出來的結果<br>
<br>
0:48:14.700,0:48:16.860<br>
其實也就是 feature transform<br>
<br>
0:48:17.520,0:48:18.580<br>
如果你觀察<br>
<br>
0:48:18.580,0:48:21.300<br>
你看 Google 那篇 paper 的話，它其實會分析一下<br>
<br>
0:48:21.300,0:48:23.740<br>
它的這個 machine 做的事情<br>
<br>
0:48:23.740,0:48:26.500<br>
它做的事情就很像是在做 feature transform<br>
<br>
0:48:26.500,0:48:29.100<br>
但是，做出來也就跟 feature transform 一樣好<br>
<br>
0:48:29.100,0:48:31.000<br>
也沒也辦法比 feature transform 做的更好<br>
<br>
0:48:31.000,0:48:33.140<br>
所以，修訊號與系統還是必要的<br>
<br>
0:48:34.780,0:48:37.820<br>
剛才講的都是語音的例子<br>
<br>
0:48:37.820,0:48:40.980<br>
那影像的話，其實也是差不多啦<br>
<br>
0:48:40.980,0:48:44.200<br>
這個我想大家應該都知道，所以<br>
<br>
0:48:44.200,0:48:47.480<br>
我們就稍微跳過去，過去影像也是<br>
<br>
0:48:47.480,0:48:49.120<br>
疊很多很多的 block<br>
<br>
0:48:49.120,0:48:52.260<br>
有很多很多人訂的、handcrafted 的 feature<br>
<br>
0:48:52.260,0:48:55.600<br>
那你只是在用很多很多的 block<br>
<br>
0:48:55.600,0:48:58.320<br>
很多很多 handcrafted 的 feature 去處理你 input 的影像<br>
<br>
0:48:58.320,0:49:01.940<br>
然後，只又在最後一層，用一個 shallow 的 classifier<br>
<br>
0:49:01.940,0:49:04.360<br>
現在，你就直接兜一個<br>
<br>
0:49:04.360,0:49:05.920<br>
很深的 network<br>
<br>
0:49:05.920,0:49:07.500<br>
input 就直接是 pixel<br>
<br>
0:49:07.500,0:49:09.520<br>
output 就直接是裡面影像是甚麼<br>
<br>
0:49:09.520,0:49:10.680<br>
就不需要抽 feature 了<br>
<br>
0:49:11.840,0:49:13.700<br>
那 deep learning  還有什麼好處呢<br>
<br>
0:49:13.700,0:49:16.260<br>
有時候我們會做<br>
<br>
0:49:16.260,0:49:18.420<br>
通常我們真正在意的 task<br>
<br>
0:49:18.420,0:49:20.820<br>
它是非常的複雜的<br>
<br>
0:49:20.820,0:49:23.380<br>
那在這種非常的複雜的 task 裡面<br>
<br>
0:49:23.380,0:49:28.660<br>
有時候非常像的 input，它會有很不一樣的output<br>
<br>
0:49:28.660,0:49:30.960<br>
舉例來說，你在做影像辨識的時候<br>
<br>
0:49:30.960,0:49:33.100<br>
這個白色的狗<br>
<br>
0:49:33.100,0:49:35.840<br>
跟北極熊其實看起來是很像的<br>
<br>
0:49:35.840,0:49:37.680<br>
它們其實是很像的<br>
<br>
0:49:37.680,0:49:39.820<br>
但是你的 machine 要知道說<br>
<br>
0:49:39.820,0:49:44.480<br>
看到這個要 output 狗，看到這個要 output 北極熊<br>
<br>
0:49:45.420,0:49:49.380<br>
有時候看起來很不一樣的東西<br>
<br>
0:49:49.380,0:49:51.960<br>
其實是一樣的，比如說，這個是火車<br>
<br>
0:49:51.960,0:49:56.360<br>
側面看是這個樣子，橫看成嶺側成峰<br>
<br>
0:49:57.000,0:49:59.580<br>
橫著看就變這個樣子<br>
<br>
0:49:59.580,0:50:02.480<br>
所以它們是不一樣的，但 output 都要是火車<br>
<br>
0:50:02.480,0:50:05.620<br>
如果你今天的 network<br>
<br>
0:50:05.620,0:50:06.740<br>
只有一層的話<br>
<br>
0:50:06.740,0:50:08.120<br>
你只能夠簡單的 transform<br>
<br>
0:50:08.120,0:50:11.360<br>
你沒有辦法把一樣的東西變得很不一樣<br>
<br>
0:50:11.360,0:50:13.580<br>
把不一樣的東西變得很像<br>
<br>
0:50:13.580,0:50:15.900<br>
你要讓原來 input 很像的東西<br>
<br>
0:50:15.900,0:50:18.160<br>
結果看起來很不像<br>
<br>
0:50:18.160,0:50:20.860<br>
你需要做很多層次的轉換<br>
<br>
0:50:20.860,0:50:23.520<br>
舉例來說，如果我們看下面這個例子<br>
<br>
0:50:23.520,0:50:25.340<br>
這個是語音的例子<br>
<br>
0:50:25.340,0:50:27.680<br>
在這個圖上，我們是吧<br>
<br>
0:50:27.680,0:50:29.380<br>
這個不是我做的<br>
<br>
0:50:29.380,0:50:32.460<br>
這個是來自於 ICASSP 2012 的一篇 paper<br>
<br>
0:50:32.460,0:50:33.720<br>
在這個圖上<br>
<br>
0:50:33.720,0:50:35.600<br>
這邊做的事情是<br>
<br>
0:50:35.600,0:50:39.400<br>
把 MFCC 投影到二維的平面上<br>
<br>
0:50:39.400,0:50:42.260<br>
那同樣的顏色<br>
<br>
0:50:42.260,0:50:44.500<br>
代表的是這個<br>
<br>
0:50:44.500,0:50:46.720<br>
不同的顏色<br>
<br>
0:50:46.720,0:50:50.920<br>
代表的是不同的人說的話<br>
<br>
0:50:50.920,0:50:52.880<br>
紅色代表是某個人說的句子<br>
<br>
0:50:52.880,0:50:54.560<br>
綠色代表是某個人說的句子<br>
<br>
0:50:54.560,0:50:55.740<br>
藍色代表是某個人說的句子<br>
<br>
0:50:55.740,0:50:58.400<br>
注意一下，這些人說的句子是不一樣的<br>
<br>
0:50:59.000,0:51:03.740<br>
在語音上，你會發現說同樣的句子<br>
<br>
0:51:03.740,0:51:05.180<br>
不同的人說<br>
<br>
0:51:05.180,0:51:08.380<br>
它的聲音訊號看起來是非常不一樣的<br>
<br>
0:51:08.380,0:51:11.260<br>
你會發現說，這個紅色看起來跟藍色間沒有什麼關係<br>
<br>
0:51:11.260,0:51:13.120<br>
藍色看起來跟綠色間沒有什麼關係<br>
<br>
0:51:13.120,0:51:15.260<br>
所以，有人看到這個圖就覺得<br>
<br>
0:51:15.260,0:51:17.160<br>
語音辨識不能做啊<br>
<br>
0:51:17.160,0:51:20.740<br>
不能做，不同的人說話的聲音太不一樣了<br>
<br>
0:51:20.740,0:51:23.180<br>
就算說同樣的句子，感覺也太不一樣不能做<br>
<br>
0:51:23.180,0:51:25.800<br>
如果你今天 learn 一個 neural network<br>
<br>
0:51:25.800,0:51:29.600<br>
如果你只看第一層的 hidden layer 的 output<br>
<br>
0:51:29.600,0:51:32.740<br>
你會發現說不同的人<br>
<br>
0:51:32.740,0:51:35.400<br>
講的話、講的同一個句子<br>
<br>
0:51:35.400,0:51:37.260<br>
還是看起來很不一樣<br>
<br>
0:51:37.260,0:51:41.460<br>
但是如果你今天看的八個 hidden layer 的 output 時候<br>
<br>
0:51:41.460,0:51:43.900<br>
你會發現說<br>
<br>
0:51:43.900,0:51:46.380<br>
不同的人說的同樣的句子<br>
<br>
0:51:46.380,0:51:49.400<br>
它自動的被 align 在一起<br>
<br>
0:51:49.400,0:51:52.900<br>
也就是說，這個 DNN 在很多的 layer 的轉換的時候<br>
<br>
0:51:52.900,0:51:55.760<br>
它把本來看起來很不像的東西<br>
<br>
0:51:55.760,0:51:58.180<br>
它知道說它們應該是一樣的<br>
<br>
0:51:58.180,0:51:59.820<br>
經過很多的 layer 的轉換以後<br>
<br>
0:51:59.820,0:52:02.040<br>
就把它們兜在一起<br>
<br>
0:52:02.040,0:52:06.500<br>
就把它們 map 在一起了<br>
<br>
0:52:06.500,0:52:08.560<br>
比如說，你看這個圖上面<br>
<br>
0:52:08.560,0:52:12.180<br>
這邊你會看到一條一條的線<br>
<br>
0:52:12.180,0:52:16.100<br>
那在這條線裡面，你會看到不同顏色的聲音訊號<br>
<br>
0:52:16.100,0:52:19.060<br>
也就是說，不同的人說同樣的話<br>
<br>
0:52:19.060,0:52:21.600<br>
經過 8 個 hidden layer 的轉換以後<br>
<br>
0:52:21.600,0:52:24.260<br>
對 neural network 來說，它就變得很像<br>
<br>
0:52:24.260,0:52:27.660<br>
本來的 input 完全不像，再通過很多個 layer 的轉換以後<br>
<br>
0:52:27.660,0:52:29.740<br>
它就變得像<br>
<br>
0:52:30.160,0:52:33.520<br>
或者是，今天這個是語音的例子<br>
<br>
0:52:33.520,0:52:36.300<br>
如果我們看 MNIST 手寫數字辨識的例子<br>
<br>
0:52:36.300,0:52:38.260<br>
其實也可以輕易地做到這些實驗<br>
<br>
0:52:38.260,0:52:40.800<br>
input 的 vector 是長這個樣子<br>
<br>
0:52:40.800,0:52:45.060<br>
input 就是 28*28 個 pixel<br>
<br>
0:52:45.060,0:52:47.260<br>
如果你把 28*28 個 pixel<br>
<br>
0:52:47.260,0:52:50.480<br>
28*28 的 vector project 到<br>
<br>
0:52:50.480,0:52:56.760<br>
二維的平面的話，那它看起來像是這個樣子<br>
<br>
0:52:56.760,0:52:58.940<br>
你會發現說，在這個圖上<br>
<br>
0:52:58.940,0:53:03.680<br>
它這邊這個 4 跟 9 幾乎是疊在一起的<br>
<br>
0:53:03.680,0:53:05.660<br>
因為 4 跟 9 很像，我們仔細想想看<br>
<br>
0:53:05.660,0:53:08.940<br>
4 跟 9 都是一個圈圈再加一條線<br>
<br>
0:53:08.940,0:53:12.440<br>
4 跟 9 很像，所以如果你光看 input 的 pixel 的話<br>
<br>
0:53:12.440,0:53:16.300<br>
4 跟 9 幾乎是疊在一起的，你幾乎沒有辦法把它分開<br>
<br>
0:53:16.300,0:53:20.160<br>
但是，如果我們看的一個 hidden layer 的 output<br>
<br>
0:53:20.160,0:53:24.120<br>
這個時候你會發現說， 4 跟 9 還是很像<br>
<br>
0:53:24.120,0:53:26.020<br>
它們還是離得很近<br>
<br>
0:53:26.020,0:53:29.120<br>
7 也跟 4 很像<br>
<br>
0:53:29.120,0:53:33.040<br>
4、7、9，它們是很像的<br>
<br>
0:53:33.040,0:53:34.860<br>
這是第一個 hidden layer 的 output<br>
<br>
0:53:34.860,0:53:38.040<br>
但是，如果我們看第二個<br>
hidden layer 的 output，你就會發現說<br>
<br>
0:53:38.040,0:53:39.840<br>
4、7、9，是逐漸被分開的<br>
<br>
0:53:39.840,0:53:43.200<br>
到第三個 hidden layer 的 output，它們又被分得更開<br>
<br>
0:53:43.200,0:53:47.360<br>
所以，如果你今天要讓不一樣的 input<br>
<br>
0:53:47.360,0:53:49.680<br>
被 merge 在一起，相當在語音舉的例子<br>
<br>
0:53:49.680,0:53:52.060<br>
或者是讓<br>
<br>
0:53:52.060,0:53:55.360<br>
你要讓原來看起來很像的 input <br>
<br>
0:53:55.360,0:53:57.060<br>
最後被分得很開<br>
<br>
0:53:57.060,0:54:01.160<br>
那你就需要好多 hidden layer 才能辦到這件事情<br>
<br>
0:54:01.860,0:54:05.480<br>
那其實還有更多用 deep learning 的理由啦<br>
<br>
0:54:05.480,0:54:08.360<br>
那這個有人寫了一篇 paper<br>
<br>
0:54:08.360,0:54:10.580<br>
Microsoft 的 researcher<br>
<br>
0:54:10.580,0:54:14.160<br>
Rich Caruana，他寫了一篇 paper，它的 title 是<br>
<br>
0:54:14.160,0:54:17.680<br>
Do Deep Nets Really Need to be Deep？<br>
<br>
0:54:17.680,0:54:20.140<br>
如果翻譯成中文就可以翻譯成這個<br>
<br>
0:54:20.140,0:54:22.920<br>
深度學習是不是過譽了<br>
<br>
0:54:25.780,0:54:27.840<br>
在這篇 paper 裡面呢<br>
<br>
0:54:27.840,0:54:29.480<br>
他做了一個很神奇<br>
<br>
0:54:29.480,0:54:33.080<br>
這篇 paper 有點舊，它大概是兩年前 publish 的<br>
<br>
0:54:33.080,0:54:35.520<br>
這篇 paper 看起來，那時候覺得很神奇<br>
<br>
0:54:35.520,0:54:36.880<br>
但現在已經是 common sense 了<br>
<br>
0:54:36.880,0:54:38.820<br>
他的做法是這樣，他說<br>
<br>
0:54:38.820,0:54:42.400<br>
我們今天來比較一下一層的 hidden layer<br>
<br>
0:54:42.400,0:54:44.200<br>
跟三層的 hidden layer <br>
<br>
0:54:44.200,0:54:46.040<br>
它們在 MNIST 還有 TIMIT 這些<br>
<br>
0:54:46.040,0:54:47.820<br>
benchmark corpus 上的差別<br>
<br>
0:54:47.820,0:54:49.640<br>
當然結果並不意外<br>
<br>
0:54:49.640,0:54:52.780<br>
當你把一個 hidden layer 跟<br>
<br>
0:54:52.780,0:54:54.820<br>
三個 hidden layer 調一樣參數的時候<br>
<br>
0:54:54.820,0:54:58.440<br>
當然是三個 hidden layer performance 是比較好的<br>
<br>
0:54:58.440,0:55:01.620<br>
其實，你也可以自己 verify 這件事情<br>
<br>
0:55:01.620,0:55:04.940<br>
如果你有真的在做 deep learning 的話<br>
<br>
0:55:04.940,0:55:08.360<br>
其實，這是 common sense，然後<br>
<br>
0:55:08.360,0:55:10.200<br>
接下來，他發現說<br>
<br>
0:55:10.200,0:55:11.560<br>
一個 hidden layer<br>
<br>
0:55:11.560,0:55:13.000<br>
參數怎麼增加<br>
<br>
0:55:13.000,0:55:14.480<br>
performance 都不好<br>
<br>
0:55:14.480,0:55:16.280<br>
它很快就 saturate 了<br>
<br>
0:55:16.280,0:55:21.120<br>
他就想說，怎麼會這樣呢<br>
<br>
0:55:21.120,0:55:24.400<br>
他就做了一件當時看起來很匪夷所思的事<br>
<br>
0:55:24.400,0:55:27.120<br>
他說，一個 hidden layer 的 neural network<br>
<br>
0:55:27.120,0:55:31.840<br>
你 learning 的 target 不要用真正的 label<br>
<br>
0:55:31.840,0:55:33.940<br>
懂嗎？就是我們 本來learning 的時候<br>
<br>
0:55:33.940,0:55:35.760<br>
你要用真正的 label 阿<br>
<br>
0:55:35.900,0:55:38.320<br>
你就看說，這個 image 有人告訴你說這個是 1<br>
<br>
0:55:38.320,0:55:39.460<br>
有人告訴你說是 0<br>
<br>
0:55:39.460,0:55:40.860<br>
你的 network 就是用這個 train 的嘛<br>
<br>
0:55:40.860,0:55:42.840<br>
這是很合理的想法，他說<br>
<br>
0:55:42.840,0:55:45.060<br>
你現在的 shallow network 不要用這個 label<br>
<br>
0:55:45.060,0:55:50.140<br>
你用三層 hidden layer 的 output 當作你的 feature<br>
<br>
0:55:50.140,0:55:53.700<br>
三層 hidden layer，你把所有的 image<br>
<br>
0:55:53.700,0:55:55.740<br>
都丟到一個 learn 好的三層 hidden layer<br>
<br>
0:55:55.740,0:55:57.000<br>
然後，得到那些 output<br>
<br>
0:55:57.000,0:55:59.940<br>
那有一些 output 是錯的，就不管它是錯的這樣<br>
<br>
0:55:59.940,0:56:03.600<br>
然後，一層的 hidden layer 就去學三層的 hidden layer<br>
<br>
0:56:03.600,0:56:05.920<br>
幫你 label 的結果，那一個錯的 ，它就學一個錯的<br>
<br>
0:56:05.920,0:56:08.540<br>
結果，它的 performance 會比較好<br>
<br>
0:56:08.540,0:56:14.220<br>
會逼近三層的 hidden layer，<br>
幾乎跟三層的 hidden layer 一樣好<br>
<br>
0:56:14.220,0:56:15.740<br>
因為，他就是想說<br>
<br>
0:56:15.740,0:56:18.960<br>
照理說，你只要一個 hidden layer 就能做到任何事<br>
<br>
0:56:18.960,0:56:21.100<br>
所以，沒有理由三層可以得到這個 performance<br>
<br>
0:56:21.100,0:56:22.580<br>
一層得不到這個 performance<br>
<br>
0:56:22.580,0:56:25.220<br>
但是，你直接 learn 一個一層的 network<br>
<br>
0:56:25.220,0:56:26.580<br>
你就是 learn 不出這個結果<br>
<br>
0:56:26.580,0:56:29.080<br>
你要去讓一層 network 模擬三層 network 的行為<br>
<br>
0:56:29.080,0:56:32.580<br>
它才能夠 learn 出這個結果<br>
<br>
0:56:32.580,0:56:34.820<br>
那有人讀了這篇 paper，他覺得說<br>
<br>
0:56:34.820,0:56:38.180<br>
Rich 的結論是 deep learning 不 work<br>
<br>
0:56:38.180,0:56:39.920<br>
其實他的結論不是 deep learning 不 work<br>
<br>
0:56:39.920,0:56:41.200<br>
我在 conference 遇過他本人<br>
<br>
0:56:41.200,0:56:42.160<br>
我問過他說<br>
<br>
0:56:42.160,0:56:44.460<br>
你的意思是 deep learning 不 work，他說不是<br>
<br>
0:56:44.460,0:56:46.700<br>
我的意思是 deep learning 是 work 的<br>
<br>
0:56:46.700,0:56:49.740<br>
就是你直接在一層 network 是 learn 不起來的<br>
<br>
0:56:49.740,0:56:50.860<br>
你要先 learn 三層的 network<br>
<br>
0:56:50.860,0:56:53.900<br>
再用一層的 network 去模擬三層 network 的行為<br>
<br>
0:56:53.900,0:56:54.820<br>
你才 learn 的起來<br>
<br>
0:56:54.820,0:56:56.740<br>
然後，這個是我在 ASRU<br>
<br>
0:56:56.740,0:56:59.240<br>
2015 的時候，聽過他的 keynote speech<br>
<br>
0:56:59.240,0:57:00.480<br>
這個時他的第一頁投影片<br>
<br>
0:57:00.480,0:57:02.500<br>
Do Deep Nets Really Need to be Deep？<br>
<br>
0:57:02.500,0:57:04.000<br>
然後，第二頁 Yes！<br>
<br>
0:57:04.000,0:57:07.160<br>
然後，我們留下很多時間給大家問問題，結束！<br>
<br>
0:57:12.500,0:57:16.240<br>
如果你想要學更多的話，你可以看一下這個<br>
<br>
0:57:16.240,0:57:20.820<br>
Bengio 的 deep learning 的 Theoretical 的 Motivations<br>
<br>
0:57:20.820,0:57:24.060<br>
他講了很多非常發人深省的想法<br>
<br>
0:57:24.060,0:57:27.460<br>
或者是，現在 deep learning 很紅了，所以有各種<br>
<br>
0:57:27.460,0:57:30.720<br>
來自其他領域的解讀，比如說從<br>
<br>
0:57:30.720,0:57:33.040<br>
物理的角度來解釋，為什麼要做 deep learning<br>
<br>
0:57:33.040,0:57:36.480<br>
從化學的角度來解釋，為什麼要做 deep learning<br>
<br>
0:57:36.480,0:57:39.680<br>
我把連結留在這邊給大家參考<br>
<br>
0:57:39.680,0:57:41.040<br>
講到這邊剛好告一個段落<br>
<br>
0:57:41.040,0:57:44.720<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
