<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.360<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.360,0:00:05.040<br>
各位同學大家好，我們來上課吧<br>
<br>
0:00:09.020,0:00:11.140<br>
今天的規劃是這樣子<br>
<br>
0:00:11.140,0:00:14.660<br>
就是我們等一下，會先公告 final<br>
<br>
0:00:14.660,0:00:19.420<br>
那 final 有三個選擇，所以是需要花時間跟大家講一下的<br>
<br>
0:00:19.420,0:00:21.420<br>
所以，今天的規劃是<br>
<br>
0:00:21.420,0:00:26.760<br>
我們等一下上課，大概上到 1: 20 以後下課<br>
<br>
0:00:26.760,0:00:32.340<br>
不對，腦殘了，到 11:20 吧<br>
<br>
0:00:32.340,0:00:36.120<br>
不是 11:20，大概是 10:20 的時候下課<br>
<br>
0:00:36.120,0:00:38.480<br>
然後，等一下<br>
<br>
0:00:38.480,0:00:41.160<br>
剩下的時間就讓助教來把<br>
<br>
0:00:41.160,0:00:44.100<br>
三個 final 都跟大家講完<br>
<br>
0:00:45.780,0:00:50.240<br>
那今天我們要講的是 Word Embedding<br>
<br>
0:00:50.240,0:00:54.820<br>
我們之前已經講了 Dimension Reduction<br>
<br>
0:00:54.820,0:00:59.580<br>
那 Word Embedding 其實是 Dimension Reduction 一個<br>
<br>
0:00:59.580,0:01:02.440<br>
非常好、非常廣為人知的應用<br>
<br>
0:01:05.920,0:01:08.620<br>
如果我們今天要你用一個 vector<br>
<br>
0:01:08.620,0:01:10.960<br>
來表示一個 word，你會怎麼做呢？<br>
<br>
0:01:10.960,0:01:12.620<br>
最 typical 的作法<br>
<br>
0:01:12.620,0:01:14.920<br>
叫做 1-of-N encoding<br>
<br>
0:01:14.920,0:01:18.200<br>
每一個 word，我們用一個 vector來表示<br>
<br>
0:01:18.200,0:01:22.160<br>
這個 vector 的 dimension，就是這個世界上<br>
<br>
0:01:22.160,0:01:24.280<br>
可能有的 word 數目<br>
<br>
0:01:24.280,0:01:26.440<br>
假設這個世界上可能有十萬個 word<br>
<br>
0:01:26.440,0:01:28.640<br>
那 1-of-N encoding 的 dimension<br>
<br>
0:01:28.640,0:01:30.500<br>
就是十萬維<br>
<br>
0:01:30.500,0:01:34.680<br>
那每一個 word，對應到其中一維<br>
<br>
0:01:34.680,0:01:38.420<br>
所以，apple 它就是第一維是 1，其他都是 0<br>
<br>
0:01:38.420,0:01:42.480<br>
bag 就是第二維是 1，cat 就是第三維是 1，以此類推，等等<br>
<br>
0:01:42.480,0:01:45.960<br>
如果你用這種方式來描述一個 word<br>
<br>
0:01:45.960,0:01:48.980<br>
你的這個 vector 一點都不 informative<br>
<br>
0:01:48.980,0:01:53.020<br>
你每一個 word，它的 vector 都是不一樣的<br>
<br>
0:01:53.020,0:01:56.020<br>
所以從這個 vector 裡面，你沒有辦法得到任何的資訊<br>
<br>
0:01:56.020,0:01:57.560<br>
你沒有辦法知道說<br>
<br>
0:01:57.560,0:01:59.180<br>
比如說，bag 跟 cat<br>
<br>
0:01:59.180,0:02:01.600<br>
bag 跟 cat，沒什麼關係<br>
<br>
0:02:01.600,0:02:03.100<br>
比如說，cat 跟 dog<br>
<br>
0:02:03.100,0:02:05.160<br>
它們都是動物這件事<br>
<br>
0:02:05.160,0:02:07.100<br>
你沒有辦法知道<br>
<br>
0:02:07.100,0:02:08.840<br>
那怎麼辦呢？<br>
<br>
0:02:09.900,0:02:13.280<br>
有一個方法就叫做建 Word Class<br>
<br>
0:02:13.280,0:02:16.480<br>
也就是你把不同的 word<br>
<br>
0:02:16.480,0:02:17.800<br>
有同樣性質的 word<br>
<br>
0:02:17.860,0:02:20.380<br>
把它們 cluster 成一群一群的<br>
<br>
0:02:20.540,0:02:25.120<br>
然後就用那一個 word 所屬的 class 來表示這個 word<br>
<br>
0:02:25.120,0:02:29.560<br>
這個就是我們之前，在做 Dimension Reduction 的時候<br>
<br>
0:02:29.560,0:02:31.600<br>
講的 clustering 的概念<br>
<br>
0:02:31.600,0:02:35.380<br>
比如說，dog, cat 跟 bird，它們都是class 1<br>
<br>
0:02:35.380,0:02:38.500<br>
ran, jumped, walk 是 class 2<br>
<br>
0:02:38.500,0:02:40.640<br>
flower, tree, apple 是class 3，等等<br>
<br>
0:02:40.640,0:02:43.940<br>
但是，光用 class 是不夠的<br>
<br>
0:02:43.940,0:02:47.020<br>
我們之前有講過說，光做 clustering 是不夠的<br>
<br>
0:02:47.020,0:02:50.440<br>
因為，如果光做 clustering 的話呢<br>
<br>
0:02:50.440,0:02:51.794<br>
我們找了一些 information<br>
<br>
0:02:51.800,0:02:54.140<br>
比如說<br>
<br>
0:02:54.140,0:02:55.660<br>
這個是屬於動物的 class<br>
<br>
0:02:55.660,0:02:57.260<br>
這是屬於植物的 class<br>
<br>
0:02:57.260,0:02:58.740<br>
它們都是屬於生物<br>
<br>
0:02:58.740,0:03:00.980<br>
但是在 class 裡面，沒有辦法呈現這一件事情<br>
<br>
0:03:00.980,0:03:04.320<br>
或者是說，class 1 是動物<br>
<br>
0:03:04.320,0:03:07.680<br>
而 class 2 代表是，動物可以做的行為<br>
<br>
0:03:07.680,0:03:10.140<br>
但是， class 3 是植物<br>
<br>
0:03:10.140,0:03:12.780<br>
class 2 裡面的行為是 class 3 裡面沒有辦法做的<br>
<br>
0:03:12.780,0:03:15.360<br>
所以，class 2 跟 class 1 是有一些關聯的<br>
<br>
0:03:15.360,0:03:17.700<br>
沒有辦法用 Word Class 呈現出來<br>
<br>
0:03:17.700,0:03:19.400<br>
所以怎麼辦呢？<br>
<br>
0:03:19.400,0:03:22.680<br>
我們需要的，是 Word Embedding<br>
<br>
0:03:22.680,0:03:24.260<br>
Word Embedding 是這樣<br>
<br>
0:03:24.260,0:03:25.840<br>
把每一個 word<br>
<br>
0:03:25.840,0:03:30.040<br>
都 project 到一個 high dimensional 的 space 上面<br>
<br>
0:03:30.040,0:03:32.180<br>
把每一個 word 都 project 到一個<br>
<br>
0:03:32.180,0:03:33.800<br>
high dimensional space 上面<br>
<br>
0:03:33.800,0:03:35.880<br>
雖然說，這邊這個 space 是 high dimensional<br>
<br>
0:03:35.880,0:03:39.720<br>
但是它其實遠比 1-of-N encoding 的 dimension 還要低<br>
<br>
0:03:39.720,0:03:42.940<br>
1-of-N encoding，你這個 vector 通常是<br>
<br>
0:03:42.960,0:03:45.420<br>
比如說，英文有 10 萬詞彙，這個就是 10 萬維<br>
<br>
0:03:45.420,0:03:47.320<br>
但是，如果是用 Word Embedding 的話呢<br>
<br>
0:03:47.320,0:03:50.840<br>
通常比如說 50 維、100維這個樣子的 dimension<br>
<br>
0:03:50.840,0:03:54.320<br>
這個是一個從 1-of-N encoding 到 Word Embedding<br>
<br>
0:03:54.320,0:03:58.260<br>
這是 Dimension Reduction 的 process<br>
<br>
0:03:58.260,0:04:00.400<br>
那我們希望在這個 Word Embedding 的<br>
<br>
0:04:00.400,0:04:02.160<br>
這個圖上<br>
<br>
0:04:02.160,0:04:04.000<br>
我們可以看到的結果是<br>
<br>
0:04:04.000,0:04:07.400<br>
同樣，類似 semantic，類似語意的詞彙<br>
<br>
0:04:07.400,0:04:09.500<br>
它們能夠在這個圖上<br>
<br>
0:04:09.500,0:04:10.660<br>
是比較接近的<br>
<br>
0:04:10.660,0:04:14.280<br>
而且在這個 high dimensional space 裡面呢<br>
<br>
0:04:14.280,0:04:15.920<br>
在這個 Word Embedding 的 space 裡面<br>
<br>
0:04:15.920,0:04:19.360<br>
每一個 dimension，可能都有它特別的含意<br>
<br>
0:04:19.360,0:04:23.600<br>
比如說，假設我們現在做完 Word Embedding 以後<br>
<br>
0:04:23.600,0:04:25.260<br>
每一個 word 的這個<br>
<br>
0:04:25.260,0:04:27.980<br>
Word Embedding 的 feature vector長的是這個樣子<br>
<br>
0:04:27.980,0:04:29.880<br>
所以，可能就可以知道說<br>
<br>
0:04:29.880,0:04:32.620<br>
這個 dimension 代表了<br>
<br>
0:04:32.620,0:04:37.620<br>
生物和其他的東西之間的差別<br>
<br>
0:04:37.620,0:04:41.660<br>
這個 dimension 可能就代表了，比如說<br>
<br>
0:04:41.660,0:04:44.460<br>
這是會動的，跟動作有關的東西<br>
<br>
0:04:44.460,0:04:46.460<br>
動物是會動的，還有這個是動作<br>
<br>
0:04:46.460,0:04:47.880<br>
跟動作有關的東西<br>
<br>
0:04:47.880,0:04:51.560<br>
和不會動的，是靜止的東西的差別，等等<br>
<br>
0:04:51.560,0:04:55.160<br>
那怎麼做 Word Embedding<br>
<br>
0:04:55.160,0:04:58.740<br>
Word Embedding 是一個 unsupervised approach<br>
<br>
0:04:58.740,0:05:00.840<br>
也就是，我們怎麼讓 machine<br>
<br>
0:05:00.840,0:05:02.760<br>
知道每一個詞彙的含義，是什麼呢？<br>
<br>
0:05:02.760,0:05:06.660<br>
你只要透過讓 machine 閱讀大量的文章<br>
<br>
0:05:06.660,0:05:08.880<br>
你只要讓 machine 透過閱讀大量的文章<br>
<br>
0:05:08.880,0:05:11.080<br>
它就可以知道，每一個詞彙<br>
<br>
0:05:11.080,0:05:15.540<br>
它的 embedding 的 feature vector 應該長什麼樣子<br>
<br>
0:05:15.540,0:05:18.480<br>
這是一個 unsupervised 的 problem<br>
<br>
0:05:18.480,0:05:20.140<br>
因為我們要做的事情就是<br>
<br>
0:05:20.140,0:05:22.200<br>
learn 一個 Neural Network<br>
<br>
0:05:22.200,0:05:23.720<br>
找一個  function<br>
<br>
0:05:23.720,0:05:26.060<br>
那你的 input 是一個詞彙<br>
<br>
0:05:26.060,0:05:30.120<br>
output 就是那一個詞彙所對應的 Word Embedding<br>
<br>
0:05:30.120,0:05:31.500<br>
它所對應的 Word Embedding<br>
<br>
0:05:31.500,0:05:33.700<br>
的那一個 vector<br>
<br>
0:05:33.700,0:05:35.860<br>
而我們手上有的 train data<br>
<br>
0:05:35.860,0:05:37.980<br>
是一大堆的文字<br>
<br>
0:05:37.980,0:05:41.360<br>
所以我們只有Input，我們只有 input<br>
<br>
0:05:41.360,0:05:44.800<br>
但是我們沒有 output ，我們沒有 output<br>
<br>
0:05:44.800,0:05:45.776<br>
我們不知道<br>
<br>
0:05:45.776,0:05:49.680<br>
每一個 Word Embedding 應該長什麼樣子<br>
<br>
0:05:49.680,0:05:51.960<br>
所以，對於我們要找的  function<br>
<br>
0:05:51.960,0:05:53.940<br>
我們只有單項<br>
<br>
0:05:53.940,0:05:56.280<br>
我們只知道輸入，不知道輸出<br>
<br>
0:05:56.280,0:06:01.280<br>
所以，這是一個 unsupervised learning 的問題<br>
<br>
0:06:01.280,0:06:04.480<br>
那這個問題要怎麼解呢？<br>
<br>
0:06:04.720,0:06:06.420<br>
我們之前有講過<br>
<br>
0:06:06.420,0:06:09.160<br>
一個 deep learning base 的 Dimension Reduction 的方法<br>
<br>
0:06:09.160,0:06:11.140<br>
叫做 Auto-encoder<br>
<br>
0:06:11.280,0:06:14.620<br>
也就是 learn 一個 network，讓它輸入等於輸出<br>
<br>
0:06:14.620,0:06:16.320<br>
這邊某一個 hidden layer 拿出來<br>
<br>
0:06:16.320,0:06:18.200<br>
就是 Dimension Reduction 的結果<br>
<br>
0:06:18.200,0:06:19.400<br>
在這個地方<br>
<br>
0:06:19.400,0:06:21.880<br>
你覺得你可以用 Auto-encoder 嗎 ?<br>
<br>
0:06:21.880,0:06:24.540<br>
給大家一秒鐘時間的想一想<br>
<br>
0:06:24.540,0:06:28.800<br>
你覺得這邊可以用 Auto-encoder 的同學，舉手一下<br>
<br>
0:06:28.800,0:06:34.060<br>
你覺得不能用 auto-encoder 的同學請舉手一下，謝謝<br>
<br>
0:06:34.060,0:06:36.040<br>
把手放下，大多數的同學都覺得<br>
<br>
0:06:36.040,0:06:38.100<br>
不能用 Auto-encoder 來處理這一問題<br>
<br>
0:06:38.100,0:06:42.000<br>
沒錯，這個問題你沒辦法用 Auto-encoder 來解<br>
<br>
0:06:42.000,0:06:43.900<br>
你沒辦法用 Auto-encoder 來解<br>
<br>
0:06:43.900,0:06:46.300<br>
這件事情有點難解釋<br>
<br>
0:06:46.300,0:06:48.160<br>
或許讓大家自己回去想一想<br>
<br>
0:06:48.160,0:06:49.240<br>
你可以想想看<br>
<br>
0:06:49.240,0:06:51.960<br>
如果你是用 1-of-N encoding 當作它的 input<br>
<br>
0:06:51.960,0:06:54.320<br>
如果你用 1-of-N encoding 當作它的 input<br>
<br>
0:06:54.320,0:06:58.180<br>
對 1-of-N encoding 來說，每一個詞彙都是 independent<br>
<br>
0:06:58.180,0:07:00.520<br>
你把這樣子的 vector 做 Auto-encoder<br>
<br>
0:07:00.520,0:07:03.200<br>
你其實，沒有辦法 learn 出<br>
<br>
0:07:03.200,0:07:05.860<br>
任何 informative 的 information<br>
<br>
0:07:05.860,0:07:09.380<br>
所以，在 Word Embedding 這個 task 裡面<br>
<br>
0:07:09.380,0:07:13.340<br>
用 Auto-encoder 是沒有辦法的<br>
<br>
0:07:13.340,0:07:15.040<br>
如果你這一邊 input 是 1-of-N encoding<br>
<br>
0:07:15.180,0:07:17.040<br>
用 Auto-encoder 是沒有辦法的<br>
<br>
0:07:17.040,0:07:19.420<br>
除非你說，你用這個<br>
<br>
0:07:19.420,0:07:22.220<br>
你用 character，比如說你用<br>
<br>
0:07:22.220,0:07:24.200<br>
character 的 n-gram 來描述一個 word<br>
<br>
0:07:24.200,0:07:27.220<br>
或許，它可以抓到一些字首、字根的含義<br>
<br>
0:07:27.220,0:07:30.300<br>
不過基本上，現在大家並不是這麼做的<br>
<br>
0:07:30.300,0:07:33.580<br>
那怎麼找這個 Word Embedding 呢<br>
<br>
0:07:33.580,0:07:36.080<br>
這邊的作法是這樣子<br>
<br>
0:07:36.080,0:07:37.680<br>
它基本的精神就是<br>
<br>
0:07:37.680,0:07:40.820<br>
你要如何了解一個詞彙的含義呢<br>
<br>
0:07:40.820,0:07:43.500<br>
你要看這個詞彙的 contest<br>
<br>
0:07:43.500,0:07:45.080<br>
每一個詞彙的含義<br>
<br>
0:07:45.080,0:07:50.560<br>
可以根據它的上下文來得到<br>
<br>
0:07:50.560,0:07:52.260<br>
舉例來說<br>
<br>
0:07:52.260,0:07:54.600<br>
假設機器讀了一段文字是說<br>
<br>
0:07:54.600,0:07:56.740<br>
馬英九520宣誓就職<br>
<br>
0:07:56.740,0:07:59.140<br>
它又讀了另外一段新聞說<br>
<br>
0:07:59.140,0:08:02.040<br>
蔡英文520宣誓就職<br>
<br>
0:08:02.040,0:08:05.360<br>
對機器來說，雖然它不知道馬英九指的是什麼<br>
<br>
0:08:05.360,0:08:07.440<br>
他不知道蔡英文指的是什麼<br>
<br>
0:08:07.440,0:08:10.460<br>
但是馬英九後面有接520宣誓就職<br>
<br>
0:08:10.460,0:08:12.700<br>
蔡英文的後面也有接520宣誓就職<br>
<br>
0:08:12.700,0:08:16.600<br>
對機器來說，只要它讀了大量的文章<br>
<br>
0:08:16.600,0:08:21.640<br>
發現說，馬英九跟蔡英文後面都有類似的 contest<br>
<br>
0:08:21.640,0:08:23.420<br>
它前後都有類似的文字<br>
<br>
0:08:23.420,0:08:25.280<br>
機器就可以推論說<br>
<br>
0:08:25.280,0:08:29.240<br>
蔡英文跟馬英九代表了某種有關係的 object<br>
<br>
0:08:29.240,0:08:32.620<br>
他們是某些有關係的物件<br>
<br>
0:08:32.620,0:08:34.340<br>
所以它可能也不知道他們是人<br>
<br>
0:08:34.340,0:08:36.840<br>
但它知道說，蔡英文跟馬英九這兩個詞彙<br>
<br>
0:08:36.840,0:08:40.220<br>
代表了，可能有同樣地位的東西<br>
<br>
0:08:40.220,0:08:44.180<br>
那怎麼來體現這一件事呢<br>
<br>
0:08:44.180,0:08:49.420<br>
怎麼用這個精神來找出 Word Embedding 的 vector 呢<br>
<br>
0:08:49.420,0:08:52.700<br>
有兩個不同體系的作法<br>
<br>
0:08:52.700,0:08:56.760<br>
一個做法叫做 Count based 的方法<br>
<br>
0:08:56.760,0:08:58.920<br>
Count based 的方法是這樣<br>
<br>
0:08:58.920,0:09:05.600<br>
如果我們現在有兩個詞彙，wi, wj<br>
<br>
0:09:05.600,0:09:09.980<br>
它們常常在同一個文章中出現<br>
<br>
0:09:09.980,0:09:12.060<br>
它們常常一起 co-occur<br>
<br>
0:09:12.060,0:09:14.580<br>
那它們的 word vector<br>
<br>
0:09:14.580,0:09:17.200<br>
我們用 V(wi) 來代表<br>
<br>
0:09:17.200,0:09:18.980<br>
wi 的 word vector<br>
<br>
0:09:18.980,0:09:22.220<br>
我們用 V(wj) 來代表，wj 的 word vector<br>
<br>
0:09:28.500,0:09:33.920<br>
如果 wi 跟 wj，它們常常一起出現的話<br>
<br>
0:09:33.920,0:09:39.820<br>
V(wi) 跟 V(wj) 它們就會比較接近<br>
<br>
0:09:39.820,0:09:42.080<br>
那這種方法<br>
<br>
0:09:42.080,0:09:43.940<br>
有一個很代表性的例子<br>
<br>
0:09:43.940,0:09:46.140<br>
叫做 Glove vector<br>
<br>
0:09:46.140,0:09:49.160<br>
我把它的 reference 附在這邊<br>
<br>
0:09:49.160,0:09:50.580<br>
給大家參考<br>
<br>
0:09:51.820,0:09:54.960<br>
那這個方法的原則是這樣子<br>
<br>
0:09:54.960,0:09:56.260<br>
假設我們知道<br>
<br>
0:09:56.260,0:09:59.480<br>
wi 的 word vector 是 V(wi)<br>
<br>
0:09:59.480,0:10:02.920<br>
wj 的 word vector 是 V(wj)<br>
<br>
0:10:02.920,0:10:06.000<br>
那我們可以計算它的 inner product<br>
<br>
0:10:06.000,0:10:12.300<br>
假設 Nij 是 wi 跟 wj<br>
<br>
0:10:12.300,0:10:16.940<br>
它們 co-occur 在同樣的 document 裡面的次數<br>
<br>
0:10:16.940,0:10:20.860<br>
那我們就希望為 wi 找一組 vector<br>
<br>
0:10:20.860,0:10:23.520<br>
為 wj 找一個組 vector<br>
<br>
0:10:23.520,0:10:26.560<br>
然後，希望這兩個<br>
<br>
0:10:26.560,0:10:28.840<br>
這兩件事情<br>
<br>
0:10:28.840,0:10:31.900<br>
越接近越好<br>
<br>
0:10:31.900,0:10:33.620<br>
你會發現說，這個概念<br>
<br>
0:10:33.620,0:10:36.960<br>
跟我們之前講的 LSA 是<br>
<br>
0:10:36.960,0:10:40.340<br>
跟我們講的 matrix factorization 的概念<br>
<br>
0:10:40.340,0:10:41.900<br>
其實是一樣的<br>
<br>
0:10:41.900,0:10:43.480<br>
其實是一樣的<br>
<br>
0:10:43.480,0:10:45.100<br>
另外一個方法<br>
<br>
0:10:45.100,0:10:48.100<br>
叫做 Prediction based 的方法<br>
<br>
0:10:48.100,0:10:52.800<br>
我發現我這一邊拼錯了<br>
<br>
0:10:52.800,0:10:54.920<br>
這應該是 Prediction based 的方法<br>
<br>
0:10:54.920,0:10:58.840<br>
那我不知道說<br>
<br>
0:10:58.840,0:11:00.700<br>
就我所知，好像沒有人<br>
<br>
0:11:00.700,0:11:02.720<br>
很認真的比較過說<br>
<br>
0:11:02.720,0:11:04.420<br>
Prediction based 方法<br>
<br>
0:11:04.420,0:11:06.100<br>
跟 Count based 的方法<br>
<br>
0:11:06.100,0:11:09.460<br>
它們有什麼樣非常不同的差異<br>
<br>
0:11:09.460,0:11:11.280<br>
或者是誰優誰劣<br>
<br>
0:11:11.280,0:11:13.540<br>
如果你有知道這方面的 information，或許<br>
<br>
0:11:13.540,0:11:15.740<br>
你可以貼在我們的社團上面<br>
<br>
0:11:15.740,0:11:19.800<br>
我講一下， Prediction based 的方法是怎麼做的呢<br>
<br>
0:11:19.800,0:11:22.820<br>
Prediction based 的方法，它的想法是這樣<br>
<br>
0:11:22.820,0:11:25.800<br>
我們來 learn 一個  neural network<br>
<br>
0:11:25.800,0:11:30.040<br>
它做的事情是 prediction，predict 什麼呢？<br>
<br>
0:11:30.040,0:11:34.340<br>
這個 neural network 做的事情是 given 前一個 word<br>
<br>
0:11:34.340,0:11:36.280<br>
假設給我一個 sentence<br>
<br>
0:11:36.280,0:11:39.300<br>
這邊每一個 w 代表一個 word<br>
<br>
0:11:39.300,0:11:44.560<br>
given w(下標 i-1)，這個 prediction 的 model<br>
<br>
0:11:44.560,0:11:47.020<br>
這個 neural network，它的工作是要<br>
<br>
0:11:47.020,0:11:50.820<br>
predict 下一個可能出現的 word 是誰<br>
<br>
0:11:50.820,0:11:54.580<br>
那我們知道說，每一個 word<br>
<br>
0:11:54.580,0:11:59.780<br>
我們都用 1-of-N encoding，可以把它表示成一個 feature vector<br>
<br>
0:11:59.780,0:12:03.360<br>
所以，如果我們要做 prediction 這一件事情的話<br>
<br>
0:12:03.360,0:12:06.920<br>
我們就是要 learn 一個 neural network<br>
<br>
0:12:06.920,0:12:08.860<br>
它的 input<br>
<br>
0:12:08.860,0:12:13.300<br>
就是 w(下標 i-1) 的 1-of-N encoding 的 feature vector<br>
<br>
0:12:13.440,0:12:15.440<br>
1-of-N encoding 的 vector<br>
<br>
0:12:15.540,0:12:18.160<br>
那它的 output 就是<br>
<br>
0:12:18.160,0:12:23.460<br>
下一個 word, wi 是某一個 word 的機率<br>
<br>
0:12:23.460,0:12:26.880<br>
也就是說，這一個 model 它的 output<br>
<br>
0:12:26.880,0:12:30.100<br>
它 output 的 dimension 就是 vector 的 size<br>
<br>
0:12:30.100,0:12:32.380<br>
假設現在世界上有 10 萬個 word<br>
<br>
0:12:32.380,0:12:34.580<br>
這個 model 的 output 就是 10 萬維<br>
<br>
0:12:34.580,0:12:37.100<br>
每一維代表了某一個 word<br>
<br>
0:12:37.100,0:12:39.160<br>
是下一個 word 的機率<br>
<br>
0:12:39.160,0:12:40.640<br>
每一維代表某一個 word<br>
<br>
0:12:40.640,0:12:43.760<br>
是會被當作 wi<br>
<br>
0:12:43.760,0:12:46.480<br>
它會是下一個 word, wi 的機率<br>
<br>
0:12:46.480,0:12:51.040<br>
所以 input 跟 output 都是 lexicon size<br>
<br>
0:12:51.040,0:12:53.040<br>
只是它們代表的意思是不一樣的<br>
<br>
0:12:53.040,0:12:56.760<br>
input 是 1-of-N encoding，output 是下一個 word 的機率<br>
<br>
0:12:56.760,0:13:00.080<br>
那假設這就是一個<br>
<br>
0:13:00.080,0:13:02.520<br>
一般我們所熟知的<br>
<br>
0:13:02.520,0:13:06.960<br>
multi-layer 的 Perceptron，一個 deep neural network<br>
<br>
0:13:06.960,0:13:10.580<br>
那你把它丟進去的時候<br>
<br>
0:13:10.580,0:13:13.120<br>
你把這個 input feature vector 丟進去的時候<br>
<br>
0:13:13.120,0:13:14.420<br>
它會通過很多 hidden layer<br>
<br>
0:13:14.420,0:13:17.760<br>
通過一些 hidden layer，你就會得到一些 output<br>
<br>
0:13:17.760,0:13:24.460<br>
接下來，我們把第一個 hidden layer 的 input 拿出來<br>
<br>
0:13:24.460,0:13:27.240<br>
我們把第一個 hidden layer 的 input 拿出來<br>
<br>
0:13:27.240,0:13:30.060<br>
假設第一個 hidden layer 的 input<br>
<br>
0:13:30.060,0:13:32.840<br>
我們這一邊寫作，它的第一個 dimension 是 Z1<br>
<br>
0:13:32.840,0:13:34.840<br>
第二個 dimension 是 Z2，以此類推<br>
<br>
0:13:34.840,0:13:36.380<br>
這邊把它寫作 Z<br>
<br>
0:13:36.380,0:13:39.620<br>
那我們用這個 Z<br>
<br>
0:13:39.620,0:13:42.340<br>
就可以代表一個 word<br>
<br>
0:13:42.340,0:13:45.400<br>
input 不同的 1-of-N encoding<br>
<br>
0:13:45.400,0:13:47.860<br>
這邊的 Z 就會不一樣<br>
<br>
0:13:47.860,0:13:51.280<br>
所以，我們就把這邊的 Z<br>
<br>
0:13:51.280,0:13:54.500<br>
拿來代表一個詞彙<br>
<br>
0:13:54.500,0:13:56.020<br>
你 input 同一個詞彙<br>
<br>
0:13:56.020,0:13:57.940<br>
它有同樣的 1-of-N encoding<br>
<br>
0:13:57.940,0:14:00.080<br>
在這邊它的 Z 就會一樣<br>
<br>
0:14:00.080,0:14:02.900<br>
input 不同的詞彙，這邊的 Z 就會不一樣<br>
<br>
0:14:02.900,0:14:04.900<br>
所以，我們就用這個 Z<br>
<br>
0:14:04.900,0:14:08.780<br>
這一個 input 1-of-N encoding 得到 Z 的這個 vector<br>
<br>
0:14:08.780,0:14:10.460<br>
來代表一個 word<br>
<br>
0:14:10.460,0:14:12.700<br>
來當作那一個 word 的 embedding<br>
<br>
0:14:12.700,0:14:14.660<br>
你就可以得到這一個現象<br>
<br>
0:14:14.660,0:14:18.320<br>
你就可以得到這樣的 vector<br>
<br>
0:14:18.320,0:14:20.520<br>
為什麼用這個 Prediction based 的方法<br>
<br>
0:14:20.520,0:14:22.580<br>
就可以得到這樣的 vector 呢<br>
<br>
0:14:22.580,0:14:24.220<br>
Prediction based 的方法<br>
<br>
0:14:24.220,0:14:26.180<br>
是怎麼體現我們說的<br>
<br>
0:14:26.180,0:14:28.420<br>
可以根據一個詞彙的上下文<br>
<br>
0:14:28.420,0:14:32.260<br>
來了解一個詞彙的涵義，這一件事情呢？<br>
<br>
0:14:32.260,0:14:34.020<br>
這邊是這樣子的<br>
<br>
0:14:34.020,0:14:37.360<br>
假設我們的 training data 裡面呢<br>
<br>
0:14:37.360,0:14:39.080<br>
有一個文章是<br>
<br>
0:14:39.080,0:14:42.300<br>
告訴我們說，馬英九跟蔡英文宣誓就職<br>
<br>
0:14:42.300,0:14:44.840<br>
另外一個是馬英九宣誓就職<br>
<br>
0:14:44.840,0:14:46.100<br>
在第一個句子裡面<br>
<br>
0:14:46.100,0:14:49.320<br>
蔡英文是 w(下標 i-1)，宣誓就職是 w(下標 i)<br>
<br>
0:14:49.320,0:14:50.660<br>
在另外一篇文章裡面<br>
<br>
0:14:50.660,0:14:54.480<br>
馬英九是 w(下標 i-1)，宣誓就職是  w(下標 i)<br>
<br>
0:14:54.480,0:14:58.440<br>
那你在訓練這個 Prediction  model 的時候<br>
<br>
0:14:58.440,0:15:02.020<br>
不管是 input 蔡英文，還是馬英九<br>
<br>
0:15:02.020,0:15:04.500<br>
不管是 input 蔡英文還是馬英九的 1-of-N encoding<br>
<br>
0:15:04.500,0:15:07.120<br>
你都會希望 learn 出來的結果<br>
<br>
0:15:07.120,0:15:11.260<br>
是宣誓就職的機率比較大<br>
<br>
0:15:11.260,0:15:12.900<br>
因為馬英九和蔡英文後面<br>
<br>
0:15:12.900,0:15:15.620<br>
接宣誓就職的機率都是高的<br>
<br>
0:15:15.620,0:15:18.720<br>
所以，你會希望說 input 馬英九跟蔡英文的時候<br>
<br>
0:15:18.720,0:15:23.120<br>
它 output，是 output 對應到宣誓就職那一個詞彙<br>
<br>
0:15:23.120,0:15:26.780<br>
它的那個 dimension 的機率是高的<br>
<br>
0:15:26.780,0:15:28.720<br>
為了要讓<br>
<br>
0:15:28.720,0:15:31.140<br>
蔡英文和馬英九雖然是不同的 input<br>
<br>
0:15:31.140,0:15:33.780<br>
但是，為了要讓最後在 output 的地方<br>
<br>
0:15:33.780,0:15:36.160<br>
得到一樣的 output<br>
<br>
0:15:36.160,0:15:40.120<br>
你就必須再讓中間的 hidden layer 做一些事情<br>
<br>
0:15:40.120,0:15:42.940<br>
中間的 hidden layer 必須要學到說<br>
<br>
0:15:42.940,0:15:44.640<br>
這兩個不同的詞彙<br>
<br>
0:15:44.640,0:15:47.000<br>
必需要把他們 project 到<br>
<br>
0:15:47.000,0:15:48.800<br>
必須要通過這個<br>
<br>
0:15:48.800,0:15:50.860<br>
必須要通過 weight 的轉換<br>
<br>
0:15:50.860,0:15:52.540<br>
通過這個參數的轉換以後<br>
<br>
0:15:52.540,0:15:56.000<br>
把它們對應到同樣的空間<br>
<br>
0:15:56.000,0:15:58.060<br>
在 input 進入 hidden layer 之前<br>
<br>
0:15:58.060,0:16:00.980<br>
必須把它們對應到接近的空間<br>
<br>
0:16:00.980,0:16:02.880<br>
這樣我們最後在 output 的時候<br>
<br>
0:16:02.880,0:16:05.720<br>
它們才能夠有同樣的機率<br>
<br>
0:16:05.720,0:16:08.740<br>
所以，當我們 learn 一個 prediction model 的時候<br>
<br>
0:16:08.740,0:16:12.100<br>
考慮一個 word 的 context這一件事情<br>
<br>
0:16:12.100,0:16:15.940<br>
就自動的，被考慮在這個 prediction model裡面<br>
<br>
0:16:15.940,0:16:17.900<br>
所以我們把這個 prediction model 的<br>
<br>
0:16:17.900,0:16:19.580<br>
第一個 hidden layer 拿出來<br>
<br>
0:16:19.580,0:16:20.980<br>
我們就可以得到<br>
<br>
0:16:21.100,0:16:24.120<br>
我們想要找的這種 word embedding 的特性<br>
<br>
0:16:24.120,0:16:27.160<br>
那你可能會想說<br>
<br>
0:16:27.160,0:16:31.260<br>
如果只用 w(下標 i-1)去 predict w(下標 i)<br>
<br>
0:16:31.260,0:16:32.980<br>
好像覺得太弱了<br>
<br>
0:16:32.980,0:16:34.980<br>
就算是人，你給一個詞彙<br>
<br>
0:16:34.980,0:16:36.380<br>
要 predict 下一個詞彙<br>
<br>
0:16:36.380,0:16:37.920<br>
感覺也很難<br>
<br>
0:16:37.920,0:16:39.080<br>
因為，如果只看一個詞彙，<br>
<br>
0:16:39.080,0:16:42.180<br>
下一個詞彙的可能性，是千千萬萬的<br>
<br>
0:16:42.180,0:16:43.400<br>
是千千萬萬的<br>
<br>
0:16:43.400,0:16:47.160<br>
那怎麼辦呢？怎麼辦呢？<br>
<br>
0:16:47.160,0:16:50.400<br>
你可以拓展這個問題<br>
<br>
0:16:50.400,0:16:52.320<br>
比如說，你可以拓展說<br>
<br>
0:16:52.320,0:16:55.320<br>
我希望 machine learn 的是 input 前面兩個詞彙<br>
<br>
0:16:55.320,0:16:59.540<br>
w(下標 i-2) 跟 w(下標 i-1)<br>
<br>
0:16:59.540,0:17:02.520<br>
predict 下一個 word, w(下標 i)<br>
<br>
0:17:02.520,0:17:05.860<br>
那你可以輕易地把這個 model 拓展到 N 個詞彙<br>
<br>
0:17:05.860,0:17:10.080<br>
一般我們，如果你真的要 learn 這樣的 word vector 的話<br>
<br>
0:17:10.080,0:17:11.800<br>
你可能會需要你的 input<br>
<br>
0:17:11.800,0:17:14.140<br>
可能通常是至少 10 個詞彙<br>
<br>
0:17:14.140,0:17:15.680<br>
你這樣才能夠 learn 出<br>
<br>
0:17:15.680,0:17:16.920<br>
比較 reasonable 的結果<br>
<br>
0:17:16.920,0:17:19.660<br>
只 input 一個或兩個，這都太少了<br>
<br>
0:17:19.660,0:17:22.500<br>
那我們這邊用 input 兩個 word 當作例子<br>
<br>
0:17:22.500,0:17:23.940<br>
那你可以輕易地把<br>
<br>
0:17:23.940,0:17:27.020<br>
這個 model 拓展到 10 個 word<br>
<br>
0:17:28.200,0:17:31.460<br>
那這邊要注意的事情是這個樣子<br>
<br>
0:17:31.460,0:17:33.740<br>
本來，如果是一般的 neural network<br>
<br>
0:17:33.740,0:17:37.440<br>
你就把 input w(下標 i-2) 和 w(下標 i-1) 的<br>
<br>
0:17:37.440,0:17:41.060<br>
1-of-N encoding 的 vector，把它接在一起<br>
<br>
0:17:41.060,0:17:43.180<br>
變成一個很長的 vector<br>
<br>
0:17:43.180,0:17:45.160<br>
直接丟都到 neural network 裡面<br>
<br>
0:17:45.160,0:17:47.400<br>
當作 input 就可以了<br>
<br>
0:17:47.400,0:17:50.140<br>
但是實際上，你在做的時候<br>
<br>
0:17:50.140,0:17:54.860<br>
你會希望 w(下標 i-2) 的<br>
<br>
0:17:54.860,0:17:57.520<br>
這個跟它相連的 weight<br>
<br>
0:17:57.520,0:17:59.780<br>
跟和 w(下標 i-1) 相連的 weight<br>
<br>
0:17:59.780,0:18:02.760<br>
它們是被 tight 在一起的<br>
<br>
0:18:02.760,0:18:05.060<br>
所謂 tight 在一起的意思是說<br>
<br>
0:18:05.060,0:18:08.420<br>
w(下標 i-2) 的第一個 dimension<br>
<br>
0:18:08.420,0:18:11.900<br>
跟第一個 hidden layer 的第一個 neuron<br>
<br>
0:18:11.900,0:18:13.340<br>
它們中間連的 weight<br>
<br>
0:18:13.340,0:18:16.480<br>
和 w(下標 i-1) 的第一個 dimension<br>
<br>
0:18:16.480,0:18:19.880<br>
和第一個 hidden layer 的 neuron，它們之間連的位置<br>
<br>
0:18:19.880,0:18:23.720<br>
這兩個 weight 必須是一樣的<br>
<br>
0:18:23.720,0:18:27.760<br>
所以，我這邊故意用同樣的顏色來表示它<br>
<br>
0:18:27.760,0:18:30.740<br>
這一個 dimension，它連到這個的 weight<br>
<br>
0:18:30.740,0:18:35.420<br>
跟第一個 dimension，它連到這邊的 weight<br>
<br>
0:18:35.420,0:18:37.240<br>
它必須是一樣的<br>
<br>
0:18:37.240,0:18:39.980<br>
所以，我這邊故意用同樣的顏色來表示他<br>
<br>
0:18:39.980,0:18:42.600<br>
這一個 dimension，它連到它的 weight<br>
<br>
0:18:42.600,0:18:44.840<br>
跟它連到它的 weight，必須是一樣的<br>
<br>
0:18:44.840,0:18:46.600<br>
以此類推<br>
<br>
0:18:46.600,0:18:50.360<br>
希望大家知道知道我的意思<br>
<br>
0:18:50.360,0:18:53.500<br>
那為什麼要這樣做呢？<br>
<br>
0:18:53.500,0:18:55.680<br>
為什麼要這樣做呢？<br>
<br>
0:18:55.680,0:18:58.220<br>
一個顯而易見的理由是這樣<br>
<br>
0:18:58.220,0:19:00.400<br>
一個顯而易見的理由是說<br>
<br>
0:19:00.400,0:19:02.900<br>
如果，我們不這麼做<br>
<br>
0:19:02.900,0:19:07.260<br>
如果我們不這麼做，你把不同的 word<br>
<br>
0:19:07.260,0:19:11.320<br>
你把同一個 word 放在<br>
<br>
0:19:11.320,0:19:15.700<br>
w(下標 i-2) 的位置跟放在 w(下標 i-1) 的位置<br>
<br>
0:19:15.700,0:19:19.460<br>
通過這個 transform 以後<br>
<br>
0:19:19.460,0:19:22.740<br>
它得到的 embedding 就會不一樣<br>
<br>
0:19:22.740,0:19:26.760<br>
如果，你必須要讓這一組 weight<br>
<br>
0:19:26.760,0:19:28.820<br>
和這一組weight 是一樣的<br>
<br>
0:19:28.820,0:19:31.760<br>
那你把一個 word 放在這邊，通過這個 transform<br>
<br>
0:19:31.760,0:19:33.640<br>
跟把一個 weight 放在這邊，通過一個 transform<br>
<br>
0:19:33.640,0:19:36.320<br>
它們得到的 weight 才會是一樣的<br>
<br>
0:19:36.320,0:19:38.360<br>
當然另外一個理由你可以說<br>
<br>
0:19:38.360,0:19:40.620<br>
我們做這一件事情的好處是<br>
<br>
0:19:40.620,0:19:43.160<br>
我們可以減少參數量<br>
<br>
0:19:43.160,0:19:46.200<br>
因為 input 這個 dimension 很大，它是十萬維<br>
<br>
0:19:46.200,0:19:48.580<br>
所以這個 feature vector，就算你這一邊是50 維<br>
<br>
0:19:48.580,0:19:52.120<br>
它也是一個非常非常、碩大無朋的 matrix<br>
<br>
0:19:52.120,0:19:54.680<br>
有一個已經覺得夠卡了<br>
<br>
0:19:54.680,0:19:57.060<br>
所以，有兩個更是吃不消<br>
<br>
0:19:57.060,0:20:00.540<br>
更何況說，我們現在 input 往往是 10 個 word<br>
<br>
0:20:00.540,0:20:02.280<br>
所以，如果我們強迫讓<br>
<br>
0:20:02.280,0:20:04.460<br>
所有的 1-of-N encoding<br>
<br>
0:20:04.460,0:20:06.240<br>
它後面接的 weight 是一樣的<br>
<br>
0:20:06.240,0:20:08.580<br>
那你就不會隨著你的 contest 的增長<br>
<br>
0:20:08.580,0:20:12.660<br>
而需要這個更多的參數<br>
<br>
0:20:12.660,0:20:16.120<br>
或許我們用 formulation 來表示<br>
<br>
0:20:16.120,0:20:17.920<br>
會更清楚一點<br>
<br>
0:20:17.920,0:20:23.280<br>
現在，假設 w(下標 i-2) 的 1-of-N encoding 就是 X2<br>
<br>
0:20:23.280,0:20:26.280<br>
w(下標 i-1) 的 1-of-N encoding 就是 X1<br>
<br>
0:20:26.280,0:20:28.760<br>
那它們的這個長度<br>
<br>
0:20:28.760,0:20:31.400<br>
都是 V 的絕對值<br>
<br>
0:20:31.400,0:20:34.220<br>
它們的長度我這邊都寫成 V 的絕對值<br>
<br>
0:20:34.220,0:20:41.200<br>
那這個 hidden layer 的 input<br>
<br>
0:20:41.200,0:20:43.200<br>
我們把它寫一個 vector, Z<br>
<br>
0:20:43.200,0:20:47.180<br>
Z 的長度，是 Z 的絕對值<br>
<br>
0:20:47.180,0:20:51.980<br>
那我們把這個 Z 跟<br>
<br>
0:20:51.980,0:20:55.740<br>
X(i-2) 跟 X(i-1) 有什麼樣的關係<br>
<br>
0:20:55.740,0:21:03.520<br>
Z 等於 X(i-2) * W1 + X(i-1) * W2<br>
<br>
0:21:03.520,0:21:09.800<br>
你把 X(i-2) * W1 + X(i-1) * W2，就會得到這個 Z<br>
<br>
0:21:09.800,0:21:11.440<br>
那現在這個 W1 跟 W2<br>
<br>
0:21:11.440,0:21:17.660<br>
它們都是一個 Z 乘上一個 V dimension 的 weight matrix<br>
<br>
0:21:17.660,0:21:19.780<br>
那在這邊，我們做的事情是<br>
<br>
0:21:19.780,0:21:23.500<br>
我們強制讓 W1 要等於 W2<br>
<br>
0:21:23.500,0:21:26.500<br>
要等於一個一模一樣的 matrix, W<br>
<br>
0:21:26.500,0:21:31.280<br>
所以，我們今天實際上在處理這個問題的時候<br>
<br>
0:21:31.280,0:21:36.580<br>
你可以把 X(i-2) 跟 X(i-1) 直接先加起來<br>
<br>
0:21:36.580,0:21:38.700<br>
因為 W1 跟 W2 是一樣的<br>
<br>
0:21:38.700,0:21:40.180<br>
你可以把 W 提出來<br>
<br>
0:21:40.180,0:21:42.920<br>
你可以把 X(i-1) 跟X(i-2) 先加起來<br>
<br>
0:21:42.920,0:21:45.360<br>
再乘上 W 的這個 transform<br>
<br>
0:21:45.360,0:21:47.980<br>
就會得到 z<br>
<br>
0:21:47.980,0:21:51.900<br>
那你今天如果要得到一個 word 的 vector 的時候<br>
<br>
0:21:51.900,0:21:55.060<br>
你就把一個 word 的 1-of-N encoding<br>
<br>
0:21:55.060,0:21:57.080<br>
乘上這個 W<br>
<br>
0:21:57.080,0:22:00.700<br>
你就可以得到那一個 word 的 Word Embedding<br>
<br>
0:22:02.960,0:22:07.360<br>
那這一邊會有一個問題，就是我們在實做上<br>
<br>
0:22:07.360,0:22:10.260<br>
如果你真的要自己實做的話<br>
<br>
0:22:10.260,0:22:13.400<br>
你怎麼讓這個 W1 跟 W2<br>
<br>
0:22:13.400,0:22:16.360<br>
它們的位 weight 一定都要一樣呢<br>
<br>
0:22:16.360,0:22:21.640<br>
事實上我們在 train CNN 的時候<br>
<br>
0:22:21.640,0:22:23.540<br>
也有一樣類似的問題<br>
<br>
0:22:23.540,0:22:25.000<br>
我們在 train CNN 的時候<br>
<br>
0:22:25.000,0:22:27.040<br>
我們也要讓 W1 跟 W2<br>
<br>
0:22:27.040,0:22:30.560<br>
我們也要讓某一些參數，它們的 weight<br>
<br>
0:22:30.560,0:22:33.380<br>
必須是一樣的<br>
<br>
0:22:33.860,0:22:36.740<br>
那怎麼做呢？這個做法是這樣子<br>
<br>
0:22:36.740,0:22:40.980<br>
假設我們現在有兩個 weight, wi 跟 wj<br>
<br>
0:22:40.980,0:22:44.000<br>
那我們希望 wi 跟 wj，它的 weight 是一樣的<br>
<br>
0:22:44.000,0:22:45.260<br>
那怎麼做呢？<br>
<br>
0:22:45.260,0:22:50.660<br>
首先，你要給 wi 跟 wj 一樣的 initialization<br>
<br>
0:22:50.660,0:22:53.240<br>
訓練的時候要給它們一樣的初始值<br>
<br>
0:22:53.240,0:22:56.480<br>
接下來，你計算 wi 的<br>
<br>
0:22:56.480,0:23:00.540<br>
wi 對你最後 cost function 的偏微分<br>
<br>
0:23:00.540,0:23:02.440<br>
然後 update wi<br>
<br>
0:23:02.440,0:23:05.380<br>
然後，你計算 wj 對 cost function 的偏微分<br>
<br>
0:23:05.380,0:23:06.720<br>
然後 update wj<br>
<br>
0:23:06.720,0:23:09.200<br>
你可能會說 wi 跟 wj <br>
<br>
0:23:09.200,0:23:11.860<br>
如果它們對 C 的偏微分是不一樣的<br>
<br>
0:23:11.860,0:23:13.400<br>
那做 update 以後<br>
<br>
0:23:13.400,0:23:16.000<br>
它們的值，不就不一樣了嗎？<br>
<br>
0:23:16.000,0:23:18.300<br>
所以，如果你只有列這樣的式子<br>
<br>
0:23:18.300,0:23:21.720<br>
wi 跟 wj 經過一次 update 以後，它們的值就不一樣了<br>
<br>
0:23:21.720,0:23:23.960<br>
initialize 值一樣也沒有用<br>
<br>
0:23:23.960,0:23:25.140<br>
那怎麼辦呢？<br>
<br>
0:23:25.140,0:23:29.860<br>
我們就把 wi 再減掉<br>
<br>
0:23:29.860,0:23:32.740<br>
再減掉 wj 對 C 的偏微分<br>
<br>
0:23:32.740,0:23:36.800<br>
把 wj 再減掉 wi 對 C 的偏微分<br>
<br>
0:23:36.800,0:23:39.940<br>
也就是說 wi 有這樣的 update <br>
<br>
0:23:39.940,0:23:42.540<br>
wj 也要有一個一模一樣的 update<br>
<br>
0:23:42.540,0:23:44.020<br>
wj 有這樣的 update<br>
<br>
0:23:44.020,0:23:46.800<br>
wi 也要有一個一模一樣的 update<br>
<br>
0:23:46.800,0:23:49.000<br>
如果你用這樣的方法的話呢<br>
<br>
0:23:49.000,0:23:52.480<br>
你就可以確保 wi 跟 wj，它們是<br>
<br>
0:23:52.480,0:23:54.820<br>
在這個 update 的過程中<br>
<br>
0:23:54.820,0:23:56.060<br>
在訓練的過程中<br>
<br>
0:23:56.060,0:23:59.700<br>
它們的 weight 永遠都是被 tight 在一起的<br>
<br>
0:23:59.700,0:24:02.860<br>
永遠都是一樣<br>
<br>
0:24:04.500,0:24:07.240<br>
那要怎麼訓練這個 network 呢？<br>
<br>
0:24:07.240,0:24:08.560<br>
這個 network 的訓練<br>
<br>
0:24:08.560,0:24:11.340<br>
完全是 unsupervised 的<br>
<br>
0:24:11.340,0:24:15.660<br>
也就是說，你只要 collect 一大堆文字的data<br>
<br>
0:24:15.660,0:24:17.500<br>
collect 文字的 data 很簡單<br>
<br>
0:24:17.500,0:24:19.340<br>
就寫一個程式上網去爬就好<br>
<br>
0:24:19.340,0:24:20.740<br>
寫一個程式爬一下<br>
<br>
0:24:20.740,0:24:22.940<br>
八卦版的 data<br>
<br>
0:24:22.940,0:24:25.320<br>
就可以爬到一大堆文字<br>
<br>
0:24:25.320,0:24:27.500<br>
然後，接下來就可以 train 你的 model<br>
<br>
0:24:27.500,0:24:29.340<br>
怎麼 train，比如說這邊有一個句子就是<br>
<br>
0:24:29.340,0:24:31.900<br>
潮水退了，就知道誰沒穿褲子<br>
<br>
0:24:31.900,0:24:34.440<br>
那你就讓你的 model<br>
<br>
0:24:34.440,0:24:38.140<br>
讓你的 neural network input "潮水" 跟 "退了"<br>
<br>
0:24:38.140,0:24:40.760<br>
希望它的 output 是 "就" 這個樣子<br>
<br>
0:24:40.760,0:24:44.760<br>
你會希望你的 output 跟"就" 的 cross entropy<br>
<br>
0:24:44.760,0:24:47.440<br>
"就" 也是一個 1-of-N encoding 來表示<br>
<br>
0:24:47.440,0:24:48.880<br>
所以，你希望你的 network 的 output <br>
<br>
0:24:48.880,0:24:51.460<br>
跟 "就" 的 1-of-N encoding<br>
<br>
0:24:51.460,0:24:53.340<br>
是 minimize cross entropy<br>
<br>
0:24:53.340,0:24:55.780<br>
然後，再來就 input "退了 " 跟 "就"<br>
<br>
0:24:55.780,0:24:59.680<br>
然後，希望它的 output 跟 "知道" 越接近越好<br>
<br>
0:24:59.680,0:25:01.560<br>
然後 output "就" 跟 "知道" <br>
<br>
0:25:01.560,0:25:04.460<br>
然後就，希望它跟 "誰" 越接近越好<br>
<br>
0:25:05.260,0:25:07.320<br>
那剛才講的<br>
<br>
0:25:07.320,0:25:09.220<br>
只是最基本的型態<br>
<br>
0:25:09.220,0:25:12.120<br>
其實這個 Prediction based 的 model <br>
<br>
0:25:12.120,0:25:15.020<br>
可以有種種的變形<br>
<br>
0:25:15.020,0:25:16.560<br>
目前我還不確定說<br>
<br>
0:25:16.560,0:25:21.140<br>
在各種變形之中哪一種是比較好的<br>
<br>
0:25:21.140,0:25:23.080<br>
感覺上，它的 performance<br>
<br>
0:25:23.080,0:25:25.420<br>
在不同的 task上互有勝負<br>
<br>
0:25:25.420,0:25:28.500<br>
所以，很難說哪一種方法一定是比較好的<br>
<br>
0:25:28.500,0:25:31.160<br>
那有一招叫做<br>
<br>
0:25:31.160,0:25:33.420<br>
Continuous bag of word, (CBOW)<br>
<br>
0:25:33.420,0:25:35.220<br>
那 CBOW 是這個樣子的<br>
<br>
0:25:35.220,0:25:38.560<br>
CBOW 是說，我們剛才是拿前面的詞彙<br>
<br>
0:25:38.560,0:25:41.100<br>
去 predict 接下來的詞彙<br>
<br>
0:25:41.100,0:25:43.040<br>
那 CBOW 的意思是說<br>
<br>
0:25:43.040,0:25:44.920<br>
我們拿某一個詞彙的 context<br>
<br>
0:25:44.920,0:25:46.500<br>
去 predict 中間這個詞彙<br>
<br>
0:25:46.500,0:25:50.320<br>
我們拿 W(i-1) 跟 W(i+1) 去 predict Wi<br>
<br>
0:25:50.320,0:25:54.220<br>
用 W(i-1) 跟 W(i+1)去 predict Wi<br>
<br>
0:25:54.220,0:25:56.460<br>
那 Skip-gram 是說<br>
<br>
0:25:56.460,0:26:02.040<br>
我們拿中間的詞彙去 predict 接下來的 context<br>
<br>
0:26:02.040,0:26:08.880<br>
我們拿 Wi 去 predict W(i-1) 跟 W(i+1)<br>
<br>
0:26:08.880,0:26:12.700<br>
也就是 given 中間的 word，我們要去 predict 它的周圍<br>
<br>
0:26:12.700,0:26:14.440<br>
會是長什麼樣子<br>
<br>
0:26:14.860,0:26:17.380<br>
講到這邊大家有問題嗎？<br>
<br>
0:26:18.700,0:26:21.140<br>
講到這邊常常會有人問我一個問題<br>
<br>
0:26:21.140,0:26:25.100<br>
假設你有足夠 word vector 相關的文獻的話<br>
<br>
0:26:25.100,0:26:26.540<br>
你可能會說<br>
<br>
0:26:26.540,0:26:29.940<br>
其實這個 network 它不是 deep 的阿<br>
<br>
0:26:29.940,0:26:31.940<br>
雖然，常常在講 deep learning 的時候<br>
<br>
0:26:31.940,0:26:33.800<br>
大家都會提到 word vector<br>
<br>
0:26:33.800,0:26:36.200<br>
把它當作 deep learning 的一個 application<br>
<br>
0:26:36.200,0:26:39.320<br>
但是，如果你真的有讀過 word vector 的文獻的話<br>
<br>
0:26:39.320,0:26:40.580<br>
你會發現說<br>
<br>
0:26:40.580,0:26:43.820<br>
這個 neural network，它不是 deep 的<br>
<br>
0:26:43.820,0:26:45.440<br>
它其實就是一個 hidden layer<br>
<br>
0:26:45.440,0:26:47.540<br>
它其實是一個 linear 的 hidden layer<br>
<br>
0:26:47.540,0:26:48.800<br>
了解嗎？就是<br>
<br>
0:26:48.800,0:26:50.800<br>
這個 neural network，它只有一個 hidden layer<br>
<br>
0:26:50.800,0:26:53.280<br>
所以，你把 word input 以後，你就得到 word embedding<br>
<br>
0:26:53.280,0:26:56.540<br>
你就直接再從那個 hidden layer，就可以得到 output <br>
<br>
0:26:56.540,0:26:59.180<br>
它不是 deep 的，為什麼呢？<br>
<br>
0:26:59.180,0:27:01.740<br>
為什麼？常常有人 問我這個問題<br>
<br>
0:27:01.740,0:27:04.380<br>
那為了回答這個問題 <br>
<br>
0:27:04.380,0:27:09.100<br>
我邀請了  Tomas Mikolov 來台灣玩這樣<br>
<br>
0:27:09.100,0:27:13.740<br>
Tomas Mikolov 就是 propose word vector 的作者<br>
<br>
0:27:13.740,0:27:16.520<br>
所以，如果你有用過 word vector 的 toolkit 的話<br>
<br>
0:27:16.520,0:27:18.600<br>
你可能有聽過他的名字<br>
<br>
0:27:18.600,0:27:21.660<br>
那就問他說，為什麼這個 model不是 deep 的呢？<br>
<br>
0:27:21.660,0:27:22.920<br>
他給我兩個答案<br>
<br>
0:27:22.920,0:27:24.340<br>
他說，首先第一個就是<br>
<br>
0:27:24.340,0:27:27.860<br>
他並不是第一個 propose word vector 的人<br>
<br>
0:27:27.860,0:27:30.240<br>
在過去就有很多這樣的概念<br>
<br>
0:27:30.240,0:27:31.940<br>
那他最 famous 的地方是<br>
<br>
0:27:31.940,0:27:35.220<br>
他把他寫的一個非常好的 toolkit 放在網路上<br>
<br>
0:27:35.220,0:27:38.060<br>
他在他的 toolkit 裡面，如果你看他的 code 的話<br>
<br>
0:27:38.060,0:27:41.320<br>
他有種種的 tip<br>
<br>
0:27:41.320,0:27:44.020<br>
所以，你自己做的時候做不出他的 performance 的<br>
<br>
0:27:44.020,0:27:46.540<br>
他是一個非常非常強 的 engineer<br>
<br>
0:27:46.540,0:27:48.700<br>
他有各種他自己直覺的 sense<br>
<br>
0:27:48.700,0:27:51.940<br>
所以你自己做，你做不出他的 performance<br>
<br>
0:27:51.940,0:27:54.520<br>
用他的 toolkit，跑出來的 performance 就是特別好<br>
<br>
0:27:54.520,0:27:57.740<br>
所以，這是一個<br>
<br>
0:27:57.740,0:27:59.360<br>
他非常厲害的地方<br>
<br>
0:27:59.360,0:28:02.540<br>
他說，在他之前其實就有很多人做過<br>
<br>
0:28:02.540,0:28:05.440<br>
word vector，也有提出類似的概念<br>
<br>
0:28:05.440,0:28:10.460<br>
他說他寫的，他有一篇 word vector 的文章跟 toolkit<br>
<br>
0:28:10.460,0:28:12.320<br>
他想要 verify 最重要的一件事情是說<br>
<br>
0:28:12.320,0:28:14.700<br>
過去其實其他人就是用 deep<br>
<br>
0:28:14.700,0:28:18.400<br>
他想要講的是說，其實這個 task<br>
<br>
0:28:18.400,0:28:20.060<br>
不用 deep 就做起來了<br>
<br>
0:28:20.060,0:28:22.840<br>
不用 deep 的好處就是減少運算量<br>
<br>
0:28:22.840,0:28:26.180<br>
所以它可以跑很大量、很大量、很大量的 data<br>
<br>
0:28:26.180,0:28:28.120<br>
那我聽他這樣講<br>
<br>
0:28:28.120,0:28:29.960<br>
我就想起來，其實過去確實是<br>
<br>
0:28:29.960,0:28:32.440<br>
有人已經做過 word vector<br>
<br>
0:28:32.440,0:28:34.380<br>
過去確實已經有做過 word vector 這件事情<br>
<br>
0:28:34.380,0:28:37.820<br>
只是那些結果沒有紅起來<br>
<br>
0:28:37.820,0:28:39.460<br>
我記得說，我大學的時候<br>
<br>
0:28:39.460,0:28:41.320<br>
就看過類似的 paper<br>
<br>
0:28:41.320,0:28:42.920<br>
我大學的時候就有看過<br>
<br>
0:28:42.920,0:28:46.520<br>
其實就是一樣，就是 learn 一個 Prediction model<br>
<br>
0:28:46.520,0:28:48.120<br>
predict 下一個 word 的做法<br>
<br>
0:28:48.180,0:28:50.000<br>
只是那個時候是 deep<br>
<br>
0:28:50.000,0:28:51.740<br>
在我大學的時候<br>
<br>
0:28:51.740,0:28:52.940<br>
那時候 deep learning 還不紅<br>
<br>
0:28:52.940,0:28:54.020<br>
我看到那一篇 paper  的時候<br>
<br>
0:28:54.020,0:28:56.560<br>
他最後講說我 train 了這個 model<br>
<br>
0:28:56.560,0:29:00.560<br>
我花了 3 週，然後我沒有辦法把實驗跑完<br>
<br>
0:29:00.560,0:29:02.540<br>
所以結果是很好的<br>
<br>
0:29:02.880,0:29:06.080<br>
就其他方法，他可以跑很多的 iteration<br>
<br>
0:29:06.080,0:29:07.860<br>
然後說這個 neural network 的方法<br>
<br>
0:29:07.860,0:29:12.560<br>
我跑了 5 個 epoch，花了 3 週，我實在做不下去<br>
<br>
0:29:12.560,0:29:14.080<br>
所以，performance 沒有特別好<br>
<br>
0:29:14.080,0:29:16.260<br>
而且想說，這是什麼荒謬的做法<br>
<br>
0:29:16.260,0:29:19.080<br>
但是，現在運算量不同<br>
<br>
0:29:19.080,0:29:20.340<br>
所以，現在要做這一件事情呢<br>
<br>
0:29:20.340,0:29:22.700<br>
都沒有問題<br>
<br>
0:29:22.700,0:29:25.600<br>
其實像 word embedding 這個概念<br>
<br>
0:29:25.600,0:29:30.820<br>
在語音界，大概是在 2010 年的時候開始紅起來的<br>
<br>
0:29:30.820,0:29:34.760<br>
那個時候我們把它叫做 continuous 的 language model<br>
<br>
0:29:34.760,0:29:35.600<br>
一開始的時候<br>
<br>
0:29:35.600,0:29:37.800<br>
也不是用 neural network 來得到這個 word embedding的<br>
<br>
0:29:37.800,0:29:39.680<br>
因為 neural network  的運算量比較大<br>
<br>
0:29:39.680,0:29:41.580<br>
所以，一開始並不是選擇 neural network<br>
<br>
0:29:41.580,0:29:44.200<br>
而是用一些其他方法來<br>
<br>
0:29:44.200,0:29:46.980<br>
一些比較簡單的方法來得到這個 word 的 embedding<br>
<br>
0:29:46.980,0:29:49.040<br>
只是，後來大家逐漸發現說<br>
<br>
0:29:49.040,0:29:51.840<br>
用 neural network 得到的結果才是最好的<br>
<br>
0:29:51.840,0:29:53.840<br>
過去其他不是 neural network 的方法<br>
<br>
0:29:53.840,0:29:55.100<br>
就逐漸式微<br>
<br>
0:29:55.100,0:29:57.320<br>
通通都變成 neural network based 的方法<br>
<br>
0:29:57.900,0:29:59.760<br>
還有一個勵志的故事<br>
<br>
0:29:59.760,0:30:02.060<br>
就是Tomas Mikolov 那個<br>
<br>
0:30:02.060,0:30:03.880<br>
word vector paper不是非常 famous 嗎？<br>
<br>
0:30:03.880,0:30:06.020<br>
它的 citation，我不知道，搞不好都有 1 萬了<br>
<br>
0:30:06.020,0:30:08.740<br>
他說他第一次投那 一篇 paper 的時候<br>
<br>
0:30:08.740,0:30:10.820<br>
他先投到一個，我已經忘記名字的<br>
<br>
0:30:10.820,0:30:12.780<br>
很小很小的會，accept rate 有 70%<br>
<br>
0:30:12.780,0:30:14.740<br>
然後就被 reject 了<br>
<br>
0:30:16.380,0:30:20.120<br>
他還得到一個 comment，就是這是什麼東西<br>
<br>
0:30:20.120,0:30:22.120<br>
我覺得這東西一點用都沒有<br>
<br>
0:30:22.120,0:30:25.380<br>
所以，這是一個非常勵志的故事<br>
<br>
0:30:27.240,0:30:29.840<br>
那我們知道說<br>
<br>
0:30:29.840,0:30:32.300<br>
word vector 可以得到一些有趣的特性<br>
<br>
0:30:32.300,0:30:34.880<br>
我們可以看到說<br>
<br>
0:30:34.880,0:30:39.400<br>
如果你把同樣類型的東西的 word vector 擺在一起<br>
<br>
0:30:39.400,0:30:42.180<br>
比如說，我們把這個 Italy<br>
<br>
0:30:42.180,0:30:44.660<br>
跟它的首都 Rome 擺在一起<br>
<br>
0:30:44.660,0:30:47.120<br>
我們把Germany 跟它的首都 Berlin 擺在一起<br>
<br>
0:30:47.120,0:30:49.800<br>
我們把 Japan<br>
<br>
0:30:49.800,0:30:51.660<br>
跟它的首都 Tokyo 擺在一起<br>
<br>
0:30:51.660,0:30:53.487<br>
你會發現說<br>
<br>
0:30:53.487,0:30:56.800<br>
它們之間是有某種固定的關係的<br>
<br>
0:30:56.800,0:30:59.860<br>
或者是，你把一個動詞的三態擺在一起<br>
<br>
0:30:59.860,0:31:03.500<br>
你會發現說，動詞的三態<br>
<br>
0:31:03.500,0:31:04.940<br>
同一個動詞的三態<br>
<br>
0:31:04.940,0:31:07.280<br>
它們中間有某種固定的關係<br>
<br>
0:31:07.280,0:31:08.660<br>
成為這個三角形<br>
<br>
0:31:09.080,0:31:11.560<br>
所以從這個 word vector 裡面呢<br>
<br>
0:31:11.560,0:31:15.080<br>
你可以 discover 你不知道的 word 跟 word 之間的關係<br>
<br>
0:31:15.080,0:31:17.540<br>
比如說，還有人發現說<br>
<br>
0:31:17.540,0:31:19.420<br>
如果你今天把<br>
<br>
0:31:19.420,0:31:23.420<br>
兩個 word vector 和 word vector 之間，兩兩相減<br>
<br>
0:31:23.420,0:31:27.140<br>
這個結果是把 word vector 跟 word vector 之間兩兩相減<br>
<br>
0:31:27.140,0:31:30.940<br>
然後 project 到一個 2 dimensional 的 space 上面<br>
<br>
0:31:30.940,0:31:33.560<br>
那你會發現說，在這一區<br>
<br>
0:31:33.560,0:31:35.880<br>
如果今天 word vector 兩兩相減<br>
<br>
0:31:35.880,0:31:38.160<br>
它得到的結果是落在這個位置的話<br>
<br>
0:31:38.160,0:31:42.780<br>
那這兩個 word vector 之間，它們就有，比如說<br>
<br>
0:31:42.780,0:31:46.920<br>
某一個 word 是包含於某一個 word 之間的關係<br>
<br>
0:31:46.920,0:31:49.960<br>
比如說，你把 (這一邊這個字比較小)<br>
<br>
0:31:49.960,0:31:53.820<br>
比如說，你把海豚跟會轉彎的白海豚相減<br>
<br>
0:31:53.820,0:31:56.400<br>
它的 vector 落在這邊<br>
<br>
0:31:56.400,0:31:59.980<br>
你把演員跟主角相減，落在這一邊<br>
<br>
0:31:59.980,0:32:02.120<br>
你把工人跟木匠相減，落在這邊<br>
<br>
0:32:02.120,0:32:06.280<br>
你把職員跟售貨員相減，落在這一邊<br>
<br>
0:32:06.280,0:32:08.720<br>
你把羊跟公羊相減，落在這邊<br>
<br>
0:32:08.720,0:32:10.360<br>
如果，某一個東西是<br>
<br>
0:32:10.360,0:32:12.500<br>
屬於另外一個東西的話<br>
<br>
0:32:12.500,0:32:13.980<br>
你把它們兩個 word vector 相減<br>
<br>
0:32:13.980,0:32:16.940<br>
它們的結果呢，會是很類似的<br>
<br>
0:32:16.940,0:32:20.200<br>
所以用 word vector 的這一個的概念<br>
<br>
0:32:20.200,0:32:24.120<br>
我們可以做一些簡單的推論<br>
<br>
0:32:24.120,0:32:27.020<br>
舉例來說， 因為我們知道說<br>
<br>
0:32:27.020,0:32:28.880<br>
比如說，hotter 的 word vector<br>
<br>
0:32:28.880,0:32:31.360<br>
減掉 hot 的 word vector 會很接近<br>
<br>
0:32:31.360,0:32:33.780<br>
bigger 的 word vector 減掉 big 的 word vector <br>
<br>
0:32:33.780,0:32:36.020<br>
或是 Rome 的 vector 減掉 Italy 的 vector<br>
<br>
0:32:36.020,0:32:38.560<br>
會很接近 Berlin 的 vector 減掉 Germany 的 vector <br>
<br>
0:32:38.560,0:32:40.960<br>
或是 King 的 vector 減掉 queen 的 vector 會很接近<br>
<br>
0:32:40.960,0:32:43.120<br>
uncle 的 vector 減掉 aunt 的 vector<br>
<br>
0:32:43.120,0:32:46.060<br>
如果有人問你說，羅馬之於義大利<br>
<br>
0:32:46.060,0:32:48.160<br>
就好像 Berlin 之於什麼？<br>
<br>
0:32:48.160,0:32:49.900<br>
智力測驗都會考這樣的問題<br>
<br>
0:32:49.900,0:32:51.560<br>
機器可以回答這種問題了<br>
<br>
0:32:51.560,0:32:54.440<br>
怎麼做呢？因為我們知道說<br>
<br>
0:32:54.440,0:32:57.700<br>
今天這個問題的答案<br>
<br>
0:32:57.700,0:33:00.180<br>
Germany 的 vector 會很接近 Berlin 的 vector<br>
<br>
0:33:00.180,0:33:03.000<br>
減掉 Rome 的 vector 加 Italy 的 vector<br>
<br>
0:33:03.000,0:33:05.860<br>
因為這 4 個 word vector 中間有這樣的關係<br>
<br>
0:33:05.860,0:33:07.880<br>
所以你可以把 Germany  放在一邊<br>
<br>
0:33:07.880,0:33:09.940<br>
把另外三個 vector 放在右邊<br>
<br>
0:33:09.940,0:33:12.640<br>
所以 Germany 的 vector 會接近 Berlin 的 vector<br>
<br>
0:33:12.640,0:33:15.160<br>
減掉 Rome 的 vector 再加上 Italy 的 vector<br>
<br>
0:33:15.160,0:33:17.520<br>
所以，如果你要回答這個問題<br>
<br>
0:33:17.520,0:33:20.420<br>
假設你不知道答案是 Germany 的話<br>
<br>
0:33:20.420,0:33:22.540<br>
那你要做的事情就是<br>
<br>
0:33:22.540,0:33:24.020<br>
計算 Berlin 的 vector<br>
<br>
0:33:24.020,0:33:27.080<br>
減掉 Rome的 vector，再加 Italy 的 vector<br>
<br>
0:33:27.080,0:33:29.420<br>
然後看看它跟哪一個 vector 最接近<br>
<br>
0:33:29.420,0:33:32.620<br>
你可能得到的答案就是 Germany<br>
<br>
0:33:32.620,0:33:36.680<br>
這邊有一個 word vector 的 demo<br>
<br>
0:33:36.680,0:33:40.520<br>
就讓機器讀了大量 PTT 的文章以後<br>
<br>
0:33:40.520,0:33:42.520<br>
它就像這樣<br>
<br>
0:33:42.840,0:33:46.000<br>
那 word vector 還可以做很多其他的事情<br>
<br>
0:33:46.000,0:33:51.280<br>
比如說，你可以把不同的語言的 word vector<br>
<br>
0:33:51.280,0:33:53.120<br>
把它拉在一起<br>
<br>
0:33:53.120,0:33:55.080<br>
如果，你今天有一個中文的 corpus<br>
<br>
0:33:55.080,0:33:56.680<br>
有一個英文的 corpus<br>
<br>
0:33:56.680,0:34:00.400<br>
你各自去、分別去 train 一組 word vector<br>
<br>
0:34:00.400,0:34:01.920<br>
你會發現說<br>
<br>
0:34:01.920,0:34:03.720<br>
中文跟英文的 word vector<br>
<br>
0:34:03.720,0:34:06.820<br>
它是完全沒有任何的關係的<br>
<br>
0:34:06.820,0:34:09.260<br>
它們的每一個 dimension<br>
<br>
0:34:09.260,0:34:11.540<br>
對應的含義並沒有任何關係，為什麼？<br>
<br>
0:34:11.540,0:34:14.080<br>
因為你要 train word vector 的時候<br>
<br>
0:34:14.080,0:34:17.240<br>
它憑藉的就是上下文之間的關係<br>
<br>
0:34:17.240,0:34:19.380<br>
所以，如果你今天的 corpus 裡面<br>
<br>
0:34:19.380,0:34:22.160<br>
沒有中文跟英文的句子混雜在一起<br>
<br>
0:34:22.160,0:34:25.120<br>
沒有中文跟英文的詞彙混雜在一起<br>
<br>
0:34:25.120,0:34:26.760<br>
那 machine 就沒有辦法判斷<br>
<br>
0:34:26.760,0:34:30.240<br>
中文的詞彙跟英文的詞彙他們之間的關係<br>
<br>
0:34:30.240,0:34:34.440<br>
但是，今天假如你已經事先知道說<br>
<br>
0:34:34.440,0:34:35.720<br>
某幾個詞彙<br>
<br>
0:34:35.720,0:34:38.920<br>
某幾個中文的詞彙和某幾個英文的詞彙<br>
<br>
0:34:38.920,0:34:40.340<br>
它們是對應在一起的<br>
<br>
0:34:40.340,0:34:42.500<br>
那你先得到一組中文的 vector <br>
<br>
0:34:42.500,0:34:43.880<br>
再得到一組英文的 vector<br>
<br>
0:34:43.880,0:34:46.280<br>
接下來，你可以再 learn 一個  model<br>
<br>
0:34:46.280,0:34:49.280<br>
它把中文和英文對應的詞彙<br>
<br>
0:34:49.280,0:34:52.220<br>
比如說，我們知道 "加大" 對應到 "enlarge"<br>
<br>
0:34:52.220,0:34:53.820<br>
"下跌" 對應到 "fall" <br>
<br>
0:34:53.820,0:34:56.580<br>
你把對應的詞彙，通過這個 projection 以後，<br>
<br>
0:34:56.580,0:34:59.220<br>
把它們 project 在 space上的同一個點<br>
<br>
0:34:59.220,0:35:01.680<br>
把它們 project 在 space 上面的同一個點<br>
<br>
0:35:01.680,0:35:06.660<br>
那在這個圖上，綠色的然後下面又有<br>
<br>
0:35:06.660,0:35:08.620<br>
這個綠色的英文的代表是<br>
<br>
0:35:08.620,0:35:13.000<br>
已經知道對應關係的中文和英文的詞彙<br>
<br>
0:35:13.000,0:35:16.820<br>
然後，如果你做這個 transform 以後<br>
<br>
0:35:16.820,0:35:19.940<br>
接下來有新的中文的詞彙和新的英文的詞彙<br>
<br>
0:35:19.940,0:35:22.260<br>
你都可以用同樣的 projection<br>
<br>
0:35:22.260,0:35:24.300<br>
把它們 project 到同一個 space 上面<br>
<br>
0:35:24.300,0:35:26.300<br>
比如說，你就可以自動知道說<br>
<br>
0:35:26.300,0:35:32.060<br>
中文的降低跟的英文的 reduce<br>
<br>
0:35:32.060,0:35:34.580<br>
它們都應該落在這個位置<br>
<br>
0:35:34.580,0:35:38.440<br>
都應該落在差不多的位置等等這樣<br>
<br>
0:35:38.440,0:35:39.940<br>
你就可以自動做到<br>
<br>
0:35:39.940,0:35:42.980<br>
比如說，類似翻譯這個樣子的效果<br>
<br>
0:35:42.980,0:35:49.280<br>
那這個 embedding不只限於文字<br>
<br>
0:35:49.280,0:35:52.420<br>
你也可以對影像做 embedding<br>
<br>
0:35:52.420,0:35:54.180<br>
這邊有一個很好的例子<br>
<br>
0:35:54.180,0:35:55.360<br>
這個例子是這樣做的<br>
<br>
0:35:55.360,0:35:58.120<br>
它說，我們先已經找到一組 word vector<br>
<br>
0:35:58.120,0:36:00.260<br>
比如說，dog 的 vector、horse 的 vector<br>
<br>
0:36:00.260,0:36:02.240<br>
auto 的 vector 和 cat 的 vector<br>
<br>
0:36:02.240,0:36:04.420<br>
它們分佈在空間上是這樣子的位置<br>
<br>
0:36:04.420,0:36:07.040<br>
接下來，你 learn 一個 model <br>
<br>
0:36:07.040,0:36:08.500<br>
它是 input 一張 image<br>
<br>
0:36:08.500,0:36:10.660<br>
output 是跟一個跟 word vector<br>
<br>
0:36:10.660,0:36:12.420<br>
一樣 dimension 的 vector<br>
<br>
0:36:12.420,0:36:14.100<br>
那你會希望說<br>
<br>
0:36:14.100,0:36:16.500<br>
狗的 vector 就散佈在狗的周圍<br>
<br>
0:36:16.500,0:36:18.660<br>
馬的 vector 就散佈在馬的周圍<br>
<br>
0:36:18.660,0:36:22.300<br>
車輛的 vector 就散佈在 auto 的周圍<br>
<br>
0:36:22.300,0:36:24.380<br>
那假設有一些 image <br>
<br>
0:36:24.380,0:36:27.300<br>
你已經知道他們是屬於哪一類<br>
<br>
0:36:27.300,0:36:30.240<br>
你已經知道說這個是狗、這個是馬、這個是車<br>
<br>
0:36:30.240,0:36:32.620<br>
你可以把它們 project 到<br>
<br>
0:36:32.620,0:36:35.120<br>
它們所對應到的 word vector 附近<br>
<br>
0:36:35.120,0:36:36.980<br>
那這個東西有什麼用呢？<br>
<br>
0:36:36.980,0:36:40.780<br>
假如你今天有一個新的 image 進來<br>
<br>
0:36:40.780,0:36:42.940<br>
比如說，這個東西，它是個貓<br>
<br>
0:36:42.940,0:36:44.060<br>
但是你不知道它是貓<br>
<br>
0:36:44.060,0:36:45.080<br>
機器不知道它是貓<br>
<br>
0:36:45.080,0:36:46.960<br>
但是你通過它們的 projection <br>
<br>
0:36:46.960,0:36:49.820<br>
把它 project 到這個 space 上以後<br>
<br>
0:36:49.820,0:36:53.500<br>
神奇的是你就會發現它可能就在貓的附近<br>
<br>
0:36:53.500,0:36:55.340<br>
那你的 machine 就會自動知道說<br>
<br>
0:36:55.340,0:36:57.520<br>
這個東西叫做貓<br>
<br>
0:36:57.520,0:37:00.180<br>
當我們一般在做影像分類的時候<br>
<br>
0:37:00.180,0:37:03.380<br>
大家都已經有做過作業三<br>
<br>
0:37:03.380,0:37:05.840<br>
作業三就是影像分類的問題<br>
<br>
0:37:05.840,0:37:07.860<br>
在做影像分類的問題的時候<br>
<br>
0:37:07.860,0:37:10.240<br>
你的 machine 其實很難去處理<br>
<br>
0:37:10.240,0:37:13.580<br>
新增加的，它沒有辦法看過的 object<br>
<br>
0:37:13.580,0:37:15.140<br>
舉例來說，作業 3 裡面<br>
<br>
0:37:15.140,0:37:17.480<br>
我們就先已經訂好 10 個 class<br>
<br>
0:37:17.480,0:37:19.160<br>
你 learn 出來的 model<br>
<br>
0:37:19.160,0:37:22.160<br>
就是只能分這 10 個 class<br>
<br>
0:37:22.160,0:37:23.900<br>
如果今天有一個新的東西<br>
<br>
0:37:23.900,0:37:25.220<br>
不在這10個 class 裡面<br>
<br>
0:37:25.220,0:37:27.420<br>
你的 model 是完全是無能為力 的<br>
<br>
0:37:27.420,0:37:28.660<br>
它根本不知道它叫做什麼<br>
<br>
0:37:28.660,0:37:30.960<br>
但是，如果你用這個方法的話<br>
<br>
0:37:30.960,0:37:32.500<br>
就算有一張 image<br>
<br>
0:37:32.500,0:37:34.660<br>
是你在 training 的時候，你沒有看過的 class<br>
<br>
0:37:34.660,0:37:36.080<br>
比如說，貓這個 image<br>
<br>
0:37:36.080,0:37:38.380<br>
它從來都沒有看過<br>
<br>
0:37:38.380,0:37:40.780<br>
但是如果貓的這個 image<br>
<br>
0:37:40.780,0:37:42.920<br>
可以 project 到 cat 的 vector 附近的話<br>
<br>
0:37:42.920,0:37:46.680<br>
你就會知道說，這一張 image 叫做 cat<br>
<br>
0:37:46.680,0:37:49.100<br>
如果你可以做到這一件事，就好像是<br>
<br>
0:37:49.100,0:37:51.920<br>
 machine 先閱讀了大量的文章以後<br>
<br>
0:37:51.920,0:37:53.880<br>
它知道說，每一個詞彙<br>
<br>
0:37:53.880,0:37:55.680<br>
指的是什麼意思<br>
<br>
0:37:55.680,0:37:59.620<br>
它知道說，狗啊，貓啊，馬啊<br>
<br>
0:37:59.620,0:38:01.760<br>
它們之間有什麼樣的關係<br>
<br>
0:38:01.760,0:38:06.280<br>
它透過閱讀大量的文章，先了解詞彙間的關係<br>
<br>
0:38:06.280,0:38:07.920<br>
接下來，在看 image 的時候<br>
<br>
0:38:07.920,0:38:10.280<br>
它就可以根據它已經閱讀得到的知識<br>
<br>
0:38:10.280,0:38:12.120<br>
去 mapping 每一個 image<br>
<br>
0:38:12.120,0:38:13.800<br>
所應該對應的東西<br>
<br>
0:38:13.800,0:38:16.780<br>
這樣就算是它看到它沒有看過的東西<br>
<br>
0:38:16.780,0:38:19.360<br>
它也可能可以把它的名字叫出來<br>
<br>
0:38:20.360,0:38:22.440<br>
那剛才講的呢<br>
<br>
0:38:22.440,0:38:24.400<br>
都是 word embedding <br>
<br>
0:38:24.400,0:38:26.920<br>
也可以做 document 的 embedding<br>
<br>
0:38:26.920,0:38:29.240<br>
不只是把一個 word 變成一個 vector<br>
<br>
0:38:29.240,0:38:33.980<br>
也可以把一個 document  變成一個 vector<br>
<br>
0:38:33.980,0:38:38.420<br>
那怎麼把一個 document 變成一個 vector 呢<br>
<br>
0:38:38.420,0:38:41.320<br>
最簡單的方法，我們之前已經講過了<br>
<br>
0:38:41.320,0:38:44.960<br>
就是把一個 document 變成一個 word<br>
<br>
0:38:44.960,0:38:46.620<br>
然後，用 Auto-encoder<br>
<br>
0:38:46.620,0:38:48.000<br>
你就可以 learn 出<br>
<br>
0:38:48.000,0:38:50.420<br>
這個 document 的 Semantic Embedding<br>
<br>
0:38:50.420,0:38:53.820<br>
但光這麼做是不夠的<br>
<br>
0:38:53.820,0:38:57.420<br>
我們光用這個 word 來描述一篇 document<br>
<br>
0:38:57.420,0:38:58.880<br>
是不夠的，為什麼呢？<br>
<br>
0:38:58.900,0:39:01.240<br>
因為我們知道說，詞彙的順序<br>
<br>
0:39:01.240,0:39:03.280<br>
代表了很重要的含<br>
<br>
0:39:03.280,0:39:04.780<br>
舉例來說<br>
<br>
0:39:04.780,0:39:07.260<br>
這一邊有兩個詞彙，有兩個句子<br>
<br>
0:39:07.260,0:39:11.900<br>
一個是： white blood cells destroying an infection<br>
<br>
0:39:11.900,0:39:14.840<br>
另外一個是：an infection destroying white blood cells<br>
<br>
0:39:14.880,0:39:18.040<br>
這兩句話，如果你看它的 bag-of-word 的話<br>
<br>
0:39:18.040,0:39:21.200<br>
它們的 bag-of-word 是一模一樣的<br>
<br>
0:39:21.200,0:39:23.860<br>
因為它們都有出現有這 6 個詞彙<br>
<br>
0:39:23.860,0:39:25.480<br>
它們都有出現這 6 個詞彙<br>
<br>
0:39:25.480,0:39:26.980<br>
只是順序是不一樣的<br>
<br>
0:39:26.980,0:39:29.080<br>
但是因為它們的順序是不一樣的<br>
<br>
0:39:29.080,0:39:30.300<br>
所以上面這一句話 <br>
<br>
0:39:30.300,0:39:33.300<br>
白血球消滅了傳染病，這個是 positive<br>
<br>
0:39:33.300,0:39:36.480<br>
下面這句話，它是 negative<br>
<br>
0:39:36.480,0:39:38.480<br>
雖然說，它們有同樣的 bag-of-word<br>
<br>
0:39:38.480,0:39:41.160<br>
它們在語意上，完全是不一樣的<br>
<br>
0:39:41.160,0:39:44.540<br>
所以，光只是用 bag-of-word<br>
<br>
0:39:44.540,0:39:48.800<br>
來描述一張 image 是非常不夠的<br>
<br>
0:39:48.800,0:39:53.280<br>
用 bag-of-word 來描述 一篇 document 是非常不足的<br>
<br>
0:39:53.280,0:39:57.580<br>
你用 bag-of-word 會失去很多重要的 information<br>
<br>
0:39:57.580,0:39:59.640<br>
那怎麼做呢？<br>
<br>
0:39:59.640,0:40:00.980<br>
我們這一邊就不細講<br>
<br>
0:40:00.980,0:40:04.520<br>
這邊就列了一大堆的 reference  給大家參考<br>
<br>
0:40:04.520,0:40:07.100<br>
上面這 3 個方法，它是 unsupervised<br>
<br>
0:40:07.100,0:40:09.120<br>
也就是說你只要 collect<br>
<br>
0:40:09.120,0:40:11.840<br>
一大堆的 document<br>
<br>
0:40:11.840,0:40:13.460<br>
你就可以讓它自己去學<br>
<br>
0:40:13.460,0:40:17.320<br>
那下面這幾個方法算是 supervised<br>
<br>
0:40:17.320,0:40:19.580<br>
因為，在這一些方法裡面<br>
<br>
0:40:19.580,0:40:21.720<br>
你需要對每一個 document<br>
<br>
0:40:21.720,0:40:22.960<br>
進行額外的 label<br>
<br>
0:40:22.960,0:40:26.320<br>
你不用 label  說，每一個 document 對應的 vector是什麼<br>
<br>
0:40:26.320,0:40:28.440<br>
但是你要給它其他的 label<br>
<br>
0:40:28.440,0:40:29.900<br>
才能夠 learn 這一些 vector<br>
<br>
0:40:29.900,0:40:32.580<br>
所以下面，不算是完全 unsupervised<br>
<br>
0:40:32.580,0:40:34.960<br>
我把 reference 列在這邊，給大家參考<br>
<br>
0:40:34.960,0:40:38.540<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
