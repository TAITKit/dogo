0:00:00.000,0:00:02.180
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.180,0:00:04.760
接下來，我們要講的是 Neighbor Embedding

0:00:04.760,0:00:08.680
我們在這個作業，最後一個作業的投影片裡面啊

0:00:08.680,0:00:10.420
助教有提到一個東西呢

0:00:10.420,0:00:12.660
叫做 t-SNE

0:00:12.660,0:00:17.780
t-SNE 我相信你在很多不同的場合也常常聽到

0:00:17.780,0:00:19.780
但是如果有人問你說，

0:00:19.780,0:00:21.780
t-SNE 是什麼的時候

0:00:21.780,0:00:23.660
大家往往都答不出來

0:00:23.680,0:00:26.920
現在我們要來講一下 t-SNE 是什麼

0:00:26.920,0:00:32.980
那 t-SNE 的 “ＮＥ” 
其實就是 Neighbor Embedding 的縮寫

0:00:32.980,0:00:35.320
那我們現在要做的事情呢

0:00:35.320,0:00:37.960
其實就是我們之前講過的降維

0:00:37.960,0:00:43.960
只是我們要做的事情是「非線性」的降維

0:00:43.960,0:00:45.960
我們想要做到的事情是說

0:00:45.960,0:00:47.520
我們知道說 Data Point

0:00:47.520,0:00:51.520
它可能是在高維空間裡面的一個 Manifold

0:00:51.520,0:00:56.460
也就是說， Data Point 的分佈
它其實是分佈在一個低維的空間裡面

0:00:56.460,0:01:00.640
只是被扭曲了塞到一個高維空間裡面

0:01:00.640,0:01:03.940
那，講到 Manifold 的時候

0:01:03.940,0:01:08.560
常常舉的例子就是這個「地球」這樣子

0:01:08.560,0:01:11.920
地球的表面就是一個 Manifold

0:01:11.920,0:01:13.920
他是一個二維的平面

0:01:13.920,0:01:17.900
但是被塞到了一個三維的空間裡面

0:01:17.900,0:01:20.660
那，在一個這樣子的情況下呢

0:01:20.660,0:01:22.660
在一個 Manifold 裡面呢

0:01:22.660,0:01:26.040
只有，這個，很近的距離的點

0:01:26.040,0:01:28.040
在很近的距離的情況下

0:01:28.040,0:01:31.580
Euclidean distance 才會成立

0:01:31.580,0:01:34.940
如果今天距離很遠的時候

0:01:34.940,0:01:39.180
這個，歐式幾何他就不一定成立

0:01:39.180,0:01:43.100
也就是說，假如我們在這個 S 形的空間裡面

0:01:43.100,0:01:46.160
取某一個點放在這邊

0:01:46.160,0:01:49.940
那我們比較，我們用 Euclidean distance 比較

0:01:49.940,0:01:50.820
這個點

0:01:50.820,0:01:52.820
跟這個點之間的距離

0:01:52.820,0:01:55.780
比較他跟他之間的距離
和他跟他之間的距離

0:01:55.780,0:01:57.780
或許這件事情是 make sense 的

0:01:57.780,0:02:00.940
他跟他比較不像
他跟他比較像

0:02:00.940,0:02:04.460
但是，如果今天是距離比較遠的時候

0:02:04.460,0:02:06.460
你要比較，這個點

0:02:06.460,0:02:08.460
跟這個點的相似程度

0:02:08.460,0:02:10.460
跟這個點跟這個點的相似的程度

0:02:10.460,0:02:11.580
你在高維的空間中

0:02:11.580,0:02:14.060
直接算他們的 Euclidean distance

0:02:14.060,0:02:16.060
就變得不 make sense 了

0:02:16.060,0:02:19.640
因為，如果根據 Euclidean distance

0:02:19.640,0:02:21.640
他跟他比較近，他跟他比較遠

0:02:21.640,0:02:25.920
但是，實際上呢

0:02:25.920,0:02:28.520
很有可能，他跟他是比較近的

0:02:28.520,0:02:30.520
他跟他是比較像的

0:02:30.520,0:02:32.520
他跟他是比較不像

0:02:32.520,0:02:35.380
所以， Manifold Learning 要做的事情

0:02:35.380,0:02:40.180
是把 S 形的這塊東西展開

0:02:40.180,0:02:44.180
把塞在高維空間裡面的低維空間「攤平」

0:02:44.180,0:02:46.180
攤平的好處就是

0:02:46.180,0:02:49.720
現在，如果我們把這個低維空間攤平以後

0:02:49.720,0:02:50.660
我們做降維

0:02:50.660,0:02:55.840
然後，把這個塞在高維空間裡面的 Manifold 攤平以後

0:02:55.840,0:02:58.820
那我們就可以在這個 Manifold 上面

0:02:58.820,0:03:00.820
用 Euclidean distance來

0:03:00.820,0:03:02.820
我們就可以在這個平面上用 Euclidean distance

0:03:02.820,0:03:06.140
在降維以後，我們就可以用 Euclidean distance

0:03:06.140,0:03:08.140
來計算點和點之間的距離

0:03:08.140,0:03:11.320
這會對接下來如果你要做 clustering

0:03:11.320,0:03:14.640
或者是，你要做接下來的 supervised learning

0:03:14.640,0:03:16.640
都是會有幫助的

0:03:16.640,0:03:18.640
那類似方法有很多

0:03:18.640,0:03:20.640
那我們今天就很快的介紹幾種方法

0:03:20.640,0:03:24.020
在最後，講一下t-SNE

0:03:24.920,0:03:28.400
那有個方法叫做 Locally Linear Embedding

0:03:28.400,0:03:30.000
這個方法意思是說

0:03:30.080,0:03:33.500
在原來的空間裡面
你的點的分佈是長這個樣子

0:03:33.500,0:03:36.060
那有某一個點，叫做 xi

0:03:36.060,0:03:40.060
然後我們先選出這個 xi 的 neighbor

0:03:40.060,0:03:42.060
然後我們叫做 xj

0:03:42.060,0:03:47.540
那接下來呢，我們要找 xi 跟 xj的關係

0:03:47.540,0:03:52.240
那 xi 跟 xj 的關係，我們寫做 “wij”

0:03:52.240,0:03:56.220
wij 代表 xi 和 xj 的關係

0:03:56.220,0:03:58.840
這個 wij 是怎麼找出來的呢？

0:03:58.840,0:04:00.840
wij 是這個樣子

0:04:00.840,0:04:02.480
我們假設說

0:04:02.480,0:04:08.460
每一個 xi，都可以用它的 neighbor

0:04:08.460,0:04:10.460
做 linear combination 以後

0:04:10.460,0:04:12.460
組合而成

0:04:12.460,0:04:14.460
這個 wij 就是

0:04:14.460,0:04:20.100
拿 j 去組合 i 的時候

0:04:20.100,0:04:27.960
wij 就是拿 xj 去組合 xi 的時候的 
linear combination 的 weight

0:04:27.960,0:04:30.140
那要找這組 wij 怎麼做呢

0:04:30.140,0:04:34.580
也就是說，我們現在要找一組 wij

0:04:34.580,0:04:41.540
這組 wij 對 xi 的所有 neighbor xj 
做 weighted sum的時候

0:04:41.540,0:04:44.820
他可以跟 xi 越接近越好

0:04:44.820,0:04:46.640
或者是 xi 減掉

0:04:46.640,0:04:49.820
summation over 所有的 j
wij 乘以 xj

0:04:49.820,0:04:53.000
他的 two norm 是越接近越好的

0:04:53.000,0:04:57.380
xi 跟 xj 的 linear combination 他們是越接近越好

0:04:57.380,0:05:02.880
然後 summation over 所有的 data point i

0:05:02.880,0:05:07.020
然後，接下來我們就要做 dimension reduction

0:05:07.020,0:05:12.120
把原來所有的 xi 跟 xj 轉成 zi 和 zj

0:05:12.120,0:05:14.940
但是，現在這邊的原則就是

0:05:14.940,0:05:18.760
從 xi 跟 xj 轉成 zi 跟 zj

0:05:18.760,0:05:22.900
他們中間的關係 "wij"，是不變的

0:05:22.900,0:05:24.900
這個東西，就是那個

0:05:24.900,0:05:28.980
白居易的長恨歌裡面講的這句話這樣子

0:05:28.980,0:05:30.660
我想那句話叫什麼

0:05:30.660,0:05:33.660
這句話是出自長恨歌的結尾，它是

0:05:35.100,0:05:40.620
臨別殷勤重寄詞，詞中有誓兩心知。
七月七日長生殿，夜半無人私語時。

0:05:40.620,0:05:46.140
在天願作比翼鳥，在地願為連理枝。
天長地久有時盡，此恨綿綿無絕期。

0:05:47.820,0:05:51.960
並不是我國文特好，謝謝～

0:05:53.240,0:05:55.760
你不要誤會可能是我國文特別強，我其實沒有很強

0:05:55.760,0:05:58.260
為什麼我會背，其實我可以從頭背到尾，為什麼呢？

0:05:59.100,0:06:02.320
因為小時候我不知道犯了什麼錯，被老師懲罰背長恨歌

0:06:02.520,0:06:04.980
然後我就整首背起來了

0:06:05.460,0:06:07.580
你小時候背的東西，到長大其實不會忘

0:06:10.100,0:06:11.740
這個是什麼意思呢？所謂的

0:06:12.300,0:06:17.300
在天就是 xi 跟 xj，在原來的 space 上面

0:06:17.820,0:06:20.380
比翼鳥就是 wij

0:06:20.920,0:06:24.460
在地就是把 xi 跟 xj transform 到另外一個 space

0:06:24.460,0:06:26.460
就是 zi 跟 zj

0:06:26.460,0:06:29.420
連理枝就是比翼鳥就等於連理枝

0:06:29.420,0:06:32.140
所以他們關係還是 Wij

0:06:33.500,0:06:36.680
所以 LLE 它做的事情是這樣的

0:06:38.360,0:06:42.120
首先 wij 在原來的 space 上面找完以後

0:06:42.120,0:06:44.120
就 fix 住它，不要去動它

0:06:44.740,0:06:48.200
接下來，你為每一個 xi 跟 xj

0:06:48.600,0:06:50.600
找另外一個 vector

0:06:50.600,0:06:52.600
我們現在要做 dimension reduction

0:06:52.600,0:06:55.540
所以你新找的 vector 要比原本的 dimension 還要小

0:06:55.540,0:06:58.780
原本 100 維，降維後要比較小，10 維、2維之類的

0:07:00.020,0:07:02.680
zi 跟 zj 是另外的 vector

0:07:03.500,0:07:04.900
那我們要找這個

0:07:05.880,0:07:07.880
zi 跟 zj 它可以 minimize 什麼呢

0:07:08.360,0:07:10.360
它可以 minimize 下面這個 function

0:07:10.960,0:07:14.680
也就是說原來這個 xi 
它可以做 linear combination 產生 x

0:07:15.380,0:07:18.140
原來這些 xj 可以做 linear combination 產生 xi

0:07:18.880,0:07:24.480
這些 zj 也可以用同樣的 linear combination產生 zi

0:07:25.580,0:07:28.420
這些 zj 也可以用同樣的 linear combination 產生 zi

0:07:28.420,0:07:31.500
我們就是要找這組 z 可以滿足 wij 給我們的 constraint

0:07:31.980,0:07:34.080
所以，現在在這個式子裡面

0:07:34.300,0:07:36.060
wij 變成是已知的

0:07:36.740,0:07:40.520
但是我們要找一組 z，讓 zj 透過 wij

0:07:41.000,0:07:44.080
做 weighted sum 以後，它可以跟 zi 越接近越好

0:07:44.180,0:07:48.360
zj 用這組 weight 做 linear combination 後，它可以跟 zi 越接近越好

0:07:48.360,0:07:51.180
然後 summation over 所有的 datapoint

0:07:52.440,0:07:56.500
那這個 LLE 你要注意一下，其實它並沒有一個

0:07:57.340,0:07:59.160
明確的 function

0:07:59.840,0:08:02.700
告訴你說我們怎麼做 dimension reduction

0:08:02.700,0:08:04.700
不像我們在做 Auto-encoder 的時候

0:08:05.540,0:08:06.940
你 learn 出一個 encoder 的 network

0:08:06.940,0:08:11.000
input 一個新的 datapoint，然後可以找到它 dimension 的結果

0:08:11.580,0:08:12.860
今天在 LLE 裡面

0:08:13.060,0:08:17.180
你並不會找到一個很明確的 function，沒有一個 function 告訴我們說

0:08:17.560,0:08:19.560
怎麼從 x 變到 z

0:08:20.260,0:08:22.520
z 就是完全憑空找出來的

0:08:24.860,0:08:28.000
其實如果你用 LLE 這種方法

0:08:28.000,0:08:30.500
如果用 LLE 或其他類似的方法會有一個好處

0:08:30.500,0:08:33.380
就算是你原來這個 xi、xj 你不知道

0:08:33.380,0:08:35.380
你只知道 wij

0:08:35.380,0:08:37.660
你不知道 xi、xj 要用什麼 vector 來描述它

0:08:37.660,0:08:39.660
你只知道他們的關係

0:08:39.660,0:08:42.520
你其實也可以用 LLE 這種方法

0:08:42.520,0:08:47.940
那 LLE 呢，你其實需要好好的調一下你的 neighbor 選幾個

0:08:47.940,0:08:53.240
你 neighbor 選的數目要剛剛好才會得到好的結果

0:08:53.240,0:08:57.360
那這邊，是從原始的 paper 裡面截出來的圖

0:08:57.360,0:08:59.620
原始的 paper 它的題目實在是太潮了

0:08:59.620,0:09:04.140
他是 “Think Globally, Fit Locally” 這樣子

0:09:04.140,0:09:06.560
太精彩了，很好的一個題目

0:09:06.560,0:09:10.180
然後，它調了不同的 K

0:09:10.180,0:09:12.720
如果你今天 K 太小

0:09:12.720,0:09:14.260
你得出來的結果會不太好

0:09:14.260,0:09:16.260
K 太大，得到的結果也不太好

0:09:16.260,0:09:18.260
為什麼 K 太大得到的結果不太好呢？

0:09:18.260,0:09:20.260
因為我們之前的假設

0:09:20.260,0:09:24.260
就是每一個這個 Euclidean distance

0:09:24.260,0:09:28.280
只在很近的距離之內，可以被這樣想

0:09:28.280,0:09:31.800
所以，這個點和點之間的

0:09:31.800,0:09:34.140
data point 和 data point 之間的關係

0:09:34.140,0:09:36.980
在 transform前後可以被 keep 住

0:09:36.980,0:09:38.980
只有在距離很近的時候

0:09:38.980,0:09:40.400
才能夠成立

0:09:40.400,0:09:42.080
當你 K 選很大的時候

0:09:42.080,0:09:43.720
你會考慮一些距離很遠的點

0:09:43.720,0:09:45.720
你會考慮一些 transform 以後

0:09:45.720,0:09:47.720
relation 沒有辦法 keep 住的點

0:09:47.720,0:09:50.600
有一些關係太弱了，transform 以後沒有辦法 keep 住

0:09:50.600,0:09:53.420
那它不是這個比翼鳥或連理枝的關係

0:09:53.420,0:09:54.740
它太弱了，無法 keep 住

0:09:54.740,0:09:56.040
但是你不應該把它考慮進來

0:09:56.040,0:09:58.560
所以你的 K，要選一個適當的值

0:10:00.060,0:10:03.280
那另外一個方法叫 Laplacian Eigenmap

0:10:03.280,0:10:04.940
它的想法是這樣子

0:10:04.940,0:10:06.940
我們之前有說

0:10:06.940,0:10:10.940
我們之前在講這個 Semi-supervised learning 的時候

0:10:10.940,0:10:13.720
我們有講過 smoothness assumption

0:10:13.720,0:10:17.160
我們有說，如果你想要比較這個點跟這個點之間的距離

0:10:17.160,0:10:20.840
只算它的 Euclidean distance 是不足夠的

0:10:20.840,0:10:26.660
你要看的是它們在這個 high density 的 region 
之間的 distance

0:10:26.660,0:10:27.800
如果兩個點之間

0:10:27.800,0:10:30.700
它們有 high density 的 connection

0:10:30.700,0:10:33.960
那它們才是真正的接近

0:10:34.980,0:10:35.560
這件事情

0:10:35.560,0:10:39.960
你可以用一個 graph 來描述這件事情

0:10:39.960,0:10:41.540
也就是說你把你的 data point

0:10:41.540,0:10:43.000
construct 成一個 graph

0:10:43.000,0:10:45.380
你算你的 data point 兩兩之間的相似度

0:10:45.380,0:10:47.600
如果相似度超過一個 thereshold

0:10:47.600,0:10:49.700
就把它們 connect 起來

0:10:49.700,0:10:50.960
而建一個 graph 的方法有很多

0:10:50.960,0:10:53.080
有很多不同的方法

0:10:53.080,0:10:54.560
總之，你就把比較近的點

0:10:54.560,0:10:56.560
把它們連起來變成一個 graph

0:10:57.720,0:11:00.740
那你把點變成 graph 以後

0:11:00.740,0:11:06.020
你考慮這個 smoothness 的距離

0:11:06.020,0:11:11.300
就可以被這個 graph 上面的 connection 來 approximate

0:11:11.300,0:11:14.760
那我們之前在講 Semi-supervised learning 的時候

0:11:14.760,0:11:15.840
我們是這樣說的

0:11:15.840,0:11:20.940
如果今天 x1 跟  x2 在 high density 的 region 上面

0:11:20.940,0:11:22.940
它們是相近的

0:11:22.940,0:11:24.940
那它們的 label

0:11:24.940,0:11:28.340
y1\head 和 y2\head，很有可能是一樣的

0:11:28.340,0:11:31.380
雖然說 Semi-supervised learning 的時候

0:11:31.380,0:11:32.360
我們可以這麼做

0:11:32.360,0:11:38.180
我們有一項是考慮有 label 的 data 的項

0:11:38.180,0:11:41.960
有另外一項是跟 labeled data 沒有關係的

0:11:41.960,0:11:45.420
我們只可以利用這個 unlabeled 的 data

0:11:45.420,0:11:47.020
那這一項的作用

0:11:47.020,0:11:51.140
它是要考慮說我們現在得到的 label 是不是 smooth 的

0:11:51.140,0:11:53.960
它的作用很像一個 regularization 的 term

0:11:53.960,0:11:55.960
那這個 S 這一項啊

0:11:55.960,0:11:57.960
這個 S 這一項啊

0:11:57.960,0:12:02.820
它等於 yi 減 yj 的這個

0:12:02.820,0:12:06.580
它跟 yi 跟 yj 的這個距離

0:12:06.580,0:12:08.200
然後乘上 wi, j

0:12:08.200,0:12:10.840
也就是說，那個 wi, j 是什麼呢

0:12:10.840,0:12:17.300
wi, j 是說，如果今天兩個 data point xi 跟 xj

0:12:17.300,0:12:18.720
他們在圖上是相連的

0:12:18.720,0:12:21.240
那 wi, j 就是他們的相似成度

0:12:21.240,0:12:23.240
如果在圖上是沒有相連的

0:12:23.240,0:12:24.140
他就是 0

0:12:24.140,0:12:25.470
然後我們說

0:12:25.470,0:12:29.980
如果今天wi 跟 wj 他們很相近的話

0:12:29.980,0:12:31.980
那 wi, j 就有一個很大的值

0:12:31.980,0:12:34.480
就是希望yi 跟 yj 越接近越好

0:12:34.480,0:12:36.160
反之如果 wi, j 是 0

0:12:36.160,0:12:39.820
那 yi 跟 yj 要是什麼值都可以

0:12:39.820,0:12:40.860
那這個 S

0:12:40.860,0:12:45.580
就是evaluate 說，你現在得到的 label 有多麽的 smooth

0:12:45.580,0:12:46.400
那麼還說

0:12:46.460,0:12:51.880
這個 S 可以寫成一個 y 的 vector 乘上 L 再乘上 y

0:12:51.880,0:12:54.520
這個 L 就是 graph 的 laplacian

0:12:54.520,0:12:56.240
L 等於 E 減 W

0:12:56.240,0:12:58.700
這個大家回去 check 一下之前的投影片就知道了

0:12:58.700,0:13:03.540
那同樣的道理可以被 apply 在完全
unsupervised 的 test 上面

0:13:03.540,0:13:04.300
我們可以說

0:13:04.300,0:13:08.420
如果 x1 跟 x2 在 high density 的 region 他們是 close 的

0:13:08.420,0:13:13.640
那我們就會希望， z1 跟 z2 他們也是相近的

0:13:13.640,0:13:17.920
所以我們可以一樣把剛才那個
smoothness 的式子寫出來

0:13:17.920,0:13:18.980
我們可以寫說

0:13:18.980,0:13:21.920
我這邊寫平方可能是比較不好啦

0:13:21.920,0:13:25.740
我這邊應該寫  Euclidean distance 比較好

0:13:25.740,0:13:28.340
或者寫他的這個 two norm distance 比較好

0:13:28.340,0:13:30.040
因為我只是從前面投影片 copy 過來

0:13:30.040,0:13:32.040
只是把 y 改成 z 而已

0:13:32.040,0:13:33.340
就忘了把它改過來

0:13:33.340,0:13:41.320
好沒關係，那今天這個把 x1 跟 x2 變成 z1 跟 z2 以後

0:13:41.320,0:13:46.580
你的這個 z1 跟 z2 應該是長什麼樣子呢

0:13:46.580,0:13:47.960
你今天應該是這樣子

0:13:47.960,0:13:49.960
如果今天 xi 跟 xj

0:13:49.960,0:13:53.440
如果這個 i 跟 j 的這兩個 datapoint 他們之間的 wi, j 很像

0:13:53.440,0:13:57.560
那 zi 跟 zj 做完 dimension reduction 以後他們的距離就很近

0:13:57.560,0:14:00.980
反之呢，如果他們的 wi, j 很小

0:14:00.980,0:14:03.600
那他們的距離要怎樣就都可以

0:14:03.600,0:14:05.920
你覺得，這樣 ok 嗎？

0:14:05.920,0:14:14.960
我們就找一個 zi 跟 zj minimize 這個 S 的值

0:14:14.960,0:14:18.000
你不覺得這麼做是有問題的嗎

0:14:19.140,0:14:22.100
給大家五秒鐘想想看，這個問題出在哪裡

0:14:24.720,0:14:26.640
其實，這個問題是這樣子

0:14:26.640,0:14:29.620
你不需要告訴我 wi, j 是什麼

0:14:29.620,0:14:31.260
我一秒就可以告訴你說

0:14:31.260,0:14:34.000
要 minimize x 的時候，我應該選什麼值

0:14:34.000,0:14:36.000
心算就可以得到了

0:14:36.000,0:14:36.880
選什麼值呢？

0:14:36.880,0:14:39.680
我把所有的 zi 跟 zj 通通設一樣的值就好了

0:14:39.680,0:14:40.520
結束

0:14:40.520,0:14:44.700
把所有 zi 跟 zj 我通通都設一樣的值，比如說通通都設0

0:14:44.700,0:14:46.980
那 S 就等於 0

0:14:46.980,0:14:49.980
這個問題就結束了

0:14:49.980,0:14:53.640
所以，光是有這個式子是不夠的

0:14:53.640,0:14:56.940
那你可能說剛剛在 Semi-supervised learning 的時候
你怎麼不講這句話呢

0:14:56.940,0:14:58.840
之前在 Semi-supervised learning 的時候

0:14:58.840,0:15:03.380
我們還有 supervised learning，"supervise" 那個 labeled data給我們的那一項

0:15:03.380,0:15:09.000
所以，如果你把所有的 y，所有的 label 都設成一樣的

0:15:09.000,0:15:12.320
那你在 supervise 那一項，你得到的 lost 就會很大

0:15:12.320,0:15:17.640
那我們要同時 balance supervise 那一項
跟 semi-supervise 那一項

0:15:17.640,0:15:21.340
我們要同時 balance supervise 那一項的 cost 
跟 regularization 的 term

0:15:21.340,0:15:25.020
所以你不會選擇讓所有的 y 通通都是一樣的

0:15:25.020,0:15:27.120
但在這邊少了 supervise 的東西

0:15:27.120,0:15:30.320
所以變成選擇所有的 z 都是一樣

0:15:30.320,0:15:32.320
反而是一個最好的 solution

0:15:32.320,0:15:34.380
所以，這件事情是行不通的

0:15:34.380,0:15:36.380
怎麼辦呢？

0:15:36.380,0:15:39.520
你要給你的 z 一些 constraint

0:15:39.520,0:15:41.240
什麼樣的 constraint 呢？

0:15:41.240,0:15:43.240
會給的 constraint 是這樣

0:15:43.240,0:15:48.960
如果今天 z 他降維以後的空間是 M 維的空間

0:15:48.960,0:15:52.380
是 M 維的空間，比方說 M 是 2 之類的

0:15:52.380,0:15:54.380
那你不會希望說

0:15:54.380,0:16:00.940
你的 z 他還分佈在一個比 M 還要小的 dimension 裡面

0:16:00.940,0:16:02.620
因為我們現在要做的事情

0:16:02.620,0:16:06.920
是希望把高維空間中塞進去的低維空間展開

0:16:06.920,0:16:09.340
那我們不希望說我們展開以後

0:16:09.340,0:16:12.840
其實展開的結果他在一個更低維的空間裡面

0:16:12.840,0:16:16.020
所以，今天假如你的 z 的 dimension 是 M 的話

0:16:16.020,0:16:19.720
你會希望你找出來的那些點

0:16:19.720,0:16:21.720
假設現在總共有 N 個點

0:16:21.720,0:16:24.000
z1 到 zN

0:16:24.000,0:16:28.020
他們做 span 以後，會等於 RM

0:16:28.020,0:16:32.940
也就是說 z，他不是活在一個比 M 維更低維的空間裡面

0:16:32.940,0:16:36.960
他就會佔據整個 M 維的空間這樣子

0:16:36.960,0:16:39.660
其實，如果你要解這個式子的話

0:16:39.660,0:16:41.660
你解一解就會發現說

0:16:41.660,0:16:49.460
你解出來的這個 z 跟我們前面看到的那個
graph laplacian L 是有關係的

0:16:49.460,0:16:55.140
他其實就是 graph laplacian 的 eigenvector

0:16:55.140,0:17:00.580
對應到比較小的 eigenvalue 的那些 eigenvector

0:17:00.580,0:17:03.160
這個大家再自己 check 一下文件就好

0:17:03.160,0:17:05.860
所以他叫做 Laplacian Eigenmap

0:17:05.860,0:17:09.460
因為我們找的是那個 laplacian matrix 的 eigenvector

0:17:09.460,0:17:11.460
如果你今天先找出 z 以後

0:17:11.460,0:17:13.460
你再去做 cluster

0:17:13.460,0:17:16.220
你先找出 z，再用 K-means 做 clustering 的話

0:17:16.220,0:17:20.940
這一招有一個很潮的名字，叫做 spectral clustering

0:17:20.940,0:17:25.060
那接下來呢，我們就要講 t-SNE

0:17:25.060,0:17:31.320
那 t-SNE 他是 T-distributed Stochastic Neighbor Embedding 的縮寫

0:17:31.320,0:17:34.880
那 t-SNE 他解決什麼樣的問題呢

0:17:34.880,0:17:38.020
前面那些問題啊，有一個最大的問題就是

0:17:38.020,0:17:42.520
他只假設相近的點應該要是接近的

0:17:42.520,0:17:47.780
但他們沒有假設說，不相近的點沒有要接近

0:17:47.780,0:17:50.840
他沒有假設說，不相近的點要分開

0:17:50.840,0:17:54.780
所以比如說你用 LLE 在 MNIST 上

0:17:54.780,0:17:56.260
你會遇到這樣的情形

0:17:56.260,0:17:59.920
他確實會把不同的點呢

0:17:59.920,0:18:05.060
他確實會把，
這個不同的顏色代表不同的 digit 不同的 class，

0:18:05.060,0:18:10.000
他確實會把不同的 class都塞在一起

0:18:10.000,0:18:13.980
然後他確實會把同個 class 的點都聚集在一起

0:18:13.980,0:18:18.940
但他沒有防止說不同 class 的點不要疊成一團

0:18:18.940,0:18:20.940
如果他們都疊成一團的時候

0:18:20.940,0:18:22.120
沒有這個顏色

0:18:22.120,0:18:25.320
這些點還是擠在一起，你還是沒有辦法分開

0:18:25.320,0:18:27.320
這是另外一個例子

0:18:27.320,0:18:29.780
做在這個 COIL-20上面

0:18:29.780,0:18:33.960
COIL 是一個 image 的 corpus

0:18:33.960,0:18:36.920
他裡面的 image 就是某一個圖

0:18:36.920,0:18:39.160
比如說一個汽車、一個玩具車

0:18:39.160,0:18:45.480
然後把它轉一圈，然後拍很多張不同的照片這樣子

0:18:45.480,0:18:49.340
這邊同樣的顏色代表同一個 object

0:18:49.340,0:18:53.000
你會發現說，它找到一些圈圈

0:18:53.000,0:18:54.700
這個圈圈代表什麼意思呢？

0:18:54.700,0:19:00.480
這個圈圈代表，同一個 object 在做旋轉的時候

0:19:00.480,0:19:02.720
它每一張圖都很像

0:19:02.720,0:19:07.720
但是，旋轉的時候每一張圖都很像

0:19:07.720,0:19:11.120
但是你看一個東西的正面和側面，他卻是非常不一樣

0:19:11.120,0:19:15.620
所以，這個圈圈就顯示你把某一張圖做旋轉以後

0:19:15.620,0:19:16.480
你所得到

0:19:16.480,0:19:18.860
在做 dimension reduction 以後

0:19:18.860,0:19:20.960
你所得到的結果

0:19:20.960,0:19:22.020
但是一樣

0:19:22.020,0:19:26.260
你會發現說，不同的 object 其實他們是擠成一團的

0:19:26.260,0:19:27.640
沒有辦法分開

0:19:27.640,0:19:31.420
所以如果做 t-SNE的話，做 t-SNE 是怎樣呢？

0:19:31.440,0:19:33.440
做 t-SNE，我們一樣是要做降維

0:19:33.440,0:19:35.520
把原來的 data point x

0:19:35.520,0:19:38.900
變成比較 low dimension 的 vector z

0:19:38.900,0:19:41.840
那在原來的 x 這個 space 上面

0:19:41.840,0:19:47.040
我們會計算所有的點的 pair，xi 和 xj 之間的 similarity

0:19:47.040,0:19:51.240
這裡先寫成 S(xi, xj)

0:19:51.240,0:19:55.680
接下來，會做一個 normalization

0:19:55.680,0:19:59.040
我們會計算一個 P(xj | xi)

0:19:59.040,0:20:03.920
這個 P(xj | xi)，他是從 xi 跟 xj 的 similarity 來的

0:20:03.920,0:20:07.100
我們等下會說這個 similarity 要怎麼算

0:20:07.100,0:20:12.800
這個 P(xj | xi)，在分子的地方是 xi 跟 xj 的 similarity

0:20:12.800,0:20:14.280
然後分母的地方

0:20:14.280,0:20:21.760
就是 summation over 除了 xi 以外，所有其他的點

0:20:21.760,0:20:24.680
和 xi 之間所算出來的距離

0:20:24.680,0:20:25.660
所以你會發現說

0:20:25.660,0:20:30.000
xi 對其他所有的 data point

0:20:30.000,0:20:33.240
它所算出來的這個 P(xj | xi)

0:20:33.240,0:20:36.060
它的 summation 應該要是 1

0:20:36.060,0:20:42.980
另外假設我們今天已經找出了一個 low dimension 的 representation 就是 zi 跟 zj 的話

0:20:42.980,0:20:44.840
我們已經把 x 變成 z 的話

0:20:44.840,0:20:47.960
那我們也可以計算 zi 和 zj 之間的 similarity

0:20:47.960,0:20:50.680
這邊 similarity 我們寫成 S'

0:20:50.680,0:20:55.000
那一樣你可以計算一個 Q( zj | zi)

0:20:55.000,0:20:59.020
他的分子的地方就是 S'( zi, zj)

0:20:59.020,0:21:06.780
分母的地方就是 summation over zi
跟所有 database 裡面的 data point zk 之間的距離

0:21:06.780,0:21:13.260
那今天有做這個 normalization 其實感覺是必要的

0:21:13.260,0:21:15.880
因為如果你又做這個 normalization 的話

0:21:15.880,0:21:18.120
因為你不知道在高維空間中算出來的距離

0:21:18.120,0:21:24.840
S(xi, xj) 跟 S’(zi, zj)，它們的 scale 是不是一樣的

0:21:24.840,0:21:27.040
如果，你今天有做這個 normalization

0:21:27.040,0:21:31.340
那你最後就可以把他們都變成機率

0:21:31.340,0:21:34.940
那這個時候他們值都會介於 0 到 1 之間

0:21:34.940,0:21:37.240
他們的 scale 會是一樣的

0:21:37.240,0:21:39.240
那接下來我們要做的事情就是

0:21:39.240,0:21:44.100
我們現在還不知道 zi 跟 zj 他們的值到底是多少

0:21:44.100,0:21:46.100
這是我們要被找出來的

0:21:46.100,0:21:48.640
那麼希望找一組 zi 跟 zj

0:21:48.640,0:21:53.000
它可以讓這一個 distribution 跟這一個 distribution

0:21:53.000,0:21:54.800
越接近越好

0:21:54.800,0:21:57.060
我們要讓原來在這個

0:21:57.060,0:22:01.020
根據 similarity 在 S 這個原來的 space 算出來 distribution

0:22:01.020,0:22:04.860
跟在這個 dimension reduction 以後的 space 算出來的 distribution

0:22:04.860,0:22:06.860
越接近越好

0:22:06.860,0:22:09.800
怎麼衡量兩個 distribution 之間的相似度呢？

0:22:09.800,0:22:14.560
可以很直覺的衡量兩個 distribution 之間的相似度的方法

0:22:14.560,0:22:18.040
就是我們之前看到的 KL divergence

0:22:18.040,0:22:20.860
所以，我們今天要做的事情

0:22:20.860,0:22:22.860
就是找一組 z

0:22:22.860,0:22:24.920
它可以做到這個

0:22:24.920,0:22:31.720
xi 的這個 distribution，跟 xi 對其他 point 的 distribution

0:22:31.720,0:22:34.140
跟 zi 對其他 point 的 distribution

0:22:34.140,0:22:37.660
這兩個 distribution 之間的 KL divergence 越小越好

0:22:37.660,0:22:40.080
summation over 所有的 data point

0:22:40.080,0:22:45.140
然後你要使得這一項，它的值越小越好

0:22:45.140,0:22:48.960
然後，我把這個值就寫在這邊

0:22:48.960,0:22:52.700
這件事情要怎麼做呢？

0:22:52.700,0:22:53.500
要怎麼做

0:22:53.500,0:22:56.680
其實這件事情，實際上並沒有很困難

0:22:56.680,0:22:58.680
實際上你就是用 Gradient descent 做的

0:22:58.680,0:23:03.720
你想想看，假設你知道這個 similarity 的 matrix

0:23:03.720,0:23:06.100
他這個式子長什麼樣子，然後這個式子長什麼樣子

0:23:06.100,0:23:11.540
那你把這些式子代進去，你把這些式子代進去

0:23:11.540,0:23:17.600
然後，接下來你只要對 z 做微分，然後做 Gradient Descent

0:23:17.600,0:23:20.020
就搞定這個問題，就結束了

0:23:21.960,0:23:24.840
那今天有一個問題就是你在做 t-SNE 的時候

0:23:24.840,0:23:28.360
他會計算所有 data point 之間的 similarity

0:23:28.360,0:23:31.120
所以它的運算量有點大，所以 t-SNE 有點麻煩

0:23:31.120,0:23:36.220
如果 data point 比較多的時候，你這電腦會跑不動

0:23:36.220,0:23:39.320
一個常見的做法是你會先做降維

0:23:39.320,0:23:41.558
比如說，你原來的 dimension 很大

0:23:41.560,0:23:44.780
你不會直接從很高的 dimension 直接做 t-SNE

0:23:44.780,0:23:47.300
因為你這樣子計算 similarity 的時間太長

0:23:47.300,0:23:50.560
你通常會先做，用比較快的方法比如說 PCA

0:23:50.560,0:23:53.680
先用 PCA 做降維，比如說，先降到 50 維

0:23:53.680,0:23:57.160
然後再用 t-SNE 從 50 維降到 2 維

0:23:57.160,0:23:59.060
這個是比較常見的做法

0:23:59.060,0:24:01.260
那其實像我們今天講的這些方法啊

0:24:01.260,0:24:02.640
你會發現說

0:24:02.640,0:24:07.560
比如說，像 t-SNE，如果你給他一個新的 data point

0:24:07.560,0:24:09.720
它是沒辦法做的，對不對

0:24:09.720,0:24:12.800
他只能夠，你給他一大群

0:24:12.800,0:24:16.900
已經先給他一大堆 x，他幫你把每一個 x 的 z 都找出來

0:24:16.900,0:24:18.620
但你找完這些 z 以後

0:24:18.620,0:24:21.140
再給他一個新的 x

0:24:21.140,0:24:25.500
你要重新跑一遍這一整套演算法，很麻煩

0:24:25.500,0:24:28.240
所以，一般你不會

0:24:28.240,0:24:36.780
一般 t-SNE 的作用比較不是用在這種 training testing 的這種 base 上面

0:24:36.780,0:24:38.780
通常比較常用的做法是

0:24:38.780,0:24:40.320
拿來做 visualization

0:24:40.320,0:24:44.020
如果你已經有一大堆的 x，他是 high dimensional 的

0:24:44.020,0:24:47.180
那你想要 visualize 他們在二維空間的分佈上是什麼樣子

0:24:47.180,0:24:48.080
你用 t-SNE

0:24:48.080,0:24:50.400
那 t-SNE 往往可以給你不錯的結果

0:24:50.400,0:24:55.340
那 t-SNE 現在可能是最多人使用的一種選擇

0:24:55.340,0:25:01.380
好那我們再來要講 t-SNE 一個非常有趣的地方

0:25:01.380,0:25:06.860
就是他的這個 similarity 的選擇是非常的神妙的

0:25:06.860,0:25:10.960
我們在這個原來的 data point 的 space 上面

0:25:10.960,0:25:12.980
你的 similarity 的選擇

0:25:12.980,0:25:16.280
它是選擇 RBF 的 function

0:25:16.280,0:25:19.780
就選擇說，我們要讓 xi，

0:25:19.780,0:25:22.440
我們這個 evaluate similarity 的方式

0:25:22.440,0:25:25.640
是計算 xi 跟 xj 的 Euclidean distance

0:25:25.640,0:25:29.620
然後取一個負號，再取 exponential

0:25:29.620,0:25:32.740
那我們之前有說如果你要在 graph 上算 similarity 的話

0:25:32.740,0:25:34.120
用這種方法比較好

0:25:34.120,0:25:38.260
因為它可以確保說只有非常相近的點才有值

0:25:38.260,0:25:40.220
那 exponential 他掉得非常快

0:25:40.220,0:25:44.780
所以只要距離一拉開，similarity 就會變得很小

0:25:44.780,0:25:49.440
那在 t-SNE之前有一個方法叫做 SNE

0:25:49.440,0:25:51.160
就把前面的 t 拿掉

0:25:51.160,0:25:53.740
SNE 就是一個很直覺的想法

0:25:53.740,0:25:57.580
在 data point 原來的 space 上用這個 evaluation 的 measure

0:25:57.580,0:26:02.520
當然在新的 space 上你用同樣的 measure 就好啦

0:26:02.520,0:26:08.200
選不同 measure，呃，你直覺就選一樣的 measure 嘛

0:26:08.200,0:26:10.380
那 t-SNE 神妙的地方就是

0:26:10.380,0:26:13.660
他在你 dimension reduction 以後的 space

0:26:13.660,0:26:16.680
他選的 measure 跟原來的 space 是不一樣的

0:26:16.680,0:26:18.820
他在 dimension reduction 以後選的 space

0:26:18.820,0:26:22.820
是這個 t-distribution 的其中一種

0:26:22.820,0:26:25.680
t-distribution 裡面有參數你可以調他

0:26:25.680,0:26:27.600
可以產生很多種不同的 distribution

0:26:27.600,0:26:29.600
t-distribution 的其中⼀一種

0:26:29.600,0:26:47.140
他是 1 除以 1 加 Euclidean distance 的平⽅

0:26:47.140,0:26:51.200
那你可能問說，為什麼要這樣做呢

0:26:51.200,0:26:53.740
我可以提供⼀個很直覺的理由

0:26:53.740,0:26:59.140
假設橫軸代表了在原來 space 上的 Euclidean distance

0:26:59.140,0:27:00.720
或者是

0:27:00.720,0:27:08.700
做 dimension reduction 以後的 Euclidean distance

0:27:08.700,0:27:14.200
那這個紅⾊這條線，是這⼀項

0:27:14.200,0:27:17.560
藍色這條線，是這⼀項

0:27:17.560,0:27:20.320
紅色這條線是 RBF function

0:27:20.320,0:27:22.900
藍色這條線是 t-distribution

0:27:22.900,0:27:25.320
你會發現說呢

0:27:25.320,0:27:30.080
如果原來在這個點跟這個點

0:27:30.080,0:27:35.020
之後做 Dimension reduction 以後

0:27:35.020,0:27:38.100
要怎麼才能維持它原來的 space 呢?

0:27:38.100,0:27:41.180
那你就把它變成這個樣⼦

0:27:41.180,0:27:44.880
它就可以維持它原來的 space

0:27:44.880,0:27:48.280
如果你要維持他們原來之間的距離

0:27:48.280,0:27:55.880
如果你今天要維持他們原來的機率的話

0:27:55.880,0:27:58.320
要維持原來他們之間相對的關係的話

0:27:58.320,0:28:02.140
那你就把他變成這樣

0:28:02.140,0:28:03.100
那你會發現說

0:28:03.100,0:28:07.780
如果本來距離比較近，他們的影響是比較⼩的

0:28:07.780,0:28:09.780
如果本來就已經有一段距離

0:28:09.780,0:28:14.540
那從原來的這個 distribution 變到 t-distribution 以後

0:28:14.540,0:28:15.900
他會被拉得很遠

0:28:15.900,0:28:18.340
t-distribution 他的尾巴特別長

0:28:18.340,0:28:20.600
所以如果你本來距離比較遠的話

0:28:21.500,0:28:24.580
變到 t-distribution 以後，他會變得更遠

0:28:24.580,0:28:27.640
也就是說，原來在高維空間裡面

0:28:27.640,0:28:29.640
如果距離很近

0:28:29.640,0:28:32.840
那做完 transform 以後他還是很近

0:28:33.180,0:28:35.400
如果原來就已經有一段距離了，有⼀個 gap

0:28:35.400,0:28:38.740
那做完 transform 以後他就會被拉得很遠

0:28:38.740,0:28:43.260
所以你會發現說 t-SNE，他畫出來的圖往往長得像這樣

0:28:43.260,0:28:48.540
他會把你的 data point 聚集成一群、一群、一群

0:28:48.540,0:28:51.540
因為你的 data point 之間本來只要有一個 gap

0:28:51.540,0:28:56.940
做完 t-SNE 以後，它就會把 gap 強化，gap 就會變得特別明顯

0:28:56.940,0:29:00.680
所以這是 t-SNE 做在 MNIST 上面的結果

0:29:00.680,0:29:03.940
那其實這個不是直接做在 MNIST 的 pixel 上⾯

0:29:03.940,0:29:06.700
直接做在 pixel 上面的 performance 看起來沒有這麼好

0:29:06.700,0:29:14.920
這是先對 pixel 做 PCA 降維以後，再做在 MNIST 上⾯

0:29:14.920,0:29:17.760
那不同顏色代表的是不同的數字

0:29:17.760,0:29:20.920
那你會發現，不同的數字會變成一群一群的

0:29:21.720,0:29:26.080
那 t-SNE 在 COIL-20 上的結果，其實還蠻驚人的

0:29:26.080,0:29:35.420
這邊的每一個圈圈就代表一個 object，他只是轉了一圈以後的結果

0:29:35.420,0:29:40.380
你會發現這邊有一些被扭曲的圈

0:29:41.320,0:29:42.740
這扭曲的圈圈是什麼意思呢？

0:29:42.740,0:29:45.640
那是因為有⼀些東西，比如說杯⼦

0:29:46.320,0:29:48.280
他的這個面長這樣

0:29:49.160,0:29:52.200
它轉 180 度過來以後，他還是長得一模⼀樣

0:29:52.920,0:29:55.900
所以你把那個杯子轉 360 度轉一圈的時候

0:29:55.900,0:29:59.360
你就會發現說，它在中間某一個地方是很像的 image

0:29:59.760,0:30:03.860
所以，你會看到說這邊有很多看起來很像 8 的符號

0:30:04.440,0:30:05.980
那這邊這很多條線

0:30:06.240,0:30:09.320
其實好像是說，有 4 部都是⼩汽⾞

0:30:09.320,0:30:11.880
4 部小汽車看起來是蠻像的

0:30:11.880,0:30:15.300
所以這邊有四條線是被 align 在一起的

0:30:16.140,0:30:20.060
這邊有一個從網路上找到的 t-SNE 動畫

0:30:20.060,0:30:22.580
他長得是這個樣⼦

0:30:24.420,0:30:26.680
你看，因為他是用 gradient descent train 的

0:30:26.680,0:30:29.940
所以你會看到，隨著 iteration 的 process

0:30:29.940,0:30:34.800
點會被分得越來越開

0:30:35.120,0:30:37.980
然後不同的點之間呢

0:30:37.980,0:30:41.620
這其實不是做在 MNIST 上，是做在另外⼀個手寫數字辨識的 corpus 上⾯

0:30:42.020,0:30:46.680
你會發現說不同的 digit 之間，他是會被分得很開的

0:30:49.120,0:30:54.660
那 t-SNE 的地方呢，我們就講到這邊

0:30:54.660,0:30:57.060
臺灣大學⼈⼯智慧中心
科技部⼈工智慧技術暨全幅健康照護聯合研究中⼼
http://aintu.tw
