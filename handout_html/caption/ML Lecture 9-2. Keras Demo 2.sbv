0:00:00.000,0:00:07.060
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:07.860,0:00:15.280
這個是我們上一次做失敗的 code 這樣子

0:00:15.740,0:00:20.480
我們疊了一些，我們要做手寫數字辨識

0:00:20.485,0:00:23.535
那我們疊的 network 呢，它的這個

0:00:23.535,0:00:26.335
hidden layer 的 size，就是 689

0:00:26.340,0:00:28.080
用 sigmoid function

0:00:28.080,0:00:31.160
我本來要疊 10 層，疊 10 層其實也不 work

0:00:31.160,0:00:32.660
就把它註解掉了

0:00:33.360,0:00:38.360
那用的 loss function 是 MSE，然後用了 SGD

0:00:38.420,0:00:41.340
等等，那結果是差的

0:00:41.480,0:00:42.320
怎麼辦？

0:00:42.320,0:00:44.840
你自己在 train network 的時候才會遇到這個問題

0:00:44.840,0:00:47.275
很多個 network train 下去，結果是差的

0:00:47.280,0:00:49.960
這個時候你就會去問老師說，怎麼辦？

0:00:49.960,0:00:53.280
每次有人問我這個問題的時候，我第一個會問你的就是

0:00:53.280,0:00:55.240
你在 training set 上

0:00:55.240,0:00:57.620
得到多少的 performance

0:00:57.620,0:00:58.640
你可能會問說

0:00:58.640,0:01:01.420
你怎麼不是問 testing set 上得到多少的 performance

0:01:01.420,0:01:04.580
而是問 training set 上，得到多少的 performance

0:01:04.580,0:01:06.920
因為如果你有看

0:01:07.000,0:01:09.460
錄影的話，你會知道說

0:01:09.460,0:01:11.740
今天 deep learning 在 training 的時候

0:01:11.740,0:01:13.780
你非常容易 train 壞掉

0:01:13.780,0:01:16.720
它跟其他方法不一樣，它跟 SVM 不一樣

0:01:16.720,0:01:19.120
SVM 是解一個 convex optimization problem

0:01:19.125,0:01:20.980
所以，你每次找的時候

0:01:20.980,0:01:23.560
它都可以找到一個 optimal 的 solution

0:01:23.560,0:01:25.165
也就是當你用 SVM 的時候

0:01:25.165,0:01:26.860
SVM 會竭盡全力

0:01:26.860,0:01:29.000
給你它可以得到最好的結果

0:01:29.100,0:01:30.880
那 deep learning 不是這樣的方法

0:01:30.880,0:01:32.320
它雖然很 powerful

0:01:32.320,0:01:35.620
但它其實就跟噴火龍一樣，你不見得能夠叫得動它

0:01:35.620,0:01:38.800
所以，你需要看一下你的 training set

0:01:38.860,0:01:41.820
來看看說你到底有沒有把它

0:01:41.820,0:01:44.540
的能力做起來

0:01:44.540,0:01:47.340
那你可能會想說，它在 training set 上 performance 好

0:01:47.380,0:01:49.380
可能只是 overfitting 阿

0:01:49.380,0:01:51.220
沒錯，它可能只是 overfitting 

0:01:51.460,0:01:54.975
但是，如果你連在 training set 上 overfitting 都做不到

0:01:54.980,0:01:59.740
你更遑論去做在 testing set 上舉一反三了

0:01:59.740,0:02:01.300
所以，我們先讓它至少

0:02:01.300,0:02:03.620
在 training set 上得到好的結果

0:02:03.620,0:02:05.860
那 training set 上得到好的結果有可能

0:02:05.860,0:02:07.560
可以舉一反三到 testing set 上

0:02:07.560,0:02:09.180
也有可能 overfitting

0:02:09.180,0:02:11.040
我們不知道，但是

0:02:11.040,0:02:13.320
如果你在 training set 上都沒有好的結果

0:02:13.320,0:02:15.580
那你其實在 testing set 上

0:02:15.660,0:02:18.320
可以得到好的結果的機會是微乎其微的

0:02:18.320,0:02:20.180
所以，如果你在 testing set 上結果不好

0:02:20.180,0:02:22.020
你應該先看看你的 training set

0:02:22.020,0:02:24.675
結果是怎麼樣，然後才看說

0:02:24.675,0:02:26.860
現在是不是 overfitting

0:02:26.860,0:02:29.520
所以，我們來看一下在 training set 上的結果吧

0:02:29.520,0:02:31.240
其實，Keras 在訓練的過程中

0:02:31.240,0:02:33.960
就已經會告訴你它在 training set 上得到的 performance

0:02:33.960,0:02:36.520
不過，我們今天特別再把 training set 的結果

0:02:36.520,0:02:39.720
再 print 出來

0:02:46.140,0:02:49.195
那我們來 print 一下 training set 的結果

0:02:49.200,0:02:51.000
只要把 x_test 改成 x_train

0:02:51.080,0:02:54.000
y 改成 y_train 就好

0:02:56.580,0:02:59.540
我們實際上來跑一下

0:03:04.540,0:03:06.780
那其實在 training 的過程中

0:03:06.940,0:03:08.740
Keras 就會告訴你說

0:03:08.740,0:03:11.320
它現在算出來的 accuracy 是多少

0:03:11.320,0:03:13.000
在每一個 η 後面

0:03:13.000,0:03:14.380
它都會告訴你說

0:03:14.380,0:03:17.640
這個 η 結束的時候，它算出來的 accuracy 是多少

0:03:17.680,0:03:21.475
那如果我們今天看這個實驗結果，你就會發現說

0:03:21.480,0:03:24.800
其實，啊！我這邊忘記把 test 改成 train

0:03:24.860,0:03:26.480
不過大家知道我的意思就好

0:03:26.480,0:03:30.880
上面這個 accuracy 是 training set 上的 accuracy

0:03:30.880,0:03:33.960
下面這個 accuracy 是 testing set 上的 accuracy

0:03:34.020,0:03:36.040
如果你只有看 testing set 上的 accuracy

0:03:36.040,0:03:38.440
你並不知道你現在是不是 overfitting

0:03:38.500,0:03:40.780
有人看到 testing set 上的 accuracy 就會說

0:03:40.780,0:03:42.820
看 testing set 上的 accuracy，它 performance 很差

0:03:42.820,0:03:44.820
就會胡亂得到一個結論說

0:03:44.820,0:03:46.560
所以，deep learning 很容易 overfitting

0:03:46.560,0:03:48.060
所以，deep learning 很不 work

0:03:48.060,0:03:48.840
那其實不是這樣

0:03:48.840,0:03:50.080
今天在這個 task 裡面

0:03:50.080,0:03:52.500
如果我們看 training set 的 accuracy 的話

0:03:52.500,0:03:55.460
你會發現 training set 的 accuracy 也是差的

0:03:55.460,0:03:58.260
這告訴我們什麼，這告訴我們 network 在 train 的時候

0:03:58.260,0:04:00.340
它就沒 train 好

0:04:00.340,0:04:02.400
它可能卡在一個 local minimum

0:04:02.400,0:04:04.420
它可能卡在一個很差的 saddle point

0:04:04.420,0:04:06.940
總之，它在 training set 上的 performance 就沒做好

0:04:06.940,0:04:07.820
所以，這個時候

0:04:07.820,0:04:09.160
你遇到的問題並不是overfitting

0:04:09.160,0:04:10.520
而是 training 沒有 train 好

0:04:10.520,0:04:12.920
你要想個辦法，先在 training set 上

0:04:12.920,0:04:14.620
得到比較好的 performance

0:04:14.620,0:04:16.660
那把這個 test 改成 train

0:04:20.100,0:04:24.020
那這邊到底少了甚麼東西呢？

0:04:24.020,0:04:27.480
其實這邊少的是 loss function

0:04:27.480,0:04:31.080
設得不對，其實我們已經有跟大家解釋過說

0:04:31.080,0:04:33.820
其實，用 mean square error 看分類的問題

0:04:33.820,0:04:35.560
你其實不會得到好的結果

0:04:35.560,0:04:37.220
我們在講 Logistic Regression 的時候

0:04:37.220,0:04:38.340
已經講過這件事了

0:04:38.340,0:04:40.140
我們現在實際來示範一下

0:04:43.460,0:04:47.600
我們就只是單純地把 mean square error 換成

0:04:48.060,0:04:50.100
這個 cross entropy

0:04:52.120,0:04:54.880
在 Keras 裡面，categorical 的 cross entropy

0:04:54.880,0:04:57.200
就是我們上課講的 cross entropy 啦

0:04:57.200,0:04:59.760
那從 mean square error 換成 cross entropy

0:04:59.760,0:05:02.360
這個你可不會覺得有甚麼特別厲害的地方

0:05:02.360,0:05:05.120
paper 也不會跟你 emphasize 這件事

0:05:05.125,0:05:07.415
但是，我們看看它有甚麼樣的差別

0:05:10.305,0:05:12.215
那我們剛才是做不起來的

0:05:17.740,0:05:19.180
我們來看一下，你看

0:05:19.360,0:05:21.740
當我們換成 cross entropy 以後

0:05:21.740,0:05:24.100
在 training set 上的 accuracy

0:05:24.100,0:05:25.240
就起飛了

0:05:25.240,0:05:26.720
現在 training set 就得到

0:05:26.720,0:05:29.320
87% 的正確率阿

0:05:31.100,0:05:34.760
這其實是個巧合，我沒有辦法特別設成這個結果

0:05:35.220,0:05:38.340
testing set 上得到 85% 的正確率，所以現在就

0:05:38.340,0:05:40.560
比較有 train 起來了

0:05:44.320,0:05:47.660
現在試一下，batch_size 會對結果造成的影響

0:05:47.660,0:05:51.280
現在，你看我們 batch_size 設 100

0:05:51.280,0:05:55.740
那我們現在呢，把 batch_size 改成 10000

0:05:56.320,0:05:59.740
把 batch_size 改成 10000，再跑跑看

0:06:00.480,0:06:02.680
剛才我們可以得到

0:06:02.760,0:06:04.700
training set 上 87%

0:06:04.700,0:06:06.980
testing set 上 85% 的正確率

0:06:06.980,0:06:08.920
你看 batch_size 設 10000

0:06:08.920,0:06:11.460
跑超快，因為你是用 GPU 平行運算

0:06:11.460,0:06:14.280
所以，在 GPU 可以平行運算的能力

0:06:14.395,0:06:16.220
它可以承受的前提之下

0:06:16.220,0:06:18.300
batch_size 越大，它其實跑得越快

0:06:18.300,0:06:19.740
但是，batch_size 一開大

0:06:20.225,0:06:21.225
performance

0:06:21.580,0:06:23.700
開太大的時候，performance 就壞掉了

0:06:23.700,0:06:25.940
那至於為什麼，我們上課有解釋過了

0:06:25.940,0:06:29.360
就會發現說，一樣的 network 架構，batch_size 一開大

0:06:29.360,0:06:30.300
結果就壞掉了

0:06:30.300,0:06:32.200
那我們試著把 batch_size 弄小一點

0:06:32.200,0:06:35.040
剛才是從 100 改到 10000

0:06:35.040,0:06:37.840
現在改回 1，現在改成 1

0:06:38.180,0:06:40.300
那改成 1 你會發現怎麼樣呢？

0:06:40.440,0:06:42.300
今天如果 batch_size 只有 1 的時候

0:06:42.300,0:06:45.260
GPU 就沒有辦法發揮它平行運算的效能

0:06:45.260,0:06:47.680
就會發現說，變得很慢這樣子

0:06:47.680,0:06:50.980
變得很慢，所以，有人他不知道說

0:06:50.980,0:06:53.860
你要能夠用 GPU 加速

0:06:53.860,0:06:58.420
前提是你在 training 的時候，batch 開大一點

0:06:58.420,0:06:59.720
GPU 才能夠真的加速

0:06:59.720,0:07:01.920
如果，你今天 batch_size 設 1

0:07:01.920,0:07:03.600
你就做 Stochastic Gradient Descent

0:07:03.600,0:07:05.840
GPU 可以對你帶來的幫助

0:07:05.840,0:07:08.200
其實就不會很大，所以你看現在

0:07:08.200,0:07:10.840
跑得非常非常慢，每一個 η 要 20 秒

0:07:10.840,0:07:13.500
我相信大家應該不會有興趣看我把它跑完

0:07:13.500,0:07:14.920
所以，我們就把它停下來

0:07:18.540,0:07:20.260
有些人可能想說

0:07:20.260,0:07:22.140
那就應該要用 deep 了吧

0:07:22.760,0:07:24.900
再加 10 層

0:07:30.200,0:07:32.920
現在改成用 10 層

0:07:35.520,0:07:38.220
我們看一下 testing 的 accuracy

0:07:38.220,0:07:39.840
先來看一下 training 的 accuracy

0:07:39.840,0:07:41.580
看一下 training 的 accuracy，就會發現說

0:07:41.580,0:07:42.860
沒做起來

0:07:42.860,0:07:45.200
卡住了

0:07:45.200,0:07:48.860
那我們在錄影裡面有解釋過為甚麼會這樣

0:07:48.860,0:07:50.280
疊 10 層的時候

0:07:50.285,0:07:52.380
會有 gradient vanishing 的問題，所以卡住了

0:07:52.380,0:07:54.520
所以，你看 testing set 的 performance

0:07:54.520,0:07:56.120
大概是 11% 的正確率

0:07:56.120,0:07:58.940
那如果你沒有 training deep learning 的概念

0:07:58.940,0:08:01.580
你可能會說，所以 10 層 overfitting

0:08:01.580,0:08:02.980
所以 performance 這麼差

0:08:02.980,0:08:04.820
但是，如果你仔細看一下 training set

0:08:04.820,0:08:07.740
它的 performance 其實也是差的

0:08:07.740,0:08:09.920
所以，這個不是 overfitting，這個是沒 train 起來

0:08:10.340,0:08:12.100
那怎麼辦？

0:08:15.780,0:08:18.340
那現在要怎麼辦呢？

0:08:18.360,0:08:21.040
我們來改一下 activation function

0:08:21.040,0:08:25.380
我們把 sigmoid 都改成 ReLU

0:08:25.420,0:08:27.960
sigmoid 現在通通改成 ReLU

0:08:27.960,0:08:29.380
再 train 一次

0:08:36.820,0:08:39.820
你會發現說，現在 training 的 accuracy 呢

0:08:39.900,0:08:43.700
它就爬起來了

0:08:45.940,0:08:48.420
現在已經跑到 98、99 這樣

0:08:50.120,0:08:52.780
你會發現說，現在 training 的 accuracy

0:08:52.780,0:08:54.380
已經將近 100% 的 testing

0:08:54.380,0:08:58.380
可以得到 95.6% 的正確率

0:09:01.400,0:09:05.500
這邊有個有趣的地方可以跟大家分享一下

0:09:05.500,0:09:08.100
現在我們的 image 阿

0:09:09.220,0:09:11.020
它是有 normalize 的

0:09:11.020,0:09:12.880
所謂有 normalize 的意思是說

0:09:12.880,0:09:16.880
每一個 pixel，我們用一個 0~1 之間的值來表示它

0:09:17.000,0:09:21.135
1 代表最黑，0 代表沒有塗黑

0:09:21.140,0:09:23.940
其實，你剛拿到一個 image 的時候

0:09:23.940,0:09:27.140
通常我們是用灰階來表示它的

0:09:27.140,0:09:31.040
也就是每一個 pixel 的值，是用 0~255 來表示它

0:09:31.040,0:09:34.020
所以我這邊特別除上 255，做 normalize 這件事

0:09:34.020,0:09:36.720
如果今天我們把 255 拿掉

0:09:36.720,0:09:38.260
會發甚麼事呢？

0:09:47.640,0:09:50.020
你會發現說，你又做不起來了

0:09:50.160,0:09:52.600
所以，這種小小的地方

0:09:52.640,0:09:54.560
只是有沒有做 normalization 的地方

0:09:54.560,0:09:57.600
對你的結果會有關鍵的影響

0:09:57.600,0:09:59.820
而這些事情，是很多人都忽略的

0:09:59.820,0:10:02.720
因為我們知道說，現在 AI 非常地潮

0:10:02.720,0:10:06.440
現在多數人的心力都集中在 AI 會不會統治世界這件事情

0:10:06.440,0:10:10.460
或講一些奇奇怪怪不符合實際的話

0:10:10.600,0:10:13.500
這個東西，像這個小小的 normalization

0:10:13.500,0:10:16.640
一點都不潮，不會統治世界的東西

0:10:16.640,0:10:18.820
但對結果其實有非常大的影響

0:10:20.795,0:10:23.725
我們把它改回去

0:10:23.760,0:10:26.980
改回去，那接下來

0:10:26.980,0:10:30.140
我想示範的一個東西是

0:10:31.040,0:10:33.180
我們把

0:10:35.280,0:10:38.100
這個時程註解起來

0:10:38.100,0:10:42.260
然後，我們再跑一次

0:10:42.260,0:10:45.100
那你會發現說，今天

0:10:46.720,0:10:48.620
在 training 的時候阿

0:10:49.660,0:10:52.000
跑得很快，我們就讓它跑完

0:10:53.985,0:10:56.220
那今天在 training 的時候

0:10:56.220,0:10:59.480
大概在第一個 epoch 的時候得到 77% 的正確率

0:10:59.480,0:11:02.580
在第二個 epoch 的時候得到 90% 的正確率

0:11:02.580,0:11:04.480
那我們現在換一下

0:11:04.480,0:11:08.200
training 的，Gradient Descent 的 strategy

0:11:08.200,0:11:09.760
把它從 SGD

0:11:09.760,0:11:11.340
改成 Adam

0:11:11.340,0:11:15.080
我上課有講過 Aden，把它改成 Adam

0:11:20.325,0:11:22.215
然後，再跑一次

0:11:24.740,0:11:27.820
你會發現說，當我們用 Adam 的時候

0:11:27.820,0:11:30.400
它可能最後收斂的地方差不多

0:11:30.400,0:11:33.980
但是，你會發現它上升的速度是變快的

0:11:35.680,0:11:37.760
我們剛才在

0:11:37.760,0:11:39.320
第一個 epoch

0:11:39.320,0:11:40.980
在還沒有用 Adam 的時候，第一個 epoch

0:11:40.980,0:11:43.100
它的正確率是 7 開頭

0:11:43.100,0:11:45.840
如果現在，有加上了

0:11:45.840,0:11:48.000
用 Adam 的話呢

0:11:48.000,0:11:51.000
在第一個 epoch，它的正確率就有 85%

0:11:51.000,0:11:53.300
第二個 epoch 就有 95%

0:11:53.300,0:11:56.580
那今天在這個 test，因為一個 epoch 跑得非常非常快

0:11:56.580,0:11:59.135
所以，你可能沒有甚麼特別的感覺

0:11:59.140,0:12:01.680
但是，如果今天一個 epoch 要跑一天

0:12:01.680,0:12:05.000
你就會覺得說有 Adam 真是好這樣子

0:12:09.080,0:12:10.780
我在 testing set 上呢

0:12:10.780,0:12:12.600
故意加上了 noise

0:12:12.600,0:12:14.380
training set 沒有 noise

0:12:14.380,0:12:15.980
testing set 的每一個 image

0:12:15.980,0:12:20.380
每一個 pixel 都故意給它加上 random 的 noise

0:12:20.580,0:12:22.180
然後我們實際來

0:12:22.180,0:12:25.220
操作一下，看看結果會掉多少

0:12:27.940,0:12:31.860
我們本來在 testing set 上已經可以得到 96% 的正確率

0:12:31.860,0:12:33.680
但現在 training 和 testing 呢

0:12:33.680,0:12:36.280
是不 match 的

0:12:38.680,0:12:41.840
所以，一做下去，結果就爛掉了

0:12:41.840,0:12:46.200
結果就爛掉了，現在 testing 只有不到 50% 的正確率

0:12:46.200,0:12:47.880
那怎麼辦呢？

0:12:47.880,0:12:51.260
我們來試一下 dropout 可以帶給我們甚麼樣的結果

0:12:51.260,0:12:53.380
我們來加一下 dropout，怎麼加 dropout 呢？

0:12:53.380,0:12:56.820
你就打 model.add

0:12:58.015,0:12:59.100
Dropout

0:12:59.100,0:13:01.060
然後，你要設一個 dropout rate

0:13:01.080,0:13:03.200
這個 dropout rate 其實就是你自己設的啦

0:13:03.200,0:13:06.020
就像是 network hidden layer 的 size 一樣

0:13:06.020,0:13:07.740
你要設多少是你自己決定的

0:13:07.740,0:13:09.600
常見的是設 0.5，不過

0:13:09.600,0:13:11.540
因為今天在這個 task 裡面

0:13:11.540,0:13:13.760
training 跟 testing 非常的 mismatch

0:13:13.760,0:13:15.800
所以，我覺得 dropout rate 可以設大一點

0:13:15.800,0:13:18.500
比如說，我設 0.7 試試看

0:13:25.960,0:13:30.300
每一個 dropout 就是加在每一個 hidden layer 後面

0:13:30.300,0:13:33.380
那這邊有一件事情，大家要注意的就是說

0:13:33.380,0:13:35.300
今天當你加了 dropout 以後

0:13:35.300,0:13:37.960
其實，training 上的 performance 是會變差的

0:13:37.960,0:13:39.080
這個很合理嘛

0:13:39.080,0:13:41.180
因為加 dropout 就是去綁住 network 的手腳

0:13:41.240,0:13:43.420
在 training 的時候，它的 performance會變差

0:13:43.420,0:13:44.660
所以，如果你今天

0:13:44.660,0:13:46.320
你的 performance 不好是來自於

0:13:46.320,0:13:47.820
你在 training set 上的 performance 不好

0:13:47.820,0:13:50.180
你不要再加 dropout，你只會越弄越差而已

0:13:50.180,0:13:52.320
你今天在 training 上已經跑得太好

0:13:52.320,0:13:54.860
它 overfitting，你才加上 dropout

0:13:54.860,0:13:57.340
我們看一下，其實剛才阿

0:13:57.340,0:13:59.400
我們的正確率都可以做到

0:13:59.400,0:14:01.020
100% 的正確率這樣

0:14:01.020,0:14:02.840
看到沒有，100% 的正確率

0:14:02.840,0:14:05.160
這個才是真正的 overfitting

0:14:05.480,0:14:08.440
所以，我們現在加了 dropout 以後

0:14:08.440,0:14:10.780
應該就跑不到 100% 的正確率了

0:14:10.780,0:14:12.100
你看剛才在 training 的時候

0:14:12.100,0:14:14.500
在最後幾個 epoch 的正確率都已經是 100%

0:14:14.700,0:14:16.040
現在加上 dropout

0:14:16.040,0:14:18.940
你就會發現說，network 就 train 不到那個 performance

0:14:22.140,0:14:24.820
在 training 的時候，就等於是綁住 network 的手腳

0:14:24.820,0:14:27.840
你就會發現說，它現在有點被卡住了

0:14:27.840,0:14:28.820
它在 training 的時候

0:14:28.820,0:14:32.060
它的正確率現在在 94, 95 中間徘徊

0:14:32.060,0:14:34.720
那今天在這個

0:14:34.720,0:14:36.620
testing training data 的時候呢

0:14:36.620,0:14:39.020
其實就不會加上 dropout 啦

0:14:39.020,0:14:40.120
所以，你會發現說

0:14:40.120,0:14:41.640
在 testing 的時候

0:14:41.700,0:14:43.120
用 testing data 的時候

0:14:43.120,0:14:44.820
它 performance 是遠比 training 的時候

0:14:44.820,0:14:47.240
所呈現的 performance 要好得多

0:14:47.240,0:14:48.580
那有加 dropout 的時候

0:14:48.580,0:14:49.780
network 的 performance 呢

0:14:49.780,0:14:51.580
network train 的時候會綁住手腳

0:14:51.580,0:14:52.840
所以它的 performance 會差一點

0:14:52.860,0:14:54.660
那你會發現說，在 testing 的時候

0:14:54.660,0:14:57.100
剛才，正確率連 50% 都不到

0:14:57.100,0:14:58.335
但加了 dropout 以後

0:14:58.335,0:15:01.425
現在就有 60% 的正確率了

0:15:01.425,0:15:02.720
那這邊就是

0:15:02.720,0:15:05.425
實際示範一下，這個

0:15:05.580,0:15:08.040
如果我們把上課教的種種 tip

0:15:08.040,0:15:10.460
真的拿來實做在 MNIST 的時候

0:15:10.460,0:15:11.920
會有甚麼樣的不同

0:15:11.920,0:15:13.600
那其實還有很多東西沒有講的

0:15:13.600,0:15:15.500
那你可以自己回去試試看

0:15:15.500,0:15:16.900
或在作業三的時候

0:15:16.900,0:15:18.740
試試看不同的 tip 對 network 會有怎麼樣的影響

0:15:18.800,0:15:21.480
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
