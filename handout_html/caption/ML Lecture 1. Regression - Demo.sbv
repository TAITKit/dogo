0:00:02.825,0:00:04.885
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:04.885,0:00:07.785
那我們在前一堂課看了regression，

0:00:07.785,0:00:09.995
我們知道怎麼用gradient descent來找出regression model的參數

0:00:10.115,0:00:12.705
這個聽起來還滿容易的。

0:00:12.945,0:00:15.895
接下來，我就舉個例子，

0:00:15.895,0:00:18.465
讓大家知道實際在做regression的時候，

0:00:18.465,0:00:20.865
你會碰到甚麼樣的困難。

0:00:24.855,0:00:27.525
這邊我們假設x data有10筆，

0:00:27.675,0:00:30.375
y data也有10筆。

0:00:30.375,0:00:33.325
那x和y之間的關係，

0:00:33.325,0:00:35.855
是ydata= b+w*xdata

0:00:35.855,0:00:37.445
是ydata= b+w*xdata

0:00:37.975,0:00:40.555
那b和w都是參數，

0:00:40.555,0:00:43.635
我們用gradient descent把b和w找出來。

0:00:43.635,0:00:46.645
我們用gradient descent把b和w找出來。

0:00:46.645,0:00:49.605
我當然知道說今天這個問題有closed-form solution，

0:00:49.605,0:00:52.515
這個b和w有更好的方法可以找出來，

0:00:52.515,0:00:55.455
那我們假裝不知道這件事，

0:00:55.455,0:00:58.205
我們要練習用gradient descent把b和w找出來。

0:00:58.205,0:00:59.205
我們要練習用gradient descent把b和w找出來。

0:01:00.635,0:01:02.415
那怎麼做呢?

0:01:02.415,0:01:04.725
gradient descent其實非常的簡單，

0:01:04.725,0:01:07.365
所以這邊我們不需要說太多。

0:01:07.365,0:01:09.885
這個程式碼還沒有超過20行。

0:01:12.305,0:01:14.545
我們先給b一個初始值，

0:01:14.545,0:01:17.265
這邊b的初始值是 -120，

0:01:17.265,0:01:20.055
我們給w一個初始值，

0:01:20.055,0:01:22.895
這邊w的初始值是-4，

0:01:22.895,0:01:26.175
我們需要一個learning rate，那learning rate我們就給他一個很小的數字

0:01:26.385,0:01:29.405
iteration就設100000個

0:01:30.885,0:01:34.165
那在每一個iteration裡面，

0:01:34.215,0:01:37.465
我們要做的事是，計算出b和w對loss的偏微分，

0:01:37.465,0:01:40.255
我們要做的事是，計算出b和w對loss的偏微分，

0:01:40.255,0:01:42.915
計算的式子在之前的課程裡面已經講過了，

0:01:42.915,0:01:46.275
計算的式子在之前的課程裡面已經講過了，

0:01:46.475,0:01:49.455
那你就把之前課程裡面導過的式子，把它寫出來就可以了

0:01:49.455,0:01:50.455
那你就把之前課程裡面導過的式子，把它寫出來就可以了

0:01:50.475,0:01:52.265
那你就把之前課程裡面導過的式子，把它寫出來就可以了

0:01:52.675,0:01:55.265
算出這個b和w對loss的偏微分以後，

0:01:55.265,0:01:57.975
算出這個b和w對loss的偏微分以後，

0:01:58.245,0:02:01.335
你就把b的偏微分乘上learning rate去update b

0:02:01.335,0:02:03.865
你就把b的偏微分乘上learning rate去update b

0:02:03.865,0:02:06.735
把w的偏微分乘上learning rate去update w

0:02:06.735,0:02:10.155
把w的偏微分乘上learning rate去update w

0:02:10.225,0:02:11.745
反覆iteration多次，

0:02:11.745,0:02:14.755
最後就可以找出b和w了。

0:02:14.755,0:02:15.755
最後就可以找出b和w了。

0:02:16.235,0:02:18.915
那我們就實際來執行一下程式，

0:02:19.895,0:02:21.875
看看我們會得到甚麼樣的結果。

0:02:22.505,0:02:24.585
這是我們得到的結果，

0:02:24.935,0:02:26.825
這個圖上面的顏色代表了不同的參數下我們會得到的loss。

0:02:27.445,0:02:30.515
這個圖上面的顏色代表了不同的參數下我們會得到的loss。

0:02:30.515,0:02:33.405
這個圖上面的顏色代表了不同的參數下我們會得到的loss。

0:02:33.405,0:02:36.195
縱軸代表w的變化，

0:02:36.385,0:02:39.595
橫軸代表b的變化，

0:02:39.595,0:02:42.345
不同的w和不同的b，我們得到不同的顏色，

0:02:42.345,0:02:45.065
也就是不同的loss。

0:02:45.065,0:02:48.095
那loss最低的點是在這個地方，

0:02:48.095,0:02:50.675
也就是b介於-180到-200之間，

0:02:50.675,0:02:53.845
w大概介於2到4之間的時候，

0:02:53.845,0:02:55.095
這個時候loss最低。

0:02:55.675,0:02:58.885
那我們初始的b和w，

0:02:59.155,0:03:00.655
在這個地方。

0:03:01.255,0:03:04.275
在做gradient descent的時候就從這個地方開始update參數，

0:03:04.275,0:03:06.955
在做gradient descent的時候就從這個地方開始update參數，

0:03:06.955,0:03:09.945
所以參數就一路從這個地方開始一直變化，

0:03:09.945,0:03:11.415
走到這邊然後向左轉。

0:03:11.785,0:03:14.385
但是過了100000次參數update以後，

0:03:14.385,0:03:17.715
我們發現說我們現在的參數，

0:03:17.715,0:03:20.445
離最佳解仍然非常的遙遠。

0:03:20.445,0:03:23.295
離最佳解仍然非常的遙遠。

0:03:23.885,0:03:27.235
怎麼辦?這顯然是learning rate不夠大，

0:03:29.445,0:03:30.545
把learning rate 調大一點。

0:03:37.735,0:03:41.095
這是learning rate 調大10倍後的結果，

0:03:41.425,0:03:44.595
你發現說最後，經過100000次參數的update以後

0:03:44.595,0:03:46.485
我們的參數在這個地方，

0:03:46.815,0:03:49.915
離最佳解稍微近了一點，

0:03:49.915,0:03:52.435
不過這邊有一個劇烈的震盪的現象發生。

0:03:52.435,0:03:54.175
不過這邊有一個劇烈的震盪的現象發生。

0:03:55.305,0:03:58.635
那我們再把learning rate稍微設大一點，

0:03:59.195,0:04:00.345
我們設再大10倍，

0:04:04.755,0:04:07.935
你發現，啊糟糕了!

0:04:07.935,0:04:08.965
learning rate再大10倍以後就太大了

0:04:10.805,0:04:13.785
從這個地方開始update參數，

0:04:13.785,0:04:16.245
結果參數一update，

0:04:16.245,0:04:18.895
它就飛到這個圖外面去了。

0:04:18.895,0:04:20.085
就飛得很遠很遠了。

0:04:20.505,0:04:23.905
所以現在learning rate太大，

0:04:23.905,0:04:26.885
如果再把它變小，

0:04:26.885,0:04:29.935
又變成跟剛才一樣還是離最佳解很遠，

0:04:30.435,0:04:31.705
怎麼辦?

0:04:32.155,0:04:35.235
這問題明明就很簡單，

0:04:35.235,0:04:38.265
只有兩個參數，

0:04:38.265,0:04:41.375
結果gradient descent搞半天都搞不定。

0:04:41.375,0:04:44.245
連兩個參數都搞不定，

0:04:44.245,0:04:47.275
之後如果再做neural network有數百萬個參數的時候，

0:04:47.275,0:04:49.895
要怎麼辦呢?

0:04:49.895,0:04:53.235
這個就是一室之不治何以天下國家為的概念。

0:04:53.375,0:04:54.525
怎麼辦?

0:04:55.305,0:04:57.725
只好放個大絕來解決這個問題。

0:04:57.725,0:04:59.445
我本來不想要用這一招的，

0:05:02.905,0:05:05.795
但是只有兩個參數的問題，

0:05:05.795,0:05:08.515
我們不解決是不行的。

0:05:08.615,0:05:11.825
怎麼辦呢?

0:05:11.925,0:05:13.975
我們要給b和w客製化的learning rate。

0:05:13.975,0:05:17.025
它們兩個的learning rate要是不一樣的。

0:05:17.565,0:05:20.835
怎麼做呢?

0:05:20.835,0:05:21.935
實作起來是滿容易的，所以現場寫一下。

0:05:28.645,0:05:31.915
我們要給b和w客製化的learning rate，

0:05:57.575,0:06:00.105
原來b和w的偏微分都是直接乘上learning rate，

0:06:00.105,0:06:03.265
原來b和w的偏微分都是直接乘上learning rate，

0:06:03.265,0:06:06.495
一個固定的learning rate，

0:06:06.495,0:06:09.365
那我們現在把它除掉不同的值，

0:06:09.365,0:06:12.535
所以它們會有不同的learning rate。

0:06:19.475,0:06:22.575
你可能看得一頭霧水，

0:06:22.575,0:06:24.965
不過沒有關係，這個就叫做AdaGrad

0:06:24.965,0:06:28.095
之後我們會再詳加解釋。

0:06:28.095,0:06:30.585
那這個learning rate就隨便設，設個1就好。

0:06:36.665,0:06:39.395
你會發現說有了新的learning rate以後，

0:06:41.855,0:06:45.345
從初始的值到終點，

0:06:45.405,0:06:46.405
我們就可以很順利的在十萬次update參數之內，走到我們的終點了

0:06:46.665,0:06:49.255
我們就可以很順利的在十萬次update參數之內，走到我們的終點了

0:06:49.255,0:06:50.255
我們就可以很順利的在十萬次update參數之內，走到我們的終點了

0:06:50.255,0:06:52.255
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:06:52.255,0:06:54.255
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
