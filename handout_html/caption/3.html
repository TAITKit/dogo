<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
0:00:02.825,0:00:04.885<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:04.885,0:00:07.785<br>
那我們在前一堂課看了regression，<br>
<br>
0:00:07.785,0:00:09.995<br>
我們知道怎麼用gradient descent來找出regression model的參數<br>
<br>
0:00:10.115,0:00:12.705<br>
這個聽起來還滿容易的。<br>
<br>
0:00:12.945,0:00:15.895<br>
接下來，我就舉個例子，<br>
<br>
0:00:15.895,0:00:18.465<br>
讓大家知道實際在做regression的時候，<br>
<br>
0:00:18.465,0:00:20.865<br>
你會碰到甚麼樣的困難。<br>
<br>
0:00:24.855,0:00:27.525<br>
這邊我們假設x data有10筆，<br>
<br>
0:00:27.675,0:00:30.375<br>
y data也有10筆。<br>
<br>
0:00:30.375,0:00:33.325<br>
那x和y之間的關係，<br>
<br>
0:00:33.325,0:00:35.855<br>
是ydata= b+w*xdata<br>
<br>
0:00:35.855,0:00:37.445<br>
是ydata= b+w*xdata<br>
<br>
0:00:37.975,0:00:40.555<br>
那b和w都是參數，<br>
<br>
0:00:40.555,0:00:43.635<br>
我們用gradient descent把b和w找出來。<br>
<br>
0:00:43.635,0:00:46.645<br>
我們用gradient descent把b和w找出來。<br>
<br>
0:00:46.645,0:00:49.605<br>
我當然知道說今天這個問題有closed-form solution，<br>
<br>
0:00:49.605,0:00:52.515<br>
這個b和w有更好的方法可以找出來，<br>
<br>
0:00:52.515,0:00:55.455<br>
那我們假裝不知道這件事，<br>
<br>
0:00:55.455,0:00:58.205<br>
我們要練習用gradient descent把b和w找出來。<br>
<br>
0:00:58.205,0:00:59.205<br>
我們要練習用gradient descent把b和w找出來。<br>
<br>
0:01:00.635,0:01:02.415<br>
那怎麼做呢?<br>
<br>
0:01:02.415,0:01:04.725<br>
gradient descent其實非常的簡單，<br>
<br>
0:01:04.725,0:01:07.365<br>
所以這邊我們不需要說太多。<br>
<br>
0:01:07.365,0:01:09.885<br>
這個程式碼還沒有超過20行。<br>
<br>
0:01:12.305,0:01:14.545<br>
我們先給b一個初始值，<br>
<br>
0:01:14.545,0:01:17.265<br>
這邊b的初始值是 -120，<br>
<br>
0:01:17.265,0:01:20.055<br>
我們給w一個初始值，<br>
<br>
0:01:20.055,0:01:22.895<br>
這邊w的初始值是-4，<br>
<br>
0:01:22.895,0:01:26.175<br>
我們需要一個learning rate，那learning rate我們就給他一個很小的數字<br>
<br>
0:01:26.385,0:01:29.405<br>
iteration就設100000個<br>
<br>
0:01:30.885,0:01:34.165<br>
那在每一個iteration裡面，<br>
<br>
0:01:34.215,0:01:37.465<br>
我們要做的事是，計算出b和w對loss的偏微分，<br>
<br>
0:01:37.465,0:01:40.255<br>
我們要做的事是，計算出b和w對loss的偏微分，<br>
<br>
0:01:40.255,0:01:42.915<br>
計算的式子在之前的課程裡面已經講過了，<br>
<br>
0:01:42.915,0:01:46.275<br>
計算的式子在之前的課程裡面已經講過了，<br>
<br>
0:01:46.475,0:01:49.455<br>
那你就把之前課程裡面導過的式子，把它寫出來就可以了<br>
<br>
0:01:49.455,0:01:50.455<br>
那你就把之前課程裡面導過的式子，把它寫出來就可以了<br>
<br>
0:01:50.475,0:01:52.265<br>
那你就把之前課程裡面導過的式子，把它寫出來就可以了<br>
<br>
0:01:52.675,0:01:55.265<br>
算出這個b和w對loss的偏微分以後，<br>
<br>
0:01:55.265,0:01:57.975<br>
算出這個b和w對loss的偏微分以後，<br>
<br>
0:01:58.245,0:02:01.335<br>
你就把b的偏微分乘上learning rate去update b<br>
<br>
0:02:01.335,0:02:03.865<br>
你就把b的偏微分乘上learning rate去update b<br>
<br>
0:02:03.865,0:02:06.735<br>
把w的偏微分乘上learning rate去update w<br>
<br>
0:02:06.735,0:02:10.155<br>
把w的偏微分乘上learning rate去update w<br>
<br>
0:02:10.225,0:02:11.745<br>
反覆iteration多次，<br>
<br>
0:02:11.745,0:02:14.755<br>
最後就可以找出b和w了。<br>
<br>
0:02:14.755,0:02:15.755<br>
最後就可以找出b和w了。<br>
<br>
0:02:16.235,0:02:18.915<br>
那我們就實際來執行一下程式，<br>
<br>
0:02:19.895,0:02:21.875<br>
看看我們會得到甚麼樣的結果。<br>
<br>
0:02:22.505,0:02:24.585<br>
這是我們得到的結果，<br>
<br>
0:02:24.935,0:02:26.825<br>
這個圖上面的顏色代表了不同的參數下我們會得到的loss。<br>
<br>
0:02:27.445,0:02:30.515<br>
這個圖上面的顏色代表了不同的參數下我們會得到的loss。<br>
<br>
0:02:30.515,0:02:33.405<br>
這個圖上面的顏色代表了不同的參數下我們會得到的loss。<br>
<br>
0:02:33.405,0:02:36.195<br>
縱軸代表w的變化，<br>
<br>
0:02:36.385,0:02:39.595<br>
橫軸代表b的變化，<br>
<br>
0:02:39.595,0:02:42.345<br>
不同的w和不同的b，我們得到不同的顏色，<br>
<br>
0:02:42.345,0:02:45.065<br>
也就是不同的loss。<br>
<br>
0:02:45.065,0:02:48.095<br>
那loss最低的點是在這個地方，<br>
<br>
0:02:48.095,0:02:50.675<br>
也就是b介於-180到-200之間，<br>
<br>
0:02:50.675,0:02:53.845<br>
w大概介於2到4之間的時候，<br>
<br>
0:02:53.845,0:02:55.095<br>
這個時候loss最低。<br>
<br>
0:02:55.675,0:02:58.885<br>
那我們初始的b和w，<br>
<br>
0:02:59.155,0:03:00.655<br>
在這個地方。<br>
<br>
0:03:01.255,0:03:04.275<br>
在做gradient descent的時候就從這個地方開始update參數，<br>
<br>
0:03:04.275,0:03:06.955<br>
在做gradient descent的時候就從這個地方開始update參數，<br>
<br>
0:03:06.955,0:03:09.945<br>
所以參數就一路從這個地方開始一直變化，<br>
<br>
0:03:09.945,0:03:11.415<br>
走到這邊然後向左轉。<br>
<br>
0:03:11.785,0:03:14.385<br>
但是過了100000次參數update以後，<br>
<br>
0:03:14.385,0:03:17.715<br>
我們發現說我們現在的參數，<br>
<br>
0:03:17.715,0:03:20.445<br>
離最佳解仍然非常的遙遠。<br>
<br>
0:03:20.445,0:03:23.295<br>
離最佳解仍然非常的遙遠。<br>
<br>
0:03:23.885,0:03:27.235<br>
怎麼辦?這顯然是learning rate不夠大，<br>
<br>
0:03:29.445,0:03:30.545<br>
把learning rate 調大一點。<br>
<br>
0:03:37.735,0:03:41.095<br>
這是learning rate 調大10倍後的結果，<br>
<br>
0:03:41.425,0:03:44.595<br>
你發現說最後，經過100000次參數的update以後<br>
<br>
0:03:44.595,0:03:46.485<br>
我們的參數在這個地方，<br>
<br>
0:03:46.815,0:03:49.915<br>
離最佳解稍微近了一點，<br>
<br>
0:03:49.915,0:03:52.435<br>
不過這邊有一個劇烈的震盪的現象發生。<br>
<br>
0:03:52.435,0:03:54.175<br>
不過這邊有一個劇烈的震盪的現象發生。<br>
<br>
0:03:55.305,0:03:58.635<br>
那我們再把learning rate稍微設大一點，<br>
<br>
0:03:59.195,0:04:00.345<br>
我們設再大10倍，<br>
<br>
0:04:04.755,0:04:07.935<br>
你發現，啊糟糕了!<br>
<br>
0:04:07.935,0:04:08.965<br>
learning rate再大10倍以後就太大了<br>
<br>
0:04:10.805,0:04:13.785<br>
從這個地方開始update參數，<br>
<br>
0:04:13.785,0:04:16.245<br>
結果參數一update，<br>
<br>
0:04:16.245,0:04:18.895<br>
它就飛到這個圖外面去了。<br>
<br>
0:04:18.895,0:04:20.085<br>
就飛得很遠很遠了。<br>
<br>
0:04:20.505,0:04:23.905<br>
所以現在learning rate太大，<br>
<br>
0:04:23.905,0:04:26.885<br>
如果再把它變小，<br>
<br>
0:04:26.885,0:04:29.935<br>
又變成跟剛才一樣還是離最佳解很遠，<br>
<br>
0:04:30.435,0:04:31.705<br>
怎麼辦?<br>
<br>
0:04:32.155,0:04:35.235<br>
這問題明明就很簡單，<br>
<br>
0:04:35.235,0:04:38.265<br>
只有兩個參數，<br>
<br>
0:04:38.265,0:04:41.375<br>
結果gradient descent搞半天都搞不定。<br>
<br>
0:04:41.375,0:04:44.245<br>
連兩個參數都搞不定，<br>
<br>
0:04:44.245,0:04:47.275<br>
之後如果再做neural network有數百萬個參數的時候，<br>
<br>
0:04:47.275,0:04:49.895<br>
要怎麼辦呢?<br>
<br>
0:04:49.895,0:04:53.235<br>
這個就是一室之不治何以天下國家為的概念。<br>
<br>
0:04:53.375,0:04:54.525<br>
怎麼辦?<br>
<br>
0:04:55.305,0:04:57.725<br>
只好放個大絕來解決這個問題。<br>
<br>
0:04:57.725,0:04:59.445<br>
我本來不想要用這一招的，<br>
<br>
0:05:02.905,0:05:05.795<br>
但是只有兩個參數的問題，<br>
<br>
0:05:05.795,0:05:08.515<br>
我們不解決是不行的。<br>
<br>
0:05:08.615,0:05:11.825<br>
怎麼辦呢?<br>
<br>
0:05:11.925,0:05:13.975<br>
我們要給b和w客製化的learning rate。<br>
<br>
0:05:13.975,0:05:17.025<br>
它們兩個的learning rate要是不一樣的。<br>
<br>
0:05:17.565,0:05:20.835<br>
怎麼做呢?<br>
<br>
0:05:20.835,0:05:21.935<br>
實作起來是滿容易的，所以現場寫一下。<br>
<br>
0:05:28.645,0:05:31.915<br>
我們要給b和w客製化的learning rate，<br>
<br>
0:05:57.575,0:06:00.105<br>
原來b和w的偏微分都是直接乘上learning rate，<br>
<br>
0:06:00.105,0:06:03.265<br>
原來b和w的偏微分都是直接乘上learning rate，<br>
<br>
0:06:03.265,0:06:06.495<br>
一個固定的learning rate，<br>
<br>
0:06:06.495,0:06:09.365<br>
那我們現在把它除掉不同的值，<br>
<br>
0:06:09.365,0:06:12.535<br>
所以它們會有不同的learning rate。<br>
<br>
0:06:19.475,0:06:22.575<br>
你可能看得一頭霧水，<br>
<br>
0:06:22.575,0:06:24.965<br>
不過沒有關係，這個就叫做AdaGrad<br>
<br>
0:06:24.965,0:06:28.095<br>
之後我們會再詳加解釋。<br>
<br>
0:06:28.095,0:06:30.585<br>
那這個learning rate就隨便設，設個1就好。<br>
<br>
0:06:36.665,0:06:39.395<br>
你會發現說有了新的learning rate以後，<br>
<br>
0:06:41.855,0:06:45.345<br>
從初始的值到終點，<br>
<br>
0:06:45.405,0:06:46.405<br>
我們就可以很順利的在十萬次update參數之內，走到我們的終點了<br>
<br>
0:06:46.665,0:06:49.255<br>
我們就可以很順利的在十萬次update參數之內，走到我們的終點了<br>
<br>
0:06:49.255,0:06:50.255<br>
我們就可以很順利的在十萬次update參數之內，走到我們的終點了<br>
<br>
0:06:50.255,0:06:52.255<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:06:52.255,0:06:54.255<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
