<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:04.560<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
<br>
0:00:04.560,0:00:12.460<br>
Lecture 4，我們想要講的就是，<br>
如何讓 machine 學會和環境作互動<br>
<br>
0:00:12.460,0:00:16.120<br>
那主要要講的就是 Reinforcement learning<br>
<br>
0:00:16.460,0:00:22.560<br>
那機器學習和環境作互動，<br>
跟一般 machine learning 的問題，有什麼不一樣呢<br>
<br>
0:00:22.660,0:00:33.180<br>
我覺得最大的差異就是，機器所採取的行為，<br>
會影響到它未來發生的事情<br>
<br>
0:00:36.200,0:00:38.820<br>
比如我們假設以影像分類的問題為例<br>
<br>
0:00:38.840,0:00:41.980<br>
來對照讓機器學習和環境作互動<br>
<br>
0:00:43.020,0:00:45.420<br>
那如果你今天是做影像的分類<br>
<br>
0:00:45.420,0:00:49.180<br>
給機器看一張圖片，<br>
讓它決定這張圖片是貓還是狗<br>
<br>
0:00:49.320,0:00:52.400<br>
那一個非常類似的 task 是要讓機器和環境互動<br>
<br>
0:00:52.400,0:00:56.580<br>
可能是給他一張圖片，假設那機器其實是自駕車好了<br>
<br>
0:00:56.860,0:01:02.020<br>
給它一張圖片，一個影像，它決定它要採取怎樣的行為<br>
<br>
0:01:02.080,0:01:04.620<br>
這跟一般的分類問題有什麼不同呢？<br>
<br>
0:01:04.720,0:01:09.540<br>
在一般的分類問題裡面，你得到輸出的結果以後<br>
<br>
0:01:09.580,0:01:13.600<br>
就沒事了，我給他一張圖片，<br>
覺得是貓就是貓，覺得是狗就是狗<br>
<br>
0:01:14.000,0:01:16.000<br>
但是在互動的問題裡面<br>
<br>
0:01:16.020,0:01:18.800<br>
假設是一個自駕車，給他看一個圖片<br>
<br>
0:01:18.800,0:01:21.320<br>
要決定現在要往左轉與往右轉<br>
<br>
0:01:21.440,0:01:25.900<br>
它的決策，會影響接下來它看到的 data<br>
<br>
0:01:25.900,0:01:30.680<br>
也就是說它如果決定往左轉，<br>
接下來它看到的就是往左轉街道的樣子<br>
<br>
0:01:30.760,0:01:33.360<br>
決定往右轉，<br>
它看到的就是往右轉街道的樣子<br>
<br>
0:01:33.880,0:01:37.320<br>
所以今天讓機器學習和環境作互動<br>
<br>
0:01:37.520,0:01:41.440<br>
跟一般的 learning problem，是不太一樣的<br>
<br>
0:01:41.440,0:01:44.200<br>
所以會需要另外來討論它<br>
<br>
0:01:44.200,0:01:45.700<br>
它最不一樣的地方就是<br>
<br>
0:01:45.700,0:01:47.960<br>
今天機器和環境作互動的時候<br>
<br>
0:01:47.960,0:01:53.340<br>
它會影響環境，它會影響它接下來的 input<br>
<br>
0:01:55.080,0:01:59.880<br>
好，那有什麼樣的例子，<br>
是機器要學習和環境作互動的呢？<br>
<br>
0:02:00.040,0:02:02.820<br>
大家都知道的就是 Alpha Go<br>
<br>
0:02:02.920,0:02:07.720<br>
在 Alpha Go 裡面，你有一個 machine 在下圍棋<br>
<br>
0:02:08.780,0:02:10.780<br>
Alpha Go 的輸入是什麼？<br>
<br>
0:02:10.800,0:02:13.940<br>
Alpha Go 的輸入，就是棋盤的盤勢<br>
<br>
0:02:14.020,0:02:15.860<br>
黑子和白子的位置<br>
<br>
0:02:15.860,0:02:20.000<br>
它的輸出就是接下來應該要落子的位置<br>
<br>
0:02:20.280,0:02:25.260<br>
那我們剛才講說，<br>
今天在機器學習和環境作互動的過程中<br>
<br>
0:02:25.280,0:02:30.460<br>
最重要的特色就是，你所採取的行為，<br>
會影響往後的發展<br>
<br>
0:02:30.700,0:02:33.260<br>
在圍棋裡面，顯然是這樣子<br>
<br>
0:02:33.380,0:02:36.740<br>
你落子的位置，當然會影響你對手落子的位置<br>
<br>
0:02:36.960,0:02:43.620<br>
你今天出手下在天元，對方一定守，跟你出手下在 5-5，對方一定守，顯然是會不一樣的<br>
<br>
0:02:46.080,0:02:50.060<br>
在做這種和環境互動的，你通常會訂一個目標<br>
<br>
0:02:50.060,0:02:52.720<br>
舉例來說在下圍棋裡面，<br>
要達成的目的，當然就是要贏棋<br>
<br>
0:02:52.720,0:02:58.060<br>
舉例來說在下圍棋裡面，<br>
要達成的目的，當然就是要贏棋<br>
<br>
0:02:58.640,0:03:03.900<br>
那在機器和環境作互動的文獻上，<br>
你常常看到一個字眼叫做 state<br>
<br>
0:03:04.040,0:03:11.640<br>
這是什麼意思，在過去，<br>
我們通常覺得說外界來的資訊太過複雜<br>
<br>
0:03:11.700,0:03:16.460<br>
我們會需要有另一個 model 幫我們把外接資訊做摘要<br>
<br>
0:03:16.540,0:03:19.720<br>
做完摘要以後，再丟給 machine<br>
<br>
0:03:19.940,0:03:23.220<br>
外界輸入資訊沒有處理的，就做 observation<br>
<br>
0:03:23.220,0:03:27.520<br>
但是今天隨著 deep learning 技術的發展，<br>
model 的能力已經越來越強<br>
<br>
0:03:27.520,0:03:32.380<br>
但是今天隨著 deep learning 技術的發展，<br>
model 的能力已經越來越強<br>
<br>
0:03:32.380,0:03:37.080<br>
我們不太再需要幫 model 另外再去做<br>
資訊的 summarization<br>
<br>
0:03:37.080,0:03:41.600<br>
反正它自己有那麼多個 layer，<br>
它自己會決定說 input 這個 raw 的 feature<br>
<br>
0:03:41.600,0:03:45.000<br>
observation 哪些資訊是可以用的，哪些是它不要的<br>
<br>
0:03:45.060,0:03:49.560<br>
所以 observation 跟 state <br>
現在這兩個詞彙基本上是混著用的<br>
<br>
0:03:49.760,0:03:54.760<br>
所以當我講 state 跟 observation <br>
其實我指的是一樣的東西<br>
<br>
0:03:55.200,0:03:58.240<br>
讓機器下圍棋，是一個機器和環境互動的例子<br>
<br>
0:03:58.500,0:04:01.740<br>
舉例來說，讓機器學會玩電玩<br>
<br>
0:04:01.900,0:04:04.700<br>
也是一個和環境作互動的例子<br>
<br>
0:04:04.700,0:04:06.740<br>
在機器學習玩電玩的時候<br>
<br>
0:04:06.740,0:04:10.480<br>
它有另外一個 AI，另外一個主機當作對手<br>
<br>
0:04:10.480,0:04:13.820<br>
機器採取的行為，當然會影響對方的回應<br>
<br>
0:04:13.880,0:04:16.500<br>
如果你想做一些 video game 的 playing 的話<br>
<br>
0:04:16.740,0:04:19.040<br>
你可以參考下面這兩個連結<br>
<br>
0:04:19.040,0:04:26.580<br>
OpenAI 它們提供了一些平台，<br>
讓你的機器可以練習去玩一些遊戲，比如說 GTA<br>
<br>
0:04:27.280,0:04:29.280<br>
有人可能會問說，讓機器學習玩遊戲，<br>
好像沒有什麼特別的<br>
<br>
0:04:32.560,0:04:37.860<br>
有人可能會問說，讓機器學習玩遊戲，<br>
好像沒有什麼特別的<br>
<br>
0:04:38.860,0:04:41.600<br>
遊戲主機裡面就有一個 AI，它也會玩遊戲嘛<br>
<br>
0:04:41.620,0:04:45.500<br>
當機器學習玩遊戲後，它看到的遊戲畫面，<br>
跟人看到的遊戲畫面，是一模一樣的<br>
<br>
0:04:46.080,0:04:54.260<br>
再來呢，它並沒有 handcrafted 的 rule<br>
<br>
0:04:54.440,0:04:56.940<br>
告訴它什麼樣的行為是好的，<br>
什麼樣的行為是不好的<br>
<br>
0:04:56.940,0:05:00.580<br>
告訴它什麼樣的行為是好的，<br>
什麼樣的行為是不好的<br>
<br>
0:05:00.720,0:05:04.020<br>
它是人告訴他說看到這樣的畫面，<br>
看到這樣的狀況，你就採取這樣的行為<br>
<br>
0:05:04.100,0:05:09.920<br>
它是人告訴他說看到這樣的畫面，<br>
看到這樣的狀況，你就採取這樣的行為<br>
<br>
0:05:10.160,0:05:12.360<br>
但是當我們讓機器自己學習的時候，<br>
沒有人告訴機器說，該採取什麼樣的行為<br>
<br>
0:05:12.360,0:05:15.040<br>
沒有人告訴機器說應該採取什麼樣的行為<br>
<br>
0:05:15.040,0:05:19.680<br>
它必須要自己 dig out 什麼樣的行為是好的，<br>
什麼樣的行為是不好的<br>
<br>
0:05:20.600,0:05:24.840<br>
但和環境作互動，還有很多例子，<br>
比如說自駕車也是一個例子<br>
<br>
0:05:24.840,0:05:27.720<br>
如果你今天要做一台自駕車的話<br>
<br>
0:05:27.740,0:05:31.800<br>
比如說看到一個紅燈，<br>
然後它就決定說，現在要踩個煞車<br>
<br>
0:05:31.800,0:05:36.240<br>
那當然，我們剛才用自駕車的例子有講過說<br>
<br>
0:05:36.240,0:05:41.520<br>
比如說看到一個紅燈，<br>
然後它就決定說，現在要踩個煞車<br>
<br>
0:05:41.520,0:05:44.840<br>
那當然，我們剛才用自駕車的例子有講過說<br>
<br>
0:05:44.840,0:05:48.980<br>
機器採取不同的行為，<br>
就會影響它接下來看到的畫面<br>
<br>
0:05:49.020,0:05:53.780<br>
隨著採取的行為不同，<br>
接下來它看到的 observation，就會不一樣<br>
<br>
0:05:54.060,0:05:58.540<br>
或者是說，假設你想要做一個 chat-bot，<br>
也是一樣<br>
<br>
0:05:58.700,0:06:00.960<br>
然後 machine 回答說，你希望從哪裡出發<br>
<br>
0:06:00.960,0:06:03.740<br>
你說，你想要訂 11/5 到台北的機票<br>
<br>
0:06:03.800,0:06:07.760<br>
然後 machine 回答說，你希望從哪裡出發<br>
<br>
0:06:08.720,0:06:11.760<br>
這個輸入，和輸出<br>
<br>
0:06:12.060,0:06:15.300<br>
這個東西，你可以想成，你有一個巨大的 network<br>
<br>
0:06:15.420,0:06:18.800<br>
輸入一個 input，然後你就可以回一個輸出<br>
<br>
0:06:18.940,0:06:24.480<br>
但是今天你的 Dialog system 它的輸出，<br>
會影響它接下來看到的 input<br>
<br>
0:06:24.620,0:06:28.560<br>
但是假如今天 dialog system 的輸出<br>
是比如說你要幾點出發呢？<br>
<br>
0:06:28.760,0:06:33.080<br>
那它接下來看到的 input，可能就是，<br>
我要從 Boston 出發<br>
<br>
0:06:33.360,0:06:38.120<br>
但是假如今天 dialog system 的輸出<br>
是比如說你要幾點出發呢？<br>
<br>
0:06:38.140,0:06:40.780<br>
它看到的輸入，可能就是我要 6 點出發<br>
<br>
0:06:40.800,0:06:43.820<br>
所以跟其他互動的 task 一樣<br>
<br>
0:06:43.860,0:06:46.180<br>
今天你要訓練一個對話系統的時候<br>
<br>
0:06:46.180,0:06:48.420<br>
環境就是指你的客人<br>
<br>
0:06:49.160,0:06:51.160<br>
但你今天採取的行為，會影響你客人的回應<br>
<br>
0:06:51.240,0:06:53.860<br>
環境就是指你的客人<br>
<br>
0:06:53.860,0:06:58.140<br>
但你今天採取的行為，會影響你客人的回應<br>
<br>
0:06:59.120,0:07:02.680<br>
好，要怎麼解這種和環境互動的問題呢？<br>
<br>
0:07:02.860,0:07:05.080<br>
你當然可以說，和環境互動的問題<br>
<br>
0:07:05.080,0:07:10.920<br>
我們就直接把它想成是一個分類的問題<br>
<br>
0:07:10.920,0:07:14.260<br>
直接把它當成是一個<br>
 supervised learning 的問題<br>
<br>
0:07:14.260,0:07:18.400<br>
用 supervised learning 的方法，去學一個 network<br>
<br>
0:07:18.400,0:07:20.240<br>
怎麼說，假設是圍棋的話<br>
<br>
0:07:20.240,0:07:24.720<br>
我們就教 machine，看到這個盤勢，你就下 3 3<br>
<br>
0:07:24.800,0:07:27.460<br>
那你就跟人做的一樣，<br>
那這不過是一個 supervised model<br>
<br>
0:07:27.460,0:07:29.860<br>
人看到這個盤勢，就會下 3 3<br>
<br>
0:07:29.860,0:07:33.480<br>
那你就跟人做的一樣，<br>
那這不過是一個 supervised model<br>
<br>
0:07:33.480,0:07:37.380<br>
看到這個盤勢，輸出 3 3 的機率要越大越好<br>
<br>
0:07:37.880,0:07:40.340<br>
好，那如果是自駕車，你就教機器說<br>
<br>
0:07:40.580,0:07:43.600<br>
看到這個畫面，你就踩煞車<br>
<br>
0:07:43.600,0:07:45.520<br>
那你說要怎麼收集這種 data 呢？<br>
<br>
0:07:45.660,0:07:51.840<br>
你就 collect 人在開車的時候，<br>
行車紀錄器的畫面，還有人的動作<br>
<br>
0:07:51.880,0:07:56.280<br>
你要調這個 network 的參數，讓它的 output 是踩煞車<br>
<br>
0:07:56.500,0:07:59.140<br>
機器要學到說，看這個畫面的時候<br>
<br>
0:07:59.140,0:08:02.820<br>
你要調這個 network 的參數，讓它的 output 是踩煞車<br>
<br>
0:08:02.980,0:08:09.760<br>
如果是對話系統的話，<br>
你就搜集很多真人的客服，跟顧客的對話<br>
<br>
0:08:09.760,0:08:13.420<br>
那你知道真人的顧客說，<br>
我想訂 11/5 到台北的機票<br>
<br>
0:08:13.420,0:08:16.160<br>
真人會問說，你要從哪裡出發？<br>
<br>
0:08:16.360,0:08:19.860<br>
那當我們用這種方式，讓機器做學習的時候<br>
<br>
0:08:19.860,0:08:22.480<br>
你要回答的跟真人客服，越像越好<br>
<br>
0:08:22.480,0:08:23.700<br>
你就這樣輸出<br>
<br>
0:08:24.100,0:08:27.560<br>
那當我們用這種方式，讓機器做學習的時候<br>
<br>
0:08:27.560,0:08:34.980<br>
這個叫做 behavior cloning，<br>
就是複製行為，複製 expert 的行為<br>
<br>
0:08:34.980,0:08:39.500<br>
我們可以告訴機器說，真人駕駛，<br>
看到這個畫面就採取怎麼樣的行為<br>
<br>
0:08:39.500,0:08:43.500<br>
真人的客服，聽到顧客說這句話，就這樣子回答<br>
<br>
0:08:43.660,0:08:48.520<br>
你就跟你的老師，你的 expert 做得一模一樣就好了<br>
<br>
0:08:48.520,0:08:51.160<br>
就希望你可以學得跟 expert 一樣厲害<br>
<br>
0:08:51.240,0:08:54.220<br>
這麼做，會有什麼樣的問題呢？<br>
<br>
0:08:54.220,0:08:59.280<br>
以下是一個 behavior cloning 的例子，<br>
那其實跟 machine learning 沒什麼關係<br>
<br>
0:09:07.940,0:09:13.880<br>
以下部分省略<br>
<br>
0:09:45.660,0:09:50.260<br>
好，所以機器在學的時候，<br>
它就跟 Shelton 一樣困惑了<br>
<br>
0:09:50.320,0:09:55.000<br>
當它有一個人類的老師的時候，<br>
人類的老師，採取的一些行為<br>
<br>
0:09:55.040,0:10:00.820<br>
機器能夠做的事情，就是相信它的老師全然是對的，<br>
然後完全去模仿它的老師<br>
<br>
0:10:01.140,0:10:05.440<br>
事實上，假設機器可以完全模仿它的老師<br>
<br>
0:10:05.440,0:10:07.760<br>
也許問題並沒有很大<br>
<br>
0:10:07.760,0:10:10.880<br>
機器只是學了一些不該學的東西而已<br>
<br>
0:10:10.920,0:10:13.160<br>
但真正害怕的 case 是什麼呢？<br>
<br>
0:10:13.160,0:10:17.760<br>
真正害怕的 case 是，<br>
機器沒有辦法完全模仿它的老師<br>
<br>
0:10:17.860,0:10:23.300<br>
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的<br>
<br>
0:10:23.300,0:10:26.920<br>
到底哪些東西該學，那些東西不該學<br>
<br>
0:10:27.060,0:10:29.060<br>
就變成是一個問題了<br>
<br>
0:10:29.400,0:10:32.160<br>
這 behavior cloning 我覺得最大的問題就是<br>
<br>
0:10:32.300,0:10:40.260<br>
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的<br>
<br>
0:10:40.260,0:10:46.740<br>
舉例來說，剛剛 Shelton 在學中文的時候，<br>
它不知道說語音是重要的，手勢是不重要的<br>
<br>
0:10:46.740,0:10:52.280<br>
對機器來說，你今天就給它一個示範，<br>
它又不知道說到底是語音重要，還是手勢重要<br>
<br>
0:10:52.280,0:10:55.480<br>
就好像有一個人說，它想要成為成功的人物<br>
<br>
0:10:55.480,0:10:57.100<br>
那整個結果就會壞掉<br>
<br>
0:10:57.180,0:11:01.680<br>
所以 behavior cloning 最大的問題就是，<br>
機器不知道說什麼是重要的，什麼是不重要的<br>
<br>
0:11:01.680,0:11:04.380<br>
它只能夠做照單全收這件事<br>
<br>
0:11:04.460,0:11:07.280<br>
就好像有一個人說，它想要成為成功的人物<br>
<br>
0:11:07.280,0:11:11.700<br>
比如說它想跟 Jobs 一樣，<br>
他就列出了 Jobs 的 20 個人格特質<br>
<br>
0:11:11.700,0:11:14.960<br>
可能包括勤奮，創造力，還有壞脾氣<br>
<br>
0:11:15.180,0:11:18.340<br>
然後他就覺得說他能力很差，<br>
三項裡面他學一樣就好了<br>
<br>
0:11:18.340,0:11:22.200<br>
他就決定他只學壞脾氣，然後就一無是處這樣子<br>
<br>
0:11:22.400,0:11:24.800<br>
所以 behavior cloning 的問題就是這樣<br>
<br>
0:11:25.620,0:11:33.820<br>
好，那在這種與環境互動的情況下，<br>
有些行為是重要的，有些行為是不重要的<br>
<br>
0:11:34.300,0:11:37.460<br>
為什麼，有些行為是重要的，有些行為是不重要的呢？<br>
<br>
0:11:37.460,0:11:42.120<br>
因為你現在你的 machine 採取的行為，<br>
會影響接下來的發展<br>
<br>
0:11:42.120,0:11:46.940<br>
所以有些行為非常的關鍵，<br>
有些行為也許沒那麼關鍵<br>
<br>
0:11:47.040,0:11:51.180<br>
如果說，behavior cloning，機器學不到這件事<br>
<br>
0:11:51.260,0:11:56.480<br>
好那要怎麼樣讓機器，真的可以在和環境互動的情況下學好呢<br>
<br>
0:11:56.680,0:12:01.140<br>
你就不能單純的考慮每一個 step<br>
<br>
0:12:01.140,0:12:05.460<br>
你就不能夠只是告訴機器說，在看到這個盤勢的時候<br>
<br>
0:12:05.480,0:12:07.240<br>
你就下在這個位置<br>
<br>
0:12:07.240,0:12:12.580<br>
你不能只把他當作一個簡單的，<br>
一般的 supervised learning 來看待<br>
<br>
0:12:12.600,0:12:14.480<br>
機器是沒有辦法學好的<br>
<br>
0:12:14.480,0:12:19.180<br>
你要讓機器把所有 actions 都當作整體來看待<br>
<br>
0:12:19.180,0:12:23.240<br>
那怎麼做到這一件事呢？有兩個方向<br>
<br>
0:12:23.240,0:12:26.820<br>
一個方向，就是大家都知道的 <br>
reinforcement learning<br>
<br>
0:12:27.240,0:12:30.880<br>
那在 reinforcement learning 裡面，機器會去跟環境互動<br>
<br>
0:12:31.000,0:12:32.820<br>
他自己去跟環境互動<br>
<br>
0:12:32.820,0:12:36.520<br>
他在跟環境互動的過程中，他會得到一些 reward<br>
<br>
0:12:36.600,0:12:40.380<br>
告訴他說，甚麼樣的行為是好的，甚麼樣的行為是不好的<br>
<br>
0:12:40.820,0:12:45.040<br>
接下來機器要自己去學習說，怎麼樣可以得到比較好的reward<br>
<br>
0:12:45.060,0:12:46.540<br>
怎麼樣多採取好的行為<br>
<br>
0:12:46.540,0:12:51.340<br>
避免採取會得到 negative reward 會得到差的評價的行為<br>
<br>
0:12:51.560,0:12:55.700<br>
另外一個也許大家可能比較沒有哪麼熟悉的，<br>
叫做 learning by demonstration<br>
<br>
0:12:55.940,0:13:00.080<br>
learning by demonstration 又叫做 imitation learning 或著是 apprenticeship learning<br>
<br>
0:13:00.380,0:13:05.920<br>
那在這種 task 裡面，<br>
機器它有一些 expert 的 demo<br>
<br>
0:13:06.180,0:13:11.120<br>
但是它今天在學習 expert 的行為的時候，<br>
它必須要有特別的學習方式<br>
<br>
0:13:11.120,0:13:16.200<br>
而不是使用照單全收，behavior cloning 的方式<br>
<br>
0:13:16.200,0:13:19.840<br>
所以 learning by demonstration，<br>
它並不是一般 supervised learning 的問題<br>
<br>
0:13:19.920,0:13:21.920<br>
機器不是複製 expert 的行為<br>
<br>
0:13:21.980,0:13:26.960<br>
而是用其他方法，來讓它可以學得跟 expert 一樣好<br>
<br>
0:13:27.220,0:13:29.940<br>
好，那我們就先來講 reinforcement learning<br>
<br>
0:13:29.940,0:13:34.740<br>
但其實我們除了講 reinforcement learning，<br>
還會講 inverse reinforcement learning 的技術<br>
<br>
0:13:34.740,0:13:39.920<br>
其實 inverse reinforcement learning 的技術，<br>
就是 learning by demonstration<br>
<br>
0:13:40.260,0:13:45.800<br>
所以，inverse reinforcement learning 的技術，<br>
就是 learning by demonstration 的其中一種<br>
<br>
0:13:46.080,0:13:49.440<br>
好，那我們先來看一下 reinforcement learning<br>
<br>
0:13:49.460,0:13:51.460<br>
等一下如果時間夠的話<br>
<br>
0:13:51.500,0:13:57.540<br>
我們就會先介紹 Actor，再介紹 Critic，<br>
然後再介紹 Actor + Critic 的方法<br>
<br>
0:13:59.140,0:14:03.600<br>
那我們分別以電玩，和下圍棋為例，<br>
來說明這3 個東西分別是什麼<br>
<br>
0:14:03.620,0:14:05.760<br>
第一個 component 是一個 actor<br>
<br>
0:14:05.760,0:14:07.720<br>
第二個 component 是一個 environment<br>
<br>
0:14:07.720,0:14:10.780<br>
第三個 component 是一個 reward function<br>
<br>
0:14:10.780,0:14:16.080<br>
那我們分別以電玩，和下圍棋為例，<br>
來說明這3 個東西分別是什麼<br>
<br>
0:14:16.180,0:14:22.549<br>
如果在電玩裡面 你的actor要做的事情 就是去操控搖桿<br>
<br>
0:14:22.549,0:14:25.380<br>
決定要向左向右，還是開火<br>
<br>
0:14:25.420,0:14:30.400<br>
如果是在下圍棋的時候，<br>
它就決定說現在要落子落在哪個位置<br>
<br>
0:14:30.880,0:14:38.000<br>
至於環境，在 Video game 裡面，<br>
你的環境，就是主機<br>
<br>
0:14:38.000,0:14:44.540<br>
machine 現在要去玩遊戲，<br>
至少要有主機跟遊戲玩，你的主機就是環境<br>
<br>
0:14:44.540,0:14:48.720<br>
那在下圍棋裡面，你的環境就是 machine 的對手<br>
<br>
0:14:48.720,0:14:50.640<br>
就是另外一個人類的對手<br>
<br>
0:14:50.940,0:14:57.220<br>
好，那 reward function 呢，在遊戲裡面，<br>
你會先訂好比如說殺一隻怪獸得 20 分等等<br>
<br>
0:14:57.240,0:14:58.560<br>
這個就是 reward function<br>
<br>
0:14:58.760,0:15:02.120<br>
那在圍棋裡面，<br>
reward function 是很明確的，就是圍棋的規則<br>
<br>
0:15:02.120,0:15:04.920<br>
下到這個地方就贏了，就得一分<br>
<br>
0:15:04.920,0:15:07.920<br>
下到這個地方就輸了，就得負一分<br>
<br>
0:15:08.120,0:15:09.900<br>
那在 reinforcement learning 的 task<br>
<br>
0:15:09.900,0:15:14.960<br>
你要注意 environment 跟 reward function <br>
是訂好的，是訂死的<br>
<br>
0:15:14.960,0:15:21.000<br>
你不能去動它，<br>
你不能去動圍棋的規則，你不能去動你的對手<br>
<br>
0:15:21.000,0:15:22.680<br>
那都是你無法控制的<br>
<br>
0:15:22.860,0:15:28.020<br>
我們要做的事情是什麼，我們唯一能夠控制的，<br>
就是 actor 它採取的行為<br>
<br>
0:15:28.020,0:15:31.440<br>
我們要做的事情是，調整 actor 採取的行為<br>
<br>
0:15:31.440,0:15:34.180<br>
使得它可以得到，最大的 reward<br>
<br>
0:15:34.580,0:15:38.080<br>
好，那我們以電玩為例，來說明一下<br>
<br>
0:15:38.080,0:15:43.420<br>
這個環境，還有 actor，<br>
還有 reward 它們互動的情形<br>
<br>
0:15:43.640,0:15:47.760<br>
假設我們現在要讓機器去玩電玩，那是什麼樣的狀況呢？<br>
<br>
0:15:47.760,0:15:54.440<br>
首先機器會先看到一個遊戲畫面，<br>
這個遊戲畫面，我們就叫做 s1<br>
<br>
0:15:54.560,0:15:59.100<br>
這個遊戲畫面，就輸給 machine，就輸入給 actor<br>
<br>
0:15:59.100,0:16:02.680<br>
actor 就要做一個決定，決定現在要做什麼<br>
<br>
0:16:02.680,0:16:09.160<br>
舉例來說，它決定說，看到這個畫面 s1，<br>
我要採取的行為，叫做 a1<br>
<br>
0:16:09.160,0:16:11.780<br>
那 a1 就是向右移動<br>
<br>
0:16:12.060,0:16:16.320<br>
好，那它向右移動以後，它會得到一個 reward<br>
<br>
0:16:16.320,0:16:21.500<br>
在每一個 time set，在每一個互動的過程中<br>
<br>
0:16:21.500,0:16:25.220<br>
reward function 都會給 machine 一個 reward<br>
<br>
0:16:25.220,0:16:28.300<br>
那在這一步，只是採取向右，<br>
所以得到的 reward 是 0<br>
<br>
0:16:28.300,0:16:31.820<br>
也就是採取向右，不會得到任何的分數，所以 r1 是 0<br>
<br>
0:16:31.820,0:16:36.600<br>
那機器採取這個行為之後，它就會看到新的遊戲畫面<br>
<br>
0:16:36.600,0:16:38.840<br>
至少它看到自己往右移了<br>
<br>
0:16:38.840,0:16:42.000<br>
這畫面是我從真實的遊戲截下來的<br>
<br>
0:16:42.000,0:16:45.080<br>
它本來在這個地方，然後它就往右移了<br>
<br>
0:16:45.200,0:16:48.880<br>
然後往右移以後，就會看到新的遊戲畫面<br>
<br>
0:16:48.880,0:16:51.440<br>
機器就會決定要採取新的行為<br>
<br>
0:16:51.540,0:16:54.260<br>
舉例來說，它可能看到新的畫面<br>
<br>
0:16:54.260,0:16:57.960<br>
這次他決定說，看到畫面 s2，它要採取 a2 這個行為<br>
<br>
0:16:57.960,0:17:03.140<br>
a2 這個行為，就是開火<br>
它決定要開火<br>
<br>
0:17:03.620,0:17:10.220<br>
好，哪假設它開火以後，它殺了一個怪<br>
<br>
0:17:10.220,0:17:13.280<br>
根據 reward function 的定義，<br>
就會告訴他說，你殺了一隻怪<br>
<br>
0:17:13.280,0:17:16.920<br>
那假設，殺那一隻怪，值 5 分<br>
那你就得到 5 分，那 r2 就是 5<br>
<br>
0:17:16.960,0:17:19.340<br>
那你又看到新的遊戲畫面，就是 s3<br>
<br>
0:17:19.340,0:17:22.900<br>
那這互動的過程，就反覆的繼續下去<br>
<br>
0:17:22.920,0:17:30.080<br>
直到說，現在呢，在某一個遊戲畫面，<br>
機器決定採取 action at，得到 reward rt<br>
<br>
0:17:30.080,0:17:34.680<br>
然後進入 terminal state，然後進入按照這個主機的設定<br>
<br>
0:17:34.680,0:17:38.660<br>
走到這邊，遊戲就結束，那互動的過程就結束了<br>
<br>
0:17:38.660,0:17:46.740<br>
那每一場遊戲，叫做一個 Episode，每一個互動，<br>
從頭到尾，在文獻上，我們叫它一個 Episode<br>
<br>
0:17:46.740,0:17:49.100<br>
在電玩裡面，每一場遊戲叫做一個 Episode<br>
<br>
0:17:49.100,0:17:52.440<br>
在圍棋裡面，每一局棋叫做一個 Episode<br>
<br>
0:17:52.680,0:17:57.380<br>
那在一個 Episode 中，我們把每一個 step 得到的 reward<br>
<br>
0:17:57.380,0:18:02.800<br>
r1 r2 r3 rT 都加起來，就叫做 total reward<br>
<br>
0:18:02.960,0:18:05.720<br>
那 total reward 我們就寫成大 R<br>
<br>
0:18:06.600,0:18:13.180<br>
那現在機器要做的事情是，<br>
我們希望大 R 就是 total reward 的值<br>
<br>
0:18:13.180,0:18:20.820<br>
在互動的過程中，越大越好<br>
<br>
0:18:21.440,0:18:26.920<br>
那我們再用另外一個簡化的圖，來說明一下 environment，actor，還有 reward function 之間的關係<br>
<br>
0:18:27.120,0:18:31.560<br>
Environment 輸出一個 state，其實就是遊戲的畫面<br>
<br>
0:18:31.780,0:18:36.220<br>
它也可以說是輸出一個 observation，我剛才講過 state 跟 observation 在我心裡面是一樣的東西<br>
<br>
0:18:36.420,0:18:43.880<br>
輸出一個遊戲畫面 s1，s1 輸入給 actor，<br>
actor 就輸出說，我要執行 a1<br>
<br>
0:18:43.880,0:18:48.220<br>
那再把 a1 輸給 environment ，<br>
environment 就說，我要執行 s2<br>
<br>
0:18:48.220,0:18:51.380<br>
s2 輸給 actor，actor 就說我要執行 a2<br>
<br>
0:18:51.380,0:18:55.140<br>
然後 environment 看你執行 a2 以後，<br>
它再說現在有新的畫面 s3<br>
<br>
0:18:55.140,0:18:57.100<br>
就這樣反覆執行下去<br>
<br>
0:18:57.340,0:18:59.340<br>
那 reward 怎麼計算呢？你有一個 reward function<br>
<br>
0:18:59.520,0:19:01.000<br>
就是這個遊戲的規則<br>
<br>
0:19:01.000,0:19:07.320<br>
結果 reward function 告訴我們說，在 state s1，採取 a1，你把 state s1 a1，丟到 reward function<br>
<br>
0:19:07.820,0:19:10.800<br>
你得到一個東西，數值是 r1，得到分數是 r1<br>
<br>
0:19:11.160,0:19:17.220<br>
在 s2 a2，在 s2 這個 state，<br>
採取 a2 這個行為，得到 reward 是 r2<br>
<br>
0:19:17.220,0:19:19.120<br>
以此類推<br>
<br>
0:19:19.120,0:19:22.900<br>
那我們如果把 state 跟 action 這個序列啊<br>
<br>
0:19:22.900,0:19:27.360<br>
就是在 state s1 採取 action a1，<br>
state 2 s2 採取 action a2 的這個序列<br>
<br>
0:19:27.360,0:19:33.640<br>
通通記錄起來，叫做一個 Trajectory<br>
<br>
0:19:34.940,0:19:39.800<br>
那在這個 Trajectory 裡面呢<br>
<br>
0:19:39.800,0:19:44.120<br>
這個 s，a 的序列叫做 Trajectory<br>
<br>
0:19:44.120,0:19:49.260<br>
那你把這些 reward 通通都加起來，<br>
就得到 total reward 大 R<br>
<br>
0:19:49.260,0:19:54.380<br>
或者你可以說，<br>
你得到某一個 trajectory tau 的 reward，R of tau<br>
<br>
0:19:54.660,0:20:06.440<br>
那我現在說我們目標，希望可以調整這個 actor，<br>
使得最後可以得到的 reward 越大越好<br>
<br>
0:20:06.840,0:20:09.900<br>
好，那接下來就是要說怎麼調整這個 actor<br>
<br>
0:20:09.960,0:20:15.340<br>
在講調整 actor 之前，<br>
我們當然要先來看一下 actor 長什麼樣子<br>
<br>
0:20:15.780,0:20:20.580<br>
好，那 actor 長什麼樣子了，<br>
actor 也是一個 neural network<br>
<br>
0:20:20.580,0:20:25.780<br>
所以其實這個 reinforcement learning <br>
從來都不是一個特別新的題目<br>
<br>
0:20:25.780,0:20:32.020<br>
現在我們講的這些技術，<br>
其實在 80 年代就已經有相當完整的版本<br>
<br>
0:20:32.020,0:20:36.940<br>
近年來，deep reinforcement  learning 突然又變得很紅，<br>
到底有什麼樣不一樣的地方<br>
<br>
0:20:37.020,0:20:41.100<br>
其實他最不一樣的地方就是，<br>
我們現在使用了 neural network<br>
<br>
0:20:41.380,0:20:46.300<br>
過去的 actor 通常都是查表<br>
而不是 neural network<br>
<br>
0:20:46.400,0:20:49.700<br>
長久以來，多數人都相信說<br>
<br>
0:20:49.700,0:20:55.320<br>
當我們把這個 actor 換成一個 <br>
non-linear 的 network 的時候，是無法 trained<br>
<br>
0:20:55.520,0:20:58.180<br>
它沒有辦法收斂，沒有辦法證明它會收斂<br>
<br>
0:20:58.180,0:21:00.860<br>
那怎麼辦，就不要用它？<br>
<br>
0:21:01.080,0:21:07.260<br>
可是後來就是 google 他們的貢獻就是，<br>
他們想了一大堆的 tip<br>
<br>
0:21:07.260,0:21:10.320<br>
讓這個 training 能夠真的 work 起來<br>
<br>
0:21:10.320,0:21:15.020<br>
所以現在，當我們講 actor 的時候，<br>
它其實都是一個 neural network<br>
<br>
0:21:16.020,0:21:20.060<br>
好，那這個 neural network <br>
它的輸入輸出分別是什麼呢？<br>
<br>
0:21:20.500,0:21:23.920<br>
actor，這個 neural network，<br>
它的輸入就是一個遊戲的畫面<br>
<br>
0:21:24.180,0:21:26.520<br>
那這個遊戲的畫面，就是由 pixel 組成的嘛<br>
<br>
0:21:26.520,0:21:30.140<br>
那你要處理影像，通常就是需要用到 CNN<br>
<br>
0:21:30.140,0:21:33.420<br>
所以這個 actor 它的前幾個 layer <br>
可能都是 CNN<br>
<br>
0:21:33.420,0:21:36.480<br>
為了要處理影像遊戲的畫面<br>
<br>
0:21:39.320,0:21:44.580<br>
好，那這個輸出呢，就是看你有幾個 actions<br>
<br>
0:21:44.580,0:21:48.580<br>
你輸出的每一個 neuron，就對應到一個 action<br>
<br>
0:21:48.580,0:21:56.300<br>
就假設說你現在可以採取的 actions，<br>
就是向左，向右跟開火<br>
<br>
0:21:56.660,0:22:01.200<br>
那你的這個 network，<br>
它的 output layer，就有 3 個 neuron<br>
<br>
0:22:01.200,0:22:07.180<br>
分別對應到向左，向右跟開火<br>
<br>
0:22:07.380,0:22:09.840<br>
好，哪假設現在輸入這個遊戲畫面<br>
<br>
0:22:10.000,0:22:14.740<br>
向左的分數是 0.7，向右的分數是 0.2，<br>
開火的分數是 0.1<br>
<br>
0:22:15.080,0:22:17.820<br>
那最後 actor 會決定採取哪一個 action 呢？<br>
<br>
0:22:17.820,0:22:24.380<br>
通常你會做一個 sampling，你就根據這個數值，<br>
產生一個 probability distribution<br>
<br>
0:22:24.380,0:22:29.800<br>
有 70% 機率向左，20%  機率向右，10% 機率開火<br>
<br>
0:22:30.540,0:22:34.600<br>
當然你也可以說，我不想要 stochastic 的 actor<br>
<br>
0:22:34.620,0:22:38.000<br>
我們剛剛有說 70% 機率向左，20% 機率向右，10% 機率開火<br>
<br>
0:22:38.000,0:22:42.140<br>
意味著說，你的 actor 在同一個畫面下，會採取不同行為<br>
<br>
0:22:42.380,0:22:44.960<br>
然後它是 stochastic，它每次採取的行為都不一樣<br>
<br>
0:22:45.600,0:22:50.000<br>
這樣當然有好處，<br>
這樣的好處就是你的對手比較不容易識破你要做的事情<br>
<br>
0:22:50.240,0:22:57.380<br>
當然如果你不喜歡這樣的話，你也可以說，看哪一個 action 得到的分數最高，我們就採取那個 action<br>
<br>
0:22:57.380,0:23:00.920<br>
這樣也可以，反正就 depend on 你要怎麼設計你的 actor<br>
<br>
0:23:00.920,0:23:02.380<br>
都是可以的<br>
<br>
0:23:03.660,0:23:08.920<br>
那過去其實會用一個 lookup table，<br>
來當作你的 actor<br>
<br>
0:23:08.920,0:23:15.520<br>
那用 lookup table 有什麼壞處呢？有人會說，如果用 network 參數會比較多<br>
<br>
0:23:15.520,0:23:16.800<br>
用 lookup table 參數會比較少<br>
<br>
0:23:16.800,0:23:18.720<br>
其實不是，用 lookup table 參數太多<br>
<br>
0:23:18.720,0:23:22.440<br>
用 lookup table 如果你 input 是遊戲畫面，<br>
根本無法處理<br>
<br>
0:23:23.260,0:23:28.200<br>
因為遊戲畫面是無窮無盡的，<br>
你根本無法窮舉所有可能發生的遊戲畫面<br>
<br>
0:23:28.200,0:23:29.940<br>
你用 lookup table 就不 work<br>
<br>
0:23:29.980,0:23:33.440<br>
但是如果你是用 neural network，<br>
就算是從來沒有看過的遊戲畫面<br>
<br>
0:23:33.440,0:23:36.560<br>
你也可以把遊戲畫面丟進去看它會得到什麼樣的結果<br>
<br>
0:23:36.900,0:23:40.320<br>
假如你這個 network train 的夠好，<br>
它有 generalization 的能力<br>
<br>
0:23:40.320,0:23:43.540<br>
那你也可能可以得到好的回應<br>
<br>
0:23:43.680,0:23:45.680<br>
那我們回到我們剛才要做的事情<br>
<br>
0:23:45.760,0:23:50.480<br>
我們說 actor/environment/reward，<br>
他們中間的互動就是這個樣子<br>
<br>
0:23:50.760,0:23:54.640<br>
那我們最想要做的事情是什麼，我們想要做的是事情是<br>
<br>
0:23:54.660,0:23:59.100<br>
希望調整 actor 的參數，<br>
注意一下我們都講說 actor 都是一個 neural network 嗎<br>
<br>
0:23:59.180,0:24:02.060<br>
neural network 就是輸入一個遊戲畫面，就是 state<br>
<br>
0:24:02.060,0:24:04.540<br>
output 就是現在要採取的一個 action a<br>
<br>
0:24:04.540,0:24:07.520<br>
所有遊戲畫面，採取一個 action a<br>
<br>
0:24:07.620,0:24:10.840<br>
actor 是一個 neural network<br>
<br>
0:24:11.380,0:24:14.700<br>
那我們希望說，現在能夠達成的目標<br>
<br>
0:24:14.820,0:24:19.520<br>
是希望整個 episode 所有 reward 合起來，<br>
它得到 total reward 大 R 的值<br>
<br>
0:24:19.520,0:24:21.140<br>
越大越好<br>
<br>
0:24:21.140,0:24:26.920<br>
你，如果看這個圖，你知道怎麼解這個問題嗎？<br>
<br>
0:24:27.040,0:24:30.980<br>
仔細想想，這個問題應該沒有那麼難<br>
對不對<br>
<br>
0:24:31.020,0:24:37.040<br>
你想想看，假設<br>
<br>
0:24:37.660,0:24:41.280<br>
reward function 也是一個 neural network<br>
<br>
0:24:41.860,0:24:43.860<br>
假設 reward function 也是一個 neural network<br>
<br>
0:24:43.860,0:24:48.320<br>
這個 neural network 就是輸入 <br>
state and action 給你一個分數<br>
<br>
0:24:48.320,0:24:50.860<br>
假設 environment 也是一個 neural network<br>
<br>
0:24:50.860,0:24:54.080<br>
給它一個 action 它就輸出一個 state<br>
<br>
0:24:54.560,0:24:58.360<br>
這一整個畫面上，這三個neural network 串起來<br>
<br>
0:24:58.360,0:25:01.480<br>
不就只是一個巨大的 neural network 而已嗎？<br>
<br>
0:25:01.860,0:25:06.440<br>
它可能在這邊有一個類似隨機的 random seed<br>
<br>
0:25:06.440,0:25:08.980<br>
當作 input 去決定初始的畫面是什麼<br>
<br>
0:25:09.340,0:25:12.560<br>
然後最後 output 就是一個數值<br>
<br>
0:25:12.660,0:25:17.040<br>
然後我們現在這個目標，<br>
就是希望這個數值，越大越好<br>
<br>
0:25:17.720,0:25:18.920<br>
大家可以想像啊<br>
<br>
0:25:18.920,0:25:24.120<br>
這是一個巨大的，假設這些東西都是 network<br>
<br>
0:25:24.120,0:25:30.400<br>
把它們通通串起來，你就只是有一個巨大的 network<br>
然後你希望它的輸出，越大越好<br>
<br>
0:25:31.100,0:25:35.240<br>
那我們剛才有講說 <br>
reward and environment 你是動不了的<br>
<br>
0:25:35.240,0:25:39.560<br>
那是別人的東西，你動不了，<br>
你唯一能調的只有 actor 參數<br>
<br>
0:25:39.880,0:25:45.320<br>
在這個巨大的 network 裡面，<br>
只有藍色的部分的參數是你是可以調的<br>
<br>
0:25:45.360,0:25:52.360<br>
你要調藍色的這部分的參數，<br>
使得最終輸出的值，越大越好<br>
<br>
0:25:52.360,0:25:56.380<br>
這個你會做嗎？<br>
這不就只是 Gradient ascent 而已嗎？<br>
<br>
0:25:57.020,0:26:00.720<br>
你想想看，你想讓這個 R 越大越好<br>
<br>
0:26:00.900,0:26:07.780<br>
然後你從這個地方，一路 back propagate 回來<br>
<br>
0:26:07.840,0:26:13.160<br>
然後你就可以調這個 actor 的參數，<br>
希望最終讓的 output 越大越好<br>
<br>
0:26:13.180,0:26:16.260<br>
這樣大家可以接受這個想法嗎？<br>
<br>
0:26:16.560,0:26:19.720<br>
你自己想想看，<br>
這東西聽起來跟 GAN 也蠻像的，那講 GAN 的時候<br>
<br>
0:26:19.720,0:26:25.080<br>
我們說有一個 generator，<br>
然後 generator 會把它的輸出接給 discriminator<br>
<br>
0:26:25.080,0:26:29.160<br>
然後 discriminator 要調整它的參數，<br>
讓輸出的值越大越好<br>
<br>
0:26:29.240,0:26:32.340<br>
那現在我們有 environment，有 reward，有 actor<br>
<br>
0:26:32.340,0:26:35.700<br>
這個 environment 跟 reward，<br>
你可以想成是 discriminator<br>
<br>
0:26:35.700,0:26:38.820<br>
我們要調 actor 的參數，actor 你可以想成是 generator<br>
<br>
0:26:38.820,0:26:41.400<br>
我們要調 actor 參數，讓它輸出的東西<br>
<br>
0:26:41.400,0:26:45.040<br>
通過 reward function 以後，output 的值，越大越好<br>
<br>
0:26:45.060,0:26:49.100<br>
這個不是跟前面 GAN 做的事情，非常地像嗎？<br>
<br>
0:26:50.280,0:26:52.280<br>
那這個東西你會不會做呢？<br>
<br>
0:26:52.440,0:26:56.100<br>
假設 reward 是一個 network，environment 是一個 network<br>
<br>
0:26:56.100,0:27:00.640<br>
用 back propagation 一路 back propagate 回去，<br>
就可以調 actor 參數，去 maximize 最終的 output<br>
<br>
0:27:00.640,0:27:03.500<br>
這個我假設你是會做的<br>
<br>
0:27:03.600,0:27:07.300<br>
那現在的難點是什麼，現在的難點是<br>
<br>
0:27:07.300,0:27:10.380<br>
這個 environment 跟 reward <br>
就不是 neural network 啊<br>
<br>
0:27:10.380,0:27:12.780<br>
它是個黑盒子<br>
<br>
0:27:12.800,0:27:14.560<br>
你根本不知道它是什麼東西<br>
<br>
0:27:14.560,0:27:17.140<br>
如果下圍棋的話，你的 environment 是個對手<br>
<br>
0:27:17.140,0:27:21.100<br>
它其實也是個 neural network 啦，<br>
不過你沒有辦法把它剖開來看就是了<br>
<br>
0:27:21.440,0:27:25.720<br>
如果下圍棋的話，<br>
你的 reward 就是圍棋的規則，那它很複雜<br>
<br>
0:27:25.720,0:27:28.760<br>
你可能也不知道怎麼<br>
把它用一個 neural network 來表示它<br>
<br>
0:27:29.800,0:27:34.520<br>
現在真正遇到的問題是，我們想要 maximize 這個 R<br>
<br>
0:27:34.660,0:27:42.220<br>
但是 reward function 它的參數我們不知道<br>
environment function 的參數，我們也不知道<br>
<br>
0:27:42.560,0:27:44.520<br>
怎麼辦？<br>
<br>
0:27:44.520,0:27:49.260<br>
本來如果這兩個東西就是 neural network，<br>
它是我們知道怎麼調整參數去 maximize 它<br>
<br>
0:27:49.260,0:27:50.680<br>
但實際上不知道<br>
<br>
0:27:50.680,0:27:53.660<br>
那怎麼辦呢，這邊遇到記得一個口訣<br>
<br>
0:27:53.660,0:27:59.220<br>
那個口訣就是，<br>
如果你今天發現你要 optimize 的 function 不能微分的話<br>
<br>
0:27:59.220,0:28:03.320<br>
就用 policy gradient 這個技術硬 train 一發，<br>
就結束了<br>
<br>
0:28:03.320,0:28:12.580<br>
如果你想要知道 policy gradient 是什麼的話，<br>
請參看下面這個連結<br>
<br>
0:28:12.840,0:28:16.980<br>
今天只要記得說，如果有叫你 maximize 一個東西<br>
<br>
0:28:16.980,0:28:18.920<br>
比如說我要 maximize 這個 R<br>
<br>
0:28:19.360,0:28:24.140<br>
但問題就是這個東西，<br>
有這個 environment 跟 reward，導致我們無法微分<br>
<br>
0:28:24.140,0:28:27.800<br>
那怎麼辦，反正就是有一招，叫做 policy gradient<br>
<br>
0:28:27.800,0:28:32.220<br>
它可以去調這個 actor 的參數<br>
讓我們最終可以 output 這個 R，就結束了<br>
<br>
0:28:32.540,0:28:40.280<br>
這個就是 reinforcement learning 做的事情<br>
<br>
0:28:43.020,0:28:48.080<br>
我知道這跟你一般平常聽到的<br>
 reinforcement learning 講法有點不太一樣<br>
<br>
0:28:48.160,0:28:55.040<br>
通常如果你看 David Silver 的 video，<br>
通常先從 Markov decision process 開始講<br>
<br>
0:28:55.040,0:29:00.420<br>
然後等你聽完 Markov decision process，以為自己聽懂了，然後就開始想睡了，然後剩下的東西，你都聽不懂了<br>
<br>
0:29:00.420,0:29:05.160<br>
然後再講一次，其實你也是聽不懂，<br>
所以這邊是採取一個不太一樣的講法<br>
<br>
0:29:05.160,0:29:09.420<br>
告訴你，其實 reinforcement learning 做的事情，<br>
就是這樣<br>
<br>
0:29:10.600,0:29:31.180<br>
Q&A 時間<br>
<br>
0:29:31.380,0:29:35.320<br>
好那我們就不要講 policy gradient 的部分，<br>
你只要記得這個口訣<br>
<br>
0:29:35.320,0:29:41.020<br>
就是發現只要是不能微分的東西，<br>
policy gradient 就是可以幫你 optimize 就是了<br>
<br>
0:29:41.520,0:29:44.880<br>
那 policy gradient，這邊其實也不是推導啦<br>
<br>
0:29:44.880,0:29:49.880<br>
這邊只是要講實作是怎麼做的，<br>
推導的部分，還有實做具體怎麼做<br>
<br>
0:29:49.880,0:29:54.760<br>
永遠可以看底下我線上課程的錄影<br>
<br>
0:29:55.240,0:30:01.040<br>
這個部分，就把它跳過<br>
<br>
0:30:01.240,0:30:05.900<br>
好，那我們剛才講了 actor 怎麼 train<br>
<br>
0:30:06.000,0:30:10.400<br>
接下來我們要講另外一個東西，這個東西叫做 critic<br>
<br>
0:30:10.700,0:30:22.920<br>
Critic 做的事情是什麼，<br>
Critic 本身並沒有辦法決定要採取哪一個 action<br>
<br>
0:30:22.960,0:30:26.520<br>
那 Critic 可以做的事情是什麼呢？Critic 可以做的事情是<br>
<br>
0:30:26.520,0:30:32.700<br>
給它一個 actor pi，<br>
它可以告訴你說這個 actor pi 有多好<br>
<br>
0:30:32.960,0:30:36.800<br>
什麼叫做這個 actor pi 有多好呢？<br>
這個 Critic 其實有很多種<br>
<br>
0:30:36.800,0:30:42.620<br>
我們今天介紹一個 state value function，寫成 V(pi) of x<br>
<br>
0:30:42.860,0:30:49.700<br>
它做的事情是，給它一個 actor pi，<br>
它告訴你說，現在在給一個 actor pi 的前提下<br>
<br>
0:30:49.700,0:30:54.580<br>
假設我們看到一個 observation or state s<br>
<br>
0:30:54.880,0:31:03.180<br>
接下來，一直到遊戲結束的時候，<br>
會得到 reward 的總和期望值有多大<br>
<br>
0:31:03.220,0:31:09.360<br>
注意一下，今天我們算的，<br>
並不是看到這個 state 之後，下一秒會得到的 reward<br>
<br>
0:31:09.360,0:31:15.960<br>
而是看到這個 state 以後，<br>
所有 accumulated 的 reward 的期望值<br>
<br>
0:31:16.200,0:31:19.640<br>
或者舉例來說，以下圍棋為例的話<br>
<br>
0:31:19.640,0:31:24.440<br>
這個 V(pi) of x 的意思就是說，<br>
假設你已經有一個下圍棋的 agent<br>
<br>
0:31:24.440,0:31:25.700<br>
叫做 pi<br>
<br>
0:31:25.780,0:31:29.620<br>
那你現在給它一個 observation，就是棋盤的盤勢<br>
<br>
0:31:29.840,0:31:33.880<br>
比如說，出手天元<br>
<br>
0:31:33.920,0:31:40.240<br>
接下來 V(pi) of x 的意思就是說，<br>
從出手下到天元，一直到遊戲結束為止<br>
<br>
0:31:40.240,0:31:46.360<br>
假設今天在圍棋裡面，遊戲結束，<br>
贏了就得到分數 1，輸了就得到分數 -1<br>
<br>
0:31:46.380,0:31:50.900<br>
在其他的狀況下，得到的分數都是 0<br>
<br>
0:31:50.920,0:31:55.080<br>
那 V(pi) of x，x 是出手天元，假設 x 出手天元的話<br>
<br>
0:31:55.080,0:32:02.600<br>
V(pi) of x 就是 假設出手下在天元，<br>
接下來獲勝的機率有多大，就是 V(pi) of x<br>
<br>
0:32:02.600,0:32:08.140<br>
或出手下在天元，你的 actor 是 pi，<br>
那你獲勝的機率有多大<br>
<br>
0:32:08.140,0:32:10.180<br>
這個就是 V(pi) of x<br>
<br>
0:32:10.500,0:32:14.900<br>
而透過圖像化的方式來畫它的話，<br>
就是有一個 function 叫做 V(pi)<br>
<br>
0:32:14.900,0:32:18.740<br>
給它一個 state，然後它就會 output 一個數值<br>
<br>
0:32:18.740,0:32:20.260<br>
叫做 V(pi) of x<br>
<br>
0:32:20.260,0:32:24.840<br>
那這個數值代表什麼？這個數值就代表了說，這個 actor<br>
<br>
0:32:24.840,0:32:30.480<br>
假設我們用 pi 這個 actor，<br>
在 state s 的時候，接下來，看到 state s<br>
<br>
0:32:30.480,0:32:34.380<br>
接下來它會得到的 reward 期望值有多大<br>
<br>
0:32:34.460,0:32:38.320<br>
舉例來說，假設你有一個很強的 actor<br>
<br>
0:32:38.320,0:32:41.500<br>
然後它看到這個遊戲的畫面，接下來還有很多怪<br>
<br>
0:32:41.500,0:32:44.660<br>
因為它有很多怪可以殺，<br>
所以接下來它可以得到很高的分數<br>
<br>
0:32:44.660,0:32:46.020<br>
所以 V(pi) of s 就很大<br>
<br>
0:32:46.140,0:32:51.020<br>
這邊舉另外一個例子，遊戲畫面剩下怪已經很少了<br>
<br>
0:32:51.080,0:32:54.700<br>
V(pi) of s 就會比較小，因為它到遊戲結束的時候<br>
<br>
0:32:54.700,0:32:58.500<br>
可以得到的分數就比較少了，<br>
因為其他怪都已經被殺完了<br>
<br>
0:32:58.500,0:33:00.880<br>
剩下的怪，已經很少了<br>
<br>
0:33:01.060,0:33:06.300<br>
這邊有一件事你要特別注意，<br>
當你看到這種 Critic 的時候<br>
<br>
0:33:06.300,0:33:11.480<br>
Critic 都是 depend on actor 的，<br>
給不同的 actor 就算是同樣的 state<br>
<br>
0:33:11.480,0:33:13.260<br>
Critic 的 output 也是不一樣的<br>
<br>
0:33:13.260,0:33:18.540<br>
所以我們說 Critic 的工作，就是衡量一個 actor 好不好<br>
<br>
0:33:18.540,0:33:23.040<br>
所以給它不同的 actor，就算是同一個 state<br>
<br>
0:33:23.040,0:33:25.560<br>
它得到的分數也是不一樣<br>
<br>
0:33:25.820,0:33:31.560<br>
我們舉一個棋靈王的例子，<br>
這例子是這樣子（只註記提到 Critic 部分）<br>
<br>
0:34:12.780,0:34:17.420<br>
所以如果把它對應到 Critic 的話<br>
<br>
0:34:17.420,0:34:22.420<br>
如果 actor 是以前的阿光，<br>
那你不應該下，大馬步飛<br>
<br>
0:34:22.420,0:34:26.060<br>
以前的阿光，<br>
如果 state 是下大馬步飛，是壞的<br>
<br>
0:34:26.060,0:34:32.200<br>
因為以前的阿光比較弱，下大馬步飛的話，<br>
這個比較複雜，所以會下的不好<br>
<br>
0:34:32.280,0:34:38.140<br>
但是因為阿光它後來變強了，所以如果是要 evaluate 變強的阿光這個 actor 的話<br>
<br>
0:34:38.140,0:34:40.320<br>
下大馬步飛，就會變得好<br>
<br>
0:34:40.320,0:34:46.900<br>
這個所要強調的意思是說，<br>
今天你的 Critic 其實會隨著 actor 的不同<br>
<br>
0:34:46.900,0:34:49.560<br>
而得到不同的分數<br>
<br>
0:34:52.960,0:34:58.140<br>
怎麼算這個 Critic，怎麼評估這個 Critic<br>
<br>
0:34:58.220,0:35:03.980<br>
有兩個方法，一個是 Monte-Carlo 的方法，<br>
另外一個是 Temporal-Difference 的方法<br>
<br>
0:35:04.140,0:35:08.720<br>
那 Monte-Carlo 的方法其實非常的直觀，它就是說<br>
<br>
0:35:08.720,0:35:13.700<br>
今天 Critic 怎麼衡量一個 actor 好不好，<br>
它就去看那個 actor 玩遊戲<br>
<br>
0:35:13.700,0:35:16.620<br>
假設我們以玩遊戲為例子，打電玩為例子<br>
<br>
0:35:16.620,0:35:23.340<br>
Critic 就去看那個 actor pi 玩遊戲，<br>
看 actor pi 玩得怎麼樣<br>
<br>
0:35:23.400,0:35:30.560<br>
那假設現在 Critic 觀察到說，<br>
actor pi 在經過這個 state Sa 以後<br>
<br>
0:35:30.560,0:35:35.680<br>
它會得到的 accumulated 的 reward，這從 state Sa 之後<br>
<br>
0:35:35.980,0:35:42.200<br>
這個 actor pi 它會得到的 reward 是 Ga，<br>
那這個 Critic 就要學說<br>
<br>
0:35:42.200,0:35:47.360<br>
如果 input state Sa，那我的 output 要跟 Ga 越接近越好<br>
<br>
0:35:47.360,0:35:51.080<br>
這不過是一個 regression 的問題嘛<br>
<br>
0:35:52.020,0:35:56.180<br>
這個 actor 要調它的參數，<br>
那它的 output 跟 Ga 越接近越好<br>
<br>
0:35:56.260,0:36:00.780<br>
那假設又觀察到說，現在 actor 跑到 state b<br>
<br>
0:36:00.780,0:36:04.560<br>
玩到遊戲結束的時候，會得到 accumulated reward Gb<br>
<br>
0:36:04.560,0:36:11.180<br>
那現在輸入 state b<br>
<br>
0:36:11.180,0:36:16.340<br>
那它的 output 就要跟 Gb 越接近越好<br>
<br>
0:36:16.840,0:36:20.140<br>
這個很直觀，這個就是 Monte-Carlo 的方法<br>
<br>
0:36:20.400,0:36:24.480<br>
另外一種叫做不直觀的方法，<br>
叫做 Temporal-Difference 的方法<br>
<br>
0:36:24.660,0:36:31.220<br>
Temporal-Difference 方法是說，<br>
現在一樣讓 Critic 去看 actor 玩遊戲<br>
<br>
0:36:31.220,0:36:33.780<br>
那 Critic 看到 actor 在做什麼呢？<br>
<br>
0:36:33.780,0:36:42.320<br>
它看到 actor 在 state st 採取 action at，<br>
接下來得到 reward rt，然後跳到 state s(t+1)<br>
<br>
0:36:42.500,0:36:48.920<br>
光看到這樣一個 data，一筆 data，<br>
之前我們在前一頁做 Monte-Carlo 的時候<br>
<br>
0:36:48.920,0:36:53.960<br>
看到某一個 state，我們必須要一直玩到遊戲結束，<br>
才知道 accumulated reward 是多少<br>
<br>
0:36:54.040,0:36:57.380<br>
但是在 temporal-difference 的時候，<br>
只要看這樣一筆 data<br>
<br>
0:36:57.940,0:37:02.800<br>
那個，Critic 就可以學了，<br>
actor 只要在某一個 state 採取某一個行為<br>
<br>
0:37:02.800,0:37:04.060<br>
Critic 就可以學了<br>
<br>
0:37:04.200,0:37:07.400<br>
為什麼Critic 就可以學呢，它就是 based on 這個式子<br>
<br>
0:37:07.660,0:37:10.820<br>
因為我們現在 V(pi) of s(t)<br>
<br>
0:37:10.820,0:37:14.900<br>
是要衡量在 s(t) 這個 state <br>
會得到的 accumulated 的 reward<br>
<br>
0:37:15.220,0:37:16.480<br>
V(pi) of s(t+1)<br>
<br>
0:37:16.480,0:37:20.620<br>
是要衡量在 s(t+1) 這個 state <br>
會得到的 accumulated 的 reward<br>
<br>
0:37:20.640,0:37:24.300<br>
今天如果我們觀察到，在 s(t) 這個 state<br>
<br>
0:37:24.300,0:37:27.400<br>
會得到 reward r(t)，跳到 s(t+1)<br>
<br>
0:37:27.400,0:37:34.860<br>
意味著說，<br>
在 s(t+1) 和 s(t) 中間它們差了 reward 就是 r(t)<br>
<br>
0:37:34.860,0:37:39.040<br>
這一項，你會得到 accumulated reward 是這一項<br>
<br>
0:37:39.040,0:37:44.340<br>
然後這一項，你得到 accumulated reward 是這一項<br>
<br>
0:37:44.340,0:37:50.540<br>
他們中間經過了得到 reward r(t) 這一件事，<br>
所以他們中間的差異，就是 r(t)<br>
<br>
0:37:50.880,0:37:53.840<br>
那你在訓練 network 的時候怎麼辦呢？<br>
<br>
0:37:53.840,0:38:03.820<br>
訓練 network 你就說，現在把 s(t) 丟進去，你就會得到一個分數 ，把 s(t+1) 丟進去，你會得到另外一個分數<br>
<br>
0:38:04.060,0:38:09.060<br>
我們希望這兩個分數的差，跟 r(t) 越接近越好<br>
<br>
0:38:09.060,0:38:14.660<br>
所以現在的訓練目標，你不知道這個實際上的是值多少，你不知道這個實際上的是值多少，不知道<br>
<br>
0:38:14.660,0:38:18.160<br>
因為還沒有玩到遊戲結束嘛，<br>
你不知道 accumulated reward 是多少<br>
<br>
0:38:18.320,0:38:21.000<br>
你學到一件事，你學到的事情是<br>
<br>
0:38:21.000,0:38:25.020<br>
雖然我不知道他們值是多少，但我知道它們差了 r(t)<br>
<br>
0:38:25.020,0:38:28.480<br>
所有就告訴 machine 說，看到 s(t) 你輸出的值<br>
<br>
0:38:28.480,0:38:34.000<br>
跟你看到 s(t+1) 你輸出的值，<br>
中間要差了 r(t)，然後 learn 下去，就結束了<br>
<br>
0:38:34.180,0:38:38.400<br>
那用 temporal-difference，<br>
有一個非常明確的好處，就是<br>
<br>
0:38:38.400,0:38:42.200<br>
今天當遊戲還沒有結束，玩到一半的時候<br>
<br>
0:38:42.200,0:38:45.280<br>
你就可以開始 update 你的 network<br>
<br>
0:38:45.280,0:38:49.280<br>
那有時候有些遊戲非常的長，如果你沒有辦法一邊玩遊戲<br>
<br>
0:38:49.280,0:38:57.220<br>
一邊 update 你的 network 的話，那你會搞太久，<br>
所以在 temporal-difference 是有它的好處的<br>
<br>
0:38:57.980,0:39:14.700<br>
略過<br>
<br>
0:39:15.900,0:39:19.760<br>
接下來，我們剛才講了一個 critic，<br>
這個 critic 是給它一個 state<br>
<br>
0:39:19.760,0:39:25.300<br>
它會衡量說，這個 state 到遊戲結束的時候<br>
會得到多少的 reward<br>
<br>
0:39:25.460,0:39:29.360<br>
但那一種 critic 沒有辦法拿來決定 action<br>
<br>
0:39:29.880,0:39:37.380<br>
但是有另外一種 critic，它可以拿來決定 action，<br>
這種 critic 我們叫做 Q function<br>
<br>
0:39:37.480,0:39:45.940<br>
這種 Q function 它的 input 就是一個 state，一個 action<br>
<br>
0:39:45.940,0:39:52.620<br>
那它到底在量什麼，它量的就是，給我一個 actor pi<br>
<br>
0:39:52.620,0:39:56.060<br>
然後量說，在給我一個 actor pi 的前提之下，<br>
<br>
0:39:56.140,0:40:02.960<br>
在 observation s，採取了 action a<br>
<br>
0:40:02.960,0:40:10.920<br>
在這個 state 採取了 action a 的話，到遊戲結束的時候，會得到多少 accumulated reward<br>
<br>
0:40:11.020,0:40:15.920<br>
之前第一它只量在 s 的時候，會得到多少的 reward<br>
<br>
0:40:15.920,0:40:20.740<br>
現在是量測，在 s 採取了 a 會得到多少 的 reward<br>
<br>
0:40:20.740,0:40:26.420<br>
當你採取的 action 不同，非常直觀，<br>
你採取 action 不同，你得到的 reward 就不一樣嘛<br>
<br>
0:40:26.520,0:40:30.400<br>
之前它沒量這件事，它沒量說你會採取哪一個 action<br>
<br>
0:40:30.720,0:40:36.300<br>
今天 Q 會量說，在 state s 採取 action a 的時候，<br>
會得到多少的 reward<br>
<br>
0:40:37.000,0:40:41.060<br>
那所以 Q 呢，理論上它會有兩個 input，s 跟 a<br>
<br>
0:40:41.060,0:40:46.380<br>
它吃 s 跟 a，決定說它要得到多少的分數<br>
<br>
0:40:46.720,0:40:51.600<br>
那有時候我們會改寫這個 Q function，<br>
假設你的 a 是可以窮舉的<br>
<br>
0:40:51.600,0:40:56.060<br>
舉例來說，在玩遊戲的時候，<br>
a 只有向左／向右，跟開火三個選擇<br>
<br>
0:40:56.160,0:41:00.100<br>
那你就可以說，我們現在呢，任一個 Q function<br>
<br>
0:41:00.100,0:41:02.660<br>
我們的 Q function 是 input 一個 state s<br>
<br>
0:41:02.660,0:41:10.820<br>
它的 output 分別就是 Q(pi) of (s, a=left)，<br>
Q(pi) of (s, a=right)，Q(pi) of (s, a=fire)<br>
<br>
0:41:10.940,0:41:15.020<br>
這樣的好處就是，你只要輸入一個 state s ，<br>
你就可以知道說<br>
<br>
0:41:15.020,0:41:22.440<br>
s 配上，向左的時候，分數是多少，s 配上向右的時候，分數是多少，s 配上開火的時候，分數是多少<br>
<br>
0:41:22.620,0:41:27.120<br>
那在這個 case 你必須要把不同的 a 一個一個帶進去，<br>
你才可以算出它的 Q function<br>
<br>
0:41:27.300,0:41:35.180<br>
但在這個 case，你只要 s 帶進去，<br>
就可以一次把不同的 action，它的分數都算出來<br>
<br>
0:41:35.820,0:41:39.220<br>
那有了這個 Q 有什麼用呢？<br>
<br>
0:41:39.220,0:41:47.140<br>
它的妙用是這個樣子，<br>
你可以用 Q function 找出一個比較好的 actor<br>
<br>
0:41:47.140,0:41:49.820<br>
這一招就叫做 Q learning<br>
<br>
0:41:50.300,0:41:58.240<br>
所以這個 Q learning 的整個 process 是這樣，<br>
你有一個已經初始的 actor pi<br>
<br>
0:41:58.240,0:42:02.260<br>
然後這個 actor pi，去玩，去跟這個環境互動<br>
<br>
0:42:02.280,0:42:07.180<br>
然後我們說，critic 的工作就是去觀察，<br>
 這個 actor pi 它跟環境的互動<br>
<br>
0:42:07.180,0:42:12.360<br>
那它可以透過 TD or MC 的方法，去學出這個 Q function<br>
<br>
0:42:12.360,0:42:18.380<br>
它可以去估測說，根據這個 pi 跟環境互動的資料，<br>
用 TD or MC 的方法<br>
<br>
0:42:18.380,0:42:22.300<br>
你可以估測說，現在給定這個 actor 的前提之下<br>
<br>
0:42:22.300,0:42:26.180<br>
在某一個 state 採取某一個 action，<br>
得到的 Q value 是多少<br>
<br>
0:42:26.180,0:42:35.480<br>
假設估出這種 Q function 以後，估測出這種 critic 以後，可以保證一件事，細節我們下一頁投影片講<br>
<br>
0:42:35.480,0:42:41.340<br>
可以保證什麼事？可以保證我們說，<br>
我們一定能夠找到一個新的 actor pi，<br>
<br>
0:42:41.520,0:42:46.140<br>
它比原來的 pi 更好，我們本來有一個 actor pi<br>
<br>
0:42:46.140,0:42:50.960<br>
我們就觀察它，去跟環境互動的狀況，然後我們估測出<br>
<br>
0:42:50.960,0:42:57.320<br>
我們認出一個 critic，它估測 actor pi，<br>
在某一個 state 採取某一個 action 的時候<br>
<br>
0:42:57.320,0:42:59.180<br>
會得到的分數<br>
<br>
0:42:59.400,0:43:02.320<br>
然後接下來，有了這個 Q 以後<br>
<br>
0:43:02.320,0:43:06.180<br>
保證我們可以找到一個，另外一個新的 actor pi prime<br>
<br>
0:43:06.180,0:43:11.080<br>
它比原來的 pi 還要好，<br>
這樣我們是不是就找到一個比較好的 actor 了<br>
<br>
0:43:11.200,0:43:17.360<br>
所以你有這個比較好的 actor pi prime 以後，<br>
你就把這個 pi 用 pi prime 取代掉<br>
<br>
0:43:17.360,0:43:24.080<br>
你有新的 actor，觀察一下，<br>
量出新的 actor 的 Q function，再找到一個更好的 actor<br>
<br>
0:43:24.080,0:43:26.620<br>
本來是 pi prime，那現在就變成 pi double prime，<br>
<br>
0:43:26.660,0:43:30.020<br>
然後再重新來一次，那你的 actor 是不是越找越好？<br>
<br>
0:43:30.080,0:43:33.240<br>
那這就是我們要的，你就可以找到越來越好的<br>
<br>
0:43:33.540,0:43:38.300<br>
所以 Q function 的精神就是這樣，<br>
重點的地方就是這一步<br>
<br>
0:43:38.500,0:43:42.240<br>
就這個打問號這一步，只要量得出 Q function<br>
<br>
0:43:42.240,0:43:46.800<br>
接下來就一定可以找到一個更好的 actor pi prime<br>
<br>
0:43:46.920,0:43:49.760<br>
好，那這件事到底是怎麼說的呢？<br>
<br>
0:43:49.880,0:43:57.340<br>
它的理論就是這個樣子，它的理論是說，<br>
證明在下一頁，證明我就不要講這樣子<br>
<br>
0:43:57.520,0:44:02.720<br>
理論是這樣，理論上是說，<br>
什麼叫做 pi prime 一定比 pi 好<br>
<br>
0:44:02.720,0:44:11.340<br>
pi prime 比 pi 好的定義是說，給所有可能的 state s<br>
<br>
0:44:11.600,0:44:16.040<br>
如果你用 pi 去玩這個遊戲，得到的 reward<br>
<br>
0:44:16.040,0:44:19.940<br>
用 pi 去玩遊戲得到的 accumulated reward，<br>
一定會小於<br>
<br>
0:44:19.940,0:44:25.920<br>
我們已經定義過 V 了嘛，這就是為什麼前面要講 V，<br>
就是為了要講這一個啦，V 的定義大家已經知道了<br>
<br>
0:44:26.180,0:44:30.540<br>
給所有可能的 state s，如果你採取 pi 這個 actor<br>
<br>
0:44:30.540,0:44:35.440<br>
跟你採取 pi prime 這個 actor，<br>
pi prime 這個 actor 得到的 accumulated reward<br>
<br>
0:44:35.440,0:44:39.640<br>
一定會大過 pi 這個 actor，<br>
所以 pi prime 會得到 reward 一定會比 pi 大<br>
<br>
0:44:39.640,0:44:44.480<br>
不管是哪一個 state，<br>
那就代表 pi prime 是一個比較好的 actor<br>
<br>
0:44:44.780,0:44:49.620<br>
那怎麼根據這個 Q 找到一個比較好的 actor pi prime 呢？<br>
<br>
0:44:49.680,0:44:53.500<br>
它的原理就是只有下面這條式子<br>
<br>
0:44:53.860,0:44:56.460<br>
這個比較好的 actor pi prime 怎麼來啊？<br>
<br>
0:44:56.460,0:45:01.060<br>
這個 pi prime 就是，給你一個 Q function<br>
<br>
0:45:01.060,0:45:06.420<br>
這個 Q function 是拿來衡量 pi 這個 actor 的 Q function<br>
<br>
0:45:06.520,0:45:09.600<br>
然後我們說，給某一個 state 的時候<br>
<br>
0:45:09.600,0:45:14.280<br>
窮舉所有可能的 action，<br>
看哪一個 action 的 Q value 最大<br>
<br>
0:45:14.280,0:45:17.820<br>
把那個 action 當作新的 actor，pi prime 的輸出<br>
<br>
0:45:17.900,0:45:22.320<br>
就我們一個新的 actor pi prime，但它其實是空的，它其實不太會做決定，它怎麼做決定<br>
<br>
0:45:22.320,0:45:26.960<br>
它都聽 Q 的，它自己其實沒有參數，它都聽 Q 的<br>
<br>
0:45:26.960,0:45:31.100<br>
所以你說給你一個 state s，<br>
pi prime 你想要做什麼樣的行為呢？<br>
<br>
0:45:31.100,0:45:37.140<br>
它就說，那我們把 Q 找出來，<br>
Q 其實只看過 pi ，它是看 pi 的<br>
<br>
0:45:37.280,0:45:41.440<br>
Q 是說，它之前看過 pi 做過的事情，它知道說<br>
<br>
0:45:41.440,0:45:45.960<br>
pi 這個 actor 在 s 採取 a 的時候，會得到多少的 reward<br>
<br>
0:45:45.960,0:45:50.180<br>
然後它說窮舉所有可能的 a，<br>
看看哪一個 a 可以讓這個 function 最大<br>
<br>
0:45:50.180,0:45:54.740<br>
然後這個 a，pi prime 就說這就是它的輸出了，<br>
然後就結束了<br>
<br>
0:45:54.880,0:45:58.020<br>
然後這個 pi prime 呢，就一定會比 pi 還要好<br>
<br>
0:46:00.680,0:46:05.440<br>
這邊有一個顯然的問題是什麼問題？<br>
就是你怎麼解這個 arg max 的 problem<br>
<br>
0:46:05.760,0:46:10.940<br>
如果 a 是 discrete 的，只有向左，向右，開火，<br>
你就只需要把向左，向右，開火<br>
<br>
0:46:10.940,0:46:14.380<br>
分別帶到 Q function 裡面，看你會得到什麼樣的結果<br>
<br>
0:46:14.380,0:46:18.540<br>
但是比較慘的地方是，Q learning 好像聽起來很厲害<br>
<br>
0:46:18.540,0:46:22.400<br>
但是如果今天你的 action 無法窮舉<br>
<br>
0:46:22.400,0:46:27.360<br>
它是 continuous 的，你就爆炸了，<br>
你就不能解這個 arg max 的 problem 了<br>
<br>
0:46:27.620,0:46:36.020<br>
那至於這個理論的證明，其實還蠻簡單，<br>
一頁就可以講完，但我們就不要講這個了<br>
<br>
0:46:36.060,0:46:40.080<br>
那 Q 怎麼量，你就可以用 TD 來量<br>
<br>
0:46:40.360,0:46:44.080<br>
那其實 Q learning 有非常非常多的 trick 啦<br>
<br>
0:46:44.080,0:46:52.540<br>
你要怎麼找那些 trick 呢？<br>
你就去找一篇 google paper 叫做 rainbow<br>
<br>
0:46:52.540,0:46:58.380<br>
然後裡面就講了，7 種不同的 DQN 的 tip<br>
<br>
0:46:58.740,0:47:01.900<br>
因為正好 7 種，就對應到彩虹的 7 個顏色<br>
<br>
0:47:01.900,0:47:06.340<br>
他在做圖的時候，每一個方法就對應到彩虹的一個顏色<br>
<br>
0:47:06.340,0:47:11.580<br>
所以他把它的 paper，就取做 rainbow<br>
<br>
0:47:11.900,0:47:17.200<br>
那我們細節，也許就不需要講，那裏面有很多的技術啦<br>
<br>
0:47:17.260,0:47:23.320<br>
我覺得比較好實作的是那個 double DQN 跟 Dueling DQN<br>
<br>
0:47:23.320,0:47:27.580<br>
那細節如果你自己要 implement DQN 的時候，<br>
你再去看看那些 paper<br>
<br>
0:47:27.620,0:47:37.280<br>
總之，DQN 有很多的 tip 可以讓他做得比較好，<br>
那這些都整理在 rainbow 那篇 paper 裡面了<br>
<br>
0:47:37.980,0:47:42.680<br>
那最後，我們要講 Actor+Critic<br>
<br>
0:47:42.680,0:47:46.860<br>
同時使用的技術，<br>
就我們剛才有講說，怎麼 learn 一個 actor<br>
<br>
0:47:46.860,0:47:54.580<br>
我們也講說怎麼 learn 一個 critic，我們也講說其實 critic 也有辦法告訴我們說要採取什麼樣的 action 才是對的<br>
<br>
0:47:54.640,0:48:01.180<br>
那接下來我們要講的是 actor+critic 的技術<br>
<br>
0:48:01.180,0:48:03.900<br>
那什麼是 Actor+Critic 的技術呢？<br>
<br>
0:48:03.900,0:48:10.500<br>
有一個非常知名的方法，叫做 A3C 也許大家都有聽過<br>
<br>
0:48:10.500,0:48:18.260<br>
那 A3C 就是，的 3 個 A 分別是什麼呢？<br>
他的前兩個 A 就是 Advantage Actor-Critic<br>
<br>
0:48:18.260,0:48:22.380<br>
所以這是 A2C，然後等一下再講第三個 A 是什麼<br>
<br>
0:48:22.880,0:48:26.140<br>
那這個 Advantage Actor-Critic 它是什麼意思呢？<br>
<br>
0:48:26.140,0:48:33.060<br>
他是說，這邊我們就沒有把細節說出來，<br>
那他的概念其實很簡單<br>
<br>
0:48:33.060,0:48:39.980<br>
我們之前在 learn 這個 actor 的時候，<br>
我們是看 reward function 的 output<br>
<br>
0:48:39.980,0:48:46.780<br>
來決定 actor 要怎麼樣 update，<br>
才可以得到最好的 reward<br>
<br>
0:48:47.160,0:48:54.600<br>
但是今天實際上在這個互動的過程中<br>
<br>
0:48:54.600,0:49:01.360<br>
有非常大的隨機性，所以直接根據互動的過程學，<br>
可能沒有辦法學得很好<br>
<br>
0:49:01.680,0:49:05.840<br>
所以 actor-critic 這種方法，<br>
他的精神，細節我們就不要講了<br>
<br>
0:49:05.840,0:49:13.320<br>
他的精神是什麼？他的精神就是，<br>
今天 actor 不要真的去看環境的 reward<br>
<br>
0:49:13.320,0:49:17.860<br>
因為環境的 reward 變化太大了，<br>
因為中間有隨機性，變化太大<br>
<br>
0:49:17.860,0:49:25.200<br>
但是我不要跟環境的 reward 學，<br>
我們只跟 critic 學，這個方法就叫 actor-critic<br>
<br>
0:49:25.200,0:49:29.460<br>
那怎麼跟 critic 學呢？其實有非常多不同的方法<br>
<br>
0:49:29.460,0:49:34.120<br>
Advantage Actor-Critic 只是眾多的方法的其中一種而已<br>
<br>
0:49:34.120,0:49:38.640<br>
那他之所以變得比較有名，<br>
是因為他的 performance，是比較好的<br>
<br>
0:49:38.660,0:49:43.460<br>
那當然還有很多其他的方法，<br>
可以讓 actor 跟著 critic 學這樣<br>
<br>
0:49:43.460,0:49:49.220<br>
那總之，只要是 actor 不是真的看環境的 reward，<br>
而是看 critic 的比較來學習的<br>
<br>
0:49:49.220,0:49:57.940<br>
就叫做 actor-critic，那其中的某有一種方法，<br>
叫做 Advantage Actor-Critic<br>
<br>
0:49:59.740,0:50:04.720<br>
好，那我們要講 A3C，<br>
我們剛才只講了 Advantage Actor-Critic，只有兩個 A<br>
<br>
0:50:04.720,0:50:07.440<br>
第三個 A 是什麼呢？第三個 A 是這個 Asynchronous<br>
<br>
0:50:07.440,0:50:12.400<br>
所以 A3C 完整的名字，<br>
叫做 Asynchronous Advantage Actor-Critic<br>
<br>
0:50:12.400,0:50:14.300<br>
Asynchronous 的意思是什麼呢？<br>
<br>
0:50:14.300,0:50:21.040<br>
Asynchronous 的意思是說，你有一個 global 的 network<br>
<br>
0:50:21.040,0:50:25.880<br>
你本來有一個 global 的 actor 跟 global 的 critic<br>
<br>
0:50:26.040,0:50:30.100<br>
那現在要去學習的時候，每到學習的時候呢<br>
<br>
0:50:30.100,0:50:34.960<br>
就去跟 global 的 actor 和 critic copy 一組參數過來<br>
<br>
0:50:34.960,0:50:42.340<br>
你可以開分身，<br>
假設你要開 N 個分身的話，就是 copy N 組參數，<br>
<br>
0:50:42.340,0:50:48.000<br>
好，那把參數 copy 完以後呢？<br>
就讓這個 actor 實際去跟環境互動<br>
<br>
0:50:48.000,0:50:52.500<br>
那有 N 個 actor，它們就 N 個 actor 各自去跟環境做互動<br>
<br>
0:50:52.500,0:50:57.980<br>
那互動完以後，就會知道說要怎麼樣，Critic 就會告訴 actor 說要怎麼樣 update 參數<br>
<br>
0:50:57.980,0:51:03.080<br>
那把這個 updated 參數，傳回去 global 的 network<br>
<br>
0:51:03.080,0:51:11.500<br>
所以每一個分身，都會傳一個 update 的方向，<br>
那把所有 update 的方向合起來<br>
<br>
0:51:11.500,0:51:16.000<br>
可以一起做 update，那你等於就是做平行的運算<br>
<br>
0:51:16.060,0:51:20.600<br>
你等於就是平行的開 N個分身學習，<br>
所以可以學得比較快<br>
<br>
0:51:20.880,0:51:25.260<br>
以下部分略過<br>
<br>
0:51:41.520,0:51:48.440<br>
這跟 Asynchronous actor critic 的方法是一模一樣的，<br>
你開越多的分身，學習的速度就越快<br>
<br>
0:51:52.940,0:51:59.540<br>
不過當然在實作上，你要做 asynchronous 這一招，<br>
前提就是，要有很多很多的 machine 這樣子<br>
<br>
0:51:59.540,0:52:03.600<br>
你想要開 8 個分身，就要 8 個 machine，<br>
開一千個分身，就要一千個 machine<br>
<br>
0:52:03.600,0:52:08.920<br>
如果你只有一台 machine，你就只能降到 A2C，<br>
你其實也沒有辦法做 A3C 就是了<br>
<br>
0:52:09.280,0:52:15.460<br>
這邊有一些同學實作做的 actor-critic <br>
在一些遊戲上的結果<br>
<br>
0:52:15.780,0:52:20.340<br>
以下部分略過<br>
<br>
0:53:41.500,0:53:45.740<br>
那這邊有一個在網路上找到的 Doom 的比賽<br>
<br>
0:53:45.740,0:53:49.840<br>
這個其實也蠻知名的，<br>
就有一個 machine 去玩 Doom 的比賽<br>
<br>
0:54:29.500,0:54:35.000<br>
那還有另外一個技術，是 actor-critic 的一個變形<br>
<br>
0:54:35.000,0:54:39.400<br>
那我之所以要把他提出來是因為，他非常地像是 GAN<br>
<br>
0:54:39.400,0:54:43.840<br>
我們剛才有講說，<br>
在做 Q-learning 的時候，我們遇到的一個問題就是<br>
<br>
0:54:43.840,0:54:46.640<br>
我們要解一個 arg max 的 problem<br>
<br>
0:54:46.800,0:54:50.960<br>
我們要找一個 action，他要讓 Q function 最大<br>
<br>
0:54:50.960,0:54:58.040<br>
但是你今天常常會遇到的一個問題就是，<br>
你沒有辦法窮舉所有的 a<br>
<br>
0:54:58.040,0:55:01.180<br>
今天如果尤其是 a 是一個 continuous vector<br>
<br>
0:55:01.180,0:55:04.660<br>
舉例來說，<br>
什麼時候你的 action output 會是 continuous？<br>
<br>
0:55:04.660,0:55:10.080<br>
舉例來說，你想要控制機器手臂，<br>
那你 output 的是關節的角度，那他就是 continuous，<br>
<br>
0:55:10.200,0:55:16.820<br>
一般的 Q-learning 只能處理 discrete 的 case，<br>
那如果要處理 continuous 要怎麼辦呢？<br>
<br>
0:55:17.080,0:55:19.080<br>
你就 learn 一個 actor<br>
<br>
0:55:19.380,0:55:24.340<br>
那這個 actor 做的事情就是，<br>
給他一個 state，那它 output 的那個 a<br>
<br>
0:55:24.340,0:55:29.720<br>
會是讓 Q function 的值最大的那個 a<br>
<br>
0:55:29.720,0:55:34.260<br>
這樣大家懂嗎？<br>
我們有一個 Q function，然後 actor 它 output 的 a<br>
<br>
0:55:34.260,0:55:37.780<br>
actor 它要去學習，它學習的目標就是希望它 output 的 a<br>
<br>
0:55:37.780,0:55:41.220<br>
會讓 Q function 的值呢，越大越好<br>
<br>
0:55:41.340,0:55:44.680<br>
那你如果仔細想想，這不就跟 GAN 是一樣的嗎？<br>
<br>
0:55:44.680,0:55:50.100<br>
如果這個東西把它當作是 Discriminator，<br>
這個東西當作是 Generator<br>
<br>
0:55:50.100,0:55:54.440<br>
Generator 要做的事情，就是產生一個 image 是 Discriminator 覺得分數好的<br>
<br>
0:55:54.440,0:55:59.760<br>
那這邊是 actor 要產生一個 action，<br>
這個 action 是 Q function 覺得分數好的<br>
<br>
0:55:59.800,0:56:07.240<br>
那這個叫做 Pathwise Derivative Policy Gradient，<br>
那其中比較著名的方法，就是 DDPG<br>
<br>
0:56:07.240,0:56:13.440<br>
那這個我們就跳過，只是要告訴大家有這個技術而已<br>
<br>
0:56:13.440,0:56:21.100<br>
那剩下的時間，我想要講一下，<br>
Inverse reinforcement learning ，它是什麼呢？<br>
<br>
0:56:21.100,0:56:25.680<br>
它是 Imitation learning 的一種，<br>
在 inverse reinforcement learning 裡面<br>
<br>
0:56:25.680,0:56:27.600<br>
你只有 environment 跟 actor<br>
<br>
0:56:27.600,0:56:32.320<br>
我們剛才講過 environment <br>
跟 actor 它們互動關係是長這個樣子<br>
<br>
0:56:32.440,0:56:37.580<br>
但是在 Inverse reinforcement learning 裡面，<br>
我們沒有 reward function<br>
<br>
0:56:37.580,0:56:43.960<br>
我們有的東西是什麼，<br>
我們有的東西就只有這個 expert demo trajectory<br>
<br>
0:56:43.960,0:56:48.560<br>
如果是遊戲的話，有專家，有高手，<br>
去把這個遊戲，玩了 N 遍<br>
<br>
0:56:48.560,0:56:52.360<br>
給 machine 看，<br>
告訴 machine 說玩這個遊戲看起來是什麼樣子<br>
<br>
0:56:52.360,0:56:55.620<br>
但是沒有 reward function<br>
<br>
0:56:55.840,0:56:59.400<br>
那你可能會說，什麼樣的狀況，<br>
會沒有 reward function 呢？<br>
<br>
0:56:59.400,0:57:04.600<br>
事實上多數生活中的 case，<br>
我們都是沒有 reward function 的<br>
<br>
0:57:04.600,0:57:09.100<br>
今天下圍棋是用 reinforcement learning，<br>
是因為圍棋的規則是明確的<br>
<br>
0:57:09.100,0:57:10.940<br>
輸就是輸，贏就是贏<br>
<br>
0:57:11.200,0:57:17.220<br>
今天玩電玩可以用 reinforcement learning 就玩電玩的規則是明確的，殺一隻怪得到幾分是訂好的<br>
<br>
0:57:17.220,0:57:22.160<br>
但在多數的 case，我們根本就不知道 reward function 是什麼，比如說，自駕車<br>
<br>
0:57:22.160,0:57:26.920<br>
撞到一個人，要扣 10000 分嗎？<br>
撞到一個狗要扣多少分呢？<br>
<br>
0:57:26.920,0:57:32.020<br>
或者是說，今天如果你拿 reinforcement learning <br>
的技術去學一個 chat bot<br>
<br>
0:57:32.020,0:57:34.760<br>
chat bot 要做到什麼樣的事情，才能得到分數呢？<br>
<br>
0:57:34.760,0:57:41.940<br>
舉例來說，它把人激怒，會扣 100 分嗎？那人掛掉電話，扣 50 分嗎？這個東西你怎麼訂，都是訂不清楚的<br>
<br>
0:57:42.060,0:57:45.780<br>
而且有時候你用一些自己訂出來的 reward<br>
<br>
0:57:45.780,0:57:50.180<br>
那如果它跟現實的狀況差很多，<br>
machine 會學出很奇怪的東西<br>
<br>
0:57:50.400,0:57:53.680<br>
舉例來說，其中一個例子就是機械公敵<br>
<br>
0:57:53.680,0:57:58.320<br>
機械公敵那部影片說，創造機器的人，它訂了 3 大法則<br>
<br>
0:57:58.320,0:58:02.160<br>
這 3 大法則你就可以想成是，只要違反這 3 大法則<br>
<br>
0:58:02.160,0:58:07.140<br>
就會得到非常 negative 的 reward，<br>
遵守這 3 大法則，就會得到 positive 的 reward<br>
<br>
0:58:07.140,0:58:11.160<br>
那機器自己想辦法根據這個規則，<br>
根據這個 reward function<br>
<br>
0:58:11.160,0:58:13.120<br>
去找出最好的 action<br>
<br>
0:58:13.120,0:58:19.600<br>
然後它就有一個神邏輯，它決定說最好的 action 就是，為了保護人類，應該把人類監禁起來<br>
<br>
0:58:19.840,0:58:25.300<br>
那這可能是一個比較極端的例子，<br>
但在真實的研究上，確實有，這樣的例子<br>
<br>
0:58:25.300,0:58:30.820<br>
舉例來說，你想讓機器學習收盤子，<br>
然後它就把盤子放到櫃子裡面，告訴他說<br>
<br>
0:58:30.820,0:58:37.080<br>
盤子放到櫃子裏面，你就可以得到一分，<br>
機器確實可以學到，為了要得到分數<br>
<br>
0:58:37.080,0:58:40.620<br>
它會把盤子放到放到櫃子裏面，但是它可能都用摔的<br>
<br>
0:58:40.620,0:58:44.480<br>
然後盤子通通都被打破了，<br>
因為你沒有告訴他說，盤子打破要扣分啊<br>
<br>
0:58:44.480,0:58:47.380<br>
那以後變成說可能盤子都打破以後，才發現<br>
<br>
0:58:47.400,0:58:54.620<br>
這樣不行，只好再加上新的 reward，<br>
就是打破盤子要扣分，但盤子都已經被打破了<br>
<br>
0:58:55.460,0:59:02.380<br>
今天我們丟很多現實的任務，<br>
是我們根本就不知道 reward function 長怎麼樣子<br>
<br>
0:59:02.380,0:59:05.600<br>
所以我們需要 inverse reinforcement learning 這個技術<br>
<br>
0:59:05.600,0:59:09.280<br>
inverse reinforcement learning 這個技術，<br>
它做的事情就是<br>
<br>
0:59:09.280,0:59:13.680<br>
在原來的 reinforcement learning 裡面，<br>
我們有 reward function，有 environment<br>
<br>
0:59:13.680,0:59:18.380<br>
根據 reward function 還有 environment，<br>
用 reinforcement learning 技術找出最好的 actor，<br>
<br>
0:59:18.540,0:59:22.860<br>
inverse reinforcement learning 技術，<br>
剛好是反過來的，我們有 actor<br>
<br>
0:59:22.860,0:59:29.600<br>
我們雖然不知道最好的 actor 是什麼，但是我們有專家，專家去玩了 N 場遊戲，告訴我們說<br>
<br>
0:59:29.600,0:59:32.460<br>
厲害的人玩這個遊戲，看起來就是怎麼樣的<br>
<br>
0:59:32.460,0:59:36.160<br>
根據專家的 demo，還有 environment<br>
<br>
0:59:36.160,0:59:42.960<br>
透過一個叫做 inverse reinforcement learning 的技術，<br>
我們可以推出 reward function 應該長什麼樣子<br>
<br>
0:59:43.360,0:59:48.460<br>
把 reward function 推出來以後，<br>
你就可以根據你推導出的 reward function<br>
<br>
0:59:48.460,0:59:54.800<br>
再去 apply reinforcement learning 的方法，<br>
去找出最好的 actor<br>
<br>
0:59:54.800,0:59:58.220<br>
所以你是用 inverse reinforcement learning <br>
的方法去推出 reward function<br>
<br>
0:59:58.220,1:00:05.060<br>
再用 reinforcement learning 的方法去找出最好的 actor<br>
<br>
1:00:05.160,1:00:09.080<br>
那 inverse reinforcement learning 是怎麼做的呢？<br>
它的原則就是<br>
<br>
1:00:09.080,1:00:13.240<br>
你的老師，就是那些 experts 他永遠是對的<br>
<br>
1:00:13.240,1:00:20.460<br>
什麼意思，就是說，現在你一開始你有一個 actor，<br>
先隨機的找出初始化一個 actor 出來<br>
<br>
1:00:20.660,1:00:26.460<br>
然後去用這個 actor 去跟環境做互動，<br>
那這個 actor 會得到很多的 trajectory<br>
<br>
1:00:26.460,1:00:34.360<br>
會得到很多的遊戲紀錄，然後接下來啊，<br>
你比較 actor 的遊戲紀錄，跟老師的遊戲紀錄<br>
<br>
1:00:34.360,1:00:41.020<br>
然後你訂一個 reward function，一定要讓，<br>
老師得到的分數，比 actor 得到的分數高<br>
<br>
1:00:41.020,1:00:46.760<br>
就是先射箭，再畫靶的概念，<br>
就是 expert 去玩一堆遊戲，他有一堆遊戲的紀錄<br>
<br>
1:00:46.760,1:00:50.900<br>
然後 actor 也去玩遊戲，也有遊戲的紀錄，那我們不知道 reward function 是什麼<br>
<br>
1:00:50.900,1:00:53.860<br>
等他們都玩完以後，再訂一個 reward function<br>
<br>
1:00:53.920,1:00:58.160<br>
訂的標準就是，老師，就是這個 expert 得到的分數<br>
<br>
1:00:58.160,1:01:02.080<br>
一定要比學生還要高這樣子，先射箭，再畫靶的概念<br>
<br>
1:01:02.480,1:01:07.880<br>
好那把靶畫完以後，學生說，好吧，<br>
雖然因為根據這個新的 reward，我是比較弱的<br>
<br>
1:01:07.880,1:01:14.140<br>
沒關係，那我就再去學習，<br>
我想要 maximize 新的 reward function<br>
<br>
1:01:14.220,1:01:20.580<br>
actor 學到 maximize 新的 reward function 以後，<br>
他就再去跟環境互動，他就會得到新的 trajectory<br>
<br>
1:01:20.580,1:01:24.180<br>
他得到新的 trajectory 以後，他本來以為，他跟老師一樣厲害了<br>
<br>
1:01:24.180,1:01:27.420<br>
但是不幸的就是，那個規則是會一直改的<br>
<br>
1:01:27.420,1:01:33.780<br>
當他變得跟老師一樣厲害以後，<br>
我們再改一下規格，讓老師算出來的分數，還是比較高<br>
<br>
1:01:33.780,1:01:39.120<br>
然後 actor 就只好很生氣的，想辦法學，<br>
想要跟老師做的一樣好<br>
<br>
1:01:39.120,1:01:44.160<br>
就反覆反覆進行這個 process，最後，<br>
就可以找到一個 reward function<br>
<br>
1:01:44.380,1:01:48.420<br>
那這整個 process，<br>
我們用圖示畫來表示一下，有一個 expert<br>
<br>
1:01:48.420,1:01:54.680<br>
他有 N 筆遊戲紀錄，然後你有一個 actor，<br>
它也有 N 筆遊戲紀錄<br>
<br>
1:01:54.680,1:02:00.500<br>
然後你要訂一個 reward function，<br>
讓 expert 得到的分數，永遠贏過 actor<br>
<br>
1:02:00.500,1:02:08.200<br>
然後你接下來，反正你去找一個 reward function，<br>
老師一定是好的，它一定是不好的<br>
<br>
1:02:08.220,1:02:10.720<br>
接下來根據這個 reward function<br>
<br>
1:02:10.720,1:02:16.260<br>
你可以去學這個 actor，根據這個 reward function，<br>
你可以去學這個 actor<br>
<br>
1:02:16.260,1:02:21.460<br>
讓這個 actor 根據這個 reward function，<br>
可以得到最好的分數<br>
<br>
1:02:21.780,1:02:26.760<br>
但等這個 actor 做得比較好之後，<br>
這個規則又變了，然後這個 process<br>
<br>
1:02:26.760,1:02:31.620<br>
又反覆的循環，這個 process，你有沒有覺得很熟悉呢？<br>
<br>
1:02:31.620,1:02:36.300<br>
它跟 GAN 的 process，其實是一模一樣的，怎麼說？<br>
<br>
1:02:36.580,1:02:39.980<br>
在 GAN 裡面，有一堆 expert 畫的圖<br>
<br>
1:02:39.980,1:02:46.240<br>
generator 會產生一堆圖，discriminator 說，<br>
只要 expert 畫的就是好的，這些就是高分<br>
<br>
1:02:46.240,1:02:49.100<br>
這些就是低分，你 learn 出一個 discriminator<br>
<br>
1:02:49.100,1:02:54.580<br>
generator 要做的事情是，調整它畫出來的圖，<br>
使得 discriminator，覺得它是高分<br>
<br>
1:02:54.580,1:02:56.440<br>
但是 generator 以為它畫的圖<br>
<br>
1:02:56.440,1:02:59.520<br>
discriminator 會給它高分，<br>
但是 discriminator 會 update 參數<br>
<br>
1:02:59.520,1:03:05.740<br>
再使得 generator 畫的圖，得到低分，<br>
然後就反覆的，不斷去畫<br>
<br>
1:03:05.740,1:03:12.100<br>
事實上，在 inverse reinforcement learning 裡面，<br>
我們只是把 generator 換個名字叫做 actor<br>
<br>
1:03:12.100,1:03:14.980<br>
把 discriminator 換個名字，叫做 reward function，<br>
<br>
1:03:15.280,1:03:20.700<br>
我們說 actor 會產生一大堆遊戲的紀錄，<br>
但是我們要訂一個 reward function<br>
<br>
1:03:20.700,1:03:26.000<br>
反正就是要讓 actor 輸，讓老師贏，<br>
然後 actor 就會修改它做的事情<br>
<br>
1:03:26.000,1:03:27.720<br>
希望可以得到比較好的分數<br>
<br>
1:03:27.720,1:03:31.100<br>
在 actor 修改以後，reward function 也會跟著修改<br>
<br>
1:03:31.100,1:03:35.520<br>
然後就這樣反覆地進行這個 process<br>
<br>
1:03:35.900,1:03:43.200<br>
在這個結束之前呢，我就給大家看一個，<br>
這個是 Berkeley 做的<br>
<br>
1:03:43.200,1:03:49.920<br>
用 inverse reinforcement learning<br>
 的技術來教機器人做一些行為<br>
<br>
1:03:50.820,1:03:59.980<br>
以下部分省略<br>
<br>
1:05:18.100,1:05:33.020<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
