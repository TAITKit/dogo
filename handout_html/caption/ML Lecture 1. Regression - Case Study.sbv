0:00:00.000,0:00:02.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.000,0:00:04.000
我們今天要講的是 Regression

0:00:04.000,0:00:06.000
等一下我會舉一個例子，

0:00:06.000,0:00:08.000
來講 Regression 是怎麼做的。

0:00:10.000,0:00:12.000
順便引出一些 machine learning 裡面

0:00:12.000,0:00:14.000
常見的重要觀念。

0:00:16.000,0:00:18.000
那regression可以做甚麼?

0:00:18.000,0:00:18.640
除了我們作業裡面要大家做的

0:00:18.640,0:00:20.000
除了我們作業裡面要大家做的

0:00:20.000,0:00:20.700
預測PM2.5這個任務以外，

0:00:20.700,0:00:22.000
預測PM2.5這個任務以外，

0:00:22.000,0:00:24.000
還有很多其他非常有用的task。

0:00:24.860,0:00:26.000
舉例來說，

0:00:26.000,0:00:28.000
如果你可以做一個股票預測的系統，

0:00:28.000,0:00:30.000
如果你可以做一個股票預測的系統，

0:00:30.000,0:00:32.000
如果你可以做一個股票預測的系統，

0:00:32.000,0:00:34.000
你要做的事情就是 :

0:00:34.000,0:00:36.000
找一個function。

0:00:36.000,0:00:36.640
這個function的input可能是，

0:00:40.000,0:00:42.000
過去十年，

0:00:42.000,0:00:44.000
各種股票起伏的資料。

0:00:44.000,0:00:44.700
或者是，

0:00:46.000,0:00:48.000
A公司併購B公司，

0:00:48.000,0:00:50.000
B公司併購C公司等等的資料。

0:00:50.000,0:00:52.000
你希望這個function在input這些資料以後，

0:00:52.000,0:00:54.000
它的output是，明天的道瓊工業指數的點數。

0:00:54.000,0:00:56.000
它的output是，明天的道瓊工業指數的點數。

0:00:56.000,0:00:58.000
它的output是，明天的道瓊工業指數的點數。

0:00:58.000,0:01:00.000
如果你可以預測這個的話，你就發了

0:01:02.000,0:01:04.000
還有別的task，

0:01:04.000,0:01:06.000
比如說現在很熱門的

0:01:06.000,0:01:08.000
無人車，自動車。

0:01:08.000,0:01:10.000
這個自動車也可以想成是一個regression的problem，

0:01:10.000,0:01:12.000
在這個regression的problem裡面，

0:01:12.000,0:01:14.000
input就是，你的無人車所看到的各種sensor :

0:01:14.000,0:01:16.000
input就是，你的無人車所看到的各種sensor :

0:01:16.000,0:01:18.000
它的紅外線感測的sensor，

0:01:18.000,0:01:20.000
它的影像的視訊的鏡頭所看到的馬路上的東西等等。

0:01:20.000,0:01:22.000
它的影像的視訊的鏡頭所看到的馬路上的東西等等。

0:01:22.000,0:01:24.000
它的影像的視訊的鏡頭所看到的馬路上的東西等等。

0:01:24.000,0:01:26.000
你的input就是這些information，

0:01:26.000,0:01:28.000
你的input就是這些information，

0:01:28.000,0:01:30.000
output就是方向盤的角度。

0:01:30.000,0:01:32.000
比如說，

0:01:32.000,0:01:34.000
要左轉50度，

0:01:34.000,0:01:36.000
還是右轉50度，

0:01:36.000,0:01:38.000
右轉50度你就當作左轉 負50度，

0:01:38.000,0:01:40.000
所以output也是一個 scalar。

0:01:40.000,0:01:42.000
所以無人車駕駛就是一個regression problem

0:01:42.000,0:01:44.000
input一些information，

0:01:44.000,0:01:46.000
output就是方向盤的角度，

0:01:46.000,0:01:48.000
它是一個數值。

0:01:48.000,0:01:50.000
或者是，

0:01:50.000,0:01:52.000
你可以做推薦系統。

0:01:52.000,0:01:54.000
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，

0:01:54.000,0:01:56.000
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，

0:01:56.000,0:01:58.000
我們都知道說，YouTube要推薦影片或者Amazon要推薦商品給你，

0:01:58.000,0:02:00.000
那推薦系統它要做的事情，

0:02:00.000,0:02:02.000
也可以想成是一個regression的問題。

0:02:02.000,0:02:04.000
就是找一個 function，

0:02:04.000,0:02:06.000
它的input是，

0:02:06.000,0:02:08.000
某一個使用者A和某一個商品B，

0:02:08.000,0:02:10.000
某一個使用者A和某一個商品B，

0:02:10.000,0:02:12.000
它的output就是，

0:02:12.000,0:02:14.000
使用者A購買商品B的可能性。

0:02:14.000,0:02:16.000
使用者A購買商品B的可能性。

0:02:16.000,0:02:18.000
如果你可以找到這樣一個function，

0:02:18.000,0:02:20.000
它可以精確的預測說，

0:02:20.000,0:02:22.000
使用者A購買商品B的可能性的話，

0:02:22.000,0:02:24.000
使用者A購買商品B的可能性的話，

0:02:24.000,0:02:26.000
那Amazon就會推薦使用者A他最有可能購買的商品。

0:02:26.000,0:02:28.000
那Amazon就會推薦使用者A他最有可能購買的商品。

0:02:28.000,0:02:30.000
那Amazon就會推薦使用者A他最有可能購買的商品。

0:02:32.000,0:02:34.000
這個是regression的種種應用，

0:02:34.000,0:02:36.000
今天我要講的是，

0:02:36.000,0:02:38.000
另外一個我覺得更實用的應用 :

0:02:38.000,0:02:40.000
就是預測寶可夢的CP值。

0:02:40.000,0:02:42.000
就是預測寶可夢的CP值。

0:02:42.000,0:02:44.000
這個大家知道是甚麼意思嗎?

0:02:44.000,0:02:46.000
我來說明一下好了:

0:02:46.000,0:02:48.000
你的CP值就是一隻寶可夢的戰鬥力。

0:02:48.000,0:02:50.000
你的CP值就是一隻寶可夢的戰鬥力。

0:02:50.000,0:02:52.000
你抓到一隻寶可夢後，

0:02:52.000,0:02:54.000
比如說，

0:02:54.000,0:02:56.000
這個是一隻妙蛙種子，

0:03:00.000,0:03:02.000
比如說這是一隻妙蛙種子，

0:03:02.000,0:03:04.000
比如說這是一隻妙蛙種子，

0:03:04.000,0:03:06.000
然後你給他吃一些星辰或糖果以後，

0:03:06.000,0:03:08.000
他就會進化成妙蛙草。

0:03:08.000,0:03:10.000
而如果他進化成妙蛙草以後，

0:03:10.000,0:03:12.000
他的CP值就變了。

0:03:12.000,0:03:14.000
他的CP值就變了。

0:03:16.000,0:03:18.000
為甚麼我們會希望能夠預測寶可夢的CP值呢?

0:03:18.000,0:03:20.000
為甚麼我們會希望能夠預測寶可夢的CP值呢?

0:03:20.000,0:03:22.000
為甚麼我們會希望能夠預測寶可夢的CP值呢?

0:03:24.000,0:03:26.000
因為如果你可以精確的預測一隻寶可夢在進化以後的CP值的話，

0:03:26.000,0:03:28.000
因為如果你可以精確的預測一隻寶可夢在進化以後的CP值的話，

0:03:28.000,0:03:30.000
你就可以評估說，

0:03:30.000,0:03:32.000
你是否要進化這隻寶可夢。

0:03:32.000,0:03:34.000
你是否要進化這隻寶可夢。

0:03:34.000,0:03:36.000
如果他是一隻CP值比較低的寶可夢的話，

0:03:36.000,0:03:38.000
你可能就把他拿去做糖果。

0:03:40.000,0:03:42.000
你就不進化他，

0:03:42.000,0:03:44.000
這樣你就可以節省一些你的糖果的資源。

0:03:44.000,0:03:44.800
你可能就會問說，

0:03:44.800,0:03:46.000
為甚麼我們要節省糖果的資源?

0:03:46.000,0:03:48.000
為甚麼我們要節省糖果的資源?

0:03:48.000,0:03:50.000
因為你這樣可以在比較短時間內，

0:03:50.000,0:03:52.000
就進化比較多強的神奇寶貝。

0:04:00.000,0:04:02.000
你就會想說為甚麼我們要比較強的寶可夢?

0:04:04.000,0:04:06.000
因為他可以去打道館。

0:04:06.000,0:04:08.000
你問為甚麼我們要去打道館?

0:04:08.000,0:04:10.000
其實我也不知道這樣。

0:04:12.000,0:04:14.000
我們今天要做的事情，就是找一個function。

0:04:14.000,0:04:16.000
這個function的input，

0:04:18.000,0:04:20.000
就是某一隻寶可夢。

0:04:20.000,0:04:22.000
它的output就是

0:04:22.000,0:04:24.000
這隻寶可夢如果我們把它進化以後，

0:04:24.000,0:04:26.000
這隻寶可夢如果我們把它進化以後，

0:04:26.000,0:04:28.000
它的CP值的數值是多少。

0:04:28.000,0:04:30.000
這是一個regression的problem，

0:04:30.000,0:04:32.000
我們的input就是某一隻寶可夢所有相關的information，

0:04:32.000,0:04:34.000
我們的input就是某一隻寶可夢所有相關的information，

0:04:34.000,0:04:36.000
比如說，

0:04:36.000,0:04:38.000
我們把一隻寶可夢用X表示，

0:04:38.000,0:04:40.000
我們把一隻寶可夢用Xcp表示。

0:04:40.000,0:04:42.000
它的CP值我們就用Xcp來表示。

0:04:42.000,0:04:44.000
它的CP值我們就用Xcp來表示。

0:04:44.000,0:04:46.000
它的CP值我們就用Xcp來表示。

0:04:46.000,0:04:48.000
我們用下標來表示某一個，

0:04:48.000,0:04:50.000
完整的東西裡面的，

0:04:50.000,0:04:52.000
某一個component，

0:04:52.000,0:04:54.000
某一個部分，我們用下標來表示。

0:04:54.000,0:04:56.000
Xcp代表某一隻寶可夢X，

0:04:56.000,0:04:58.000
它在進化前的CP值。

0:05:00.000,0:05:02.000
比如說，這個妙蛙種子，

0:05:02.000,0:05:04.000
它CP值是14，

0:05:04.000,0:05:04.760
Xs代表這一隻寶可夢X，是屬於哪一個物種。

0:05:04.760,0:05:06.000
Xs代表這一隻寶可夢X，是屬於哪一個物種。

0:05:06.000,0:05:08.000
Xs代表這一隻寶可夢X，是屬於哪一個物種。

0:05:08.000,0:05:10.000
Xs代表這一隻寶可夢X，是屬於哪一個物種。

0:05:10.000,0:05:12.000
Xs代表這一隻寶可夢X，是屬於哪一個物種。

0:05:12.000,0:05:14.000
比如說這是妙蛙種子。

0:05:14.000,0:05:16.000
Xhp代表這一隻表可夢，它的hp值是多少，

0:05:16.000,0:05:18.000
Xhp代表這一隻表可夢，它的hp值是多少，

0:05:18.000,0:05:20.000
它的生命值是多少。

0:05:22.000,0:05:24.000
這個妙蛙種子的生命值是10。

0:05:24.000,0:05:26.000
Xw代表它的重量，

0:05:26.000,0:05:28.000
Xh代表它的高度。

0:05:30.000,0:05:32.000
可以看看你抓的寶可夢是不是特別大隻或特別小隻。

0:05:36.000,0:05:38.000
那output是進化後的CP。

0:05:38.000,0:05:40.000
這個進化後的CP值，

0:05:40.000,0:05:42.000
就是一個數值。

0:05:42.000,0:05:44.000
就是一個scalar，

0:05:44.000,0:05:46.000
我們把它用Y來表示。

0:05:46.000,0:05:46.620
這麼解這個問題呢?

0:05:50.000,0:05:52.000
我們第一堂課就講過說，做machine learning就是三個步驟，

0:05:52.000,0:05:54.000
第一個步驟就是，

0:05:54.000,0:05:56.000
找一個model；

0:05:56.000,0:05:58.000
第二個步驟是，

0:05:58.000,0:06:00.000
model就是一個function set，

0:06:00.000,0:06:02.000
第二個步驟就是，

0:06:02.000,0:06:04.000
定義function set裡面某一個function，

0:06:04.000,0:06:06.000
我們拿一個function出來

0:06:06.000,0:06:08.000
可以要evaluate它的好壞；

0:06:08.000,0:06:10.000
第三步驟就是找一個最好的function。

0:06:12.980,0:06:14.000
首先我們就從第一個步驟開始。

0:06:14.000,0:06:16.000
我們要找一個function set，

0:06:16.000,0:06:18.000
這個function set就是所謂的model。

0:06:18.000,0:06:20.000
在這個task裡面，

0:06:20.000,0:06:22.000
我們的function set，

0:06:22.000,0:06:24.000
應該長甚麼樣子呢?

0:06:24.000,0:06:26.000
一個 input一隻寶可夢，

0:06:28.000,0:06:30.000
output進化後的CP值的function，

0:06:30.000,0:06:32.000
output進化後的CP值的function，

0:06:32.000,0:06:34.000
應該長甚麼樣子呢?

0:06:34.000,0:06:36.000
這邊就先亂寫一個簡單的。

0:06:36.000,0:06:38.000
比如說我們認為說，

0:06:38.000,0:06:40.000
進化後的CP值Y，

0:06:40.000,0:06:42.000
等於某一個常數項B加上某一個數值W，

0:06:42.000,0:06:44.000
等於某一個常數項b加上某一個數值w，

0:06:44.000,0:06:46.000
等於某一個常數項b加上某一個數值w，

0:06:46.000,0:06:48.000
乘上現在輸入的寶可夢的X它在進化前的CP值，

0:06:48.000,0:06:50.000
乘上現在輸入的寶可夢的X它在進化前的CP值，

0:06:50.000,0:06:52.000
乘上現在輸入的寶可夢的X它在進化前的CP值，

0:06:54.000,0:06:56.000
這個Xcp代表進化前的CP值，

0:06:56.000,0:06:58.000
這個Y是進化後的CP值。

0:07:00.000,0:07:02.000
這個w和b是參數，

0:07:06.000,0:07:08.000
w和b可以是任何的數值。

0:07:08.000,0:07:10.000
在這個model裡面，

0:07:10.000,0:07:12.000
w和b是未知的，

0:07:12.000,0:07:14.000
你可以把任何的數字填進去，

0:07:14.000,0:07:16.000
填進不同的數值，

0:07:16.000,0:07:18.000
你就得到不同的function。

0:07:18.000,0:07:20.000
比如說你可以有一個f1，

0:07:20.000,0:07:22.000
f1是b=10，w=9

0:07:26.000,0:07:28.000
你可以有另外一個function f2，

0:07:28.000,0:07:30.000
這個f2是b=9.8，w=9.2

0:07:30.000,0:07:32.000
這個f2是b=9.8，w=9.2

0:07:32.000,0:07:34.000
你有一個f3，

0:07:34.000,0:07:36.000
它是b= -0.8，w=-1.2

0:07:36.000,0:07:38.000
它是b= -0.8，w=-1.2

0:07:38.000,0:07:40.000
如果今天你的w和b可以代任何值的話，'

0:07:44.000,0:07:46.000
其實你這個function set裡面，

0:07:46.000,0:07:48.000
可以有無窮無盡的function，

0:07:48.000,0:07:50.000
有無窮多的function。

0:07:50.000,0:07:52.000
你用這個式子y=b+w × Xcp

0:07:52.000,0:07:54.000
你用這個式子y=b+w × Xcp

0:07:54.000,0:07:56.000
代表這些function所成的集合。

0:07:58.000,0:08:00.000
當然在這些function裡面，

0:08:00.000,0:08:02.000
比如說f1，f2，f3裡面

0:08:04.000,0:08:06.000
你會發現有一些function顯然不太可能是正確的。

0:08:08.000,0:08:10.000
比如說f3不太可能是正確的。

0:08:10.000,0:08:12.000
因為我們知道說CP值其實是正，

0:08:12.000,0:08:14.000
乘以-1.2就變成是負的

0:08:14.000,0:08:16.000
所以進化以後CP值就變成是負的，

0:08:16.000,0:08:18.000
這樣顯然是說不通的。

0:08:18.000,0:08:20.000
這個就是我們等一下要用靠training data來告訴我們說，

0:08:22.000,0:08:24.000
在這個function set裡面，

0:08:24.000,0:08:26.000
哪一個function才是合理的function。

0:08:28.000,0:08:30.000
這樣子的model，

0:08:30.000,0:08:32.000
這個y=b+w × Xcp這樣子的model，

0:08:36.000,0:08:38.000
它是一種linear的model。

0:08:38.000,0:08:40.000
所謂的linear的model的意思是，

0:08:40.000,0:08:42.000
簡單來說，

0:08:42.000,0:08:44.000
如果我們可以把一個function，

0:08:44.000,0:08:46.000
我們把現在我們要找的function，

0:08:48.000,0:08:50.000
寫成y=b+ ∑WiXi

0:08:50.000,0:08:52.000
寫成y=b+ ∑WiXi

0:08:54.000,0:08:56.000
那它是一個linear的function。

0:08:58.000,0:09:00.000
這邊的Xi指的是你input 的X的各種attribute。

0:09:04.000,0:09:06.000
比如說你input寶可夢的各種不同的屬性，

0:09:06.000,0:09:08.000
比如說身高或體重等等。

0:09:08.000,0:09:10.000
這些東西我們叫做feature。

0:09:12.000,0:09:14.000
從input的object裡面，

0:09:14.000,0:09:16.000
抽出來的各種數值當作function的input，

0:09:16.000,0:09:18.000
這些東西叫做feature。

0:09:20.000,0:09:22.000
這個Wi和b，

0:09:22.000,0:09:24.000
這個Wi叫做weight，

0:09:26.000,0:09:28.000
這個b叫做bias。

0:09:34.000,0:09:36.000
接下來我們要收集training data，才能夠找這個function。

0:09:38.000,0:09:40.000
這是一個supervised learning 的task，

0:09:40.000,0:09:42.000
所以我們收集的是function的input，

0:09:42.000,0:09:44.000
和function的output。

0:09:44.000,0:09:46.000
因為是regression的task，

0:09:46.000,0:09:48.000
所以function的output是一個數值。

0:09:50.000,0:09:52.000
舉例來說，你就抓了一隻，

0:09:52.000,0:09:54.000
這個是傑尼龜，

0:09:54.000,0:09:56.000
它進化後這個是卡咪龜，

0:10:00.000,0:10:02.000
卡咪龜進化是水箭龜。

0:10:06.000,0:10:08.000
這個function的input在這邊就是這隻傑尼龜。

0:10:08.000,0:10:10.000
那我們用X1來表示它。

0:10:10.000,0:10:12.000
我們用上標來表示一個完整的object的編號。

0:10:12.000,0:10:14.000
我們用上標來表示一個完整的object的編號。

0:10:14.000,0:10:16.000
我們用上標來表示一個完整的object的編號。

0:10:16.000,0:10:18.000
我們用上標來表示一個完整的object的編號。

0:10:18.000,0:10:20.000
剛才我們有看到用下標來表示一個 component，

0:10:20.000,0:10:22.000
一個完整object裡面的component。

0:10:24.000,0:10:26.000
我們用上標來表示一個完整object的編號，

0:10:26.000,0:10:28.000
所以這是第一個X

0:10:28.000,0:10:30.000
這是一隻傑尼龜

0:10:30.000,0:10:32.000
那它進化以後的CP是973

0:10:34.000,0:10:36.000
所以我們function的output，

0:10:36.000,0:10:38.000
應該看到X1就output數值973。

0:10:40.000,0:10:42.000
那這個973我們用Y1 hat 來代表，

0:10:42.000,0:10:44.000
那這個973我們用Y1 hat 來代表，

0:10:46.000,0:10:48.000
這邊用Y來代表function的output，用上標來代表一個完整的個體。

0:10:48.000,0:10:50.000
這邊用Y來代表function的output，用上標來代表一個完整的個體。

0:10:50.000,0:10:52.000
因為我們今天考慮的output是scalar，

0:10:54.000,0:10:56.000
所以它其實裡面沒有component，

0:10:56.000,0:10:58.000
它就是一個簡單的數值。

0:10:58.000,0:11:00.000
但是我們未來如果在考慮structured learning的時候，

0:11:00.000,0:11:02.000
我們output的object可能是有structure的。

0:11:02.000,0:11:04.000
我們output的object可能是有structure的。

0:11:04.000,0:11:06.000
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。

0:11:06.000,0:11:08.000
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。

0:11:08.000,0:11:10.000
所以我們還是會需要上標下標來表示一個完整的output的object，還有它裡面的component。

0:11:12.000,0:11:14.000
我們用Y hat1來表示這個數值979。

0:11:18.000,0:11:20.000
只有一隻不夠要收集很多。

0:11:20.000,0:11:22.000
比如說再抓一隻伊布，

0:11:22.000,0:11:24.000
那這個伊布就是X2。

0:11:24.000,0:11:26.000
那它進化以後可以變成雷精靈，

0:11:26.000,0:11:28.000
那雷精靈的CP值是1420，

0:11:30.000,0:11:32.000
這個1420就是Y2 hat。

0:11:32.000,0:11:34.000
我們用hat來代表說這個是一個正確的值，

0:11:34.000,0:11:36.000
我們用hat來代表說這個是一個正確的值，

0:11:36.000,0:11:38.000
是我們實際觀察到function該有的output。

0:11:38.000,0:11:40.000
是我們實際觀察到function該有的output。

0:11:42.000,0:11:44.000
你可能以為說這只是個例子，

0:11:44.000,0:11:46.000
這不只是一個例子，

0:11:46.000,0:11:48.000
我是有真正的data的。

0:11:50.000,0:11:52.000
今天其實我是想要發表，

0:11:52.000,0:11:54.000
我在神奇寶貝上面的研究成果這樣。

0:12:00.000,0:12:02.000
那我們就收集10隻神奇寶貝，

0:12:06.000,0:12:08.000
這10隻寶可夢就是從編號1到編號10

0:12:08.000,0:12:10.000
這10隻寶可夢就是從編號1到編號10

0:12:10.000,0:12:12.000
這10隻寶可夢就是從編號1到編號10

0:12:14.000,0:12:16.000
每一隻寶可夢我們都讓它進化以後，

0:12:16.000,0:12:18.000
我們就知道它進化後的CP值

0:12:18.000,0:12:20.000
就是Y1 hat 到 Y10 hat

0:12:20.000,0:12:22.000
就是Y1 hat 到 Y10 hat

0:12:22.000,0:12:24.000
這個是真正的data，

0:12:24.000,0:12:26.000
你可能會問說怎麼只抓10隻呢?

0:12:26.000,0:12:28.000
你不知道抓這個很麻煩嗎?

0:12:30.000,0:12:32.000
其實老實說這也不是我自己抓的，

0:12:32.000,0:12:34.000
網路上有人分享他抓出來的數據。

0:12:34.000,0:12:36.000
我拿他的數據來做一下，

0:12:36.000,0:12:38.000
其實他也沒有抓太多次，

0:12:38.000,0:12:40.000
因為抓這個其實是很麻煩的。

0:12:40.000,0:12:42.000
並不是抓來就好，你要把它進化以後，

0:12:42.000,0:12:44.000
你才知道function的output是多少。

0:12:44.000,0:12:46.000
所以收集這個data並沒有那麼容易。

0:12:48.000,0:12:50.000
所以就收集了10隻 神奇寶貝，

0:12:50.000,0:12:52.000
它進化後的CP值。

0:12:54.000,0:12:56.000
那如果我們把這十隻神奇寶貝的information畫出來的話，

0:12:56.000,0:12:58.000
這個圖上每一個藍色的點，

0:12:58.000,0:13:00.000
這個圖上每一個藍色的點，

0:13:00.000,0:13:02.000
代表一隻寶可夢。

0:13:04.000,0:13:06.000
然後他的X軸，

0:13:06.000,0:13:08.000
X軸代表的是這一隻寶可夢他的CP值，

0:13:08.000,0:13:10.000
X軸代表的是這一隻寶可夢他的CP值，

0:13:10.000,0:13:12.000
X軸代表的是這一隻寶可夢他的CP值，

0:13:12.000,0:13:14.000
這個我們一抓來的時候我們就知道了

0:13:14.000,0:13:16.000
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。

0:13:16.000,0:13:18.000
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。

0:13:18.000,0:13:20.000
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。

0:13:20.000,0:13:22.000
然後他的Y軸代表如果你把這隻寶可夢進化以後，進化後的CP值。

0:13:22.000,0:13:24.000
這個用 ŷ 來表示。

0:13:26.000,0:13:28.000
所以10隻寶可夢

0:13:28.000,0:13:30.000
這邊我們有10個點

0:13:34.000,0:13:36.000
這個CP值其實就特別高，

0:13:36.000,0:13:38.000
這隻其實是伊布

0:13:40.000,0:13:42.000
伊布其實不是很容易抓的

0:13:42.000,0:13:44.000
這邊每一個點就是某第n隻寶可夢的CP值，

0:13:46.000,0:13:48.000
這邊每一個點就是某第n隻寶可夢的CP值，

0:13:48.000,0:13:50.000
跟它進化以後的 ŷ。

0:13:50.000,0:13:52.000
那我們用X上標n和下標cp，

0:13:52.000,0:13:54.000
來代表第n筆data，他的某一個component，也就是他的CP值。

0:13:54.000,0:13:56.000
來代表第n筆data，他的某一個component，也就是他的CP值。

0:13:56.000,0:13:58.000
來代表第n筆data，他的某一個component，也就是他的CP值。

0:13:58.000,0:14:02.000
接下來，

0:14:02.000,0:14:04.000
有了這些training data以後，

0:14:04.000,0:14:06.000
我們就可以定義一個function的好壞，

0:14:08.000,0:14:10.000
我們就可以知道一個function是多好或者是多不好，

0:14:12.000,0:14:14.000
知道說這裡面每一個function是多好或者是多不好。

0:14:14.000,0:14:16.000
怎麼做呢?

0:14:16.000,0:14:18.000
我們要定義一個

0:14:18.000,0:14:20.000
另外一個function，

0:14:20.000,0:14:22.000
叫做 loss function，

0:14:22.000,0:14:24.000
這邊寫作大寫的L

0:14:24.000,0:14:26.000
我們這裡定了一個function set，

0:14:26.000,0:14:28.000
這裡面有一大堆的function。

0:14:28.000,0:14:30.000
這邊我們要再定另外一個function

0:14:30.000,0:14:32.000
另外一個function叫做loss function，

0:14:32.000,0:14:34.000
我們寫成大寫的L。

0:14:34.000,0:14:35.580
這個loss function的input，

0:14:35.580,0:14:38.000
他是一個很特別function，

0:14:38.000,0:14:40.000
這個loss function他是function的function，

0:14:40.000,0:14:42.000
大家知道今天我的意思嗎?

0:14:44.000,0:14:46.000
他的input就是一個function，

0:14:46.000,0:14:48.000
他的output 就是一個數值告訴我們說

0:14:48.000,0:14:50.000
現在input的這個function他有多不好

0:14:50.000,0:14:52.000
現在input的這個function他有多不好

0:14:52.000,0:14:54.000
我們這邊是用多不好來表示。

0:14:56.000,0:14:58.000
所以這個loss function他是一個function的function，

0:14:58.000,0:15:00.000
他就是吃一個function當作input

0:15:00.000,0:15:02.000
他的output就是這個function有多不好。

0:15:02.000,0:15:04.000
他的output就是這個function有多不好。

0:15:06.000,0:15:08.000
所以你可以寫成這樣，

0:15:08.000,0:15:10.000
這個L他的input就是某一個function f

0:15:10.000,0:15:12.000
你知道一個function，

0:15:12.000,0:15:14.000
他又是由這個function裡面的兩個參數b和w來決定的

0:15:14.000,0:15:16.000
他又是由這個function裡面的兩個參數b和w來決定的

0:15:16.000,0:15:18.000
這個f是由b和w來決定的

0:15:18.000,0:15:20.000
所以input這個f，

0:15:20.000,0:15:22.000
就等於input這個f裡面的b和w。

0:15:22.000,0:15:24.000
就等於input這個f裡面的b和w。

0:15:24.000,0:15:26.000
所以你可以說loss function，

0:15:26.000,0:15:28.000
他是在衡量一組參數的好壞，

0:15:28.000,0:15:30.000
他是在衡量一組參數的好壞，

0:15:30.000,0:15:32.000
衡量一組b和w的好壞。

0:15:34.000,0:15:36.000
那怎麼定這個loss function呢?

0:15:38.000,0:15:40.000
loss function你其實可以隨自己的喜好，

0:15:40.000,0:15:42.000
定義一個你覺得合理的function。

0:15:44.000,0:15:46.000
不過我們這邊就用比較常見的作法 :

0:15:46.000,0:15:48.000
怎麼定呢?

0:15:48.000,0:15:50.000
你就把這個input的w和b，

0:15:50.000,0:15:52.000
你就把這個input的w和b，

0:15:52.000,0:15:54.000
實際地代入y=b+w × Xcp 這個function裡面。

0:15:54.000,0:15:56.000
實際地代入y=b+w × Xcp這個function裡面。

0:16:00.000,0:16:02.000
你把w乘上第n隻寶可夢的CP值，

0:16:02.000,0:16:04.000
你把w乘上第n隻寶可夢的CP值，

0:16:04.000,0:16:06.000
再加上這個constant b，

0:16:06.000,0:16:08.000
然後你就得到說 :

0:16:08.000,0:16:10.000
如果我們使用這一組w，

0:16:10.000,0:16:12.000
如果我們使用這一組w，

0:16:12.000,0:16:14.000
這一個w和b，

0:16:14.000,0:16:16.000
來當作我們的function，

0:16:16.000,0:16:18.000
來預測寶可夢它進化以後的CP值的話，

0:16:18.000,0:16:20.000
來預測寶可夢它進化以後的CP值的話，

0:16:20.000,0:16:22.000
這個預測的值Y的數值是多少。

0:16:22.000,0:16:24.000
這個預測的值Y的數值是多少。

0:16:26.000,0:16:28.000
這個裡面的括號，比較小的括號，

0:16:28.000,0:16:30.000
這個裡面的括號，比較小的括號，

0:16:30.000,0:16:32.000
他輸出的數值是我們用現在的function來預測的話，

0:16:32.000,0:16:34.000
他輸出的數值是我們用現在的function來預測的話，

0:16:34.000,0:16:36.000
他輸出的數值是我們用現在的function來預測的話，

0:16:36.000,0:16:38.000
我們得到的輸出是甚麼。

0:16:38.000,0:16:40.000
那 ŷ是真正的數值。

0:16:40.000,0:16:42.000
我們把真正的數值，

0:16:42.000,0:16:44.000
減掉預測的數值，

0:16:44.000,0:16:46.000
再取平方，

0:16:50.000,0:16:52.000
這個就是估測的誤差。

0:16:54.000,0:16:56.000
我們再把我們手上的10隻寶可夢的估測誤差，

0:16:56.000,0:16:58.000
都合起來，

0:17:00.000,0:17:02.000
就得到這個loss function。

0:17:02.000,0:17:04.000
那這個定義我相信你是不太會有問題的，

0:17:04.000,0:17:06.000
因為它非常的直覺估測 :

0:17:06.000,0:17:08.000
如果我使用某一個function，

0:17:08.000,0:17:10.000
它給我們的估測誤差越大，

0:17:10.000,0:17:12.000
那當然這個function就越不好。

0:17:12.000,0:17:14.000
那當然這個function就越不好。

0:17:14.000,0:17:16.000
所以我們就用估測誤差來定義一個loss function。

0:17:16.000,0:17:18.000
當然你可以選擇其他可能性。

0:17:26.000,0:17:28.000
再來我們有了這個loss function以後，

0:17:30.000,0:17:32.000
如果你還是有一些困惑的話，

0:17:32.000,0:17:34.000
我們可以把這個loss function的形狀畫出來。

0:17:36.000,0:17:38.000
這個loss function L(w,b)

0:17:38.000,0:17:40.000
它input就是兩個參數w和b

0:17:42.000,0:17:44.000
所以我們可以把這個L(w,b)對w和b把它做圖

0:17:44.000,0:17:46.000
所以我們可以把這個L(w,b)對w和b把它做圖

0:17:46.000,0:17:48.000
所以我們可以把這個L(w,b)對w和b把它做圖

0:17:48.000,0:17:50.860
把它畫出來。

0:17:50.860,0:17:52.000
在這個圖上的每一個點，

0:17:52.000,0:17:54.000
就代表著一個組w跟b，

0:17:54.000,0:17:56.000
也就是代表某一個function。

0:17:56.000,0:17:58.000
比如說，紅色這個點，

0:17:58.000,0:18:00.000
紅色這個點就代表著，

0:18:00.000,0:18:02.000
這個b=-180， 這個w=-2的時候所得到的function。

0:18:02.000,0:18:04.000
這個b=-180， 這個w=-2時候所得到的function。

0:18:04.000,0:18:06.000
這個b=-180， 這個w=-2時候所得到的function。

0:18:08.000,0:18:10.000
就y= -180−2×Xcp這個function。

0:18:10.000,0:18:12.000
就y= -180−2×Xcp這個function。

0:18:12.000,0:18:14.000
這圖上每一個點都代表著一個function。

0:18:16.000,0:18:18.000
顏色代表了，

0:18:18.000,0:18:20.000
現在如果我們使用這個function，

0:18:22.000,0:18:24.000
根據我們定義的loss function，

0:18:24.000,0:18:26.000
它有多糟糕，

0:18:26.000,0:18:28.000
它有多不好。

0:18:30.000,0:18:32.000
這個顏色越偏紅色代表數值越大，

0:18:34.000,0:18:36.000
所以在這一群的function，

0:18:36.000,0:18:38.000
他們loss非常大，也就是它們是一群不好的function。

0:18:40.000,0:18:42.000
最好的function落在哪裡呢?

0:18:42.000,0:18:44.000
越偏藍色代表那一個function越好

0:18:44.000,0:18:46.000
越偏藍色代表那一個function越好

0:18:48.000,0:18:50.000
所以最好的function其實落在這個位子

0:18:50.000,0:18:52.000
如果你選這個function的話，

0:18:52.000,0:18:54.000
它是可以讓你loss最低的一個function。

0:18:54.000,0:18:56.000
它是可以讓你loss最低的一個function。

0:19:00.000,0:19:02.000
接下來，我們已經定好了我們的loss function，

0:19:02.000,0:19:04.000
接下來，我們已經定好了我們的loss function，

0:19:06.000,0:19:08.000
可以衡量我們的model裡面每一個function的好壞。

0:19:10.000,0:19:12.000
接下來我們要做的事情就是，

0:19:12.000,0:19:14.000
從這個function set裡面，

0:19:14.000,0:19:16.000
從這個function set裡面，

0:19:16.000,0:19:18.000
挑選一個最好的function。

0:19:18.000,0:19:20.000
挑選一個最好的function。

0:19:22.000,0:19:24.000
所謂挑選最好的function這一件事情，

0:19:26.000,0:19:28.000
如果你想要把它寫成formulation的話，

0:19:28.000,0:19:30.000
如果你想要把它寫成equation的話，

0:19:30.000,0:19:32.000
那寫起來是甚麼樣子呢?

0:19:32.000,0:19:36.000
它寫起來就是，你要我們定的那個loss function長這樣，

0:19:38.000,0:19:40.000
那你要找一個f，

0:19:40.000,0:19:42.000
它可以讓L(f)最小，

0:19:44.000,0:19:46.000
這個可以讓L(f)最小的function，

0:19:46.000,0:19:48.000
我們就寫成f*。

0:19:48.000,0:19:50.000
或者是，

0:19:50.000,0:19:52.000
我們知道f是由兩個參數w和b表示，

0:19:52.000,0:19:54.000
我們知道f是由兩個參數w和b表示，

0:19:54.000,0:19:56.000
今天要做的事情就是，

0:19:56.000,0:19:58.000
窮舉所有的w和b，

0:19:58.000,0:20:00.000
看哪一個w和b代入L(w,b)，

0:20:02.000,0:20:04.000
可以讓這個loss的值最小。

0:20:06.000,0:20:08.000
那這一個w跟b就是最好的w跟b，

0:20:08.000,0:20:10.000
那麼寫成w*

0:20:10.000,0:20:12.000
跟b*。

0:20:12.000,0:20:14.000
或者是我們把L這個function列出來，

0:20:16.000,0:20:18.000
L這個function我們知道它長的就是這個樣子，

0:20:20.000,0:20:22.000
那我們就是把w和b，用各種不同的數值代到這個function裡面，

0:20:22.000,0:20:24.000
看哪一組w跟b，可以給我們最好的結果。

0:20:26.840,0:20:28.000
如果你修過線性代數的話，

0:20:30.000,0:20:32.000
其實這個對你來說應該完全不是問題，對不對?

0:20:32.000,0:20:34.000
這個是有closed-form solution的

0:20:34.000,0:20:36.000
像我相信你可能不記得它長甚麼樣子了

0:20:38.000,0:20:40.000
所謂的closed-form solution意思是說，

0:20:42.000,0:20:44.000
你只要把10隻寶可夢的CP值

0:20:44.000,0:20:46.000
跟他們進化後的ŷ

0:20:48.000,0:20:50.000
你只要把這些數值代到某一個function裡面，

0:20:52.000,0:20:54.000
它output就可以告訴你最好的w和b是甚麼。

0:20:54.000,0:20:56.000
如果你修過線代的話，

0:20:56.000,0:20:58.000
其實你理論上是應該是知道要怎麼做的。

0:21:00.000,0:21:02.000
我假設你已經忘記了，

0:21:02.000,0:21:04.000
那我們要教你另外一個做法。

0:21:04.000,0:21:06.000
這個做法叫做

0:21:06.000,0:21:08.000
gradient descent。

0:21:08.000,0:21:10.000
這邊要強調的是，gradient descent

0:21:10.000,0:21:12.000
不是只適用於解這一個function。

0:21:12.000,0:21:14.000
不是只適用於解這一個function。

0:21:14.000,0:21:16.000
解這一個function是比較容易的，

0:21:16.000,0:21:18.000
你有修過線代你其實就會了。

0:21:18.000,0:21:20.000
但是gradient descent它厲害的地方是，

0:21:22.000,0:21:24.000
只要你這個L是可微分的

0:21:24.000,0:21:26.000
不管它是甚麼function，

0:21:28.000,0:21:30.000
gradient descent都可以拿來處理這個function，

0:21:32.000,0:21:34.000
都可以拿來幫你找可能是比較好的function或者是參數。

0:21:34.000,0:21:36.000
都可以拿來幫你找可能是比較好的function或者是參數。

0:21:38.000,0:21:40.000
那我們來看一下gradient descent是怎麼做?

0:21:40.000,0:21:42.000
我們先假設一個比較簡單的task，

0:21:44.000,0:21:46.000
在這個比較簡單的task裡面，

0:21:46.000,0:21:48.000
我們的loss function L(w)，

0:21:48.000,0:21:50.000
我們的loss function L(w)，

0:21:50.000,0:21:52.000
它只有一個參數w。

0:21:52.000,0:21:54.000
那這個L(w)必然是不需要是我們之前定出來的那個loss function，

0:21:54.000,0:21:56.000
那這個L(w)必然是不需要是我們之前定出來的那個loss function，

0:21:56.000,0:21:58.000
它可以是任何function，

0:21:58.000,0:22:00.000
只要是可微分的就行了。

0:22:00.000,0:22:02.000
那我們現在要解的問題是，

0:22:02.000,0:22:04.000
找一個w，

0:22:04.000,0:22:06.000
讓這個L(w)最小。

0:22:06.000,0:22:08.000
這件事情怎麼做呢?

0:22:08.000,0:22:10.000
那暴力的方法就是，

0:22:10.000,0:22:12.000
窮舉所有w可能的數字，

0:22:12.000,0:22:14.000
把所有w可能的數值，

0:22:14.000,0:22:16.000
從負無限大到無限大，

0:22:16.000,0:22:18.000
一個一個值都代到loss function裡面去，

0:22:18.000,0:22:20.000
試一下這個loss function的value，

0:22:20.000,0:22:22.000
你就會知道說，哪一個w的值，

0:22:22.000,0:22:24.000
可以讓loss最小。

0:22:24.000,0:22:26.000
如果你做這件事的話，你就會發現說，

0:22:26.000,0:22:28.000
比如說這裡這個w的值，

0:22:28.000,0:22:30.000
可以讓loss最小。

0:22:30.000,0:22:32.000
但是這樣做是沒有效率的，

0:22:32.000,0:22:34.000
怎麼做比較有效率呢?

0:22:34.000,0:22:36.000
這就是gradient descent要告訴我們的。

0:22:36.000,0:22:38.000
這個作法是這樣子的 :

0:22:40.000,0:22:40.500
我們首先隨機選取一個初始的點。

0:22:42.000,0:22:44.000
我們首先隨機選取一個初始的點。

0:22:46.000,0:22:48.000
比如這邊隨機選取的是，

0:22:48.000,0:22:50.000
w0

0:22:54.000,0:22:56.000
其實你也不一定要隨機選取，

0:22:56.000,0:22:58.000
其實有可能有一些其他的方法，

0:22:58.000,0:23:00.000
可以讓你找的值是比較好的。

0:23:00.000,0:23:02.000
這個之後再提，

0:23:02.000,0:23:04.000
現在就想成是，

0:23:04.000,0:23:06.000
隨機選取一個初始的點w00

0:23:06.000,0:23:08.000
接下來，

0:23:08.000,0:23:10.000
在這個初始的w0這個位置，

0:23:10.000,0:23:12.000
我們去計算一下，

0:23:12.000,0:23:14.000
w這個參數

0:23:24.000,0:23:26.000
我們要計算在w=w0這個位置，

0:23:26.000,0:23:28.000
我們要計算在w=w0這個位置，

0:23:28.000,0:23:30.000
參數w對loss function的微分。

0:23:30.000,0:23:32.000
參數w對loss function的微分。

0:23:34.000,0:23:36.000
如果你對微分不熟的話，

0:23:36.000,0:23:38.000
反正我們這邊要找的就是切線斜率。

0:23:40.000,0:23:42.000
如果今天這個切線斜率是負的的話，

0:23:42.000,0:23:44.000
如果今天這個切線斜率是負的的話，

0:23:44.000,0:23:46.000
那顯然就是，

0:23:46.000,0:23:48.000
所以從這個圖上就可以很明顯地看到

0:23:48.000,0:23:50.000
如果切線斜率是負的的話，

0:23:50.000,0:23:52.000
顯然左邊loss是比較高的，

0:23:52.000,0:23:54.000
右邊loss是比較低的，

0:23:54.000,0:23:56.000
那我們要找loss比較低的function，

0:23:58.000,0:24:00.000
所以你應該增加你的w值，

0:24:00.000,0:24:02.000
你應該增加w0值。

0:24:04.000,0:24:06.000
反之，如果今天算出來的斜率是正的，

0:24:06.000,0:24:08.000
代表跟這條虛線反向，

0:24:08.000,0:24:08.960
也就是右邊高左邊低的話，

0:24:08.960,0:24:09.460
也就是右邊高左邊低的話，

0:24:09.460,0:24:10.000
也就是右邊高左邊低的話，

0:24:10.000,0:24:12.000
也就是右邊高左邊低的話，

0:24:12.000,0:24:14.000
那我們顯然應該減少w的值。

0:24:14.000,0:24:15.000
把我們的參數往左邊移動，

0:24:15.000,0:24:16.000
把我們的參數往左邊移動，

0:24:16.000,0:24:18.000
把我們的參數減小。

0:24:18.000,0:24:18.500
或者是，

0:24:18.500,0:24:20.000
假如你對微分也不熟切線也不熟的話，

0:24:20.000,0:24:22.000
假如你對微分也不熟切線也不熟的話，

0:24:22.000,0:24:22.920
那你就想成是，

0:24:22.920,0:24:24.000
有一個人，

0:24:24.000,0:24:26.000
站在w0這個點。

0:24:26.000,0:24:27.920
他往前後各窺視了一下，

0:24:28.000,0:24:30.000
他往前後各窺視了一下，

0:24:30.000,0:24:32.000
看一下他往左邊走一步，

0:24:32.000,0:24:34.000
loss會減少，

0:24:34.000,0:24:36.000
往左邊走一步loss會減少，

0:24:36.000,0:24:36.880
還是往右邊走一步，

0:24:36.880,0:24:38.000
loss會減少。

0:24:38.000,0:24:40.000
如果往右邊走一步loss會減少的話，

0:24:40.000,0:24:42.000
那他就會往右邊走一步。

0:24:42.000,0:24:43.100
總之在這個例子裏面，

0:24:43.100,0:24:44.000
我們的參數是增加的，

0:24:44.000,0:24:46.760
我們的參數是會增加的，

0:24:46.760,0:24:47.600
會往右邊移動。

0:24:48.000,0:24:48.500
那怎麼增加呢?

0:24:50.000,0:24:52.000
應該要增加多少呢?

0:24:52.000,0:24:53.080
這邊的增加量，

0:24:53.080,0:24:54.000
我們寫成

0:24:56.000,0:24:58.000
有關gradient descent 的theory，

0:24:58.000,0:25:00.000
我們留到下次再講，

0:25:00.000,0:25:02.000
我們今天就講一下它的操作是甚麼樣子。

0:25:04.000,0:25:06.000
如果我們往右邊踏一步的話，

0:25:06.000,0:25:08.000
應該要踏多少呢?

0:25:08.000,0:25:10.000
這個踏一步的step size，

0:25:10.000,0:25:12.000
取決於兩件事。

0:25:12.000,0:25:14.000
第一件事情是，

0:25:14.000,0:25:16.000
現在的微分值有多大。

0:25:16.000,0:25:18.000
現在的dL/dw有多大

0:25:18.000,0:25:20.000
現在的dL/dw有多大

0:25:20.000,0:25:22.000
如果微分值越大，

0:25:22.000,0:25:24.000
代表現在在一個越陡峭的地方，

0:25:24.000,0:25:26.000
那它的移動的距離就越大，

0:25:26.000,0:25:28.000
反之就越小。

0:25:28.000,0:25:30.000
反之就越小。

0:25:30.000,0:25:32.000
那還取決於另外一件事情，

0:25:32.000,0:25:32.900
這個另外一件事情，

0:25:32.900,0:25:34.000
是一個常數項，

0:25:34.000,0:25:36.000
是一個常數項。

0:25:36.000,0:25:38.000
這個常數項這個η，

0:25:38.000,0:25:40.000
我們把它叫做learning rate。

0:25:40.000,0:25:42.000
這個learning rate決定說，

0:25:42.000,0:25:44.000
我們今天踏一步，

0:25:44.000,0:25:46.000
不只是取決於我們現在微分值算出來有多大，

0:25:46.000,0:25:48.000
不只是取決於我們現在微分值算出來有多大，

0:25:48.000,0:25:50.000
還取決於我們一個

0:25:50.000,0:25:52.000
事先就定好的數值。

0:25:52.000,0:25:54.000
這個learning rate是一個事先定好的數值。

0:25:54.000,0:25:56.000
如果這個事先定好的數值你給它定大一點的話，

0:25:56.000,0:25:58.000
如果這個事先定好的數值你給它定大一點的話，

0:25:58.000,0:26:00.000
那今天踏出一步的時候，

0:26:00.000,0:26:02.000
參數更新的幅度就比較大，

0:26:02.000,0:26:04.000
反之參數更新的幅度就比較小。

0:26:04.000,0:26:06.000
如果參數更新的幅度比較大的話，

0:26:06.000,0:26:08.000
你learning rate大一點的話，

0:26:08.000,0:26:10.000
那學習的效率，學習的速度就比較快。

0:26:10.000,0:26:12.000
那學習的效率，學習的速度就比較快。

0:26:12.000,0:26:14.000
所以這個參數η，

0:26:14.000,0:26:16.000
我們就稱之為learning rate。

0:26:16.000,0:26:18.000
所以，

0:26:18.000,0:26:20.000
現在我們已經算出，

0:26:20.000,0:26:22.000
在w0這個地方，

0:26:22.000,0:26:24.000
我們應該把參數更新η乘上dL/dw

0:26:24.000,0:26:26.000
我們應該把參數更新η乘上dL/dw

0:26:26.000,0:26:28.000
我們應該把參數更新η乘上dL/dw

0:26:28.000,0:26:30.000
我們應該把參數更新η乘上dL/dw

0:26:30.000,0:26:32.000
所以你就把原來的參數w0減掉η乘以dL/dw

0:26:32.000,0:26:34.000
所以你就把原來的參數w0減掉η乘以dL/dw

0:26:34.000,0:26:36.000
所以你就把原來的參數w0減掉η乘以dL/dw

0:26:36.000,0:26:38.000
所以你就把原來的參數w0減掉η乘以dL/dw

0:26:38.000,0:26:40.000
這邊會有一項減的，

0:26:42.000,0:26:44.000
因為如果我們這個微分算出來是負的的話，

0:26:46.000,0:26:48.000
要增加這個w的值，

0:26:48.000,0:26:48.840
如果算出來是正的的話，

0:26:48.840,0:26:50.000
要減少w的值。

0:26:52.000,0:26:54.000
所以這一項微分值，跟我們的增加減少是反向的，

0:26:54.000,0:26:56.000
所以我們前面需要乘以一個負號。

0:26:58.000,0:27:00.000
那我們把w0更新以後，

0:27:00.000,0:27:02.000
變成w1，

0:27:02.000,0:27:04.000
接下來就是重複剛才看到的步驟，

0:27:04.000,0:27:06.000
接下來就是重複剛才看到的步驟，

0:27:06.000,0:27:08.000
重新去計算一次

0:27:08.000,0:27:10.000
在w=w1這個地方，

0:27:10.000,0:27:12.000
所算出來的微分值。

0:27:14.000,0:27:16.000
假設這個微分值算出來是這樣子的，

0:27:16.000,0:27:18.000
這個微分值仍然建議我們，

0:27:18.000,0:27:20.000
應該往右移動我們的參數，

0:27:20.000,0:27:22.000
只是現在移動的幅度，

0:27:22.000,0:27:24.000
可能是比較小的。

0:27:24.000,0:27:26.000
因為這個微分值，

0:27:26.000,0:27:28.000
相較於前面這一項，

0:27:28.000,0:27:30.000
是比較小的。

0:27:30.000,0:27:32.000
那你就把w1−ηdL/dw，

0:27:32.000,0:27:34.000
然後變成w2。

0:27:36.000,0:27:38.000
那這個步驟就反覆不斷地執行下去，

0:27:38.000,0:27:40.000
那這個步驟就反覆不斷地執行下去，

0:27:42.000,0:27:44.000
經過非常非常多的iteration後，

0:27:44.000,0:27:46.000
經過非常多次的參數更新以後，

0:27:48.000,0:27:50.000
假設經過t次的更新，

0:27:50.000,0:27:52.000
這個t是一個非常大的數字，

0:27:52.000,0:27:54.000
最後你會到一個local minimum的地方。

0:27:54.000,0:27:56.000
所謂local minimum的地方，

0:27:56.000,0:27:58.000
就是這個地方的微分是0，

0:27:58.000,0:28:00.000
就是這個地方的微分是0，

0:28:00.000,0:28:02.000
所以你接下來算出的微分都是0了，

0:28:02.000,0:28:04.000
所以你的參數接下來就會卡在這邊，

0:28:04.000,0:28:06.000
就沒有辦法再更新了。

0:28:06.000,0:28:08.000
這件事情你可能會覺得不太高興，

0:28:10.000,0:28:12.000
因為這邊其實有一個local minimum，

0:28:14.000,0:28:16.960
你找出來的跟gradient descent找出來的solution，

0:28:16.960,0:28:18.000
你找出來的參數，

0:28:18.000,0:28:20.000
它其實不是最佳解。

0:28:20.000,0:28:23.000
你只能找到 local minimum，

0:28:23.000,0:28:24.000
你沒有辦法找到global minimum。

0:28:24.000,0:28:26.000
但幸運的是，這件事情在regression上面，

0:28:26.000,0:28:28.000
不是一個問題。

0:28:28.000,0:28:30.000
因為在regression上面，

0:28:30.000,0:28:32.000
在linear regression上面，

0:28:32.000,0:28:34.000
它是沒有local minimum，

0:28:34.000,0:28:36.000
等下這種事情我們會再看到。

0:28:38.000,0:28:40.000
今天我們剛才討論的是，

0:28:40.000,0:28:41.940
只有一個參數的情形，

0:28:42.000,0:28:44.000
那如果是有兩個參數呢

0:28:44.000,0:28:46.000
那如果是有兩個參數呢

0:28:46.000,0:28:48.000
我們今天真正要處理的事情，

0:28:48.000,0:28:50.000
是有兩個參數的問題，

0:28:50.000,0:28:52.000
也就是w跟b。

0:28:52.000,0:28:54.000
其實有兩個參數，

0:28:54.000,0:28:56.000
從一個參數推廣到兩個參數，

0:28:56.000,0:28:58.000
其實是沒有任何不同的。

0:28:58.000,0:29:00.000
首先你就隨機選取兩個初始值，

0:29:00.000,0:29:02.000
首先你就隨機選取兩個初始值，

0:29:02.000,0:29:04.000
w0和b0，

0:29:04.000,0:29:06.000
接下來，

0:29:06.000,0:29:08.000
你就計算，

0:29:08.000,0:29:10.000
在w=w0，b=b0的時候

0:29:12.000,0:29:14.000
w對loss的偏微分，

0:29:14.000,0:29:16.000
你在計算w=w0，b=b0的時候，

0:29:16.000,0:29:18.000
你在計算w=w0，b=b0的時候，

0:29:18.000,0:29:20.000
b對L的偏微分。

0:29:22.000,0:29:24.000
接下來，你計算出這兩個偏微分之後，

0:29:24.000,0:29:26.000
你就分別去更新w0和b0這兩個參數，

0:29:26.000,0:29:28.000
你就分別去更新w0和b0這兩個參數，

0:29:28.000,0:29:30.000
你就把w0減掉η乘上w對L的偏微分，

0:29:30.000,0:29:32.000
你就把w0減掉η乘上w對L的偏微分，

0:29:32.000,0:29:34.000
你就把w0減掉η乘上w對L的偏微分，

0:29:34.000,0:29:36.000
得到w1。

0:29:36.000,0:29:38.000
你就把b0減掉η乘上b對L的偏微分，

0:29:38.000,0:29:40.000
你就把b0減掉η乘上b對L的偏微分，

0:29:40.000,0:29:42.000
你就把b0減掉η乘上b對L的偏微分，

0:29:42.000,0:29:44.000
你就得到b1。

0:29:44.000,0:29:46.000
這個步驟你就反覆的持續下去，

0:29:46.000,0:29:48.000
這個步驟你就反覆的持續下去，

0:29:48.000,0:29:50.000
接下來，

0:29:50.000,0:29:52.000
你算出b1和w1以後，

0:29:52.000,0:29:54.000
你就再計算一次w和L 的偏微分

0:29:54.000,0:29:56.000
只是現在是計算w=w1，b=b1的時候的偏微分，

0:29:56.000,0:29:58.000
只是現在是計算w=w1，b=b1的時候的偏微分，

0:29:58.000,0:30:00.000
所以這項偏微分跟這項偏微分的值不是一樣的，

0:30:00.000,0:30:02.000
這是在不同位置算出來的。

0:30:02.000,0:30:04.000
接下來你有了w1和b1以後，

0:30:04.000,0:30:06.000
接下來你有了w1和b1以後，

0:30:06.000,0:30:10.000
你就計算w1和b1在w=w1,b=b1的時候，

0:30:10.000,0:30:12.000
w對L的偏微分，

0:30:12.000,0:30:14.000
還有w1和b1在w=w1,b=b1的時候，

0:30:14.000,0:30:16.000
b對L的偏微分。

0:30:18.000,0:30:20.000
接下來你就更新參數，

0:30:20.000,0:30:22.000
你就把w1減掉η乘上算出來的微分值，

0:30:22.000,0:30:24.000
你就把w1減掉η乘上算出來的微分值，

0:30:24.000,0:30:26.000
你就得到w2。

0:30:26.000,0:30:28.000
你把b1減掉η乘上微分值就得到b2。

0:30:28.000,0:30:30.000
你把b1減掉η乘上微分值就得到b2。

0:30:30.000,0:30:32.000
你就反覆進行這個步驟，

0:30:34.000,0:30:36.000
最後你就可以找到一個loss相對比較小的w值跟b的值，

0:30:36.000,0:30:38.000
最後你就可以找到一個loss相對比較小的w值跟b的值，

0:30:38.000,0:30:40.000
這邊要補充說明的是，

0:30:40.000,0:30:42.000
所謂的gradient descent的gradient指的是甚麼呢?

0:30:42.000,0:30:44.000
所謂的gradient descent的gradient指的是甚麼呢?

0:30:44.000,0:30:46.000
其實gradient就是這個倒三角   ∇L

0:30:46.000,0:30:48.000
其實gradient就是這個倒三角   ∇L

0:30:48.000,0:30:50.000
我知道大家已經，

0:30:50.000,0:30:52.000
很久沒有學微積分了，

0:30:52.000,0:30:54.000
所以我猜你八成不記得∇L是甚麼。

0:30:54.000,0:30:56.000
這個∇L就是，

0:30:58.000,0:31:00.000
你把w對L的偏微分和b對L的偏微分排成一個vector，

0:31:00.000,0:31:02.000
你把w對L的偏微分和b對L的偏微分排成一個vector，

0:31:02.000,0:31:04.000
你把w對L的偏微分和b對L的偏微分排成一個vector，

0:31:04.000,0:31:06.000
你把w對L的偏微分和b對L的偏微分排成一個vector，

0:31:06.000,0:31:08.000
這一項就是gradient。

0:31:08.000,0:31:10.000
因為我們在整個process裡面，

0:31:10.000,0:31:12.000
我們要計算w對L的偏微分和b對L的偏微分，

0:31:12.000,0:31:14.000
我們要計算w對L的偏微分和b對L的偏微分，

0:31:14.000,0:31:16.000
這個就是gradient。

0:31:16.000,0:31:18.000
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。

0:31:18.000,0:31:20.000
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。

0:31:20.000,0:31:22.000
所以這門課如果沒必要的話，我們就盡量不要把這個大家不熟悉的符號弄出來。

0:31:22.000,0:31:24.000
只是想要讓大家知道說gradient指的就是這個東西。

0:31:26.000,0:31:28.000
那我們來visualize一下剛才做的事情。

0:31:28.000,0:31:30.000
那我們來visualize一下剛才做的事情。

0:31:30.000,0:31:32.000
剛才做的事情像是這樣 :

0:31:32.000,0:31:34.000
有兩個參數w和b，

0:31:34.000,0:31:36.000
有兩個參數w和b，

0:31:36.000,0:31:38.000
這兩個參數決定了一個function長甚麼樣。

0:31:40.000,0:31:42.000
那在這個圖上的顏色，

0:31:42.000,0:31:44.000
這個圖上的顏色代表loss function的數值。

0:31:44.000,0:31:46.000
這個圖上的顏色代表loss function的數值。

0:31:46.000,0:31:48.000
這個圖上的顏色代表loss function的數值。

0:31:48.000,0:31:50.000
越偏藍色代表loss越小，

0:31:50.000,0:31:52.000
那我們隨機選取一個初始值，

0:31:54.000,0:31:56.000
是在左下角紅色的點這個地方。

0:31:56.000,0:31:56.660
接下來，

0:31:56.660,0:31:58.000
你就去計算，

0:31:58.000,0:32:00.000
在紅色這個點，

0:32:08.000,0:32:10.000
b對loss的偏微分還有w對loss的偏微分

0:32:12.000,0:32:14.000
然後你就把參數更新，

0:32:14.000,0:32:16.000
這個η乘上b對loss的偏微分。

0:32:16.000,0:32:20.000
還有η乘上w對loss的偏微分。

0:32:20.000,0:32:22.000
還有η乘上w對loss的偏微分。

0:32:24.000,0:32:26.000
如果你對偏微分比較不熟的話，

0:32:26.000,0:32:26.840
其實這個方向，

0:32:28.000,0:32:30.000
這個gradient的方向，

0:32:32.000,0:32:34.000
其實就是等高線的法線方向。

0:32:34.000,0:32:36.000
其實就是等高線的法線方向。

0:32:36.000,0:32:38.000
其實就是等高線的法線方向。

0:32:40.000,0:32:42.000
那我們就可以更新這個參數，

0:32:42.000,0:32:44.000
從這個地方，

0:32:44.000,0:32:46.000
到這個地方。

0:32:46.000,0:32:48.000
接下來你就再計算一次偏微分，

0:32:48.000,0:32:50.000
這個偏微分告訴你說現在應該往這個方向，

0:32:50.000,0:32:52.000
這個偏微分告訴你說現在應該往這個方向，

0:32:52.000,0:32:54.000
更新你的參數。

0:32:54.000,0:32:56.000
你就把你的參數從這個地方移到這個地方。

0:32:56.000,0:32:58.000
接下來它再告訴你說，

0:32:58.000,0:32:58.920
應該這樣子走，

0:32:58.920,0:33:00.000
應該這樣子走，

0:33:00.000,0:33:02.000
然後你就把參數從這個地方

0:33:02.000,0:33:04.000
再更新到這個地方。

0:33:04.000,0:33:06.000
那gradient descent有一個讓人擔心的地方。

0:33:08.000,0:33:10.000
就是如果今天你的loss function 長的是這個樣子，

0:33:10.000,0:33:12.000
就是如果今天你的loss function 長的是這個樣子，

0:33:12.000,0:33:14.000
就是如果今天你的loss function 長的是這個樣子，

0:33:14.000,0:33:16.000
如果今天w和b對這個loss L，它看起來是這個樣子，

0:33:16.000,0:33:18.000
如果今天w和b對這個loss L，它看起來是這個樣子，

0:33:18.000,0:33:20.000
如果今天w和b對這個loss L，它看起來是這個樣子，

0:33:20.000,0:33:22.000
那你就麻煩了。

0:33:22.000,0:33:24.000
那你就麻煩了。

0:33:24.000,0:33:26.000
這個時候如果你的隨機取捨值是在這個地方，

0:33:26.000,0:33:28.000
這個時候如果你的隨機取捨值是在這個地方，

0:33:28.000,0:33:30.000
那按照gradient建議你的方向，

0:33:30.000,0:33:32.000
按照今天這個偏微分建議你的方向，

0:33:32.000,0:33:33.800
你走走走走走，

0:33:34.000,0:33:36.000
就找到這個function。

0:33:38.000,0:33:40.000
如果你隨機取捨的地方是在這個地方，

0:33:40.000,0:33:42.000
那根據gradient的方向，

0:33:42.000,0:33:44.000
你走走走就走到這個地方。

0:33:44.000,0:33:44.840
所以變成說這個方法你找到的結果，

0:33:44.840,0:33:46.000
所以變成說這個方法你找到的結果，

0:33:46.000,0:33:48.000
是看人品的。

0:33:50.000,0:33:52.000
這個讓人非常非常的擔心，

0:33:52.000,0:33:54.000
但是在linear regression裡面，

0:33:54.000,0:33:56.000
你不用太擔心。

0:33:56.000,0:33:58.000
為甚麼呢?

0:33:58.000,0:34:00.000
因為在linear regression裡面，

0:34:00.000,0:34:02.000
你的這個loss function L，

0:34:02.000,0:34:04.000
你的這個loss function L，

0:34:04.000,0:34:06.000
它是convex。

0:34:06.000,0:34:08.000
如果你定義你loss的方式跟我在前幾頁投影片講的是一樣的話，

0:34:08.000,0:34:10.000
如果你定義你loss的方式跟我在前幾頁投影片講的是一樣的話，

0:34:10.000,0:34:12.000
那一個loss是convex的。

0:34:12.000,0:34:14.000
那一個loss是convex的。

0:34:14.000,0:34:16.000
如果你不知道convex是甚麼的話，

0:34:16.000,0:34:16.800
換句話說，

0:34:16.800,0:34:18.000
它是沒有local的optimal的位置，

0:34:18.000,0:34:20.000
它是沒有local的optimal的位置，

0:34:20.000,0:34:22.000
或者是，

0:34:22.000,0:34:24.000
如果我們把圖畫出來的話，

0:34:24.000,0:34:26.000
它長的就是這樣子。

0:34:26.000,0:34:28.000
它的等高線就是一圈一圈橢圓形的。

0:34:28.000,0:34:30.000
它的等高線就是一圈一圈橢圓形的。

0:34:30.000,0:34:32.000
它的等高線就是一圈一圈橢圓形的。

0:34:32.000,0:34:34.000
所以它是沒有local optimal的地方，

0:34:34.000,0:34:36.000
所以你隨便選一個起始點，

0:34:36.000,0:34:40.000
根據gradient descent所幫你找出來的最佳的參數

0:34:40.000,0:34:42.000
根據gradient descent所幫你找出來的最佳的參數

0:34:42.000,0:34:44.000
根據gradient descent所幫你找出來的最佳的參數

0:34:44.000,0:34:46.000
你最後找出來的都會是同一組參數。

0:34:52.000,0:34:54.000
我們來看一下它的formulation。

0:34:54.000,0:34:56.000
其實這個式子是非常簡單的，

0:34:56.000,0:34:58.000
假如你要實際算一下w對L的偏微分和b對L的偏微分

0:34:58.000,0:35:00.000
假如你要實際算一下w對L的偏微分和b對L的偏微分

0:35:00.000,0:35:02.000
假如你要實際算一下w對L的偏微分和b對L的偏微分

0:35:02.000,0:35:04.000
這個式子長的是甚麼樣子呢?

0:35:04.000,0:35:06.000
這個式子長的是甚麼樣子呢?

0:35:08.000,0:35:10.000
這個L我們剛才已經看到了，

0:35:10.000,0:35:12.000
它是長這個樣子。

0:35:12.000,0:35:14.000
它是估測誤差的平方和。

0:35:16.000,0:35:18.000
如果我們把它對w做偏微分，

0:35:18.000,0:35:20.000
我們得到甚麼樣的式子呢?

0:35:22.000,0:35:24.000
這個其實非常簡單，

0:35:24.000,0:35:26.000
我相信有修過微積分的人都可以秒算。

0:35:28.000,0:35:30.000
你就把這個2移到左邊，

0:35:32.000,0:35:34.000
你要對w做偏微分，

0:35:36.000,0:35:38.000
你就先把括號裡面這一項先做偏微分，

0:35:38.000,0:35:40.000
你把2移到左邊，

0:35:40.000,0:35:42.000
你得到這樣子的結果。

0:35:42.000,0:35:44.000
接下來考慮括號裡面的部分，

0:35:44.000,0:35:46.000
括號裡面的部分，

0:35:46.000,0:35:48.000
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的

0:35:48.000,0:35:50.000
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的

0:35:50.000,0:35:52.000
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的

0:35:52.000,0:35:54.000
只有負的 𝑤 ∙ 𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的

0:35:54.000,0:35:56.000
只有負的 𝑤 ∙𝑥上標𝑛 下標𝑐𝑝這一項是跟w有關的

0:35:56.000,0:35:58.000
所以如果你把括號裡面的equation對w做偏微分的話，

0:35:58.000,0:36:00.000
所以如果你把括號裡面的equation對w做偏微分的話，

0:36:00.000,0:36:02.000
所以如果你把括號裡面的equation對w做偏微分的話，

0:36:02.000,0:36:04.000
你得到的值就是負的 𝑥上標𝑛 下標𝑐𝑝

0:36:04.000,0:36:06.000
所以partial w partial L，

0:36:08.000,0:36:10.000
w對L的偏微分它的式子就是長這樣

0:36:10.000,0:36:12.000
w對L的偏微分它的式子就是長這樣

0:36:14.000,0:36:16.000
如果你要算d對L的偏微分的話，

0:36:16.000,0:36:16.980
也非常簡單，

0:36:18.000,0:36:20.000
你就把2移到前面，

0:36:20.000,0:36:22.000
把2移到前面，

0:36:22.000,0:36:24.000
變成這個樣子。

0:36:24.000,0:36:26.000
然後你再把這個括號裡面的值，

0:36:26.000,0:36:28.000
對b做偏微分。

0:36:28.000,0:36:30.000
括號裡面只有負b這一項，

0:36:30.000,0:36:32.000
跟我們要做偏微分的這個b有關，

0:36:32.000,0:36:34.000
跟我們要做偏微分的這個b有關，

0:36:34.000,0:36:36.000
所以－b對b做偏微分得到的值，

0:36:36.000,0:36:38.000
是－1

0:36:38.000,0:36:40.000
然後就結束了。

0:36:40.000,0:36:42.000
所以有了gradient descent，

0:36:42.000,0:36:43.580
你就知道說怎麼算偏微分，

0:36:44.000,0:36:46.000
那你就可以找一個最佳的function。

0:36:46.000,0:36:48.000
那你就可以找一個最佳的function。

0:38:26.000,0:38:28.000
那結果怎麼樣呢?

0:38:28.000,0:38:30.000
我們的model長這樣，

0:38:30.000,0:38:32.000
然後費盡一番功夫以後，

0:38:34.000,0:38:36.000
你找出來的最好的b跟w，

0:38:36.000,0:38:38.000
根據training data分別是b= －188.4，w=2.7

0:38:38.000,0:38:40.000
根據training data分別是b= －188.4，w=2.7

0:38:40.000,0:38:42.000
根據training data分別是b= －188.4，w=2.7

0:38:42.000,0:38:46.000
如果你把這一個function: y=b+w×Xcp

0:38:46.000,0:38:48.000
如果你把這一個function: y=b+w×Xcp

0:38:48.000,0:38:50.000
把它的b跟w值畫到圖上的話，

0:38:50.000,0:38:52.000
它長的是這個樣子。

0:38:52.000,0:38:54.000
這一條紅色的線，

0:38:54.000,0:38:56.000
那妳可以計算一下，

0:38:56.000,0:38:58.000
你會發現說這一條紅色的線，

0:39:00.000,0:39:02.000
沒有辦法完全正確的評定，

0:39:02.000,0:39:04.000
所有的寶可夢的進化後的CP值。

0:39:06.000,0:39:08.000
如果你想要知道說他做的有多不好的話，

0:39:08.000,0:39:10.000
或者是多好的話，

0:39:10.000,0:39:12.000
你可以看一下，

0:39:12.000,0:39:14.000
你可以計算一下你的error。

0:39:14.000,0:39:16.000
你的error就是，

0:39:16.000,0:39:18.000
你計算一下每一個藍色的點跟這個紅色的點之間的距離，

0:39:18.000,0:39:20.000
你計算一下每一個藍色的點跟這個紅色的點之間的距離，

0:39:20.000,0:39:22.000
第一個藍色的點跟這個紅色的線的距離，

0:39:22.000,0:39:24.000
第一個藍色的點跟這個紅色的線的距離，

0:39:24.000,0:39:26.780
是e1

0:39:26.780,0:39:28.000
第二個藍色的點跟紅色的線的距離是e2

0:39:28.000,0:39:30.000
以此類推，

0:39:30.000,0:39:32.000
所以有e1到e10。

0:39:32.000,0:39:34.000
那平均的training data的error，

0:39:34.000,0:39:35.600
就是summation e1到e10。

0:39:36.000,0:39:38.000
這邊算出來是31.9

0:39:38.000,0:39:40.000
這邊算出來是31.9

0:39:42.000,0:39:44.000
但是這個並不是我們真正關心的，

0:39:44.000,0:39:46.000
因為你真正關心的是，

0:39:46.000,0:39:48.000
generalization的case。

0:39:48.000,0:39:50.000
也就是說，

0:39:50.000,0:39:52.000
假設你今天抓到一隻新的寶可夢以後，

0:39:54.000,0:39:56.000
如果使用你現在的model去預測的話，

0:39:58.000,0:40:00.000
那做出來你估測的誤差到底有多少。

0:40:00.000,0:40:02.000
那做出來你估測的誤差到底有多少。

0:40:02.000,0:40:04.000
所以真正關心的是，那些你沒有看過的新的data，

0:40:04.000,0:40:06.000
這邊我們叫做testing data，

0:40:06.000,0:40:08.000
這邊我們叫做testing data，

0:40:08.000,0:40:09.960
它的誤差是多少。

0:40:10.000,0:40:12.000
所以這邊又抓了另外10隻寶可夢，

0:40:12.000,0:40:14.000
所以這邊又抓了另外10隻寶可夢，

0:40:14.000,0:40:16.000
當作testing data。

0:40:16.000,0:40:16.500
這10隻寶可夢跟之前拿來做訓練的10隻，

0:40:16.500,0:40:18.000
不是同樣的10隻。

0:40:22.000,0:40:24.000
其實這新抓的10隻跟剛才看到的10隻的分布，

0:40:24.000,0:40:26.000
其實這新抓的10隻跟剛才看到的10隻的分布，

0:40:26.000,0:40:28.000
其實是還滿像的

0:40:28.000,0:40:30.000
它們就是這個圖上的10個點。

0:40:30.000,0:40:30.920
那你會發現說，

0:40:30.920,0:40:32.000
我們剛才在訓練資料上找出來這條紅色的線，

0:40:32.000,0:40:34.000
我們剛才在訓練資料上找出來這條紅色的線，

0:40:34.000,0:40:36.000
我們剛才在訓練資料上找出來這條紅色的線，

0:40:36.000,0:40:38.000
其實也可以大致上預測，

0:40:38.000,0:40:40.000
在我們沒有看過的寶可夢上，

0:40:40.000,0:40:42.000
它的進化後的CP值。

0:40:44.000,0:40:46.000
如果你想要量化它的錯誤的話，

0:40:46.000,0:40:48.000
那就計算一下它的錯誤。

0:40:48.000,0:40:50.000
它錯誤算出來是35.0。

0:40:50.000,0:40:52.000
它錯誤算出來是35.0。

0:40:52.000,0:40:54.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:40:54.000,0:40:56.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:40:56.000,0:40:58.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:40:58.000,0:41:00.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:41:00.000,0:41:02.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:41:02.000,0:41:04.000
這個值是比我們剛才在training data 上看到的error還要稍微大一點

0:41:04.000,0:41:06.000
因為可以想想看我們最好的function是在training data上找到的

0:41:06.000,0:41:08.000
所以在training data上面算出來的error，

0:41:08.000,0:41:10.000
本來就應該比testing data上面算出來的error還要稍微大一點

0:41:10.000,0:41:12.000
本來就應該比testing data上面算出來的error還要稍微大一點

0:41:12.000,0:41:14.000
本來就應該比testing data上面算出來的error還要稍微大一點

0:41:14.000,0:41:16.000
本來就應該比testing data上面算出來的error還要稍微大一點

0:41:18.000,0:41:20.000
有沒有辦法做得更好呢?

0:41:22.000,0:41:24.000
如果你想要做得更好的話，

0:41:24.000,0:41:26.000
接下來你要做的事情就是，

0:41:26.000,0:41:28.000
重新去設計你的model。

0:41:28.000,0:41:30.000
如果你觀察一下data你會發現說，

0:41:34.000,0:41:36.000
在原進化前的CP值特別大的地方，

0:41:36.000,0:41:38.000
還有進化前的CP值特別小的地方，

0:41:38.000,0:41:40.000
預測是比較不準的。

0:41:42.000,0:41:44.000
在這個地方和這個地方，預測是比較不準的。

0:41:46.000,0:41:48.000
那你可以想想看說

0:41:50.000,0:41:52.000
任天堂在做這個遊戲的時候，

0:41:52.000,0:41:54.000
它背後一定是有某一支程式，

0:41:54.000,0:41:56.000
去根據某一些hidden 的factor。

0:41:56.000,0:41:58.000
去根據某一些hidden 的factor。

0:41:58.000,0:42:00.000
去根據某一些hidden 的factor。

0:42:00.000,0:42:02.000
比如說，去根據原來的CP值和其他的一些數值，

0:42:02.000,0:42:04.000
generate 進化以後的數值。

0:42:04.000,0:42:06.000
generate 進化以後的數值。

0:42:08.000,0:42:10.000
所以到底它的function長甚麼樣子?

0:42:10.000,0:42:10.940
從這個結果看來，

0:42:10.940,0:42:12.000
那個function可能不是這樣子一條直線，

0:42:12.000,0:42:14.000
那個function可能不是這樣子一條直線，

0:42:14.000,0:42:16.000
那個function可能不是這樣子一條直線，

0:42:16.000,0:42:18.000
它可能是稍微更複雜一點。

0:42:18.000,0:42:20.000
它可能是稍微更複雜一點。

0:42:22.000,0:42:24.000
所以我們需要有一個更複雜的model。

0:42:24.000,0:42:24.880
舉例來說，

0:42:24.880,0:42:26.000
我們這邊可能需要引入二次式。

0:42:26.000,0:42:28.000
我們這邊可能需要引入二次式。

0:42:28.000,0:42:30.000
我們這邊可能需要引入二次式。

0:42:30.000,0:42:31.260
我們今天，

0:42:31.260,0:42:32.000
可能需要引入(Xcp)²這一項

0:42:32.000,0:42:34.000
可能需要引入(Xcp)²這一項

0:42:34.000,0:42:36.000
我們重新設計了一個model，

0:42:36.000,0:42:38.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:38.000,0:42:40.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:40.000,0:42:42.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:42.000,0:42:44.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:44.000,0:42:46.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:46.000,0:42:48.000
這個model它寫成y=b+w1×Xcp+w2×(Xcp)²

0:42:48.000,0:42:50.000
我們加了後面這一項

0:42:50.000,0:42:52.000
如果我們有了這個新的function，

0:42:52.000,0:42:54.000
你可以用我們剛才講得一模一樣的方式，

0:42:54.000,0:42:56.000
去define一個function 的好壞，

0:42:56.000,0:42:58.000
然後用gradient descent，

0:42:58.000,0:43:00.000
找出一個，

0:43:00.000,0:43:02.000
在你的function set裡面最好的function。

0:43:02.000,0:43:04.000
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3

0:43:04.000,0:43:06.000
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3

0:43:06.000,0:43:08.000
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3

0:43:08.000,0:43:10.000
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3

0:43:10.000,0:43:12.000
根據training data找出來的最好的function是b=−10.3, w1=1.0, w2=2.7×10^-3

0:43:12.000,0:43:14.000
如果我們把這個最好的function畫在圖上的話，

0:43:14.000,0:43:16.000
它長的是這個樣子。

0:43:16.000,0:43:18.000
它長的是這個樣子。

0:43:18.000,0:43:18.800
你就會發現說，

0:43:18.800,0:43:20.000
現在我們有了這條新的曲線，

0:43:20.000,0:43:22.000
現在我們有了這條新的曲線，

0:43:22.000,0:43:24.000
我們有了這個新的model，

0:43:24.000,0:43:26.000
它的預測在training data上面看起來是更準一點。

0:43:26.000,0:43:28.000
它的預測在training data上面看起來是更準一點。

0:43:28.000,0:43:30.000
它的預測在training data上面看起來是更準一點。

0:43:30.000,0:43:32.000
在training data上面你得到的average error現在是15.4。

0:43:32.000,0:43:34.000
在training data上面你得到的average error現在是15.4。

0:43:34.000,0:43:34.500
但我們真正關心的，

0:43:36.000,0:43:38.000
是testing data

0:43:38.000,0:43:40.000
那我們就把同樣的model

0:43:40.000,0:43:42.000
再apply到testing data上。

0:43:44.000,0:43:46.000
我們在testing data上apply同樣這條紅色的線，

0:43:46.000,0:43:48.000
然後去計算它的average error。

0:43:48.000,0:43:50.000
那我們現在得到的是18.4

0:43:52.000,0:43:54.000
在剛才如果我們沒有考慮(Xcp)²的時候

0:43:54.000,0:43:56.000
在剛才如果我們沒有考慮(Xcp)²的時候

0:43:56.000,0:43:58.000
算出來的average error是30左右

0:43:58.000,0:44:00.000
現在有考慮平方項得到的是18.4

0:44:00.000,0:44:02.000
現在有考慮平方項得到的是18.4

0:44:02.000,0:44:04.000
現在有考慮平方項得到的是18.4

0:44:04.000,0:44:06.000
那有沒有可能做得更好呢?

0:44:06.000,0:44:08.000
比如說我們可以考慮一個更複雜的model。

0:44:08.000,0:44:10.980
比如說我們可以考慮一個更複雜的model。

0:44:10.980,0:44:12.000
我們引入不只是(Xcp)²，

0:44:12.000,0:44:14.000
我們引入不只是(Xcp)²，

0:44:14.000,0:44:16.000
我們引入(Xcp)³。

0:44:16.000,0:44:18.000
我們引入(Xcp)³。

0:44:18.000,0:44:20.000
所以我們現在的model長的是這個樣子。

0:44:22.000,0:44:22.800
你就用一模一樣的方法，

0:44:22.800,0:44:24.000
你就可以根據你的training data，

0:44:24.000,0:44:26.000
找到在這一個 function set裡面，

0:44:26.000,0:44:28.000
在這個model 裡面最好的一個function。

0:44:28.000,0:44:30.000
在這個model 裡面最好的一個function。

0:44:30.000,0:44:32.000
那找出來是這樣 :

0:44:32.000,0:44:32.500
b=6.4，w1=0.66,

0:44:32.500,0:44:34.000
w2=4.3×10^-3 ，w3=-1.8×10^-6

0:44:34.000,0:44:36.000
w2=4.3×10^-3 ，w3=-1.8×10^-6

0:44:36.000,0:44:38.000
w2=4.3×10^-3 ，w3=-1.8×10^-6

0:44:38.000,0:44:40.000
所以你發現w3其實是它的值比較小

0:44:42.000,0:44:44.000
它可能是沒有太大的影響

0:44:46.000,0:44:48.000
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的

0:44:48.000,0:44:50.000
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的

0:44:50.000,0:44:52.000
作出來的線其實跟剛才看到的二次的線是沒有太大的差別的

0:44:52.000,0:44:54.000
那做出來看起來像是這個樣子。

0:44:56.000,0:44:58.000
那這個時候average error算出來是15.3

0:44:58.000,0:45:00.000
如果你看testing data的話，

0:45:00.000,0:45:02.000
如果你看testing data的話，

0:45:02.000,0:45:04.000
testing data算出來的average error是18.1。

0:45:04.000,0:45:06.640
testing data算出來的average error是18.1。

0:45:06.640,0:45:08.000
跟剛剛二次的，

0:45:08.000,0:45:10.000
有考慮(Xcp)²的結果比起來是稍微好一點點。

0:45:10.000,0:45:12.000
有考慮(Xcp)²的結果比起來是稍微好一點點。

0:45:12.000,0:45:14.000
有考慮(Xcp)²的結果比起來是稍微好一點點。

0:45:14.000,0:45:16.000
剛才前一頁是18.3，有考慮三次項後是18.1

0:45:16.000,0:45:18.000
剛才前一頁是18.3，有考慮三次項後是18.1

0:45:18.000,0:45:20.000
是稍微好一點點。

0:45:20.000,0:45:22.000
那有沒有可能是更複雜的model呢?

0:45:22.000,0:45:24.000
那有沒有可能是更複雜的model呢?

0:45:26.000,0:45:28.000
或許在寶可夢的那個程式背後，

0:45:28.000,0:45:30.000
它產生進化後的CP值用的是一個更複雜的一個function

0:45:30.000,0:45:32.000
它產生進化後的CP值用的是一個更複雜的一個function

0:45:32.000,0:45:34.000
它產生進化後的CP值用的是一個更複雜的一個function

0:45:34.000,0:45:34.720
或許它不只考慮了三次，

0:45:34.720,0:45:36.000
或者它不只考慮了(Xcp)³

0:45:36.000,0:45:38.000
或者它不只考慮了(Xcp)³

0:45:38.000,0:45:40.000
或許它考慮的是四次方也說不定

0:45:40.000,0:45:42.000
或許它考慮的是四次方也說不定

0:45:42.000,0:45:42.520
那你就用同樣的方法，

0:45:44.000,0:45:46.000
再把這些參數 :b、w1、w2、w3和w4都找出來，

0:45:46.000,0:45:48.000
再把這些參數 :b、w1、w2、w3和w4都找出來，

0:45:48.000,0:45:50.000
那你得到的function長這個樣子。

0:45:50.000,0:45:52.000
那你得到的function長這個樣子。

0:45:54.000,0:45:56.000
你發現它在training data上它顯然可以做得更好。

0:45:56.000,0:45:58.000
在input的CP值比較小的這些寶可夢，

0:45:58.000,0:46:00.000
在input的CP值比較小的這些寶可夢，

0:46:00.000,0:46:02.000
在input的CP值比較小的這些寶可夢，

0:46:02.000,0:46:04.000
這些顯然是一些綠毛蟲之類的東西，

0:46:04.000,0:46:06.000
這些顯然是一些綠毛蟲之類的東西，

0:46:06.000,0:46:08.000
這些顯然是一些綠毛蟲之類的東西，

0:46:08.000,0:46:10.000
它在這邊是predict更準的。

0:46:10.000,0:46:12.000
它在這邊是predict更準的。

0:46:12.000,0:46:14.000
所以現在的average error是14.9，

0:46:14.000,0:46:16.000
所以現在的average error是14.9，

0:46:16.000,0:46:18.000
剛才三次的是15.3

0:46:18.000,0:46:20.000
剛才三次的是在training data上是15.3

0:46:24.000,0:46:26.000
現在四次的時候在training data上是14.9

0:46:26.000,0:46:28.000
但是我們真正關心的是testing

0:46:30.000,0:46:32.000
我們真正關心的是如果沒有看過的寶可夢，

0:46:32.000,0:46:34.000
我們能夠多精確的預測它進化後的CP值。

0:46:34.000,0:46:36.000
我們能夠多精確的預測它進化後的CP值。

0:46:36.000,0:46:36.720
所以，

0:46:36.720,0:46:38.000
我們發現說，

0:46:40.000,0:46:42.000
如果我們看沒有看過的寶可夢的話，

0:46:42.000,0:46:44.000
我們得到的average error是多少呢?

0:46:46.000,0:46:48.000
我們得到的average error其實是28.8

0:46:48.000,0:46:50.000
前一頁做出來已經是18.3了

0:46:50.700,0:46:52.000
就我們用三次的時候，

0:46:52.000,0:46:54.000
在testing data上面做出來已經是18.3了

0:46:56.000,0:46:58.000
但是我們換了一個更複雜的model以後，

0:46:58.000,0:47:00.660
做出來是28.8

0:47:00.660,0:47:02.000
結果竟然變得更糟了!

0:47:02.000,0:47:04.000
結果竟然變得更糟了!

0:47:04.000,0:47:06.000
我們換了一個更複雜的model，

0:47:06.000,0:47:08.000
在training data上給我們比較好的結果

0:47:08.000,0:47:10.000
但在testing data上，

0:47:10.000,0:47:12.000
看起來結果是更糟的。

0:47:12.000,0:47:14.000
那如果換再更複雜的model會怎樣呢?

0:47:14.000,0:47:16.000
有沒有可能是五次式，

0:47:16.000,0:47:18.000
有沒有可能是五次式，

0:47:18.000,0:47:20.000
有沒有可能它背後的程式是如此的複雜，

0:47:22.000,0:47:24.000
原來的CP值的一次、兩次、三次、四次到五次

0:47:24.000,0:47:26.000
原來的CP值的一次、兩次、三次、四次到五次

0:47:26.000,0:47:28.000
那這個時候，

0:47:28.000,0:47:30.000
我們把最好的function找出來，

0:47:34.000,0:47:36.000
你會發現它最好的function在training data上長得像是這樣子。

0:47:36.000,0:47:38.000
你會發現它最好的function在training data上長得像是這樣子。

0:47:38.000,0:47:40.000
你會發現它最好的function在training data上長得像是這樣子。

0:47:40.000,0:47:42.000
這個是一個合理的結果嗎?

0:47:42.000,0:47:44.000
你會發現說，

0:47:44.000,0:47:46.000
在原來的CP值是500左右，

0:47:46.000,0:47:48.000
在原來的CP值是500左右，

0:47:48.000,0:47:50.000
500左右可能就是伊布之類的東西。

0:47:52.000,0:47:54.000
在原來的CP值是500左右的寶可夢，

0:47:54.000,0:47:56.000
根據你現在的model預測出來，

0:47:56.000,0:47:58.000
它的CP值竟然是負的。

0:48:00.000,0:48:02.000
但是在training data上面，

0:48:02.000,0:48:04.000
但是在training data上面，

0:48:06.000,0:48:09.000
我們可以算出來的error是12.8，

0:48:09.000,0:48:10.000
比我們剛才用四次式，

0:48:10.000,0:48:12.000
得到的結果又再更好一些。

0:48:14.000,0:48:16.000
那在testing的結果上是怎樣呢

0:48:18.000,0:48:20.000
如果我們把這個我們找出來的function，

0:48:20.000,0:48:22.000
apply到新的寶可夢上面，

0:48:22.000,0:48:24.000
你會發現結果怎麼爛掉了啊

0:48:26.000,0:48:28.000
至少這一隻大概是伊布吧

0:48:28.000,0:48:30.000
這隻伊布，

0:48:30.000,0:48:32.000
它預測出來進化後的CP值，

0:48:32.000,0:48:34.000
它預測出來進化後的CP值，

0:48:34.000,0:48:36.000
是非常的不准。

0:48:36.000,0:48:38.000
照理說有1000多，

0:48:38.000,0:48:40.000
但是你的model卻給它一個負的預測值。

0:48:40.000,0:48:42.000
但是你的model卻給它一個負的預測值。

0:48:42.000,0:48:44.000
所以算出來的average error非常大，

0:48:44.000,0:48:46.000
有200多。

0:48:46.000,0:48:48.000
所以當我們換了一個更複雜的model，

0:48:48.000,0:48:48.800
考慮到五次的時候，

0:48:50.000,0:48:52.000
結果又更加糟糕了。

0:48:56.000,0:48:58.000
所以到目前為止，我們試了五個不同的model。

0:48:58.000,0:49:00.000
那這五個model，

0:49:00.000,0:49:02.000
如果你把他們分別的在training data上面的average error都畫出來的話，

0:49:02.000,0:49:04.000
如果你把他們分別的在training data上面的average error都畫出來的話，

0:49:04.000,0:49:06.000
如果你把他們分別的在training data上面的average error都畫出來的話，

0:49:06.000,0:49:08.000
如果你把他們分別的在training data上面的average error都畫出來的話，

0:49:08.000,0:49:10.000
你會得到這樣子的一張圖。

0:49:12.000,0:49:14.000
從高到低

0:49:16.000,0:49:18.000
也就是說，如果你考慮一個最簡單的model，

0:49:18.000,0:49:20.000
這個時候error是比較高的；

0:49:20.000,0:49:21.520
model稍微複雜一點，

0:49:22.000,0:49:24.000
error稍微下降；

0:49:24.000,0:49:26.000
然後model越複雜，

0:49:26.000,0:49:28.000
在這個training data上的error就會越來越小。

0:49:28.000,0:49:30.000
在這個training data上的error就會越來越小。

0:49:30.000,0:49:32.000
那為甚麼會這樣呢?

0:49:32.000,0:49:32.880
這件事情倒是非常的直覺，

0:49:32.880,0:49:33.380
非常容易解釋。

0:49:36.000,0:49:38.000
假設黃色的這個圈圈，

0:49:38.000,0:49:40.000
我們故意用一樣的顏色

0:49:40.000,0:49:42.000
黃色這個圈圈代表這一個式子，

0:49:42.000,0:49:44.000
黃色這個圈圈代表這一個式子，

0:49:44.000,0:49:46.000
有考慮三次的式子，

0:49:46.000,0:49:48.000
所形成的function space。

0:49:50.000,0:49:52.000
那四次的式子所形成的function space，

0:49:52.000,0:49:54.000
就是這個綠色的圈圈。

0:49:54.000,0:49:56.000
它是包含黃色的圈圈的，

0:49:56.000,0:49:56.920
這個事情很合理，

0:49:56.920,0:49:58.000
因為你只要把w4設為0，

0:49:58.000,0:50:00.000
因為你只要把w4設為0，

0:50:00.000,0:50:02.000
是四次的這個式子就可以變成三次的式子。

0:50:02.000,0:50:04.000
是四次的這個式子就可以變成三次的式子。

0:50:06.000,0:50:08.000
所以三次的式子都包含在這個四次的式子裡面，

0:50:08.000,0:50:10.000
所以三次的式子都包含在這個四次的式子裡面，

0:50:10.000,0:50:12.000
黃色的圈圈都包含在綠色的圈圈裏面。

0:50:14.000,0:50:16.000
那如果我們今天考慮更複雜的五次的式子的話，

0:50:16.000,0:50:18.000
它又可以包含所有四次的式子。

0:50:18.000,0:50:20.000
它又可以包含所有四次的式子。

0:50:26.000,0:50:28.000
所以今天如果你有一個越複雜的model，

0:50:30.000,0:50:32.000
它包含了越多的function的話，

0:50:36.000,0:50:40.000
那理論上你就可以找出一個function，

0:50:40.000,0:50:42.000
它可以讓你的error rate越來越低。

0:50:42.000,0:50:44.000
你的function如果越複雜，你的candidate如果越多，

0:50:44.000,0:50:44.800
你當然可以找到一個function，

0:50:44.800,0:50:46.000
讓你的error rate越來越低。

0:50:46.000,0:50:48.000
讓你的error rate越來越低。

0:50:48.000,0:50:50.000
當然這邊的前提就是，

0:50:50.000,0:50:52.000
你的gradient descent要能夠真正幫你找出best function的前提下，

0:50:52.000,0:50:54.000
你的gradient descent要能夠真正幫你找出best function的前提下，

0:50:54.000,0:50:54.640
你的function越複雜，

0:50:54.640,0:50:56.000
可以讓你的error rate在training function上越低。

0:50:56.000,0:50:58.000
可以讓你的error rate在training function上越低。

0:50:58.000,0:51:00.000
可以讓你的error rate在training function上越低。

0:51:00.000,0:51:02.000
但是在testing data上面，

0:51:02.000,0:51:04.000
看起來的結果是不一樣的。

0:51:04.000,0:51:06.000
在testing data上面看起來的結果是不一樣的。

0:51:06.000,0:51:08.000
在testing data上面看起來的結果是不一樣的。

0:51:08.000,0:51:08.500
在training data上，

0:51:08.500,0:51:10.000
你會發現說

0:51:10.000,0:51:12.000
model越來越複雜你的error越來越低；

0:51:12.000,0:51:14.000
model越來越複雜你的error越來越低；

0:51:14.000,0:51:16.000
但是在testing data上，

0:51:16.000,0:51:19.040
在到第三個式子為止，

0:51:19.040,0:51:20.000
你的error是有下降的。

0:51:22.000,0:51:24.000
但是到第四個和第五個的function的時候，

0:51:24.000,0:51:26.000
error就暴增。

0:51:26.000,0:51:28.000
然後把它的圖試著畫在左邊這邊。

0:51:28.000,0:51:30.000
然後把它的圖試著畫在左邊這邊。

0:51:30.000,0:51:32.000
藍色的是training data上對不同function的error，

0:51:32.000,0:51:34.000
藍色的是training data上對不同function的error，

0:51:34.000,0:51:36.000
橙色的是testing data上對不同function的error。

0:51:36.000,0:51:38.000
橙色的是testing data上對不同function的error。

0:51:38.000,0:51:39.000
你會發現說，

0:51:39.000,0:51:40.000
今天在五次的時候，

0:51:40.000,0:51:42.000
在testing上是爆炸的，

0:51:42.000,0:51:44.000
它就突破天際沒辦法畫在這張圖上。

0:51:44.000,0:51:46.000
它就突破天際沒辦法畫在這張圖上。

0:51:46.000,0:51:48.000
那所以我們今天，

0:51:48.000,0:51:50.000
得到一個觀察，

0:51:50.000,0:51:52.000
雖然說越複雜的model可以在training data上面給我們越好的結果，

0:51:52.000,0:51:54.000
雖然說越複雜的model可以在training data上面給我們越好的結果，

0:51:54.000,0:51:56.000
但這件事情也沒有甚麼，

0:51:56.000,0:51:58.000
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。

0:51:58.000,0:52:00.000
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。

0:52:00.000,0:52:02.000
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。

0:52:02.000,0:52:04.000
因為越複雜的model並不一定能夠在testing data上給我們越好的結果。

0:52:06.000,0:52:08.000
這件事情就叫做overfitting。

0:52:08.000,0:52:10.000
就複雜的model在training data上有好的結果，

0:52:10.000,0:52:12.000
但在testing data上不一定有好的結果。

0:52:14.000,0:52:16.000
這件事情就叫做overfitting。

0:52:16.000,0:52:18.000
比如當我們用第四個和第五個式子的時候，

0:52:20.000,0:52:22.000
我們就發生overfitting的情形。

0:52:24.000,0:52:26.000
那為甚麼會有overfitting這個情形呢?

0:52:26.000,0:52:28.000
為甚麼更複雜的model它在training上面得到比較好的結果，

0:52:28.000,0:52:30.000
為甚麼更複雜的model它在training上面得到比較好的結果，

0:52:30.000,0:52:32.000
在testing上面不一定得到比較好的結果呢?

0:52:32.000,0:52:34.000
這個我們日後再解釋。

0:52:34.000,0:52:36.000
但是你其實是可以想到很多很直觀的，

0:52:36.000,0:52:38.000
在training data上面得到比較好的結果，

0:52:38.000,0:52:40.000
在training data上面得到比較好的結果，

0:52:40.000,0:52:41.660
在訓練的時候得到比較好的結果

0:52:42.000,0:52:44.000
但在測試的時候不一定會得到比較好的結果。

0:52:44.000,0:52:44.840
比如說，

0:52:44.840,0:52:46.000
你有沒有考過駕照?

0:52:48.000,0:52:50.000
考駕照不是都要去那個駕訓班嗎?

0:52:52.000,0:52:54.000
駕訓班不是都在那個場內練習嗎

0:52:54.000,0:52:56.000
你在場內練習不是都很順?

0:52:56.000,0:52:56.600
練習非常非常多次以後，

0:52:56.600,0:52:58.000
你就會得到很奇怪的技能。

0:52:58.000,0:52:58.760
你就學到說，

0:52:58.760,0:53:00.000
比如說，

0:53:00.000,0:53:02.000
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，

0:53:02.000,0:53:04.000
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，

0:53:04.000,0:53:06.000
當我後照鏡裡面看到路邊小丸子的貼紙對到正中間的時候，

0:53:06.000,0:53:08.000
就把方向盤左轉半圈這樣子。

0:53:08.000,0:53:10.000
你就學到這種技能，

0:53:10.000,0:53:12.000
所以你在測試訓練的時候，

0:53:12.000,0:53:14.000
在駕訓班的時候你可以做得很好。

0:53:14.000,0:53:16.000
但在路上的時候你就做不好。

0:53:16.000,0:53:18.000
像我就不太會開車，

0:53:18.000,0:53:20.000
雖然我有駕照。

0:53:20.000,0:53:22.000
所以我都在等無人駕駛車出來。

0:53:28.000,0:53:30.000
所以overfitting是很有可能會發生的。

0:53:30.000,0:53:32.000
所以model不是越複雜越好，

0:53:34.000,0:53:36.000
我們必須選一個剛剛好，

0:53:36.000,0:53:38.000
沒有非常複雜也沒有很複雜的model，

0:53:38.000,0:53:40.000
沒有非常複雜也沒有很複雜的model，

0:53:40.000,0:53:42.000
你要選一個最適合的model。

0:53:42.000,0:53:44.000
比如說在這個case裡面，

0:53:46.000,0:53:48.000
當我們選一個三次式的時候，

0:53:48.000,0:53:50.000
在這個case裡面當我們選一個三次式的時候，

0:53:52.000,0:53:54.000
可以給我們最低的error。

0:53:54.000,0:53:56.000
所以如果今天可以選的話，

0:53:56.000,0:53:58.000
我們就應該選擇三次的式子來作為我們的model，

0:53:58.000,0:54:00.000
我們就應該選擇三次的式子來作為我們的model，

0:54:00.000,0:54:02.000
來作為我們的function set。

0:54:04.000,0:54:06.000
你以為這樣就結束了嗎?

0:54:06.000,0:54:08.000
其實還沒有。

0:54:08.000,0:54:10.000
剛才只收集了10隻寶可夢，其實太少了

0:54:12.000,0:54:14.000
當我們收集到60隻寶可夢的時候，

0:54:14.000,0:54:16.000
你會發現說剛才都是白忙一場。

0:54:18.000,0:54:20.000
你仔細想 : 當我們收集60隻寶可夢，

0:54:22.000,0:54:24.000
你把它的原來的CP值和進化後的CP值，

0:54:24.000,0:54:26.000
你把它的原來的CP值和進化後的CP值，

0:54:26.000,0:54:28.000
畫在這個圖上，

0:54:28.000,0:54:30.000
你會發現說他們中間有一個非常奇妙的關係。

0:54:30.000,0:54:32.000
你會發現說他們中間有一個非常奇妙的關係。

0:54:32.000,0:54:34.000
它顯然不是甚麼一次二次三次一百次式，

0:54:34.000,0:54:36.000
顯然都不是，

0:54:38.000,0:54:40.000
中間有另外一個力量，

0:54:42.000,0:54:44.940
這個力量不是CP值它在影響著進化後的數值，

0:54:44.940,0:54:46.000
到底是甚麼呢?

0:54:46.000,0:54:48.000
其實非常的直覺，

0:54:48.000,0:54:50.000
就是寶可夢的物種。

0:54:52.000,0:54:54.000
這邊我們把不同的物種用不同的顏色來表示，

0:54:54.000,0:54:56.000
這邊我們把不同的物種用不同的顏色來表示，

0:55:00.000,0:55:02.000
藍色是波波，

0:55:02.000,0:55:04.000
波波進化後是比比鳥，

0:55:04.000,0:55:06.000
比比鳥進化是大比鳥。

0:55:16.000,0:55:18.000
這個黃色的點是獨角蟲，

0:55:18.000,0:55:20.000
這個黃色的點是獨角蟲，

0:55:20.000,0:55:22.000
獨角蟲進化後是鐵殼蛹，

0:55:22.000,0:55:24.000
鐵殼蛹進化後是大針蜂。

0:55:28.000,0:55:30.000
然後綠色的是綠毛蟲，

0:55:30.000,0:55:32.000
綠毛蟲進化是鐵甲蛹，

0:55:32.000,0:55:34.000
鐵甲蛹進化是巴大蝴。

0:55:34.000,0:55:36.000
紅色的是伊布，

0:55:36.000,0:55:38.000
伊布可以進化成雷精靈、火精靈或水精靈等等

0:55:40.000,0:55:42.000
你可能說怎麼都只有這些路邊就可以見到的，

0:55:42.000,0:55:44.000
因為抓乘龍快龍是很麻煩的，

0:55:44.000,0:55:45.100
所以就只有這些而已。

0:55:52.000,0:55:54.000
所以剛才只考慮CP值這件事，

0:55:54.000,0:55:56.000
只考慮進化前的CP值顯然是不對的。

0:55:56.000,0:55:58.000
只考慮進化前的CP值顯然是不對的。

0:56:02.000,0:56:04.000
因為這個進化後的CP值受到物種的影響其實是很大的。

0:56:04.000,0:56:06.000
因為這個進化後的CP值受到物種的影響其實是很大的。

0:56:06.000,0:56:08.000
或者是比原來的CP值

0:56:08.000,0:56:10.000
產生非常關鍵性的影響。

0:56:10.000,0:56:11.240
所以我們在設計model的時候，

0:56:14.000,0:56:16.000
剛才那個model設計的是不好的。

0:56:16.000,0:56:16.880
剛才那個model就好像是，

0:56:18.000,0:56:20.000
你想要海底撈針，

0:56:20.000,0:56:22.000
從function set裡面撈出一個最好的model

0:56:22.000,0:56:22.640
那其實裡面model通通都不好，

0:56:22.640,0:56:24.000
所以針根本就不在海裡，

0:56:26.000,0:56:28.000
所以你要重新設計一下你的function set。

0:56:30.000,0:56:32.000
所以這邊就重新設計一下function set，

0:56:32.000,0:56:34.000
我們的function set， input x跟output y，

0:56:34.000,0:56:36.000
這個input寶可夢和output進化後的CP值有甚麼關係呢?

0:56:36.000,0:56:38.000
這個input寶可夢和output進化後的CP值有甚麼關係呢?

0:56:38.000,0:56:40.000
它的關係是這樣 :

0:56:40.000,0:56:42.000
如果今天輸入的寶可夢x，

0:56:42.000,0:56:44.000
如果今天輸入的寶可夢x，

0:56:46.000,0:56:48.000
它的物種是屬於波波的話，

0:56:50.000,0:56:52.000
這個Xs代表說這個input x 的物種，

0:56:56.000,0:56:58.000
那他的輸出y=b1+w1×Xcp。

0:56:58.000,0:57:00.000
那他的輸出y=b1+w1×Xcp。

0:57:00.000,0:57:02.000
那他的輸出y=b1+w1×Xcp。

0:57:02.000,0:57:04.000
那如果它是獨角蟲的話，

0:57:06.000,0:57:08.000
y=b2+w2×Xcp。

0:57:08.000,0:57:10.000
如果它是綠毛蟲的話，

0:57:10.000,0:57:12.000
就是b3+w3×Xcp。

0:57:12.000,0:57:14.000
如果它是伊布的話，

0:57:14.000,0:57:15.600
就用另外一個式子。

0:57:16.000,0:57:18.000
也就是不同的物種，我們就看它是哪一個物種，

0:57:20.000,0:57:20.740
我們就代不同的linear function，

0:57:20.740,0:57:22.000
我們就代不同的linear function，

0:57:22.000,0:57:24.000
然後得到不同的y作為最終的輸出。

0:57:24.000,0:57:26.000
然後得到不同的y作為最終的輸出。

0:57:26.000,0:57:28.000
你可能會問一個問題說，

0:57:30.000,0:57:32.000
你把if 放到整個function裡面，

0:57:32.000,0:57:34.000
這樣你不就不是一個linear model了嗎?

0:57:36.000,0:57:38.000
function裡面有if 你搞得定嗎?

0:57:38.000,0:57:40.000
你可以用微分來做嗎?

0:57:40.000,0:57:42.000
你可以用剛才的gradient descent來算參數對loss function微分嗎?

0:57:42.000,0:57:44.000
你可以用剛才的gradient descent來算參數對loss function微分嗎?

0:57:44.000,0:57:46.000
其實是可以的。

0:57:46.000,0:57:48.000
這個式子你可以把它改寫成一個linear function。

0:57:50.000,0:57:52.000
寫起來就是這樣 :

0:57:54.000,0:57:56.000
這個有一點複雜但沒有關係。

0:57:56.000,0:57:58.000
我們先來觀察一下δ這個function。

0:57:58.000,0:58:00.000
如果你有修過信號的處理，我想應該知道δ這個function是指甚麼。

0:58:02.000,0:58:04.000
今天這個δ這個function的意思是說，

0:58:04.000,0:58:06.000
δ of Xs等於比比鳥的意思就是說，

0:58:06.000,0:58:08.000
δ of Xs等於比比鳥的意思就是說，

0:58:10.000,0:58:12.000
假如我們今天輸入的這隻寶可夢是比比鳥的話，

0:58:12.000,0:58:14.000
假如我們今天輸入的這隻寶可夢是比比鳥的話，

0:58:14.000,0:58:16.000
這個δ function它的output就是1。

0:58:16.000,0:58:18.000
這個δ function它的output就是1。

0:58:18.000,0:58:20.000
反之如果是其他種類的寶可夢的話，

0:58:20.000,0:58:22.000
它δ function的output就是0。

0:58:24.000,0:58:26.000
所以我們可以把剛才那個有 if的式子，

0:58:26.000,0:58:28.000
寫成像這邊這個樣子。

0:58:28.000,0:58:30.000
寫成像這邊這個樣子。

0:58:30.000,0:58:30.740
你的寶可夢進化後的CP值，

0:58:32.000,0:58:34.000
等於b1×δ(比比鳥)這樣子，

0:58:34.000,0:58:36.000
等於b1×δ(比比鳥)這樣子，

0:58:36.000,0:58:38.000
等於b1×δ(比比鳥)這樣子，

0:58:38.000,0:58:40.000
等於b1×δ(比比鳥)這樣子，

0:58:40.000,0:58:42.000
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，

0:58:42.000,0:58:44.000
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，

0:58:44.000,0:58:46.000
然後加上w1×δ(比比鳥)×這隻寶可夢的CP值，

0:58:46.000,0:58:48.000
加上b2×δ(獨角蟲)，

0:58:48.000,0:58:50.000
加上b2×δ(獨角蟲)，

0:58:50.000,0:58:52.000
加上w2×δ(獨角蟲)，

0:58:52.000,0:58:54.000
再乘上它的CP值，

0:58:54.000,0:58:56.000
然後接下來考慮綠毛蟲，

0:58:58.000,0:59:00.000
然後接下來考慮伊布。

0:59:02.000,0:59:03.780
你可能會想說這個跟剛剛那個式子哪裡一樣了呢?

0:59:06.000,0:59:08.000
你想想看，假如我們今天輸入的那一隻神奇寶貝，

0:59:10.000,0:59:12.000
假如我們今天輸入的那一隻寶可夢，

0:59:12.000,0:59:14.000
是比比鳥的話，

0:59:14.000,0:59:16.000
假如Xs等於比比鳥的話，

0:59:18.000,0:59:20.000
意味著這兩個function會是1，

0:59:22.000,0:59:24.000
這兩個δ function如果input是比比鳥的話就是1，

0:59:24.000,0:59:26.000
其他δ function就是0。

0:59:26.000,0:59:28.760
其他δ function就是0。

0:59:28.760,0:59:30.000
那乘上0的項，

0:59:32.000,0:59:34.000
乘上0的項就當作沒看到，

0:59:36.000,0:59:40.000
其實就變成y=b1+w1×Xcp

0:59:42.000,0:59:44.000
所以對其他種類的寶可夢來說也是一樣。

0:59:46.000,0:59:48.000
所以當我們設計這個function的時候，

0:59:50.000,0:59:52.000
我們就可以做到我們剛才在前一頁design的那一個有if的function。

0:59:52.000,0:59:54.000
我們就可以做到我們剛才在前一頁design的那一個有if的function。

0:59:54.000,0:59:54.500
那事實上這一個function，

0:59:54.500,0:59:56.000
它就是一個linear function。

0:59:56.000,0:59:58.000
它就是一個linear function。

0:59:58.000,1:00:00.000
這個function就是一個linear function。

1:00:00.000,1:00:02.000
怎麼說呢?

1:00:02.000,1:00:04.000
前面這個b1 w1到b4 w4就是我們的參數，

1:00:04.000,1:00:06.000
前面這個b1 w1到b4 w4就是我們的參數，

1:00:06.000,1:00:08.000
而後面這一項δ或者是δ乘以Xcp

1:00:08.000,1:00:10.000
而後面這一項δ或者是δ乘以Xcp

1:00:10.000,1:00:12.000
而後面這一項δ或者是δ乘以Xcp

1:00:12.000,1:00:14.000
不同的δ，

1:00:14.000,1:00:16.000
跟不同的δ乘以Xcp，

1:00:16.000,1:00:18.000
就是後面這個Xi這一項feature。

1:00:18.000,1:00:20.000
就是後面這個Xi這一項feature。

1:00:20.000,1:00:22.000
這個藍色框框裡面的這些，

1:00:22.000,1:00:24.000
其實就是feature。

1:00:24.000,1:00:26.000
所以這個東西它也是linear model。

1:00:26.000,1:00:28.000
所以這個東西它也是linear model。

1:00:28.000,1:00:30.000
那有了這些以後，

1:00:30.000,1:00:32.000
我們做出來的結果怎麼樣呢?

1:00:36.000,1:00:38.000
這個是在training data 上的結果，

1:00:40.000,1:00:42.000
在training data 上面，

1:00:42.000,1:00:44.000
我們知道不同種類的寶可夢，

1:00:44.000,1:00:46.000
它用的參數就不一樣。

1:00:48.000,1:00:50.000
所以不同種類的寶可夢，

1:00:54.000,1:00:56.000
它的線是不一樣的，

1:00:58.000,1:01:00.000
它的model的那條line是不一樣的。

1:01:02.000,1:01:04.000
藍色這條線是比比鳥的線，

1:01:04.000,1:01:06.000
綠色這條線是綠毛蟲的線，

1:01:08.000,1:01:10.000
黃色獨角蟲的線跟綠毛蟲的線其實是重疊的，

1:01:10.000,1:01:12.000
黃色獨角蟲的線跟綠毛蟲的線其實是重疊的，

1:01:12.000,1:01:14.000
紅色這條線是伊布的線。

1:01:14.000,1:01:16.000
所以就發現說，

1:01:16.000,1:01:18.000
當我們分不同種類的寶可夢來考慮的時候，

1:01:20.000,1:01:22.000
我們的model在training data上面可以得到更低的error。

1:01:22.000,1:01:24.000
我們的model在training data上面可以得到更低的error。

1:01:24.000,1:01:26.000
你發現說現在這幾條線，

1:01:26.000,1:01:28.000
是把training data fit得更好，

1:01:28.000,1:01:30.000
是把training data 解釋得更好，

1:01:30.000,1:01:32.000
如果說我們這麼做有考慮到寶可夢的種類的時候，

1:01:32.000,1:01:34.000
如果說我們這麼做有考慮到寶可夢的種類的時候，

1:01:36.000,1:01:38.000
我們得到的average error是3.8，在training data上。

1:01:38.000,1:01:40.000
但我們真正在意的是，

1:01:40.000,1:01:42.000
它能不能夠預測新看到的寶可夢，

1:01:44.000,1:01:46.000
也就是testing data上面的結果。

1:01:46.000,1:01:46.820
那在testing data上面，

1:01:46.820,1:01:48.000
它的結果是這個樣子：

1:01:50.000,1:01:52.000
一樣是這三條線，

1:01:54.000,1:01:56.000
發現說它也把在testing data上面的那些寶可夢fit得很好。

1:01:56.000,1:01:58.000
發現說它也把在testing data上面的那些寶可夢fit得很好。

1:01:58.000,1:02:00.000
然後它的average error是14.3

1:02:00.000,1:02:02.000
然後它的average error是14.3

1:02:02.000,1:02:04.000
這比我們剛才可以做好的18點多還要更好。

1:02:04.000,1:02:06.000
這比我們剛才可以做好的18點多還要更好。

1:02:06.000,1:02:08.000
但是如果你再觀察這個圖的話，

1:02:10.000,1:02:12.000
感覺應該是還有一些東西是沒有做好的。

1:02:12.000,1:02:14.000
感覺應該是還有一些東西是沒有做好的。

1:02:14.000,1:02:16.000
感覺應該是還有一些東西是沒有做好的。

1:02:18.000,1:02:20.000
我仔細想想看，我覺得伊布這邊應該就沒救了。

1:02:20.000,1:02:22.000
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。

1:02:22.000,1:02:24.000
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。

1:02:24.000,1:02:26.000
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。

1:02:26.000,1:02:28.000
因為我認為伊布會有很不一樣的CP值是因為進化成不同種類的精靈。

1:02:28.000,1:02:30.000
所以如果你沒有考慮這個factor的話，應該就沒救了。

1:02:34.000,1:02:36.000
但是我覺得這邊有一些還沒有fit很好的地方，

1:02:38.000,1:02:40.000
有一些值還是略高或略低於這條直線。

1:02:42.000,1:02:44.000
所以這個地方搞不好還是有辦法解釋的。

1:02:44.000,1:02:46.000
當然有一個可能是，

1:02:46.000,1:02:48.000
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化

1:02:48.000,1:02:50.000
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化

1:02:50.000,1:02:52.000
這些略高略低於，我們現在找出來這個藍色綠色線的這個model的變化

1:02:54.000,1:02:56.000
這個difference其實來自於random的數值，

1:02:58.000,1:03:00.000
就是每次那個寶可夢的程式產生進化後的CP值的時候，

1:03:00.000,1:03:02.000
就是每次那個寶可夢的程式產生進化後的CP值的時候，

1:03:02.000,1:03:04.000
它其實有加一個random的參數。

1:03:04.000,1:03:06.000
但也有可能是其實不是random的參數，

1:03:06.000,1:03:08.000
它還有其他的東西在影響著寶可夢進化後的CP值。

1:03:08.000,1:03:10.000
它還有其他的東西在影響著寶可夢進化後的CP值。

1:03:10.000,1:03:12.000
它還有其他的東西在影響著寶可夢進化後的CP值。

1:03:12.000,1:03:14.000
它還有其他的東西在影響著寶可夢進化後的CP值。

1:03:14.000,1:03:16.000
有什麼其他可能的參數呢?

1:03:16.000,1:03:18.000
比如說，

1:03:20.000,1:03:22.000
會不會進化後的CP值是跟weight有關係的?

1:03:22.000,1:03:24.000
會不會進化後的CP值是跟weight有關係的?

1:03:28.000,1:03:30.000
會不會進化後的CP值是跟它的高度有關係的?

1:03:30.000,1:03:32.000
會不會進化後的CP值是跟它的高度有關係的?

1:03:34.000,1:03:36.000
會不會進化後的CP值是跟它的HP有關係的?

1:03:36.000,1:03:38.000
會不會進化後的CP值是跟它的HP有關係的?

1:03:38.000,1:03:40.000
其實我們不知道，

1:03:40.000,1:03:42.000
我又不是大木博士我怎麼會知道這些事情，

1:03:44.000,1:03:46.000
所以如果你有domain knowledge的話，

1:03:46.000,1:03:48.000
你就可能可以知道說

1:03:48.000,1:03:50.000
你應該把甚麼樣的東西

1:03:50.000,1:03:52.000
加到你的model裡面去。

1:03:52.000,1:03:54.000
但是我又沒有domain knowledge，

1:03:54.000,1:03:56.000
那怎麼辦呢?

1:03:56.000,1:03:58.000
沒關係，有一招就是把你所有想到的東西，

1:03:58.000,1:04:00.000
通通塞進去，

1:04:00.000,1:04:02.000
我們來弄一個最複雜的function，然後看看會怎樣?

1:04:02.000,1:04:04.000
這個function我寫成這樣 :

1:04:04.000,1:04:06.000
如果它是比比鳥的話，

1:04:08.000,1:04:10.000
它的CP值我們就先計算一個y'，

1:04:10.000,1:04:12.000
它的CP值我們就先計算一個y'，

1:04:12.000,1:04:14.000
這個y'不是最後這個y，

1:04:14.000,1:04:16.000
這個y'還要做別的處理才能夠變成y。

1:04:20.000,1:04:22.000
我們就說，如果這個是比比鳥的話，

1:04:22.000,1:04:24.000
這其實不是比比鳥，這應該是波波，

1:04:24.000,1:04:26.000
因為比比鳥是進化後的。

1:04:26.000,1:04:28.000
這隻應該是波波。

1:04:28.000,1:04:30.000
那y'=b1+w1×Xcp+W5×(Xcp)²

1:04:30.000,1:04:32.000
那y'=b1+w1×Xcp+W5×(Xcp)²

1:04:32.000,1:04:34.000
那y'=b1+w1×Xcp+W5×(Xcp)²

1:04:34.000,1:04:36.000
我們就是不只要考慮CP值，

1:04:36.000,1:04:38.000
也要考慮CP值的平方。

1:04:40.000,1:04:42.000
如果是綠毛蟲，用另外一個式子。

1:04:42.000,1:04:44.000
如果是獨角蟲，用另外一個式子。

1:04:44.000,1:04:46.000
如果是伊布，用另外一個式子。

1:04:48.000,1:04:50.000
最好我們再把y'做其他的處理，

1:04:50.000,1:04:52.000
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10

1:04:52.000,1:04:54.000
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10

1:04:54.000,1:04:56.000
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10

1:04:56.000,1:04:58.000
我們把y'，再加上HP值，它的生命值乘上w9，再加上生命值的平方乘上w10

1:05:00.000,1:05:02.000
再加上高度乘上w11，

1:05:02.000,1:05:04.000
再加上高度的平方乘上w12，

1:05:04.000,1:05:06.000
再加上它的weight乘上w13，

1:05:06.000,1:05:08.000
再加上weight平方乘上w14，

1:05:08.000,1:05:10.000
這些東西合起來，

1:05:10.000,1:05:12.000
才是最後output的y。

1:05:12.000,1:05:14.000
所以這整個式子裡面，

1:05:14.000,1:05:16.000
其實也沒有很多個參數，

1:05:16.000,1:05:18.000
就是14+4=18

1:05:18.000,1:05:20.000
跟你們作業比起來，幾百個參數比起來，

1:05:20.000,1:05:22.000
其實也不是一個太複雜的model。

1:05:24.000,1:05:26.000
那我們現在有個這麼複雜的function，

1:05:26.000,1:05:28.000
在training data上我們得到的error，

1:05:28.000,1:05:30.000
期望應該就是非常的低。

1:05:32.000,1:05:34.000
我們果然得到一個非常低的error，

1:05:34.000,1:05:36.000
這個function你可以把它寫成線性的式子，

1:05:36.000,1:05:38.000
就跟剛才一樣，

1:05:38.000,1:05:40.000
這邊我就不解釋了。

1:05:40.000,1:05:42.000
那這麼一個複雜的function，

1:05:44.000,1:05:46.000
理論上我們可以得到非常低的training error。

1:05:46.000,1:05:48.000
training error算出來是1.9，

1:05:48.000,1:05:50.000
那你可以期待你在testing set上，

1:05:52.000,1:05:54.000
也算出很低的training error嗎?

1:05:54.000,1:05:56.000
倒是不見得。這麼複雜的model，

1:05:56.000,1:05:58.000
很以可能會overfitting。

1:05:58.000,1:05:59.060
你很有可能會得到，

1:06:00.000,1:06:02.000
在testing data上得到很糟的數字。

1:06:02.000,1:06:06.000
我們今天得到的數值很糟是102.3這樣子，

1:06:06.000,1:06:08.000
結果壞掉了

1:06:08.000,1:06:10.000
怎麼辦呢?

1:06:10.000,1:06:12.000
如果你是大木博士的話，

1:06:14.000,1:06:16.000
你就可以刪掉一些你覺得沒有用的input，

1:06:16.000,1:06:18.000
你就可以刪掉一些你覺得沒有用的input，

1:06:18.000,1:06:20.000
然後就得到一個簡單的model，

1:06:20.000,1:06:22.000
避免overfitting的情形。

1:06:22.000,1:06:24.000
但是我不是大木博士，

1:06:24.000,1:06:26.000
所以我有用別的方法來處理這個問題。

1:06:28.000,1:06:30.000
這招叫做regularization。

1:06:30.000,1:06:32.000
regularization要做的事情是，

1:06:32.000,1:06:34.000
我們重新定義了step 2的時候，

1:06:34.000,1:06:36.000
我們重新定義了step 2的時候，

1:06:36.000,1:06:38.000
我們對一個function是好還是壞的定義。

1:06:38.000,1:06:40.000
我們對一個function是好還是壞的定義。

1:06:40.000,1:06:42.000
我們重新redefine我們的loss function。

1:06:42.000,1:06:44.000
我們重新redefine我們的loss function。

1:06:44.000,1:06:46.000
然後，我們重新redefine我們的loss function，

1:06:48.000,1:06:50.000
把一些knowledge放進去，

1:06:50.000,1:06:52.000
讓我們可以找到比較好的function。

1:06:52.000,1:06:54.000
什麼意思呢?

1:06:54.000,1:06:56.000
假設我們的model in general寫成這樣 :

1:06:56.000,1:06:58.000
y=b+∑WiXi

1:06:58.000,1:07:00.000
我們原來的loss function，

1:07:00.000,1:07:02.000
我們原來的loss function，

1:07:02.000,1:07:04.000
它只考慮了error這件事。

1:07:04.000,1:07:06.000
原來的loss function只考慮了prediction的結果減掉正確答案的平方，

1:07:06.000,1:07:08.000
原來的loss function只考慮了prediction的結果減掉正確答案的平方，

1:07:08.000,1:07:10.000
原來的loss function只考慮了prediction的結果減掉正確答案的平方，

1:07:10.000,1:07:12.000
只考慮了prediction的error。

1:07:12.000,1:07:14.000
那regularization它就是加上一項額外的term，

1:07:14.000,1:07:16.000
那regularization它就是加上一項額外的term，

1:07:16.000,1:07:18.000
這一項額外的term是λ∑(Wi)²，

1:07:18.000,1:07:20.000
這一項額外的term是λ∑(Wi)²，

1:07:20.000,1:07:22.000
這一項額外的term是λ∑(Wi)²，

1:07:22.000,1:07:24.000
這一項額外的term是λ∑(Wi)²，

1:07:24.000,1:07:26.000
λ是一個常數，

1:07:26.000,1:07:28.000
這個是等一下我們要手調一下看要設多少。

1:07:28.000,1:07:30.000
那∑(Wi)²就是把這個model裡面所有的Wi，

1:07:30.000,1:07:32.000
那∑(Wi)²就是把這個model裡面所有的Wi，

1:07:32.000,1:07:34.000
那∑(Wi)²就是把這個model裡面所有的Wi，

1:07:34.000,1:07:36.000
都算一下平方以後加起來。

1:07:36.000,1:07:38.000
都算一下平方以後加起來。

1:07:38.000,1:07:40.000
那這個合起來才是我們的loss function。

1:07:40.000,1:07:42.000
那這個合起來才是我們的loss function。

1:07:42.000,1:07:44.000
前面這一項我們剛才解釋過，

1:07:44.000,1:07:46.000
所以我相信你是可以理解的。

1:07:46.000,1:07:48.000
error越小就代表當然是越好的function，

1:07:48.000,1:07:50.000
error越小就代表當然是越好的function，

1:07:50.000,1:07:52.000
但是，

1:07:52.000,1:07:54.000
為甚麼我們期待一個參數的值越小，

1:07:54.000,1:07:56.000
為甚麼我們期待一個參數的值越小，

1:07:56.000,1:07:58.000
為甚麼我們期待一個參數的值越小，

1:07:58.000,1:08:00.000
參數的值越接近0的function呢?

1:08:02.000,1:08:04.000
這件事情你就比較難想像。

1:08:04.000,1:08:06.000
為甚麼我們期待一個參數值接近0的function呢?

1:08:08.000,1:08:10.000
當我們加上這一項的時候，

1:08:10.000,1:08:12.000
我們就是預期說我們要找到的那個function，

1:08:12.000,1:08:14.000
它的那個參數越小越好。

1:08:14.000,1:08:16.000
它的那個參數越小越好。

1:08:18.000,1:08:20.000
當我們加上這一項的時候，

1:08:22.000,1:08:24.000
你知道參數值比較接近0的function，

1:08:26.000,1:08:28.000
它是比較平滑的。

1:08:28.000,1:08:30.000
所謂的比較平滑的意思是，

1:08:30.000,1:08:32.000
當今天的輸入有變化的時候，

1:08:34.000,1:08:36.000
output對輸入的變化是比較不敏感的。

1:08:36.000,1:08:38.000
output對輸入的變化是比較不敏感的。

1:08:38.000,1:08:40.000
為甚麼參數小就可以達到這個效果呢?

1:08:40.000,1:08:40.500
你可以想想看，

1:08:40.500,1:08:42.000
假設這個是我們的model，

1:08:46.000,1:08:48.000
現在input有一個變化，

1:08:48.000,1:08:50.000
比如說我們對某一個Xi加上ΔXi

1:08:50.000,1:08:52.000
比如說我們對某一個Xi加上ΔXi

1:08:52.000,1:08:54.000
比如說我們對某一個Xi加上ΔXi

1:08:54.000,1:08:56.000
比如說我們對某一個Xi加上ΔXi

1:08:56.000,1:08:58.000
這時候對輸出會有什麼變化呢?

1:08:58.000,1:09:00.000
這時候輸出的變化，

1:09:00.000,1:09:02.000
就是ΔXi乘上Wi

1:09:02.000,1:09:04.000
就是ΔXi乘上Wi

1:09:04.000,1:09:06.000
你的輸入變化ΔXi

1:09:06.000,1:09:14.000
輸出就是Wi乘上ΔXi

1:09:14.500,1:09:16.000
你會發現說如果今天你的Wi越小越接近0的話，

1:09:16.000,1:09:18.000
如果你的Wi越接近0的話，

1:09:20.000,1:09:22.000
它的變化就越小。

1:09:22.000,1:09:24.000
如果Wi越接近0的話，

1:09:26.000,1:09:28.000
輸出對輸入就越不sensitive。

1:09:28.000,1:09:30.000
輸出對輸入就越不sensitive。

1:09:30.000,1:09:32.000
所以今天Wi越接近0，

1:09:32.000,1:09:34.000
我們的function就是一個越平滑的function。

1:09:34.000,1:09:36.000
我們的function就是一個越平滑的function。

1:09:36.000,1:09:38.000
現在的問題就是，

1:09:40.000,1:09:42.000
為甚麼我們喜歡比較平滑的function?

1:09:42.000,1:09:44.000
這可以有不同的解釋，你可以這樣想，

1:09:46.000,1:09:48.000
如果我們今天有一個比較平滑的function的話，

1:09:48.000,1:09:50.000
如果我們今天有一個比較平滑的function的話，

1:09:52.000,1:09:54.000
那平滑的function對輸入是比較不敏感的。

1:09:54.000,1:09:56.000
所以今天如果我們的輸入，

1:09:56.000,1:09:58.000
被一些雜訊所干擾的話，

1:09:58.000,1:10:00.000
如果今天雜訊干擾的我們的輸入，

1:10:00.000,1:10:02.000
在我們測試的時候，

1:10:02.000,1:10:04.000
那一個比較平滑的function，

1:10:04.000,1:10:05.080
它會受到比較少的影響，

1:10:08.000,1:10:10.000
而給我們一個比較好的結果。

1:10:10.000,1:10:12.000
接下來我們就要來看看說，

1:10:14.000,1:10:16.000
如果我們加入了regularization的項，

1:10:18.000,1:10:20.000
對我們最終的結果會有甚麼樣的影響?

1:10:22.000,1:10:24.000
這個是實驗的結果。

1:10:24.000,1:10:26.000
我們就把λ從0、1、10一直調到100000

1:10:26.000,1:10:28.000
我們就把λ從0、1、10一直調到100000

1:10:28.000,1:10:30.000
我們就把λ從0、1、10一直調到100000

1:10:30.000,1:10:32.000
我們就把λ從0、1、10一直調到100000

1:10:32.000,1:10:34.000
所以λ值越大，

1:10:34.000,1:10:36.000
代表說今天我們的

1:10:36.000,1:10:38.000
代表說今天我們的

1:10:38.000,1:10:40.000
我們現在loss有兩項，

1:10:40.000,1:10:42.000
一項是考慮error，一項是考慮多smooth

1:10:42.000,1:10:44.000
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大

1:10:44.000,1:10:46.000
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大

1:10:46.000,1:10:48.000
λ值越大代表考慮smooth的那個regularization那一項它的影響力越大

1:10:48.000,1:10:50.000
所以當λ值越大的時候，

1:10:50.000,1:10:52.000
我們找到的function就越平滑。

1:10:54.000,1:10:56.000
如果我們看看在training data上的error的話，

1:10:56.000,1:10:58.000
我們看看在training data上的error的話，

1:10:58.000,1:11:00.000
我們會發現說，

1:11:00.000,1:11:02.000
如果function越平滑，

1:11:02.000,1:11:04.000
我們在training data上得到的error其實是越大的。

1:11:04.000,1:11:06.000
我們在training data上得到的error其實是越大的。

1:11:06.000,1:11:08.000
但是這件事情是非常合理的，

1:11:08.000,1:11:10.000
因為當λ越大的時候，

1:11:10.000,1:11:12.000
我們就越傾向於考慮W本來的值，

1:11:12.000,1:11:14.000
我們就越傾向於考慮W本來的值，

1:11:14.000,1:11:16.000
我們就越傾向於考慮W本來的值，

1:11:16.000,1:11:18.000
我們就傾向考慮W的值而減少考慮我們的error。

1:11:18.000,1:11:20.000
我們就傾向考慮W的值而減少考慮我們的error。

1:11:20.000,1:11:22.000
所以今天如果λ越大的時候，

1:11:22.000,1:11:24.000
我們考慮error就愈少，

1:11:24.000,1:11:26.000
所以我們本來在training data上得到的error就越大。

1:11:26.000,1:11:28.000
所以我們本來在training data上得到的error就越大。

1:11:28.000,1:11:30.000
但是有趣的是，

1:11:30.000,1:11:32.000
雖然在training data上得到的error就越大，

1:11:32.000,1:11:34.000
但是在testing data上面得到的error可能是會比較小。

1:11:34.000,1:11:36.000
但是在testing data上面得到的error可能是會比較小。

1:11:36.000,1:11:38.000
比如說我們看這邊的例子，

1:11:38.000,1:11:40.000
原來λ=0，

1:11:40.000,1:11:42.000
就是沒有regularization的時候error是102

1:11:42.000,1:11:44.000
λ=1就變成68，

1:11:44.000,1:11:46.000
到10就變成25

1:11:46.000,1:11:48.000
到100就變成11.1

1:11:48.000,1:11:50.000
但是λ太大的時候，

1:11:50.000,1:11:52.000
到1000的時候，

1:11:52.000,1:11:54.000
error又變大變成12.8一直到26.8。

1:11:54.000,1:11:56.000
error又變大變成12.8一直到26.8。

1:11:56.000,1:11:58.000
那這個結果是合理，

1:12:00.000,1:12:02.000
我們比較喜歡比較平滑的function，

1:12:02.000,1:12:04.000
比較平滑的function它對noise比較不sensitive，

1:12:04.000,1:12:06.000
所以當我們增加λ的時候，

1:12:06.000,1:12:08.000
你的performance是越來越好，

1:12:10.000,1:12:12.000
但是我們又不喜歡太平滑的function，

1:12:14.000,1:12:16.000
因為最平滑的function是甚麼?

1:12:16.000,1:12:18.000
最平滑的function就是一條水平線啊，

1:12:18.000,1:12:20.000
一條水平線是最平滑的function。

1:12:20.000,1:12:22.000
如果你的function是一條水平線的話，

1:12:22.000,1:12:24.000
那它啥事都幹不成，

1:12:24.000,1:12:26.000
所以如果今天function太平滑的話，

1:12:26.000,1:12:28.000
你反而會在testing set上又得到糟糕的結果。

1:12:28.000,1:12:30.000
你反而會在testing set上又得到糟糕的結果。

1:12:32.000,1:12:34.000
所以現在的問題就是，

1:12:36.000,1:12:38.000
我們希望我們的model多smooth呢?

1:12:38.000,1:12:40.000
我們希望我們的model多smooth呢?

1:12:42.000,1:12:44.000
我們希望我們今天找到的function有多平滑呢?

1:12:46.000,1:12:48.000
這件事情就變成是你們要調λ來解決這件事情，

1:12:48.000,1:12:50.000
這件事情就變成是你們要調λ來解決這件事情，

1:12:50.000,1:12:52.000
你必須要調整λ來決定你的function的平滑程度。

1:12:52.000,1:12:54.000
你必須要調整λ來決定你的function的平滑程度。

1:12:54.000,1:12:56.000
比如說你可能調整一下參數以後發現說，

1:12:58.000,1:13:00.000
今天training都隨著λ增加而增加，

1:13:00.000,1:13:02.000
testing隨著λ先減少後增加。

1:13:02.000,1:13:04.000
testing隨著λ先減少後增加。

1:13:04.000,1:13:06.000
在這個地方有一個轉折點，

1:13:06.000,1:13:08.000
是可以讓我們的testing error最小，

1:13:08.000,1:13:10.000
是可以讓我們的testing error最小，

1:13:10.000,1:13:12.000
你就選λ=100來得到你的model。

1:13:12.000,1:13:14.000
你就選λ=100來得到你的model。

1:13:14.000,1:13:16.000
你就選λ=100來得到你的model。

1:13:16.000,1:13:18.000
這邊還有一個有趣的事實，

1:13:18.000,1:13:20.000
很多同學其實都知道regularization，

1:13:22.000,1:13:22.660
你有沒有發現，

1:13:26.000,1:13:28.000
這邊我沒有把b加進去，

1:13:28.000,1:13:30.000
我剛剛突然想到一件事情，

1:13:30.000,1:13:32.000
我其實在前面那個gradient descent的投影片裡面，

1:13:32.000,1:13:34.000
有一個地方寫錯了，

1:13:34.000,1:13:36.000
然後有同學提醒我，

1:13:36.000,1:13:38.000
以後你如果有發現我投影片有寫錯的話，

1:13:40.000,1:13:42.000
以後告訴我就把你的投影片寫在投影片這樣。

1:13:46.000,1:13:48.000
這邊你發現我沒有加上b，

1:13:48.000,1:13:50.000
為甚麼呢?

1:13:50.000,1:13:52.000
你覺得是我寫錯了的同學，

1:13:52.000,1:13:54.000
你覺得是我忘了加上去的同學舉手一下，

1:13:58.000,1:14:00.000
你覺得本來就不需要加上b的同學舉手一下

1:14:02.000,1:14:04.000
這邊我覺得我沒有寫錯。

1:14:04.000,1:14:06.000
事實上很多人可能不知道這件事，

1:14:06.000,1:14:08.000
在做regularization的時候，

1:14:10.000,1:14:12.000
其實是不需要考慮bias這一項的。

1:14:12.000,1:14:14.000
首先，如果你自己做實驗的話你會發現，

1:14:14.000,1:14:16.000
不考慮bias， performance會比較好。

1:14:16.000,1:14:18.000
再來為甚麼不考慮bias呢?

1:14:18.000,1:14:20.000
因為我們今天預期的是，

1:14:22.000,1:14:24.000
我們要找一個比較平滑的function。

1:14:26.000,1:14:28.000
你調整bias的這個b的大小，

1:14:28.000,1:14:30.000
跟一個function的平滑的程度是沒有關係的。

1:14:30.000,1:14:32.000
跟一個function的平滑的程度是沒有關係的。

1:14:32.000,1:14:34.000
調整bias值的大小時你只是把function上下移動而已，

1:14:34.000,1:14:36.000
調整bias值的大小時你只是把function上下移動而已，

1:14:36.000,1:14:38.000
對function的平滑程度是沒有關係的。

1:14:38.000,1:14:40.000
所以有趣的是，這很多人都不知道，

1:14:40.000,1:14:42.000
在做regularization的時候，

1:14:42.000,1:14:44.000
你是不用考慮bias的。

1:14:46.000,1:14:48.000
總之，

1:14:48.000,1:14:50.000
搞了半天以後，

1:14:50.000,1:14:52.000
我最後可以做到，

1:14:52.000,1:14:54.000
我們的testing error是11.1。

1:14:56.000,1:14:58.000
那在我們請助教公告作業之前，

1:14:58.000,1:15:00.000
我們就說一下今天的conclusion :

1:15:02.000,1:15:04.000
首先感謝大家來參加我對寶可夢研究的發表會，

1:15:06.000,1:15:08.000
那我今天得到的結論就是，

1:15:10.000,1:15:12.000
寶可夢進化後的CP值，

1:15:12.000,1:15:14.000
跟他進化前的CP值，

1:15:14.000,1:15:16.000
還有它是哪個物種，是非常有關係的。

1:15:18.000,1:15:20.000
知道這兩件事情幾乎可以決定進化後的CP值。

1:15:20.000,1:15:20.780
但是我認為，

1:15:22.000,1:15:24.000
可能應該還有其他的factors。

1:15:24.000,1:15:26.000
我們剛剛看到說我們加上其他甚麼高度啊體重啊HP以後，

1:15:26.000,1:15:28.000
我們剛剛看到說我們加上其他甚麼高度啊體重啊HP以後，

1:15:28.000,1:15:30.000
是有比較好的，如果我們加入regularization的話。

1:15:30.000,1:15:32.000
不過我data有點少，

1:15:32.000,1:15:34.000
所以我沒有那麼confident就是了。

1:15:38.000,1:15:40.000
然後再來呢就是，

1:15:42.000,1:15:44.000
我們今天講了gradient descent的作法，

1:15:44.000,1:15:46.000
就是告訴大家怎麼做

1:15:48.000,1:15:50.000
那我們以後會講它的原理還有技巧。

1:15:56.000,1:15:58.000
我們今天講了overfitting和regularization，

1:15:58.000,1:16:00.000
我們今天講了overfitting和regularization，

1:16:00.000,1:16:02.000
介紹一下表象上的現象，

1:16:02.000,1:16:04.000
未來會講更多它背後的理論。

1:16:04.000,1:16:06.000
再來最後我們有一個很重要的問題，

1:16:06.000,1:16:08.000
首先我覺得我這個結果應該還滿正確的，

1:16:08.000,1:16:10.000
因為你知道網路上有很多的CP的預測器，

1:16:10.000,1:16:12.000
因為你知道網路上有很多的CP的預測器，

1:16:12.000,1:16:14.000
那些CP的預測器你在輸入的時候，

1:16:16.000,1:16:18.000
你只要輸入你的寶可夢的物種和它現在的CP值，

1:16:18.000,1:16:20.000
你只要輸入你的寶可夢的物種和它現在的CP值，

1:16:20.000,1:16:22.000
它就可以告訴你進化以後的CP值。

1:16:22.000,1:16:24.000
它就可以告訴你進化以後的CP值。

1:16:24.000,1:16:26.000
所以我認為你要預測進化以後的CP值，

1:16:26.000,1:16:28.000
應該是要知道原來的CP值和它的物種，

1:16:28.000,1:16:30.000
就可以知道大部分。

1:16:32.000,1:16:34.000
不過我看那些預測器預測出來的誤差，

1:16:34.000,1:16:36.000
都是給你一個range，

1:16:36.000,1:16:38.000
它都沒有辦法給你一個更準確的預測。

1:16:38.000,1:16:40.000
如果考慮更多的factor更多的input，

1:16:40.000,1:16:42.000
比如說HP甚麼啊，

1:16:42.000,1:16:44.000
或許可以預測的更準就是了。

1:16:44.000,1:16:46.000
但最後的問題就是，

1:16:46.000,1:16:48.000
我們在testing data上面，

1:16:48.000,1:16:50.000
在我們testing的10隻寶可夢上，

1:16:50.000,1:16:52.000
我們得到的average error最後是11.1。

1:16:52.000,1:16:54.000
如果我把它做成一個系統，

1:16:54.000,1:16:56.000
放到網路上給大家使用的話，

1:16:58.000,1:17:00.000
你覺得如果我們看過沒有看到的data，

1:17:04.000,1:17:06.000
那我們得到的error會預期高過11.1還是低於11.1

1:17:06.000,1:17:08.000
那我們得到的error會預期高過11.1還是低於11.1

1:17:10.000,1:17:12.000
還是理論上期望值應該是一樣的。

1:17:14.000,1:17:16.000
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，

1:17:16.000,1:17:18.000
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，

1:17:18.000,1:17:20.000
你知道我的training data是裡面只有四種，裡面都沒有甚麼乘龍卡比之類的，

1:17:20.000,1:17:22.000
我們就假設使用者只能夠輸入那四種，

1:17:22.000,1:17:24.000
它不會輸入乘龍卡比這樣。

1:17:24.000,1:17:26.000
在這個情況下，

1:17:28.000,1:17:30.000
你覺得如果我們今天把這個系統放到線上，

1:17:30.000,1:17:32.000
給大家使用的話，

1:17:32.000,1:17:34.000
我們今天得到的CP值，

1:17:36.000,1:17:38.000
會比我今天在testing set上看到的高還是低還是一樣?

1:17:40.000,1:17:42.000
你覺得一樣的同學舉手一下

1:17:44.000,1:17:46.000
你覺得會比現在看到的低，舉手一下

1:17:54.000,1:17:56.000
你覺得會比我們今天看到的11.1還要高的舉手一下

1:18:00.000,1:18:02.000
我們之後會解釋，

1:18:06.000,1:18:08.000
我們今天其實用了testing set來選model

1:18:08.000,1:18:10.000
就我們今天得到的結果其實是

1:18:10.000,1:18:12.000
如果我們真的把系統放在線上的話

1:18:14.000,1:18:16.000
預期應該會得到比我們今天看到的11.1還要更高的error rate

1:18:18.000,1:18:20.000
這個時候我們需要validation觀念來解決這個問題

1:18:22.000,1:18:24.000
這個我們就下一堂課再講。

1:18:24.000,1:18:26.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

1:18:26.000,1:18:28.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

1:18:28.000,1:18:30.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
