0:00:00.000,0:00:03.060
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心

0:00:03.060,0:00:05.060
好，我們要來上課囉！

0:00:06.660,0:00:11.520
剩下的時間，我們要來進入新的主題

0:00:11.520,0:00:14.060
我們要來講分類這件事情

0:00:14.580,0:00:19.320
在分類這件事情呢，我們要找的是一個 function

0:00:19.320,0:00:23.440
它的 input 是一個 object x

0:00:23.440,0:00:27.160
它的 output 是這個 object 屬於哪一個 class

0:00:27.160,0:00:28.980
屬於 n 個 class 的哪一個

0:00:28.980,0:00:31.780
那這樣的 task 有很多的 application

0:00:31.780,0:00:33.780
信手拈來就一大堆

0:00:33.780,0:00:35.280
比如說，在金融上

0:00:35.280,0:00:38.300
他們可以用 classification 的 model

0:00:38.300,0:00:41.520
來決定要不要貸款給某一個人

0:00:41.520,0:00:44.880
你就找一個 function，它的 input 是某一個人的 income

0:00:44.880,0:00:48.520
他的 saving、工作啊、他的年紀阿

0:00:48.520,0:00:51.120
還有他過去 financial 的 record

0:00:51.120,0:00:53.380
他過去有沒有欠債阿，等等

0:00:53.380,0:00:57.000
那 output 就是要借錢給他，或是不借錢給他

0:00:57.000,0:01:01.000
這是 binary classification 的 problem，
也就是 accept 或是 refuse

0:01:01.000,0:01:03.440
那或者是拿來做醫療的診斷，比如說

0:01:03.440,0:01:07.460
input 就是某一個人的症狀，還有他的年紀、性別

0:01:07.460,0:01:09.620
過去就醫的歷史阿，等等

0:01:09.840,0:01:13.460
或者是來做，手寫的是數字、文字的辨識

0:01:13.460,0:01:16.580
那 output 就是，他生的是哪一種病

0:01:16.620,0:01:21.760
自動來做醫療的診斷

0:01:21.760,0:01:25.280
比如說，你就手寫一個字給機器看

0:01:25.280,0:01:27.860
看到這張圖，這是 "金" 這樣子

0:01:28.140,0:01:31.920
你知道，鄉民都叫我 "大金"，這是金這樣子

0:01:31.920,0:01:34.780
然後，output 就是

0:01:35.380,0:01:39.060
這個字、這個 image，它是屬於哪一個 class

0:01:39.060,0:01:41.820
如果你是做中文的手寫辨識的話

0:01:41.820,0:01:45.380
那中文有至少 8000 個 character

0:01:45.380,0:01:48.480
那就是一個 8000 個 class 的 classification

0:01:48.480,0:01:50.500
你的 model 要從這 8000 個 class 裡面

0:01:50.500,0:01:52.320
選一個 class 當作 output

0:01:52.320,0:01:56.080
或者是做人臉辨識，input 一個人臉

0:01:56.080,0:01:58.800
然後告訴他說，這個人臉是誰的

0:01:59.220,0:02:02.340
好，那我們要用的是怎麼樣的 Example Application 呢？

0:02:02.340,0:02:04.660
其實這個也是寶可夢的例子啦

0:02:04.660,0:02:09.440
你可能以為說，我只有前面 predict CP 值那裡

0:02:09.440,0:02:12.660
其實，我還有很多其他的例子這樣

0:02:12.660,0:02:16.180
好，我又做了一些有關寶可夢的研究

0:02:16.240,0:02:17.860
我這個研究是這樣子的

0:02:17.860,0:02:20.540
我們知道說寶可夢有不同的屬性

0:02:20.540,0:02:23.120
有幾種呢？有 18 種屬性

0:02:23.120,0:02:29.360
包括水、火、電阿，還有草、冰......等等

0:02:29.360,0:02:32.940
總共有 18 種屬性，到第六代為止，總共有 18 種屬性

0:02:32.940,0:02:36.220
我們現在要做的是一個分類的問題

0:02:36.220,0:02:38.880
這個分類的問題就是，要找一個 function

0:02:38.880,0:02:42.160
這個 function 的 input 就是某一隻寶可夢

0:02:42.160,0:02:45.460
然後，它的 output 就是要告訴你說，這一隻寶可夢

0:02:45.460,0:02:47.320
它是屬於哪一種 type

0:02:47.320,0:02:49.260
比如說，input 給它一隻皮卡丘

0:02:49.260,0:02:50.700
它 output 就是雷

0:02:50.700,0:02:55.160
input 給它一個傑尼龜 ，它 output 就是水

0:02:55.160,0:02:58.700
input 給它一個妙蛙種子，output 就是草

0:02:58.700,0:03:01.420
所以是一個 classification 的問題

0:03:01.420,0:03:04.080
那要怎麼樣做這個問題呢？

0:03:04.080,0:03:07.840
你現在的第一個問題，就是怎麼把一隻寶可夢當作

0:03:07.840,0:03:09.240
function 的 input

0:03:09.240,0:03:12.600
你要把一個東西當作 function 的 input，它得數值化

0:03:12.600,0:03:17.900
你要用數字來表示一隻寶可夢，
你才能夠把它放到一個 function 裡面

0:03:18.180,0:03:21.240
那我們要怎麼把一隻寶可夢用數字來表示呢？

0:03:21.240,0:03:26.240
一隻寶可夢，其實它有很多的特性

0:03:26.240,0:03:29.000
這些特性是可以數值化的

0:03:29.000,0:03:31.560
比如說，它整體的強度

0:03:31.560,0:03:35.640
我先說一下，這並不是那個 Pokemon go 的那個東西

0:03:35.640,0:03:37.580
這個是那個 Pokemon 的電玩

0:03:37.580,0:03:40.240
你聽不懂我講這個，就算了

0:03:40.240,0:03:41.940
就假設沒聽到這句話

0:03:41.940,0:03:49.060
那一隻寶可夢，它其實可以用一組數字來描述它的特性

0:03:49.060,0:03:50.800
這組數字代表什麼呢？

0:03:50.800,0:03:54.980
比如說，這個寶可夢，它 total 的 strong

0:03:54.980,0:03:57.980
就是它有多強這樣子，你可以用一組數字來表示它

0:03:58.120,0:04:00.920
它的生命值，你可以用數字來表示它

0:04:01.160,0:04:04.600
它的攻擊力，你可以用數字來表示它

0:04:04.600,0:04:07.320
它的防禦力，你可以用數字來表示它

0:04:07.320,0:04:11.060
它的特殊攻擊力，就是它用特殊攻擊的時候

0:04:11.060,0:04:15.600
這我也不知道怎麼解釋， 
反正就是另外一個特殊攻擊的攻擊力

0:04:15.600,0:04:18.220
特殊攻擊的防禦力

0:04:18.220,0:04:21.540
還有它的速度，速度可以決定說

0:04:21.540,0:04:25.480
就是兩隻寶可夢相遇的時候，誰可以先攻擊

0:04:26.180,0:04:28.300
比如說，皮卡丘

0:04:28.300,0:04:31.420
它整體強的程度是 320

0:04:31.420,0:04:34.160
它 HP 是 35，攻擊力是 55

0:04:34.160,0:04:37.100
防禦力是40，特殊攻擊力是 50

0:04:37.100,0:04:41.800
然後，它的特殊防禦力是 50，速度是 90

0:04:42.180,0:04:46.200
所以一隻皮卡丘，我們就可以用一個 vector 來描述它

0:04:46.200,0:04:49.980
這個 vector 裡面總共有 7 個數值

0:04:49.980,0:04:56.520
所以，1 隻寶可夢，它就是 7 個數字所組成的一個 vector

0:04:56.520,0:04:59.280
所以 1 隻皮卡丘，可以用這 7 個數字來描述它

0:04:59.280,0:05:01.300
我們現在要問的問題就是

0:05:01.300,0:05:04.420
我們能不能把 7 個數字，輸進一個 function

0:05:04.420,0:05:05.580
這個 function 就告訴我們說

0:05:05.580,0:05:09.880
它的 output 是哪一種種類的寶可夢

0:05:09.880,0:05:12.040
那你可能會問這樣的問題

0:05:12.040,0:05:14.500
這件事情的重要性，到底在哪裡？

0:05:14.500,0:05:16.960
這件事情，是非常重要的

0:05:16.960,0:05:19.860
因為當兩隻寶可夢相遇，在決鬥的時候

0:05:19.860,0:05:22.580
他們之間是有屬性相剋的關係

0:05:22.580,0:05:25.380
這個是 18*18 的屬性相剋表

0:05:25.380,0:05:28.940
因為，你知道說，總共有 18 隻寶可夢

0:05:28.940,0:05:32.700
總共有 18 種 type，所以是 18*18 的屬性相剋表

0:05:32.700,0:05:36.300
比如說，今天這個格鬥系遇到

0:05:36.300,0:05:38.360
左邊，這個是攻擊方

0:05:38.360,0:05:40.040
然後，上面這個是防禦方

0:05:40.040,0:05:43.240
所以，格鬥系遇到一般的時候，他的攻擊力就 *2

0:05:43.240,0:05:47.240
這樣，你看得懂這個圖了嗎？所以，這個是很重要的

0:05:47.240,0:05:48.860
那你可能會問說

0:05:48.860,0:05:52.740
這個寶可夢屬性，不是寶可夢圖鑑上都有了嗎？

0:05:52.740,0:05:55.260
綠色寶可夢屬性，有什麼意義呢？

0:05:55.260,0:05:56.900
這件事情是有很大的意義的

0:05:56.900,0:05:59.260
因為，你有可能在決鬥中的時候，遇到

0:05:59.260,0:06:01.560
對方出的是圖鑑上沒有的

0:06:01.560,0:06:03.020
你沒有見過的寶可夢

0:06:03.020,0:06:05.240
如果你現在有這個預測的 model 的話

0:06:05.240,0:06:08.440
你就可以預測說，它出的寶可夢是哪一種屬性的

0:06:08.440,0:06:11.040
你就可以用正確的屬性來對付它

0:06:11.040,0:06:14.880
所以，這個是有非常廣泛地運用的

0:06:14.880,0:06:18.360
而且我發現說，寶可夢圖鑑，其實是有影像辨識的功能

0:06:18.360,0:06:19.920
它影像辨識的功能很強

0:06:19.920,0:06:22.800
照一張圖，它就可以告訴你說，它是哪一種寶可夢

0:06:22.800,0:06:26.360
我們應該要把這個 prediction 的 model 
加到寶可夢的圖鑑裡面

0:06:26.360,0:06:28.420
它就可以幫我們 predict 新的寶可夢

0:06:28.420,0:06:34.600
那，怎麼完成這個任務

0:06:36.060,0:06:38.400
首先，我們要先收集 data

0:06:38.400,0:06:44.980
比如說，你就說，我把寶可夢編號 400 以下的，
當作 training data

0:06:44.980,0:06:47.180
把編號 400 以上的，當作 testing data

0:06:47.180,0:06:50.000
你就假設說，因為它的寶可夢其實是越來越多的

0:06:50.000,0:06:51.860
我記得我小時候只有 150 隻

0:06:51.860,0:06:55.760
後來發現，越來越多寶可夢，現在已經有 800 隻了

0:06:55.760,0:06:57.300
所以寶可夢是不斷增加的

0:06:57.300,0:07:00.880
所以編號比較前面的，是比較早發現的那些寶可夢

0:07:00.880,0:07:03.800
所以，模擬說，我們已經發現那些寶可的情況下

0:07:03.800,0:07:05.460
如果看到新的寶可夢的時候

0:07:05.460,0:07:08.920
所以，你要收集 data，比如說

0:07:09.540,0:07:11.460
我們能不能夠預測說，它是屬於哪一種屬性的

0:07:11.460,0:07:14.160
我們的 data 就是一些 pair

0:07:14.160,0:07:16.300
告訴我們說 function 的 input, output 是甚麼

0:07:16.300,0:07:18.440
比如說，input 皮卡丘，就要 output 電

0:07:18.440,0:07:20.320
input 傑尼龜，就要 output 水

0:07:20.320,0:07:22.740
input 妙蛙種子，就要 output 草

0:07:23.320,0:07:25.900
那怎麼解這個 classification 的問題？

0:07:25.900,0:07:27.100
有人會這麼想

0:07:27.100,0:07:29.000
假設有人沒有學過 classification

0:07:29.000,0:07:31.220
他學過 Regression，他就說

0:07:31.220,0:07:34.900
Classification 就當作 Regression 的問題來硬解

0:07:34.900,0:07:36.120
怎麼硬解呢？

0:07:36.120,0:07:38.900
我們用 binary 的 classification 來當作例子

0:07:38.900,0:07:44.340
假設我們現在只要 output：
input 的 x 屬於 class 1 或 class 2

0:07:44.340,0:07:46.900
那你就把他當作一個 Regression 的問題

0:07:46.900,0:07:53.060
task 1 就代表說它的 target，也就是 y\head 是 1

0:07:53.060,0:07:57.420
task 2 就代表說，它的 target 是 -1

0:07:57.420,0:08:00.700
就這樣，你就當作 Regression 的 problem，
一直 train 下去

0:08:00.700,0:08:02.740
然後，train 完這個 model 以後呢

0:08:02.740,0:08:04.080
在 testing 的時候

0:08:04.080,0:08:07.420
因為 Regression 的 output 不會正好是 0 或是 1 阿

0:08:07.420,0:08:11.400
它是一個 number，它是一個數值

0:08:11.400,0:08:14.760
如果這個數值比較接近 1 的話，就說是 class 1

0:08:14.760,0:08:18.320
如果這個數值比較接近 -1 的話，就說是 class 2

0:08:18.320,0:08:22.120
所以，你可以想成說，現在就是以 0 為分界

0:08:22.120,0:08:24.940
如果你的 Regression model output 是大於 0 的話

0:08:24.940,0:08:26.500
就比較接近 1

0:08:26.500,0:08:28.380
你的 model 就會說是 class 1

0:08:28.380,0:08:32.880
如果小於 0 的話，就比較接近 -1，
所以 model 會說是 class 2

0:08:32.880,0:08:34.720
如果你這麼做的話

0:08:34.720,0:08:38.820
會遇到什麼樣的問題呢？

0:08:38.820,0:08:40.120
你會遇到這樣的問題

0:08:40.780,0:08:43.640
假設說，我們現在的 model 是一個

0:08:43.640,0:08:46.940
我們現在的 model，input 和 output 的關係

0:08:46.940,0:08:50.360
y = b + w1*x1 + w2*x2

0:08:50.360,0:08:55.700
所以，input 這兩個 feature，也就是 x1 跟 x2

0:08:56.760,0:09:01.140
我們現在有兩個 class，紅色是 class 1、藍色是 class 2

0:09:01.560,0:09:03.680
那如果你用 Regression 來想的話

0:09:03.680,0:09:08.500
藍色的那些 object，藍色的那些東西

0:09:08.500,0:09:10.380
input 到這個 Regression 的 model

0:09:10.380,0:09:12.660
我們都希望它越接近 1 越好

0:09:12.660,0:09:15.900
紅色這些東西，input 到 Regression 的 model

0:09:15.900,0:09:18.400
我們都希望它越接近 -1 越好

0:09:18.400,0:09:20.900
這件事，可能是做得到的

0:09:20.900,0:09:22.700
如果你把這些 data

0:09:22.700,0:09:27.900
真的找出這個 b, w1, w2 的話

0:09:27.900,0:09:29.600
那你會發現說呢

0:09:29.600,0:09:35.920
b + w1*x1 + w2*x2 (投影片上寫錯)

0:09:35.920,0:09:40.120
這個式子等於 0 的線，是綠色這條

0:09:40.120,0:09:43.420
也就是 class 1 和 class 2 的分界點

0:09:43.420,0:09:46.660
是在綠色的這條線上

0:09:46.660,0:09:49.700
這看起來、聽起來滿好的

0:09:50.180,0:09:52.600
但是，你可能會遇到這樣的問題

0:09:52.600,0:09:56.120
假設，你今天 class 1 的分佈不是這樣子

0:09:56.120,0:10:00.540
假設你 class 1 的分佈是這樣子，你就麻煩了

0:10:00.540,0:10:01.620
為什麼？

0:10:01.620,0:10:06.460
因為，如果你用綠色的這條線，所代表的 model 的話

0:10:06.460,0:10:09.580
注意一下，這個是二維的

0:10:09.580,0:10:14.580
綠色這條線，只是代表說這個 model 的值是 0

0:10:14.580,0:10:17.160
就是這個 y 的值，是 0

0:10:17.160,0:10:19.320
你的 Regression 的 output 是 0

0:10:19.320,0:10:23.440
左上角這邊，代表這個 Regression 的 output 是小於 0

0:10:23.440,0:10:27.020
右邊上角這邊，代表這個 Regression 的 output 是大於 0

0:10:27.620,0:10:30.260
那綠色這條線，會有什麼問題呢？

0:10:30.260,0:10:32.260
綠色這條線的問題就是

0:10:32.260,0:10:40.380
左上角 < 0，右上角 > 0

0:10:40.380,0:10:43.220
越偏右下，它的值就越大

0:10:43.220,0:10:49.000
所以，如果今天是考慮右下角這些點的話

0:10:49.000,0:10:53.020
它用這個綠色的 model，它做 Regression 的時候

0:10:53.020,0:10:55.700
它的 output 可能會是遠大於 1 的

0:10:55.700,0:10:57.700
但是，如果你用 Regression 的話

0:10:57.700,0:11:01.540
你會希望，藍色的點都越接近 1 越好

0:11:01.540,0:11:05.980
太大也不好，它要越接近 1 越好

0:11:05.980,0:11:08.340
太小不行、太大也不行

0:11:08.340,0:11:12.360
所以變成說，這些遠大於 1 的點

0:11:12.360,0:11:16.720
它其實對 Regression 來說，是 error，是錯的

0:11:16.720,0:11:18.780
這些點是不好的

0:11:18.780,0:11:24.420
所以，你今天如果拿這樣兩群藍色的點跟紅色的點

0:11:24.420,0:11:26.780
去做 Regression 的時候

0:11:26.780,0:11:28.820
你得到的線，不會是綠色這條

0:11:28.820,0:11:32.260
雖然綠色這條，你用直覺看、你用眼睛一看就會知道說

0:11:32.260,0:11:34.260
它是一個比較好的 boundary

0:11:34.260,0:11:36.540
但是，如果你用 Regression 刃下去的話

0:11:36.540,0:11:39.200
不會是綠色這條，它會是紫色這條

0:11:39.200,0:11:44.420
因為，它會覺得說，我把綠色這條線，往右偏一點

0:11:44.420,0:11:48.660
這樣的好處就是，這邊這些藍色的點

0:11:48.660,0:11:50.920
它的值就沒有那麼大

0:11:50.920,0:11:52.860
它的值就會壓小

0:11:52.860,0:11:55.220
就讓他們，比較接近 1

0:11:55.220,0:12:00.000
結果，這樣子的 function 反而對 Regression 來說

0:12:00.000,0:12:02.400
是一個比較好的 function

0:12:03.000,0:12:07.440
也就是說，Regression 那個定義 function 好壞的方式

0:12:07.440,0:12:10.360
就 classification 來說，不適用

0:12:10.360,0:12:13.080
今天這個 problem，對 Regression 來說

0:12:13.080,0:12:15.960
紫色的，是一個好的 function

0:12:16.960,0:12:21.100
但是，顯然對 classification 來說，
綠色的才是一個好的 function

0:12:21.260,0:12:23.660
但是，如果你當作 Regression 的 problem 來做

0:12:23.660,0:12:26.640
套用到 Regression，一樣的作法的時候

0:12:26.640,0:12:28.820
你得到的結果會是不好的

0:12:28.820,0:12:33.980
因為 Regression 對 model 好壞的定義，
是不適合用在這個地方的

0:12:33.980,0:12:38.060
所以，如果你用 Regression 的話

0:12:38.060,0:12:41.200
你的 Regression model 它會懲罰那些

0:12:41.200,0:12:45.800
太正確、那些 output 值太大的那些點

0:12:45.800,0:12:47.900
這樣，反而你得到的結果是不好的

0:12:48.300,0:12:49.960
那，還有另外一個問題

0:12:49.960,0:12:53.640
其實，這種硬把 Classification 當作 Regression 來做

0:12:53.640,0:12:57.300
我還看過有人會真的這麼做

0:12:57.300,0:12:59.520
不果我勸你不要這麼做

0:12:59.520,0:13:03.020
比如說，你今天有 Multiple class 的話

0:13:03.020,0:13:07.020
那你可能說，我把 class 1 當作 target 是 1

0:13:07.020,0:13:08.400
class 2 當作 target 是 2

0:13:08.400,0:13:10.620
class 3 當作 target 是 3

0:13:10.620,0:13:12.100
這樣子做事會有問題的

0:13:12.200,0:13:14.680
因為當你這樣子做的時候，你就假設說

0:13:14.680,0:13:17.440
class 3 和 class 2 是比較近的

0:13:17.440,0:13:18.680
它們有某種關係

0:13:18.680,0:13:20.800
class 2 和 class 1 是比較近的

0:13:20.800,0:13:22.320
它們有某種關係

0:13:22.320,0:13:25.440
但是，實際上，如果這種關係不存在的話

0:13:25.440,0:13:29.940
class 1, class 2, class 3
他們中間並沒有某種特殊的 relation

0:13:29.940,0:13:32.540
並沒有誰應該跟誰比較有關係的話

0:13:32.540,0:13:35.920
你這樣子把它當作一個 Regression 的問題來處理

0:13:35.920,0:13:38.320
你就沒有辦法得到一個好的結果

0:13:38.820,0:13:42.400
那我們這邊，應該要怎麼做呢？

0:13:42.400,0:13:45.060
理想上的做法，是這個樣子

0:13:46.120,0:13:49.160
找一個 function，這個 function 裡面呢

0:13:49.160,0:13:54.040
這在做 Regression 的時候，它的 output 是 real number

0:13:54.040,0:13:57.940
對不對？但是在 classification 的時候

0:13:57.940,0:14:00.580
它的 output 是 discrete

0:14:00.580,0:14:02.680
它是某一個 class，它是 discrete

0:14:02.680,0:14:04.920
那我要想辦法讓 model 做到這件事情

0:14:04.920,0:14:07.040
你當然可以有不同的想法

0:14:07.040,0:14:08.480
那一個可能的想法是

0:14:08.480,0:14:11.860
假如是二元分類的問題，是 binary classification 的話

0:14:11.860,0:14:15.040
我就說，我們要找的 function f 裡面

0:14:15.040,0:14:16.560
內建另外一個 function g

0:14:16.560,0:14:20.100
希望它是自動，當然我們的 g 
也是根據 training data 被找出來的

0:14:20.100,0:14:24.480
如果 g 代進 x，x 代進去的值大於 0 的話

0:14:24.480,0:14:26.500
那就說是 class 1

0:14:26.500,0:14:29.080
否則呢，就說是 class 2

0:14:29.520,0:14:31.220
那在 training 的時候

0:14:31.220,0:14:35.000
我們的 loss 應該怎麼定義才好呢？

0:14:35.000,0:14:39.360
我們的 loss 應該定義成，我們可以把 loss 定義成呢

0:14:39.360,0:14:42.480
如果我選了某一個 function f

0:14:42.480,0:14:44.240
它在我們的 training data 上面

0:14:44.240,0:14:47.100
predict 錯誤的次數

0:14:47.100,0:14:48.520
我們當然希望說

0:14:48.520,0:14:51.580
我們找出來的 function

0:14:51.580,0:14:59.000
它在 data 上的錯誤次數越小，代表它的 loss 越小

0:14:59.000,0:15:01.040
你就可以把這個式子寫成這樣

0:15:01.040,0:15:03.580
summation over 所有的 training example

0:15:03.580,0:15:09.440
δ (f(x^n) ≠ (y\head)^n)

0:15:09.440,0:15:14.380
就是如果 f(x^n) 的 output 跟
正確答案 (y\head)^n 不一樣的話

0:15:14.380,0:15:16.740
這個 δ 就是 1，否則就是 0

0:15:16.740,0:15:18.740
如果你把它全部 sum 起來的話

0:15:18.740,0:15:22.460
就是你用這個 function f，在 training data 上面

0:15:22.460,0:15:25.740
它會分類錯誤的次數

0:15:25.740,0:15:28.120
那當然希望這個值，越小越好

0:15:28.120,0:15:32.300
但是，如果要你解這個 function，你現在八成不會

0:15:32.300,0:15:34.480
因為，我們學過的是 Gradient Descent

0:15:34.480,0:15:36.080
你可能會用 Gradient Descent 解

0:15:36.080,0:15:38.240
但是，這個沒辦法微分阿

0:15:38.240,0:15:42.220
這個就沒辦法微分，這個也沒辦法微分，通通不能微分

0:15:42.220,0:15:43.400
不知道怎麼做

0:15:43.400,0:15:46.200
其實，這個是有方法，比如說

0:15:46.200,0:15:49.700
Perceptron 就是一個方法，SVM 就是一個方法

0:15:49.700,0:15:53.160
那我們今天先不講這個方法

0:15:53.160,0:15:56.420
我們今天先來講另外一個 solution

0:15:56.420,0:16:00.660
這個 solution，我們先用機率的觀點來看待它

0:16:00.660,0:16:03.120
之後我們會說，這樣的 solution

0:16:03.120,0:16:07.740
也是跟 machine learning 的 3 個 step，其實是一樣的

0:16:08.380,0:16:12.640
我們先這樣看，就是有兩個盒子

0:16:12.640,0:16:16.200
我們來回憶一下，我們國中的時候，機率會問的問題

0:16:16.720,0:16:19.900
有兩個盒子，盒子一裡面有藍球和綠球

0:16:19.900,0:16:21.900
盒子二裡面也有藍球跟綠球

0:16:21.900,0:16:26.220
假設我不告訴你說，我從哪一個盒子裡面，
挑一顆球出來

0:16:26.220,0:16:29.880
但是，我告訴你說，我從這兩個盒子的某一個盒子裡面

0:16:29.880,0:16:33.220
隨機抽取一顆球出來，它是藍色的

0:16:33.220,0:16:38.060
那這顆藍色的球，它從盒子 1

0:16:38.060,0:16:41.240
跟從盒子 2 抽出來的機率，分別是多少？

0:16:41.240,0:16:43.340
我相信這個，小學生就可以回答我

0:16:43.340,0:16:46.680
假如說，你告訴我說

0:16:46.680,0:16:50.200
你從盒子 1 裡面，抽一顆球的機率是 2/3

0:16:50.200,0:16:52.660
你從盒子 2 裡面，抽一顆球的機率是 1/3

0:16:52.660,0:16:57.020
這告訴我說，在盒子 1 裡面，藍球占 4/5

0:16:57.020,0:16:58.740
綠球占 4/5

0:16:58.740,0:17:03.080
盒子 2 裡面，藍球占 2/5，綠球占 3/5

0:17:03.080,0:17:04.740
那你就可以輕易計算說

0:17:04.740,0:17:07.440
如果我今天得到一顆藍球

0:17:07.440,0:17:11.480
他從盒子 1 裡面，抽出來的機率

0:17:11.480,0:17:15.600
這個機率就是，這個國小就應該有教過了

0:17:16.560,0:17:18.920
至少國中有教過吧，對不對？

0:17:18.920,0:17:20.780
大雞應該都會，就是

0:17:20.780,0:17:24.140
Given 一顆藍球，他從 B1 裡面 sample 出來的機率

0:17:24.140,0:17:27.420
就是這個樣子，這個沒什麼好解釋的

0:17:27.420,0:17:29.120
我相信大家都秒懂，這樣

0:17:29.340,0:17:32.300
那這個跟分類，有什麼關係呢？

0:17:32.460,0:17:37.060
如果我們把盒子換成分類的話

0:17:37.060,0:17:42.780
把盒子 1 跟盒子 2，換成類別 1 跟類別 2

0:17:42.780,0:17:45.000
換成類別 1 跟類別 2

0:17:45.000,0:17:48.940
這個時候呢，給我一個 x

0:17:49.340,0:17:51.940
就是，我們要分類的那個對象

0:17:51.940,0:17:54.440
比如說，今天我們的例子就是，分類一隻寶可夢

0:17:54.440,0:17:58.120
給我一隻寶可夢，他從某一個 class 裡面

0:17:58.120,0:18:01.300
sample 出來的機率是多少呢？

0:18:01.840,0:18:03.840
那我們需要知到哪些值？

0:18:04.040,0:18:06.880
我們需要知道 class 1

0:18:06.880,0:18:10.600
我從 class 1 裡面，抽一個 x 出來的機率

0:18:10.860,0:18:15.320
我們要知道從 class 2 裡面，抽一個 x 出來的機率

0:18:15.320,0:18:20.300
我們要知道說，從 class 1 裡面，抽一個 x 出來

0:18:20.300,0:18:24.320
從 class 1 裡面，抽出我們現在考慮的這個 x 的機率

0:18:24.680,0:18:28.260
我們要知道從這個 class 2 裡面

0:18:28.260,0:18:31.000
抽出我現在考慮的這個 x 的機率

0:18:31.000,0:18:34.400
如果有這些，當給我們一個 x 的時候

0:18:34.400,0:18:35.920
有了這 4 個數值

0:18:35.920,0:18:41.060
我們就可以計算，這個 x 是屬於 x1 的機率

0:18:41.060,0:18:43.740
怎麼算呢？x 屬於 x1 的機率

0:18:43.740,0:18:48.140
就是 case 1 本身的機率

0:18:48.140,0:18:53.200
乘上 case 1 sample 一個 object 出來，是 x 的機率

0:18:53.200,0:18:56.800
再除掉 case 1 本身的機率

0:18:56.800,0:19:00.040
乘上 case 1 sample 一個 object 出來，是 x 的機率

0:19:00.040,0:19:02.420
加上 case 2 本身的機率

0:19:02.420,0:19:07.960
乘上從 case 2 裡面 sample 出一個 object ，是 x 的機率

0:19:07.960,0:19:10.280
所以，我們現在的問題就是

0:19:10.280,0:19:13.040
如果我們知道這個機率的話

0:19:13.040,0:19:16.640
問題就解決了，因為給我一個寶可夢 x

0:19:16.640,0:19:20.260
我就可以看說，它從哪一個 case 來的機率最大

0:19:20.260,0:19:23.100
那機率最大那個 case，就是正確答案

0:19:23.100,0:19:25.580
那現在的問題是

0:19:25.580,0:19:27.360
我們如果要算這個值

0:19:27.360,0:19:30.340
那我們就要算這 4 個值

0:19:30.340,0:19:33.120
假設我們是考慮一個二元分類問題的話

0:19:33.120,0:19:35.520
我們就需要算這 4 個值

0:19:35.520,0:19:37.240
好，那這 4 個值怎麼來呢？

0:19:37.240,0:19:43.140
我們就希望從我們的 training data，去把這些值估測出來

0:19:43.720,0:19:48.900
那這一整套想法，叫做 Generative model

0:19:48.900,0:19:51.520
為甚麼它叫做 Generative model 呢？

0:19:51.520,0:19:57.080
因為有這個 model 的話，你可以拿它來 generate 一個 x

0:19:57.080,0:19:58.320
什麼意思呢？

0:19:58.320,0:20:03.360
你可以計算，某一個 x 出現的機率

0:20:03.360,0:20:06.420
如果你可以計算每一個 x 出現的機率

0:20:06.420,0:20:07.920
你就知道 x 的 distribution

0:20:07.920,0:20:14.380
你就可以用這個 distribution 來產生 x、sample x 出來

0:20:14.700,0:20:16.440
這一個機率是甚麼呢？

0:20:16.440,0:20:19.880
這個機率很簡單，它就是你從 c1 裡面

0:20:19.880,0:20:22.160
挑一個 x 出來的機率

0:20:22.160,0:20:24.320
乘上 c1 挑出 x 的機率

0:20:24.320,0:20:27.060
加上你從 case 2 裡面，挑一個 x 的機率

0:20:27.060,0:20:29.240
乘上 case 2 產生 x 的機率

0:20:29.240,0:20:31.560
你有這些機率

0:20:31.560,0:20:33.100
你就可以算這個

0:20:33.280,0:20:36.200
你就可以算某一個 x 出現的機率

0:20:36.200,0:20:38.540
你就可以自己產生 x

0:20:38.540,0:20:42.300
所以，這個東西，叫做 Generative model

0:20:43.400,0:20:48.020
那我們先來看一下 P(C1) 跟 P(C2)

0:20:48.020,0:20:52.820
我們先來算一下 P(C1) 跟 P(C2) 它出現的機率

0:20:52.820,0:20:55.520
那這個機率呢，叫做 Prior

0:20:55.520,0:21:00.240
他們呢，是比較好算的

0:21:00.240,0:21:03.580
假設我們今天考慮的這兩個 class 呢

0:21:03.580,0:21:06.880
分別是水系跟一般系

0:21:06.880,0:21:09.780
class 1 就是指水系的神奇寶貝

0:21:09.780,0:21:11.940
class 2 就是指一般系的神奇寶貝

0:21:11.940,0:21:14.720
另外 16 隻寶可夢，我們就無視它

0:21:14.720,0:21:17.780
先考慮一個二元分類的問題

0:21:17.780,0:21:22.960
那我們現在呢，把編號 ID，再圖鑑裡面編號 < 400 的

0:21:22.960,0:21:26.920
水系的和一般系的，就當作是 training data

0:21:26.920,0:21:28.540
剩下的當作是 testing data

0:21:28.540,0:21:30.080
如果你想要做的更嚴謹一點的話

0:21:30.080,0:21:33.680
你可以再把 training data 裡面
切一份 validation data 出來

0:21:34.100,0:21:38.040
好，那 training data 裡面呢，總共有 79 隻是水系的

0:21:38.040,0:21:39.700
總共有 61 隻一般系

0:21:39.700,0:21:43.800
你可能會問說，為什麼選水系
跟一般系當作二元分類問題

0:21:43.800,0:21:48.220
因為我其實統計了一下 18 種寶可夢，每個種類的數目

0:21:48.220,0:21:50.220
水系跟一般系是最多的

0:21:50.220,0:21:52.740
所以，我們就先選水系跟一般系

0:21:52.740,0:21:57.140
不過，我試了一下，如果你要把 18 種
都分類正確，好像是做不太出來的

0:21:57.140,0:22:00.880
因為它們中間，做不太出來這樣子

0:22:00.880,0:22:03.480
你就先考慮，分兩個 class 就好

0:22:05.080,0:22:09.300
如果我們現在知道說，training data 裡面

0:22:09.300,0:22:11.580
79 隻水系、61 隻一般系

0:22:11.580,0:22:16.980
那從這個第一類裡面

0:22:16.980,0:22:21.340
從 class 1 裡面，sample 出一隻寶可夢的機率是多少呢

0:22:21.340,0:22:26.340
是不是就是 79 / (79+61)，算出來是 0.56

0:22:26.540,0:22:30.480
那從 class 2，sample 出一隻寶可夢的機率

0:22:30.480,0:22:34.240
就是 61 / (79+61)，就是 0.44

0:22:34.500,0:22:38.920
這個是比較容易、比較簡單可以理解的

0:22:38.920,0:22:42.780
好，那再來我們的問題是這樣子

0:22:42.780,0:22:47.440
怎麼計算說，如果給我某一個 class

0:22:47.440,0:22:52.060
某一隻寶可夢，是從這個 class sample 出來的機率

0:22:52.060,0:22:54.820
比如說，如果給你一個海龜

0:22:54.820,0:22:57.080
我也不知道這個海龜應該叫什麼名字

0:22:57.080,0:22:59.500
給一隻海龜，它是從

0:22:59.500,0:23:02.200
就是從水系的神奇寶貝裡面

0:23:02.200,0:23:04.020
挑一隻神奇寶貝出來

0:23:04.020,0:23:10.380
它是海龜的機率，到底應該有多大呢？

0:23:11.100,0:23:13.740
那我們現在的 training data 是長這個樣子

0:23:13.740,0:23:16.920
屬於水系的神奇寶貝有 79 隻

0:23:17.160,0:23:21.800
所以有傑尼龜、可達鴨、蚊香蝌蚪之類的

0:23:21.800,0:23:25.240
這些我是認識的啦，我小時候在卡通有看到

0:23:25.240,0:23:26.940
這個是編號後面的

0:23:26.940,0:23:29.140
我小時候卡通沒有看到，所以我也不知道它叫什麼名字

0:23:29.140,0:23:30.900
它也不在這 79 隻裡面

0:23:30.900,0:23:33.240
那我到底要怎麼算說

0:23:35.020,0:23:40.300
從水系的神奇寶貝裡面挑一隻挑出來，是海龜的機率呢

0:23:40.380,0:23:42.500
你可能會想說

0:23:42.500,0:23:45.120
這 79 隻神奇寶貝又沒海龜

0:23:45.120,0:23:47.660
所以挑一隻出來，是海龜的機率根本是 0 阿

0:23:47.660,0:23:50.240
可是這海龜是水系的

0:23:50.240,0:23:53.300
我一看它的臉，我就知道它是水系的 XD

0:23:54.760,0:23:57.580
那它就是水系的，所以

0:23:57.580,0:24:01.100
你說，它從水系的裡面挑出來是 0 也不對阿

0:24:01.100,0:24:02.720
所以怎麼辦？

0:24:02.720,0:24:05.400
首先就是，每一隻神奇寶貝

0:24:05.400,0:24:08.180
每一隻寶可夢，我們剛才講過說

0:24:08.180,0:24:11.300
它都用一個向量來描述

0:24:11.300,0:24:15.820
這個向量裡面的值，就是它的各種的特徵值

0:24:15.820,0:24:19.680
所以，這個 vector，我們又稱之為一個 feature

0:24:19.680,0:24:24.020
所以，每一個寶可夢，都是用一堆 feature 來描述它

0:24:24.920,0:24:27.820
然後呢，我們就真的把那些水系的神奇寶貝

0:24:27.820,0:24:33.260
水系的寶可夢，它們的防禦力和特殊防禦力畫出來

0:24:33.260,0:24:36.880
其實，每一隻寶可夢有 7 個不同的數值阿

0:24:36.880,0:24:41.340
不過 7 個沒辦法畫，我就先畫防禦力跟特殊防禦力就好

0:24:41.340,0:24:45.800
值得強調的是，這邊的是真正的 data 這樣

0:24:45.800,0:24:50.680
如果你想要載完整的寶可夢的 data 的話

0:24:50.680,0:24:53.920
這個投影片最後也有附一個連結

0:24:53.920,0:24:57.020
你是可以載到完整的 data

0:24:58.280,0:25:02.280
那我們就把 79 隻寶可夢

0:25:02.280,0:25:09.180
它的防禦力跟特殊防禦力，都先畫在這張圖

0:25:09.180,0:25:10.640
二維的平面上

0:25:10.640,0:25:15.400
所以這二維的平面上，每一個點，就代表了一隻寶可夢

0:25:15.400,0:25:17.880
比如說，這個點是可達鴨

0:25:17.880,0:25:20.480
它的防禦力是 48，特殊防禦力是 50

0:25:20.480,0:25:22.080
這個點是傑尼龜

0:25:22.080,0:25:25.140
它的防禦力是 65，特殊防禦力是 64

0:25:25.140,0:25:27.260
可是，現在的問就是

0:25:27.260,0:25:30.180
如果給我們一個新的點

0:25:30.180,0:25:35.300
這個點是代表一隻
沒有在我們的 training data 裡面的寶可夢

0:25:35.300,0:25:39.040
是我們沒有看過的寶可夢，比如這隻海龜

0:25:39.040,0:25:43.680
它的防禦力是 103，特殊防禦力是 45

0:25:43.680,0:25:46.100
它的位置大概在這個地方

0:25:47.820,0:25:51.600
從水系裡面，挑到這隻神奇寶貝，它是水系的機率

0:25:52.120,0:25:53.780
到底應該是多少

0:25:53.780,0:25:55.240
那你不可以說它是 0 ，你不能說

0:25:55.240,0:26:01.020
這個 training data 裡面從來沒有
出現這隻海龜，所以它的機率就是 0

0:26:01.020,0:26:02.340
這樣顯然是不對的

0:26:02.340,0:26:04.040
你要想個辦法估測說

0:26:04.040,0:26:07.500
從這些我們已經有的 神奇寶貝裡面，估測說

0:26:07.500,0:26:09.980
如果從水系的神奇寶貝裡面

0:26:09.980,0:26:14.000
挑一隻出來，它是這個海龜的機率，到底有多少？

0:26:14.780,0:26:18.340
好，那怎麼辦呢？

0:26:18.340,0:26:20.040
你可以想像說

0:26:20.040,0:26:22.480
這 79 隻神奇寶貝

0:26:22.480,0:26:25.700
其實只是冰山的一角

0:26:25.700,0:26:30.180
就是水系的神奇寶貝，是從一個機率的分布裡面

0:26:30.180,0:26:33.280
水系神奇寶貝它的防禦力跟特殊防禦力

0:26:33.280,0:26:38.040
是從一個 Gaussian 的 distribution 裡面，sample 出來的

0:26:38.040,0:26:41.220
我們只是 sample 了 79 個點以後

0:26:41.220,0:26:44.200
得到的分佈長這個樣子

0:26:44.200,0:26:47.420
但是，從 Gaussian 的 distribution 裡面

0:26:47.420,0:26:50.820
sample 出這個點的機率，不是 0

0:26:50.820,0:26:56.680
我們假設說，這 79 個點是從一個 Gaussian 的 distribution 裡面 sample 出來的

0:26:56.740,0:26:58.460
再來，我們要做的事情就是

0:26:58.460,0:27:02.960
如果給我這 79 個點

0:27:02.960,0:27:07.360
我們怎麼找到那個 Gaussian 的 distribution

0:27:07.740,0:27:12.520
那我這邊還做了幾頁投影片要說 Gaussian distribution

0:27:12.520,0:27:15.120
那我覺得這應該，不用講吧

0:27:15.120,0:27:18.240
假設你不知道 Gaussian distribution 是什麼的話

0:27:18.240,0:27:21.380
你就想成它是一個 function

0:27:21.380,0:27:24.460
這個 function 的 input 就是一個 vector x

0:27:24.460,0:27:27.820
在這邊呢，代表某一隻寶可夢的數值

0:27:27.820,0:27:34.520
它的 output 就是這一隻寶可夢，
從這一個 distribution 裡面

0:27:34.520,0:27:39.180
這一個 x 從這一個 distribution 裡面，
被sample 出來的機率

0:27:39.180,0:27:41.820
其實嚴格說起來，這個東西並不是機率

0:27:41.820,0:27:44.460
它是 probability 的 density

0:27:44.600,0:27:48.920
它跟機率是成正比的，但它並不 exactly 就是機率

0:27:48.920,0:27:54.440
但是這邊，為了讓大家不要太混亂，
我們就假設它是機率

0:27:54.440,0:27:57.540
好，那這個機率的分佈呢

0:27:57.540,0:27:59.620
它是由兩個東西決定

0:27:59.620,0:28:02.860
一個東西，叫做 mean，這邊寫成 μ

0:28:02.860,0:28:07.540
另外一個東西，叫做 variance，寫做 Σ，它是一個 matrix

0:28:07.540,0:28:11.260
mean 是一個 vector，Σ 是一個 matrix

0:28:11.260,0:28:19.960
所以你把 μ 跟 Σ 代入這個看起來有點複雜的 function

0:28:19.960,0:28:23.380
那它就會有不同的形狀

0:28:23.380,0:28:27.120
同樣的 x，如果有不同的 μ 跟 Σ

0:28:27.120,0:28:31.900
那你代進同樣的 x，它 output 的
機率分布呢，就會是不一樣的

0:28:31.900,0:28:34.660
下面呢，就舉幾個例子

0:28:34.660,0:28:37.880
比如說，同樣的 Σ、不同的 μ

0:28:37.880,0:28:41.880
代表說他們機率分布最高點的地方，是不一樣的

0:28:41.880,0:28:45.140
比如說，同樣的 μ、不同的 Σ

0:28:45.140,0:28:47.480
代表說機率分布的最高點是一樣的

0:28:47.480,0:28:52.920
但是，它們的分布，散的程度是不一樣的

0:28:53.440,0:28:56.020
那接下來的問題就是

0:28:56.760,0:29:01.400
我們假設有一個 Gaussian 存在

0:29:01.400,0:29:04.960
從這個 Gaussian 裡面 ，

0:29:04.960,0:29:08.440
sample 出這 79 個點

0:29:08.440,0:29:15.540
那，到底這個 Gaussian長什麼樣子呢？

0:29:15.540,0:29:18.140
如果我們可以找到這個 Gaussian 的話

0:29:18.140,0:29:20.300
假設我們可以根據這 79 個點

0:29:20.300,0:29:23.760
估測出這個 Gaussian 的 μ (mean)

0:29:23.760,0:29:28.500
應該在這個位置，是 [75, 71.3]

0:29:29.040,0:29:33.300
它的 Σ 應該是這樣的分布

0:29:33.300,0:29:38.500
它的 x 跟 y 是有一些 correlation 的

0:29:38.500,0:29:41.800
但是沒有 x^2 跟 y^2 這邊這麼大

0:29:41.800,0:29:43.420
大概是長這樣子

0:29:43.420,0:29:46.740
那給我們一個新的點 x

0:29:47.160,0:29:49.700
它是我們過去從來沒有看過的點

0:29:49.700,0:29:52.860
它不在這個 sampling 裡面，
它不在這 79 個 sampling 裡面

0:29:52.860,0:29:56.140
但是，如果我們知道 μ 跟 x 的話

0:29:56.340,0:30:00.420
我們就可以把 Gaussian distribution 的 function 寫出來

0:30:00.420,0:30:03.060
知道 μ 跟 x，我們就可以把這個function 寫出來

0:30:03.060,0:30:04.780
這個 function 是 depend on μ 跟 x 的

0:30:04.780,0:30:07.600
所以，我們把它寫成 f (下標 μ 跟 Σ(x))

0:30:07.600,0:30:09.760
你把這個 x 代進去

0:30:09.760,0:30:12.600
經過一串複雜的運算以後

0:30:12.600,0:30:15.040
那你就可以算出呢

0:30:15.040,0:30:18.740
某一個 x 從這個 Gaussian 裡面

0:30:18.740,0:30:22.360
從這個 mean 是 μ、它的 covariance matrix 
是 Σ 的 Gaussian 裡面

0:30:22.360,0:30:24.920
被 sample 出來的機率

0:30:24.920,0:30:28.600
那如果你對這個 function 沒什麼概念的話呢

0:30:28.600,0:30:29.840
你就可以想像說

0:30:29.840,0:30:34.280
如果 x 越接近中心點，越接近 μ 這個地方

0:30:34.280,0:30:37.300
它 sample 出來的機率當然是比較大的

0:30:37.300,0:30:41.680
像這個 x 在這麼遠的地方，
它 sample 出來的機率就是比較小的

0:30:43.340,0:30:44.940
那再來有一個問題就是

0:30:44.940,0:30:48.340
怎麼找這個 μ 跟怎麼找這個 Σ ？

0:30:48.340,0:30:53.400
這邊用的這個概念呢，叫做 Maximum Likelihood

0:30:53.660,0:30:57.660
你可以想像說，這 79 個點，其實可以從

0:30:57.660,0:31:00.900
任何一個 Gaussian 裡面，被 sample 出來

0:31:00.900,0:31:01.960
對不對？

0:31:01.980,0:31:05.300
任何一個 Gaussian 都有可能sample 出這 79 個點

0:31:05.300,0:31:08.100
不管你是 μ 在這個位置

0:31:08.100,0:31:11.180
然後，它的 covariance matrix 長這個樣子

0:31:11.180,0:31:13.760
還是 μ 在這個位置

0:31:13.760,0:31:16.140
它的 covariance 長這個樣子

0:31:16.140,0:31:19.960
它都有可能sample 出這 79 個點

0:31:19.960,0:31:23.780
對不對？因為你從 Gaussian 裡面 sample 出一個 point

0:31:23.780,0:31:27.220
它可以是整個空間上的任何一個點

0:31:27.220,0:31:30.340
只是有些地方機率很低，有些地方機率很高

0:31:30.340,0:31:33.440
但沒有一個地方的機率是 exactly 等於 0 的

0:31:33.440,0:31:36.280
所以，雖然說，右上角這個 Gaussian

0:31:36.280,0:31:41.160
右上角這個 Gaussian，它 sample 出
左下角這個點的機率很低

0:31:41.160,0:31:45.560
但是，並不代表說，這個機率是 0

0:31:45.560,0:31:52.600
但是，雖然說每一個 Gaussian 都
有可能 sample 出這 79 個點

0:31:52.600,0:31:58.100
但是，他們 sample 出這 79 個點的可能性是不一樣的

0:31:58.100,0:32:02.320
他們 sample 出這 79 個點的 Likelihood 是不一樣的

0:32:02.320,0:32:05.040
顯然說，如果你的 Gaussian 是這個的話

0:32:05.040,0:32:07.880
他 sample 出這 79 個點的 Likelihood 就比較高

0:32:07.880,0:32:09.280
如果你的 Gaussian 是這個的話

0:32:09.280,0:32:14.400
他 sample 出這 79 個點的機率是比較低的

0:32:14.980,0:32:22.400
所以說，今天給我們某一個 Gaussian 的 μ 跟 Σ

0:32:22.400,0:32:26.600
我們就可以算這個 Gaussian 的 likelihood

0:32:26.600,0:32:30.160
也就是說，給我一個 Gaussian 的 μ 跟 Σ

0:32:30.160,0:32:32.080
我們就可以算這個 Gaussian

0:32:32.080,0:32:36.980
sample 出這 79 個點的機率

0:32:36.980,0:32:39.600
那這個 likelihood、這個可能性呢

0:32:39.600,0:32:41.920
我們可以把它寫成這樣一個式子

0:32:41.920,0:32:43.400
這個可能性呢

0:32:43.800,0:32:47.480
這邊呢，我們也用了 L，因為我想不到更好的 notation

0:32:47.480,0:32:50.420
可能會跟 loss function 有點混淆

0:32:50.420,0:32:53.900
但是，Likelihood 用別的 notation 又很怪

0:32:53.900,0:32:55.460
還是用 L

0:32:55.460,0:32:57.660
那這個 L，它的 input 就是

0:32:57.660,0:33:02.040
Gaussian 的 mean(μ) 跟 covariance (Σ)

0:33:02.040,0:33:06.880
L 做的事就是把這個 μ 跟 Σ，
代到這個 likelihood 的 function 裡面

0:33:06.880,0:33:08.700
那它會告訴我們說

0:33:08.700,0:33:12.500
這個 μ 跟 Σ，它 sample 出這 79 個點的機率

0:33:12.500,0:33:14.260
到底有多大？

0:33:14.520,0:33:16.280
它的可能性到底有多大？

0:33:16.280,0:33:17.660
這個東西怎麼算？

0:33:17.660,0:33:19.220
這個點，這個東西就是這樣算

0:33:19.220,0:33:25.900
因為所有的 79 個點是獨立被 sample 出來的

0:33:25.900,0:33:30.960
所以，今天這個 Gaussian，它 sample 出這 79 個點的機率

0:33:30.960,0:33:35.000
就是，這個 Gaussian sample 出第 1 個點的機率

0:33:35.000,0:33:37.320
乘上 sample 出第 2 個點的機率

0:33:37.320,0:33:38.940
乘上 sample 出第 3 個點的機率

0:33:38.940,0:33:42.700
一直到 sample 出第 79 個點的機率

0:33:43.880,0:33:47.800
那所以我們有 79 隻水系的神奇寶貝

0:33:47.800,0:33:51.520
我們知道，它是從某一個 Gaussian，被 sample 出來的

0:33:51.520,0:33:54.100
我們接下來要做的事情，就是

0:33:54.100,0:33:56.280
找到那一個 Gaussian

0:33:56.500,0:33:58.520
找一個 Gaussian

0:33:58.520,0:34:03.840
那個 Gaussian，它 sample 出這 79 個點的機率

0:34:03.840,0:34:05.300
是最大的

0:34:05.300,0:34:09.320
它 sample 出這 79 個點的 Likelihood 是最大的

0:34:09.320,0:34:13.120
那這個 Gaussian，我們就當作是

0:34:13.120,0:34:15.860
sample 出這 79 個點的 Gaussian

0:34:15.860,0:34:19.060
那這個  Likelihood 最大的 Gaussian 呢

0:34:19.060,0:34:23.160
我們寫作 (μ*, Σ*)

0:34:23.160,0:34:24.920
所以，我們現在要做的事情是這樣

0:34:24.920,0:34:26.700
Likelihood 的 function 寫做這樣子

0:34:26.700,0:34:28.780
那這每一個 f，如果你想知道的話

0:34:28.780,0:34:30.620
它很複雜，是寫成這個樣子

0:34:30.620,0:34:34.440
你就把這個 x 代進去，然後再算出它的 x^2 代進去

0:34:34.440,0:34:35.460
然後你就算出它

0:34:35.660,0:34:40.880
然後呢，我們要窮舉所有的 μ

0:34:40.880,0:34:45.760
窮舉所有的 Σ，看哪一個可以讓上面的 likelihood 的式子最大

0:34:45.760,0:34:49.700
它就是我們要找的 μ* 跟 Σ*

0:34:49.700,0:34:53.800
它就是我們認為最有可能產生這 79 個點的 μ* 跟 Σ*

0:34:53.800,0:34:58.780
我們就當作這 79 個點，是從這個 μ*, Σ* sample 出來的

0:34:58.780,0:35:02.380
這個東西，怎麼做呢？

0:35:02.380,0:35:06.460
其實這樣子講，如果你爽的話，你就用微分解一下

0:35:06.460,0:35:08.120
找那個極值的地方這樣

0:35:08.120,0:35:11.820
秒解這樣，你也可以背個公式解

0:35:11.820,0:35:14.200
怎麼秒解，就是

0:35:14.200,0:35:16.760
哪一個 μ* 可以讓這個最大呢？

0:35:16.760,0:35:18.520
這個結果是很直覺的

0:35:18.520,0:35:21.180
就是平均值可以讓它最大

0:35:21.180,0:35:24.980
所以，你就把 79 個 x 平均起來

0:35:24.980,0:35:32.400
你就把 79 個 x 當作是 vector 加起來，除 79，就得到 μ*

0:35:32.480,0:35:34.280
平均就是 μ*

0:35:34.280,0:35:39.160
如果你不爽的話，你就把這個式子取個微分阿

0:35:39.160,0:35:42.140
對 μ 取個微分，然後找它微分是 0 的點

0:35:42.140,0:35:44.060
解出來就是你的 μ*

0:35:44.920,0:35:50.080
Σ* 是甚麼呢？你先把 μ* 算出來

0:35:50.080,0:35:55.700
然後對所有的 x^n，你都算 (x^n - μ*)

0:35:55.700,0:35:59.440
乘 (x^n - μ*)^T (的 transpose)

0:35:59.440,0:36:06.680
你就算說，假設 x^n 的 mean 是 μ* 的話

0:36:06.680,0:36:10.460
的 covariance，那你算出來呢，就是 Σ*

0:36:10.460,0:36:15.340
那如果你不爽的話，就把這些值對 Σ* 做微分

0:36:15.340,0:36:19.280
對 Σ* 做微分，然後解它微分是 0 的點

0:36:19.280,0:36:21.180
你就解出來這個

0:36:21.180,0:36:26.380
有了這些以後

0:36:26.380,0:36:28.320
我們就真的去算一下

0:36:28.320,0:36:30.640
這個是真正的結果

0:36:30.940,0:36:35.120
79 隻水系的神奇寶貝，79 隻水系的寶可夢

0:36:35.200,0:36:38.300
算出來的 μ 是這樣子

0:36:38.300,0:36:41.420
算出來的 Σ 是這樣子

0:36:41.420,0:36:42.580
也就是說呢

0:36:42.580,0:36:47.120
假設這 79 隻水系的神奇寶貝

0:36:47.120,0:36:48.480
是從這個 Gaussian sample 出來的話

0:36:48.480,0:36:51.700
那最有可能 sample 出這 79 個點的 Gaussian

0:36:51.700,0:36:55.880
它的 mean 是 μ1，它的 covariance 是 Σ1

0:36:56.240,0:36:59.200
那如果你看 class 2 的話

0:37:00.320,0:37:03.120
class 2 是一般系的神奇寶貝

0:37:03.120,0:37:05.560
有幾隻呢？有 61 隻

0:37:05.560,0:37:09.460
那我們一樣算它的 mean 跟 variance

0:37:09.460,0:37:12.440
這 61 隻一般系的神奇寶貝

0:37:12.440,0:37:14.000
最有可能 sample 出它的 Gaussian

0:37:14.000,0:37:17.920
它的 mean 是長這樣，它的 variance 是長這樣

0:37:17.920,0:37:19.700
有了這些以後

0:37:20.380,0:37:24.280
就結束了，我們就可以做分類的問題了

0:37:24.280,0:37:25.280
怎麼做呢？

0:37:25.280,0:37:27.180
我們說要做分類的問題

0:37:27.180,0:37:30.900
我們只要算出 P(C1|x)

0:37:30.900,0:37:33.660
給我一個 x，它是從 C1 來的機率

0:37:33.660,0:37:36.060
那這整項可以寫成這樣子

0:37:36.760,0:37:40.420
只要我們最後算出來這一項，大於 0.5 的話

0:37:40.420,0:37:43.220
那 x 就屬於 class 1

0:37:44.760,0:37:49.140
那 P(C1) 很容易算，就是這麼回事

0:37:49.140,0:37:51.660
那 P(C2) 我們算過，就是這麼回事

0:37:51.660,0:37:54.940
P(x|C1) 怎麼算呢？

0:37:54.940,0:37:56.420
我們已經找出

0:37:56.420,0:37:59.300
我們已經假設說這個東西

0:37:59.300,0:38:01.620
它就是一個 Gaussian distribution

0:38:01.620,0:38:05.520
這個 Gaussian distribution 的 mean 跟 variance

0:38:05.520,0:38:07.880
分別就是 μ1 跟 Σ1

0:38:07.880,0:38:09.860
這我們剛才已經算出來過了

0:38:09.860,0:38:11.620
因為我們剛才已經根據

0:38:11.620,0:38:16.800
class 1 所有的、那 79 隻寶可夢的分布

0:38:16.800,0:38:18.800
知道說，他們是從一個

0:38:18.800,0:38:23.840
mean 是 μ1，covariance 是 Σ1 的 
distribution 裡面 sample 出來的

0:38:23.840,0:38:25.700
那如果是這一項呢？

0:38:25.700,0:38:31.660
P(x|C2)，那我們也知道說，
它的這個 Gaussian distribution

0:38:31.660,0:38:35.060
它是從 mean 是 μ^2, covariance 是 Σ^2

0:38:35.060,0:38:37.380
的 distribution 裡面 sample 出來的

0:38:37.380,0:38:41.760
有了這些以後，問題就解決了

0:38:41.760,0:38:43.460
那結果怎麼樣呢？

0:38:43.460,0:38:44.860
我是真的有做的

0:38:45.360,0:38:47.840
藍色的點是

0:38:47.840,0:38:50.940
這個橫軸跟縱軸

0:38:50.940,0:38:54.140
分別就是防禦力跟特殊防禦力

0:38:54.560,0:38:59.080
藍色的點，是水系的神奇寶貝的分布

0:38:59.080,0:39:03.160
紅色的點，是一般系的神奇寶貝的分布

0:39:03.160,0:39:05.340
看到這個結果，我有點緊張

0:39:05.340,0:39:09.780
因為覺得分不出來，我用人眼看就知道不太 ok

0:39:10.320,0:39:13.300
那我們就真的計算一下

0:39:13.300,0:39:16.620
在這個二維平面上

0:39:16.620,0:39:20.600
每一個點，我都當作一個 x

0:39:20.600,0:39:24.980
進去我都可以算一個，它是 C1 的機率對不對？

0:39:24.980,0:39:30.740
這個圖上的每一個點，我都可以算它是 C1 的機率

0:39:30.740,0:39:34.440
那這個機率呢，用顏色來表示

0:39:34.440,0:39:37.720
紅色就代表說，在這個區域呢

0:39:37.720,0:39:42.360
是 class 1，是水系神奇寶貝的機率是比較大的

0:39:42.360,0:39:47.320
在這個地方呢，水系神奇寶貝的機率是比較小的

0:39:47.320,0:39:48.740
你看這很合理嘛

0:39:48.740,0:39:51.500
因為，水系神奇寶貝在這邊的分布還是比較多

0:39:51.500,0:39:54.560
在這邊比較多，所以這個地方機率是比較大的

0:39:55.100,0:39:58.220
那現在，因為我們處理的是分類的問題

0:39:58.220,0:40:01.220
我們算這個機率，我們是要 output 說是哪一類

0:40:01.220,0:40:05.520
所以我們說，機率大於 0.5，就是類別一

0:40:05.520,0:40:09.160
也就是紅色這個區間，就是類別一

0:40:09.160,0:40:12.400
機率小於 0.5，就是藍色這個區間

0:40:12.400,0:40:15.660
他們就是類別二，這個是類別一

0:40:15.660,0:40:17.380
你會發現說

0:40:17.380,0:40:19.400
如果你看藍色的點的話

0:40:19.400,0:40:22.320
是比較多藍色的點，在這個紅色的區間

0:40:22.320,0:40:26.280
那紅色的點，是比較多在這個藍色的區間

0:40:26.280,0:40:27.640
那也有一些到紅色的區間

0:40:27.640,0:40:32.260
有點難搞，因為他們中間沒有一個明確的 boundary

0:40:33.240,0:40:36.680
那把它 apply 到 testing set 上

0:40:36.680,0:40:40.300
現在，testing set 就是編號大於 400 那些寶可夢

0:40:40.300,0:40:44.860
把他們整個 class 1 跟 class 2 的寶可夢
畫在這個二維平面上

0:40:44.860,0:40:46.620
那 boundary 是一樣的

0:40:46.620,0:40:49.000
這個 boundary 是一樣的

0:40:49.000,0:40:52.200
你會發現說，分的不甚太好

0:40:52.200,0:40:54.860
正確率是 47 %

0:40:55.120,0:40:57.620
那你有一點擔心

0:40:57.620,0:41:02.000
會不會是這題有可能分不出來，這也是有可能的

0:41:02.000,0:41:06.080
但是我想說我們現在只看了二維的空間，對不對

0:41:06.080,0:41:08.180
機器學習厲害的地方就是

0:41:08.180,0:41:09.820
因為我們讓機器處理這個問題

0:41:09.820,0:41:14.000
所以高維空間也可以處理，不是只處理二維的空間而已

0:41:14.000,0:41:15.940
所以我們看一下高維的空間

0:41:15.940,0:41:18.237
事實上，每一隻神奇寶貝(寶可夢呢)

0:41:18.237,0:41:21.160
它是分布在一個七維的空間裡面

0:41:21.160,0:41:25.200
如果只用二維的空間分不出來，
可是搞不好七維分的出來阿

0:41:25.200,0:41:28.720
就是這個紅色跟藍色搞不好在高維的空間上看到

0:41:28.720,0:41:30.420
一個是這樣，一個是這樣

0:41:30.420,0:41:32.560
現在從上面往下看，就覺得疊在一起

0:41:32.560,0:41:34.800
高維空間上，搞不好是分開的

0:41:34.800,0:41:36.820
搞不好在七維空間上，是分開的

0:41:36.820,0:41:41.160
所以，每一個寶可夢都是用七個數值來表示

0:41:41.160,0:41:44.480
所以，每一個寶可夢都是存在七維空間中的一個點

0:41:44.480,0:41:49.860
我們一樣可以算 class 跟 class 2 在七維空間中

0:41:49.860,0:41:55.360
sample 出那些點的 μ1 跟 μ2

0:41:55.360,0:41:57.320
μ1 跟 μ2 都是七維

0:41:57.320,0:42:02.980
Σ1 跟 Σ2 都是 7*7 的 matrix

0:42:02.980,0:42:07.180
然後你就做一發，正確率就 50%，很糟

0:42:07.180,0:42:10.440
這樣就跟你 random 猜，大概也是這個樣子

0:42:10.440,0:42:15.900
然後就 so sad 這樣，然後我們下周再看看要怎麼改進它

0:42:15.940,0:42:17.180
謝謝

0:42:30.700,0:42:35.020
各位同學大家好，我們就開始上課吧

0:42:35.020,0:42:38.460
上次我們講到哪裡呢？我們講到說

0:42:39.800,0:42:46.580
如果我們想要做，寶可夢的屬性的分類的話

0:42:46.580,0:42:52.380
我們可以假設一個機率模型

0:42:52.380,0:42:55.640
那我們把這個機率模型裡面呢

0:42:55.640,0:42:58.600
拆成有 required probability

0:42:58.600,0:43:01.760
跟每一個 class 自己的 distribution

0:43:01.760,0:43:06.860
那每一個 class 自己的機率呢，就用 Gaussian 來假設它

0:43:06.860,0:43:08.940
那經過一番運算

0:43:08.940,0:43:12.140
我們算出每一個 class 的 required

0:43:12.140,0:43:16.080
跟每一個 class 的 Gaussian distribution 以後呢

0:43:16.080,0:43:17.960
做一下

0:43:21.880,0:43:24.840
講到這邊，做一下之後發現呢

0:43:24.840,0:43:26.280
結果壞掉了

0:43:26.280,0:43:29.620
就算是我用了全部寶可夢的 7 個 feature

0:43:29.620,0:43:32.740
還是壞掉了

0:43:32.740,0:43:35.120
那怎麼辦呢？

0:43:42.500,0:43:48.740
那其實呢，當你用這樣子的 
probability generated 的 model 的時候

0:43:48.740,0:43:53.400
像我在上一堂課裡面用的那種模型

0:43:53.400,0:43:54.920
是比較少見的

0:43:54.920,0:43:56.680
其實你不常看到

0:43:56.680,0:44:02.400
給每一個 Gaussian 都有自己的 mean 跟自己的 variance

0:44:02.640,0:44:05.340
每一個 Gaussian 都有自己的 mean 跟自己的 variance

0:44:05.340,0:44:09.940
class 1 有一個 μ1，有一個 Σ1

0:44:09.940,0:44:12.800
class 2 有一個 μ2、 Σ2

0:44:12.800,0:44:15.720
比較常見的做法是，不同的 class

0:44:15.720,0:44:20.640
可以 share 同一個 covariance 的 matrix

0:44:20.640,0:44:23.360
首先。你想想看，covariance matrix

0:44:23.360,0:44:26.360
它其實是跟你 input 的 feature size

0:44:26.360,0:44:30.940
是跟它的平方成正比的

0:44:30.940,0:44:34.820
所以，covariance matrix 當你的 feature size 很大的時候

0:44:34.820,0:44:37.880
它的增長呢，其實是可以非常快的

0:44:37.880,0:44:39.660
所以在這個情況下呢

0:44:39.660,0:44:45.200
如果你把兩個不同的 Gaussian 
都給它不同的 covariance matrix

0:44:45.200,0:44:48.240
那你的 model 參數可能就太多了，model 參數多

0:44:48.240,0:44:52.240
variance 就大，也就是容易 overfitting

0:44:52.240,0:44:56.040
所以，如果我們要有效減少參數的話

0:44:56.040,0:44:58.300
我們可以給這兩個 class

0:44:58.300,0:45:02.520
就是屬於水系的神奇寶貝和屬於一般系的神奇寶貝

0:45:02.520,0:45:09.400
它們的描述這兩個 class 的 feature 分布的 Gaussian

0:45:09.400,0:45:13.540
故意給他們同樣的 covariance matrix

0:45:13.540,0:45:16.480
強迫他們共用 covariance matrix

0:45:16.860,0:45:21.600
這樣子呢，你就只需要比較少的 parameter

0:45:21.600,0:45:24.540
就可以來 model 這一個模型了

0:45:25.240,0:45:26.740
這甚麼意思呢？

0:45:26.740,0:45:32.680
也就是說，現在我們有 79 隻水系的寶可夢

0:45:32.680,0:45:36.420
我們假設它是從一個 mean 是 μ1

0:45:36.420,0:45:42.500
covariance 是 Σ 的 Gaussian 所 generate 出來的

0:45:42.980,0:45:47.380
那另外，這邊是多少隻呢？

0:45:47.380,0:45:49.460
這邊應該是 61 隻

0:45:49.460,0:45:53.780
我們這邊給它編號從 80 到 140

0:45:53.900,0:45:56.860
有另外 61 隻是屬於一般系的寶可夢

0:45:56.860,0:46:02.260
我們假設這些寶可夢他們的屬性的這個分布呢

0:46:02.260,0:46:05.320
是從另外一個 Gaussian 所 generate 出來的

0:46:05.320,0:46:08.920
另外一個 Gaussian，它的 mean 是 μ2

0:46:08.920,0:46:11.300
但是它的 covariance matrix

0:46:11.300,0:46:13.880
跟 generate 前一個

0:46:13.880,0:46:16.720
跟 generate class 1，跟 generate 水屬性的寶可夢

0:46:16.720,0:46:20.040
他們用的 covariance matrix 是同一個

0:46:20.040,0:46:24.100
這兩個 class 他們 share 同一個 covariance matrix

0:46:24.620,0:46:28.520
如果這樣子的話，你怎麼計算 Likelihood 呢？

0:46:28.520,0:46:34.000
如果你現在要計算，某一組 μ1, μ2 和 Σ

0:46:34.000,0:46:37.220
generate 這總共兩個 case 合起來

0:46:37.220,0:46:41.540
140 筆 data 的可能性的話

0:46:41.540,0:46:44.520
你就像下面這樣計算

0:46:44.780,0:46:48.520
這個計算方法就是

0:46:48.520,0:46:54.240
計算如果你今天用 μ1 跟 Σ1 產生 x^1 的機率

0:46:54.240,0:46:57.820
乘上用 μ1 跟 Σ1 產生 x^2 的機率

0:46:57.820,0:47:03.460
剛才都一直唸 Σ1 不好意思

0:47:03.460,0:47:05.580
這邊不是 Σ1，這個是只有 Σ 而已

0:47:05.580,0:47:09.320
因為這兩個 class，是共用同一個 covariance matrix

0:47:10.160,0:47:13.660
現在呢，用 μ1 跟 Σ 產生 x^1

0:47:13.660,0:47:17.480
到用 μ1 跟 Σ 產生 x^79

0:47:17.480,0:47:24.160
那如果是第一個 class 的 x 用這個方法來產生

0:47:24.160,0:47:26.760
如果是第一個 class 的 x 呢

0:47:26.760,0:47:29.700
你就用 μ2 跟 Σ 產生 x^80

0:47:29.700,0:47:32.140
用 μ2 跟 Σ 產生 x^81

0:47:32.140,0:47:35.660
到用 μ2 跟 Σ 產生 x^140

0:47:35.840,0:47:38.320
那在這個式子裡面呢 μ1, μ2

0:47:38.320,0:47:40.960
你要怎麼算 μ1, μ2 呢？

0:47:40.960,0:47:46.060
你要怎麼找一個 μ1, μ2 跟 Σ 讓
這個 Likelihood 的 function 最大呢？

0:47:46.060,0:47:48.180
那 μ1, μ2 的算法

0:47:48.180,0:47:52.720
跟我們之前沒有把 class 1 跟 class 2 的 covariance

0:47:52.720,0:47:57.620
tight 在一起的時候，那個算式是一模一樣的

0:47:57.620,0:48:02.340
你就只要把 class 1 裡面的 x 平均起來就變 μ1

0:48:02.340,0:48:05.440
class 2 裡面的 x 平均起來就變 μ2

0:48:05.440,0:48:07.980
唯一不一樣的是

0:48:09.000,0:48:11.860
(右下角)我們要把它按接受這樣子

0:48:17.580,0:48:20.600
那唯一不一樣的是

0:48:20.600,0:48:22.460
Σ 嗯？

0:48:25.320,0:48:27.420
唯一不一樣的是 Σ

0:48:27.420,0:48:31.160
因為我們現在 Σ 要同時考慮這兩個 class

0:48:31.160,0:48:33.040
所以它當然是不一樣的

0:48:33.040,0:48:35.660
那這個 Σ 的式子應該長甚麼樣子呢？

0:48:35.660,0:48:39.920
這個 Σ 的式子，這個結果非常的直觀

0:48:39.920,0:48:42.840
如果你想要看它的推導的話

0:48:42.840,0:48:46.540
我這邊引用的就是 Bishop 這本教科書

0:48:46.540,0:48:50.620
以後如果要引用的話，盡量引 Bishop，為甚麼呢？

0:48:50.620,0:48:53.660
因為它在網路上，有 available 的版本

0:48:58.060,0:49:01.660
這個 Σ 應該長甚麼樣子呢？

0:49:01.660,0:49:02.980
這個結果非常的直觀

0:49:02.980,0:49:06.600
你就把原來我們根據這些 data

0:49:06.600,0:49:09.880
所算出來的 covariance matrix (Σ^1)

0:49:09.880,0:49:11.880
跟根據這些 data

0:49:11.880,0:49:14.260
所算出來的 covariance matrix，Σ^2

0:49:14.260,0:49:18.440
weighted by 他們 element 的數目

0:49:18.440,0:49:21.480
你這個 class 1 有 79 個

0:49:21.480,0:49:23.100
所以你就把 Σ^1 * 79

0:49:23.100,0:49:26.780
class 1 有 61 個

0:49:26.780,0:49:29.140
所以你就把 Σ^2 * 61，再取平均

0:49:29.140,0:49:33.300
你就把原來這兩個 Gaussian，各自算的 covariance matrix

0:49:33.300,0:49:35.360
加權平均，就會得到

0:49:35.360,0:49:38.760
如果你要求他們用共同的 Gaussian 的時候

0:49:38.760,0:49:41.200
所得到的 covariance matrix

0:49:42.860,0:49:45.040
那我們來看一下結果

0:49:45.040,0:49:48.380
假設我們仍然是用兩個 feature

0:49:48.380,0:49:52.360
用 Defense 跟 SP Defense 的話

0:49:52.360,0:49:56.840
後來我發現，我這兩個 example 選的不是很好

0:49:56.840,0:49:58.920
因為如果你看 Defense 跟 SP Defense

0:49:58.920,0:50:03.000
你是沒有辦法把水系跟一般系的神奇寶貝分開

0:50:03.000,0:50:04.460
後來我研究了一下，我覺得

0:50:04.460,0:50:08.100
好像用一般攻擊力和一般防禦力

0:50:08.100,0:50:11.440
合起來呢，就可以分的滿開的這樣子

0:50:11.440,0:50:14.400
但是，我怎麼會事先知道這件事呢？

0:50:14.400,0:50:17.220
我又不是大木博士，對不對？

0:50:17.220,0:50:23.940
那如果我們今天共用 covariance matrix 會發生甚麼事？

0:50:23.940,0:50:27.760
在沒有共用之前，class 1 跟 class 2 的 boundary

0:50:27.760,0:50:30.700
是這條，是這個曲線

0:50:30.700,0:50:33.820
如果我們今天共用同一個 covariance matrix 的話

0:50:33.820,0:50:35.460
你會發現說

0:50:35.460,0:50:40.140
他們的 boundary，變成是一個直線

0:50:40.140,0:50:43.460
假設你把這兩個不同的 class

0:50:43.460,0:50:46.500
強迫他們的 covariance matrix 必須共用同一個的話

0:50:46.500,0:50:51.080
那你今天在分類的時候，你的 boundary 就會變成是

0:50:51.080,0:50:52.940
一條直線

0:50:52.940,0:50:55.080
所以，像這樣子的 model

0:50:55.080,0:50:57.540
我們也稱之它為 linear 的 model

0:50:57.540,0:51:00.400
你可能會想說，Gaussian 甚麼的不是 linear 的阿

0:51:00.400,0:51:05.020
但是，它分兩個 class 的 boundary 是 linear 的

0:51:05.020,0:51:08.900
所以，這樣的 model，我們也稱它為 linear 的 model

0:51:09.920,0:51:14.160
如果今天兩個 class，
你用不同的 covariance matrix 的話呢

0:51:14.160,0:51:15.940
它們就不是 linear 的 model

0:51:16.280,0:51:19.800
如果，我們考慮所有的 feature 會怎麼樣呢？

0:51:19.800,0:51:21.920
如果我們考慮所有的 feature 的話

0:51:21.920,0:51:24.500
原來我們只得到 50% 正確率

0:51:24.500,0:51:27.520
但是，神奇的是，當我們共用 covariance matrix 的時候

0:51:27.520,0:51:29.760
我們就得到 79% 的正確率了

0:51:29.760,0:51:31.940
顯然是有分對東西

0:51:32.340,0:51:35.380
那你說，為甚麼會做到這樣子呢，那這就很難分析了

0:51:35.380,0:51:39.420
因為，這個是在高維空間中發生的事情

0:51:39.420,0:51:42.300
是在 7 維空間中發生的事情

0:51:42.300,0:51:46.580
我們很難知道說，這個 boundary 是怎麼切的

0:51:46.580,0:51:49.880
但是，這個就是 machine learning fancy 的地方

0:51:49.880,0:51:54.160
就是，人沒有辦法知道怎麼做

0:51:54.160,0:51:56.400
但是，machine 可以幫我們做出來

0:51:56.400,0:52:00.560
如果今天 feature 很少，人一看就知道怎麼做

0:52:00.560,0:52:03.060
那其實可以不用用上 machine learning，對不對？

0:52:03.060,0:52:06.980
所以，現在可以得到 73% 的正確率

0:52:07.980,0:52:13.680
我們來回顧一下，我們講得這個

0:52:13.680,0:52:16.000
機率的模型

0:52:16.000,0:52:19.900
那我們講說 machine learning 就是 3 個 step

0:52:19.900,0:52:23.720
那這個機率模型呢，它其實也是 3 個 step

0:52:23.720,0:52:28.520
首先，你有一個 model，
這個 model 就是你的 function set

0:52:28.960,0:52:32.580
這個 function set 裡面的 function 都長甚麼樣子呢？

0:52:32.580,0:52:36.260
這個 function set 裡面的 function 都長下面這個樣子

0:52:36.260,0:52:38.500
input 一個 x

0:52:38.500,0:52:43.980
我們有 class 1 的 required probability

0:52:43.980,0:52:46.080
class 2 的 required probability

0:52:46.080,0:52:49.040
class 1 產生 x 的 probability distribution

0:52:49.040,0:52:51.400
class 2 產生 x 的 probability distribution

0:52:51.400,0:52:56.600
這些 required probability 和 probability distribution

0:52:56.600,0:52:59.840
就是 model 的參數

0:52:59.840,0:53:03.580
你選擇不同的 probability distribution

0:53:03.580,0:53:07.560
你就得到不同的 function

0:53:07.560,0:53:10.740
那你把這些不同的 probability distribution

0:53:10.740,0:53:14.160
就像 Gaussian 你選不同的 mean 
跟不同的 covariance matrix

0:53:14.160,0:53:16.780
你就得到不同的 probability distribution

0:53:16.780,0:53:18.980
你把這些不同的 probability distribution 積分起來

0:53:18.980,0:53:21.520
就是一個 model，就是一個 function set

0:53:22.000,0:53:24.500
那怎麼決定是哪一個 class 呢？

0:53:24.500,0:53:29.820
如果 P(x|C1) 這個 posterior probability > 0.5 的話呢

0:53:29.820,0:53:33.240
就 output class 1，反之呢，就 output class 2

0:53:33.240,0:53:35.360
這個是 function 的樣子

0:53:35.360,0:53:37.620
接下來呢，我們要找

0:53:37.620,0:53:43.140
evaluate function set 裡面每一個 function 的好壞

0:53:43.140,0:53:44.700
那怎麼 evaluate 呢？

0:53:44.700,0:53:47.240
在這個機率模型裡面

0:53:47.240,0:53:49.440
假設我們今天使用 Gaussian 的話

0:53:49.440,0:53:53.560
那我們要 evaluate 的對象，
其實就是 Gaussian 裡面的參數

0:53:53.560,0:53:57.040
也就是 mean 跟 covariance matrix

0:53:57.040,0:53:59.260
那今天呢，我們就是說

0:53:59.260,0:54:02.520
如果一個 mean 跟一個 covariance matrix

0:54:02.520,0:54:07.440
你用這些參數來定義你的 probability distribution

0:54:07.440,0:54:13.180
而它可以產生我們的 training data 的 likelihood

0:54:13.180,0:54:16.100
就是這組參數的好壞

0:54:16.100,0:54:18.240
所以，我們要做的事情就是

0:54:18.240,0:54:21.180
找一個 probability distribution

0:54:21.180,0:54:26.180
它可以最大化產生這些 data 的 likelihood

0:54:26.980,0:54:29.940
這個是定義 function 的好壞

0:54:29.940,0:54:31.760
定義一組參數的好壞

0:54:31.760,0:54:34.160
最後，怎麼找出一組最好的參數呢？

0:54:34.160,0:54:37.540
你就看看前面的投影片，它的結果是很 trivial 的

0:54:38.740,0:54:41.160
那有人就會問說

0:54:41.160,0:54:45.940
為甚麼要用 Gaussian，為甚麼不選別的這樣子？

0:54:46.160,0:54:47.800
簡單的答案就是

0:54:47.800,0:54:52.520
如果我選了別的機率模型，你也會問我同樣的問題

0:54:52.520,0:54:57.640
其實你永遠可以選一個，你自己喜歡的

0:54:57.640,0:54:59.800
這個 probability distribution

0:54:59.800,0:55:02.520
這個是你自己決定的

0:55:02.520,0:55:05.440
這個不是人工智慧

0:55:05.440,0:55:10.440
是你人的智慧，去決定說你要選哪一個人的模型

0:55:10.440,0:55:12.100
是比較適合的

0:55:12.100,0:55:16.060
那你選擇比較簡單的機率模型，參數比較少的

0:55:16.060,0:55:18.520
那你的 bias 就大、variance 就小

0:55:18.520,0:55:21.160
那你選擇複雜的，你 bias 就小、variance 就大

0:55:21.160,0:55:23.480
那你可能就要用 data set 決定一下

0:55:23.480,0:55:25.700
你要用怎麼樣的機率模型，是比較好的

0:55:26.060,0:55:30.980
那我們有另外一種常見的假設是這樣

0:55:30.980,0:55:33.420
假設我們的這個 x

0:55:33.420,0:55:36.720
我們知道 x 是由一組 feature 來描述它的

0:55:36.720,0:55:41.120
那剛才在寶可夢的例子裡面，x 可以有 7 個數值

0:55:41.120,0:55:42.600
7 個參數

0:55:42.600,0:55:47.040
那我們假設，每一個 dimension

0:55:47.040,0:55:53.800
它從機率模型，產生出來的機率是 independent 的

0:55:53.800,0:55:57.160
所以，這個 x 產生的機率

0:55:57.160,0:56:00.340
可以拆解成，x1 產生的機率

0:56:00.340,0:56:03.540
乘上 x2 產生的機率，乘上 xk 產生的機率

0:56:03.540,0:56:07.240
一直到乘上 xK 產生的機率

0:56:07.240,0:56:10.840
如果我們假設，這些機率分布是 independent 的話

0:56:10.840,0:56:13.260
每一個 dimension 分布是 independent 的話

0:56:13.260,0:56:16.060
我們可以做這樣子的假設

0:56:16.700,0:56:22.080
那今天你可以說，每一個機率

0:56:22.080,0:56:25.640
就是 x1 產生的機率、x2 產生的機率， xk 產生的機率

0:56:25.640,0:56:29.960
他們分別都是一維的 Gaussian

0:56:29.960,0:56:31.800
一維的 Gaussian，大家知道意思嗎？

0:56:31.800,0:56:33.960
如果你這樣假設的話，等於是說

0:56:35.060,0:56:39.440
我們之前討論的都是 multi-variable 的 Gaussian 嘛

0:56:39.440,0:56:41.240
都是多維度的 Gaussian

0:56:41.240,0:56:45.640
如果你假設說，每一個 dimension 分開的 model

0:56:45.640,0:56:48.120
他們都是一維的 Gaussian 的話，意思就是說

0:56:48.120,0:56:50.120
原來那個高維度的 Gaussian，它的 covariance matrix

0:56:50.120,0:56:54.000
它的 covariance matrix，變成是 diagonal

0:56:54.000,0:56:56.860
在不是對角線的地方，值都是 0

0:56:56.860,0:56:58.180
只有對角線的地方，有值

0:56:58.180,0:57:02.640
這樣你就可以更減少你的參數量

0:57:02.640,0:57:04.660
你就可以得到一個更簡單的模型

0:57:04.660,0:57:07.380
那如果試一下這個，試一下這個結果是壞的

0:57:07.380,0:57:09.540
所以看來這個模型太簡單了

0:57:09.540,0:57:14.660
model 不同的 feature 間的 covariance

0:57:14.660,0:57:15.780
我看也是必要的

0:57:15.780,0:57:19.680
我覺得，比如說，像是戰鬥跟防禦力是有正相關的

0:57:19.680,0:57:23.020
他們這個 model 之間的 covariance，看來還是必要的

0:57:23.020,0:57:25.440
那你也不一定要用 Gaussian

0:57:25.440,0:57:28.500
有很多時候你憑直覺就知道應該用 Gaussian

0:57:28.500,0:57:33.020
比如說，今天假設你有某個 feature，它是 binary 的

0:57:33.020,0:57:37.520
有某個 feature，它代表的是：是或不是

0:57:37.520,0:57:40.480
或是它的 output 就是 0 跟 1 這樣

0:57:40.480,0:57:45.340
比如說，有一隻寶可夢，它是神獸還是不是神獸

0:57:45.340,0:57:48.820
之類的，這個就是 binary 的 feature

0:57:48.820,0:57:50.340
如果是 binary 的 feature 的話

0:57:50.340,0:57:56.180
你說它是用 Gaussian distribution 產生的

0:57:56.180,0:57:57.700
就太自欺欺人了

0:57:57.740,0:58:00.620
所以，它應該不太可能是用 Gaussian 所產生的

0:58:00.620,0:58:02.440
這個時候，你就會假設別的 distribution

0:58:02.440,0:58:04.840
比如說，假設你的 feature 是 binary 的

0:58:04.840,0:58:07.340
它 output，要馬是 0，要馬是 1

0:58:07.340,0:58:11.440
這個時候，你可能就會選擇說，
它是一個 Bernoulli distribution

0:58:11.440,0:58:13.960
而不是一個 Gaussian distribution

0:58:14.520,0:58:19.480
如果我們今天假設所有的 feature

0:58:19.480,0:58:22.900
它都是 independent 產生的

0:58:22.900,0:58:26.580
我們不 model feature 和 feature 間 covariance 的關係

0:58:26.580,0:58:30.260
那我們用這種方法做分類的話

0:58:30.260,0:58:34.400
我們叫做用 Naive Bayes Classifier

0:58:34.400,0:58:38.960
它前面有一個 Naive，因為它真的很 naive

0:58:38.960,0:58:40.580
它真的很簡單，這樣

0:58:40.580,0:58:44.860
那你可能會常聽到，有人說 Naive Bayes Classifier 很強

0:58:44.860,0:58:50.360
其實它強不強是 depend on 你的假設是不是精準的

0:58:50.360,0:58:55.660
如果你今天假設不同的 dimension 之間是 independent

0:58:55.660,0:58:57.800
這件事情是很切合實際的

0:58:57.800,0:59:02.620
那 Naive Bayes Classifier 確實可以
給你提供很好的 performance

0:59:02.620,0:59:05.500
那如果這個假設是很不成立的話

0:59:05.500,0:59:09.180
那 Naive Bayes Classifier 它的 bias 就太大了

0:59:09.180,0:59:11.040
它就不是一個好的 Classifier

0:59:11.960,0:59:14.320
接下來呢，我們要做的分析是

0:59:14.320,0:59:17.520
我們要分析這項 Posterior Probability

0:59:17.520,0:59:22.660
我們在做一些整理以後，我們會發現一些有趣的現象

0:59:23.460,0:59:26.560
這一項呢，大家應該都沒有甚麼問題

0:59:26.560,0:59:29.920
把他們上下同除分子

0:59:29.920,0:59:33.060
上下同除分子，我們把他們上下

0:59:33.060,0:59:37.040
都同除 P(x|C1)*P(C1)

0:59:37.040,0:59:38.940
所以，分子的地方就變成 1

0:59:38.940,0:59:41.080
分母的地方就變成 1 加

0:59:41.080,0:59:46.920
[P(x|C2) * P(C2)] / [P(x|C1) * P(C1)]

0:59:47.620,0:59:50.740
那我們假設，這一項阿

0:59:50.740,0:59:56.680
這一項取 natural log 以後阿

0:59:56.680,0:59:57.900
它等於 z

0:59:57.900,0:59:59.900
我們假設這一項取 natural log 以後，它等於 z

1:00:00.840,1:00:04.600
那我們就可以把這個 Posterior Probability

1:00:04.600,1:00:08.900
1 / (1 + exp(-z))

1:00:08.900,1:00:13.280
這個 z 是這一項

1:00:13.280,1:00:15.320
那你把這一項放進去

1:00:15.320,1:00:18.780
乘負號，就是上下顛倒，再取 exponential

1:00:18.780,1:00:23.000
把 exponential 跟 natural log 抵銷，你就得到這一項

1:00:23.000,1:00:25.240
然後你就得到 Posterior Probability

1:00:25.240,1:00:27.440
相信這個，大家應該沒有甚麼問題

1:00:27.440,1:00:30.760
這個 function，它的 input 是 z

1:00:30.760,1:00:34.760
這個 function，叫做 sigmoid function

1:00:34.760,1:00:38.880
如果你把它 output 對 z 的關係作圖的話

1:00:38.880,1:00:41.420
你就會發現是這個樣子

1:00:41.420,1:00:44.400
也就是 z 趨近無窮大的時候

1:00:44.400,1:00:46.000
它的 output 就趨近於 1

1:00:46.000,1:00:48.100
z 趨近負無窮大的時候

1:00:48.100,1:00:50.580
它的 output 就趨近於 0

1:00:51.140,1:00:53.740
接下來，我們要做的事情是

1:00:53.740,1:00:57.240
我們要把這個 z 算一下

1:00:57.240,1:01:01.280
它到底應該長甚麼樣子

1:01:01.280,1:01:03.860
我們來算一下，這個 z 應該長甚麼樣子

1:01:03.860,1:01:06.520
接下來這邊呢，是數學比較多

1:01:06.520,1:01:08.460
如果你覺得這很無聊的話

1:01:08.460,1:01:11.380
你就睡一下，聽一下結論就好

1:01:12.180,1:01:14.960
那這個 z 應該長甚麼樣子呢？

1:01:14.960,1:01:17.460
我們已經知道這個 Posterior probability

1:01:17.460,1:01:19.340
它是一個 z 的 sigmoid function

1:01:19.360,1:01:21.960
z 長甚麼樣子？

1:01:21.960,1:01:27.160
我們把相乘的部分，取 ln，所以就變成相加

1:01:27.160,1:01:31.240
那 P(C1) / P(C2) 是甚麼呢？

1:01:31.240,1:01:36.380
我們都知道說，這邊 N1 代表 Class 1

1:01:36.380,1:01:39.020
它在 training data 裡面出現的數目

1:01:39.020,1:01:43.040
N2 代表 Class 2，它在 training data 裡面出現的次數

1:01:43.040,1:01:46.140
所以，P(C1) 就是 N1 / (N1 + N2)

1:01:46.140,1:01:48.000
P(C2) 就是 N2 / (N1 + N2)

1:01:48.000,1:01:51.400
分母的地方消掉，所以得到 N1/N2

1:01:51.400,1:01:53.700
這個是小學生的數學

1:01:53.700,1:01:57.240
那 P(x|C1) 是甚麼呢？

1:01:57.240,1:02:00.120
我們說它是一個 probability distribution

1:02:00.660,1:02:03.140
這個 Gaussian 的 distribution

1:02:03.140,1:02:07.860
P(x|C1) 是另一個 Gaussian 的 distribution

1:02:08.540,1:02:10.980
如果我們把它相除

1:02:10.980,1:02:15.000
我們把這兩個 Gaussian probability 相除，再取 ln

1:02:15.000,1:02:16.420
會得到甚麼式子呢？

1:02:16.420,1:02:21.620
就得到這樣子，把這個放在上面，把這個放在下面

1:02:21.620,1:02:26.000
那這一項跟 distribution 是沒關係的

1:02:26.000,1:02:28.800
就把它消掉

1:02:28.800,1:02:32.560
然後，這一項把它提出來

1:02:32.560,1:02:36.240
相乘變相加，把這一項提出來

1:02:36.700,1:02:38.760
然後這個 exp 的部分呢

1:02:38.760,1:02:41.840
相除等於 exp 裡面的相減

1:02:41.840,1:02:44.660
這個，也沒什麼特別的

1:02:44.660,1:02:48.520
把他們分開相乘，變相加

1:02:48.520,1:02:52.020
相乘變相加，得到這樣

1:02:53.380,1:02:56.500
那接下來呢？接下來你就

1:02:56.500,1:03:00.440
做一些運算，把它展開

1:03:00.440,1:03:04.040
你可能想要知道說 (x - μ1)^T

1:03:04.040,1:03:07.680
乘上 (Σ1)^(-1)，再乘上 (x - μ1)

1:03:07.680,1:03:09.280
它應該長甚麼樣子

1:03:09.280,1:03:12.160
把它展開

1:03:12.160,1:03:15.780
所以，這個乘這個乘這個，就得到它

1:03:15.780,1:03:19.520
這個乘這個乘這個，就得到它

1:03:19.520,1:03:23.000
這個乘這個乘這個，就得到它，這樣

1:03:23.000,1:03:26.360
那中間這兩項呢，是可以合併的

1:03:26.360,1:03:28.640
是可以合併的，他們其實是一樣的

1:03:28.640,1:03:31.140
那你就得到這樣子式子

1:03:31.140,1:03:34.680
所以，這一項展開，就變成這樣

1:03:34.680,1:03:37.260
那這一項展開呢？

1:03:38.220,1:03:40.600
這一項展開呢？因為這個跟這個

1:03:40.600,1:03:43.700
只差了一個，把 1 換成 2 嘛

1:03:43.700,1:03:47.540
所以你就把，下面這個式子的 1 都換成 2

1:03:47.540,1:03:49.720
就行了

1:03:50.100,1:03:54.920
所以，z 這一項呢，它寫成這樣

1:03:54.920,1:04:00.500
前面是跟 Σ1 和 Σ2 有關

1:04:00.500,1:04:05.420
前面是 Σ1 和 Σ2 的 determinant 相除

1:04:06.140,1:04:10.880
後面呢，把 -1/2 乘進去

1:04:10.880,1:04:15.340
把 -1/2 乘這項，你得到這一項

1:04:16.080,1:04:21.120
把 -1/2 乘進去，把 -1/2 乘這項，你得到這一項

1:04:21.120,1:04:25.360
最後呢，再加上這個機率

1:04:25.360,1:04:27.800
你就知道 z 是多少了

1:04:27.800,1:04:31.180
那如果你剛才沒有聽得很懂，沒有關係

1:04:31.180,1:04:32.960
那其實沒有特別重要

1:04:32.960,1:04:35.700
反正就是經過一番運算以後，我們知道 z

1:04:35.700,1:04:38.460
哇！長得很複雜，長這個樣子

1:04:39.040,1:04:40.800
但是，我們剛才有說過呢

1:04:40.800,1:04:44.360
一般我們會假設，covariance matrix 是共用的

1:04:44.360,1:04:49.320
所以，Σ1 = Σ2 = Σ

1:04:49.820,1:04:53.220
在這個情況下，我們就可以簡化上面這個式子

1:04:53.220,1:04:55.580
我們就可以簡化成

1:04:55.580,1:04:58.680
Σ2 的 determinant 除以 Σ1的 determinant

1:04:58.680,1:05:01.980
如果 Σ1 = Σ2 的話，它就可以被消掉

1:05:01.980,1:05:07.140
這邊有一項， -1/2 * x^T * (Σ1)^-(1) * x

1:05:07.140,1:05:11.520
這邊有一項， 1/2 * x^T * (Σ2)^-(1) * x

1:05:11.520,1:05:15.580
如果 Σ1 跟 Σ2 是一樣的話呢，它們也可以被消掉

1:05:15.580,1:05:18.580
所以，我們最後得到的結果呢，就只剩下

1:05:18.580,1:05:23.360
1, 2, 3, 4, 5，5 項

1:05:23.500,1:05:26.580
然後，你會發現說

1:05:26.580,1:05:31.280
只有這一項跟這一項，是跟 x 有關的

1:05:31.280,1:05:32.820
是跟 x 有關的

1:05:32.820,1:05:35.820
這三項，是跟 x 無關的

1:05:35.820,1:05:37.900
這兩項，最後都有乘上 x

1:05:37.900,1:05:42.620
所以，先把這兩項集合起來

1:05:40.520,1:05:45.440
這兩項集合起來把它的 x 提出來

1:05:45.440,1:05:51.280
所以這兩項集合起來，就變成呢

1:05:51.280,1:05:55.240
因為  Σ1 = Σ2 ，都有乘上 x

1:05:55.240,1:05:59.060
所以 Σ 跟 x 可以提出來，變成  Σ^(-1) * x

1:05:59.060,1:06:02.940
這邊有 μ1 的 transpose，跟 μ2 的 transpose

1:06:02.940,1:06:04.580
這邊是相減，所以

1:06:04.580,1:06:10.840
這項跟這項合起來，
就變成 (μ1 - μ2) 的 transpose * Σ^(-1) * x

1:06:10.840,1:06:13.820
剩下這三項，就把它原封不動地擺在後面

1:06:14.460,1:06:17.000
接下來呢，我們假設說

1:06:17.920,1:06:24.220
這個東西，(μ1 - μ2)^T *  Σ^(-1)

1:06:24.220,1:06:27.120
它合起來就是一個 vector

1:06:27.120,1:06:30.780
假設你把 μ1 算出來，把 μ2 算出來，把 Σ 算出來

1:06:30.780,1:06:34.800
那你再代到這個式子裡面，把 Σ 做 inverse

1:06:34.800,1:06:39.140
把 (μ1 - μ2) 做 transpose，那你就得到一個 vector

1:06:39.140,1:06:43.080
把那個 vector 叫做 W^T

1:06:43.080,1:06:46.700
後面這項，你可覺得看起來很可怕

1:06:46.700,1:06:48.820
但它其實很簡單

1:06:48.820,1:06:51.400
因為，我們從這邊開始看

1:06:51.400,1:06:55.540
這是一個 vector，這是一個 matrix

1:06:55.540,1:06:59.060
這是一個 vector，把他們三個乘起來以後

1:06:59.060,1:07:01.100
你得到的其實就是一個 scalar

1:07:01.660,1:07:04.540
那它其實不是甚麼複雜的東西，它是一個數字

1:07:04.540,1:07:08.780
你把這一項乘這一項乘這一項

1:07:08.780,1:07:13.840
你把 vector 的 transpose 乘上 matrix 的 inverse

1:07:13.840,1:07:16.260
再乘上一個 vector，它也是一個 scalar

1:07:16.260,1:07:19.740
那這個 ln (N1/N2)，它也是一個 scalar

1:07:19.740,1:07:23.640
所以你只是把這 3 個數字加起來而已，它就是個數字

1:07:23.640,1:07:28.060
所以我們就拿 b 來代表這個看起來很複雜的數字

1:07:28.060,1:07:33.460
假設你知道 μ1,  μ2 跟 Σ，那這一項其實就是個 vector

1:07:33.460,1:07:37.400
這一項其實就是個 scalar

1:07:37.400,1:07:39.660
它就是一個數字而已

1:07:39.660,1:07:42.180
所以呢，我們知道說

1:07:42.180,1:07:44.700
我們可以把 posterior probability

1:07:44.700,1:07:46.800
這項機率呢，寫成 σ(z)

1:07:46.800,1:07:48.980
z 呢，又可以寫成這樣子

1:07:48.980,1:07:53.300
所以，我們其實可以把這個 posterior probability

1:07:53.300,1:07:55.460
就簡單寫成 σ(w * x + b)

1:07:55.460,1:08:04.100
w 跟 x 的 inner product，再加上一個常數 b

1:08:04.100,1:08:08.780
我們可以把 z 寫成 w 跟 x 的 inner product

1:08:08.780,1:08:10.800
再加上一個常數 b

1:08:10.800,1:08:13.760
其實這個 posterior probability

1:08:13.760,1:08:15.920
它根本就沒有這麼複雜

1:08:15.920,1:08:19.120
它寫起來呢，就是這個樣子

1:08:19.120,1:08:21.600
所以，從這個式子，你就可以看出來說

1:08:21.600,1:08:27.160
為甚麼我們今天把 Σ1 跟 Σ2 共用的時候

1:08:27.160,1:08:29.800
假設 Σ1 必須等於 Σ2 的時候

1:08:29.800,1:08:34.940
你的 class 1 跟 class 2 的 boundary 會是 linear

1:08:34.940,1:08:38.340
你從這個式子呢，就可以很明顯地看出這件事

1:08:39.220,1:08:42.640
那再 generative model 裡面

1:08:42.640,1:08:47.140
我們做的事情是，我們用某些方法

1:08:47.140,1:08:50.360
去找出上面這個式子裡面的

1:08:50.360,1:08:52.740
N1, N2, μ1, μ2, Σ

1:08:52.740,1:08:56.360
找出這些以後，你就算出 w，你就算出 b

1:08:56.360,1:08:59.860
你把它代進這個式子，你就可以算機率

1:09:01.100,1:09:03.860
但是，如果你看到這個式子的話

1:09:03.860,1:09:05.920
你可能就可以有一個直覺的想法

1:09:05.920,1:09:07.980
為甚麼要這麼麻煩呢？

1:09:07.980,1:09:11.820
假設我們最終的目標，都是要找一個 vector w

1:09:11.820,1:09:14.040
都是要找一個 constant b

1:09:14.040,1:09:17.640
我們何必先去搞個機率

1:09:17.640,1:09:21.480
算出一些 μ, Σ 甚麼的

1:09:21.480,1:09:25.380
然後再把它搞起來，再得到 w 跟 b

1:09:25.380,1:09:27.480
這不是捨近求遠嗎？

1:09:27.480,1:09:29.500
做一件你根本就不需要做的事

1:09:29.500,1:09:31.820
最後你只需要 w 跟 b 嘛

1:09:31.820,1:09:35.560
所以，我們能不能夠直接把 w 跟 b 找出來

1:09:35.560,1:09:37.640
這個呢，就是我們下一份投影片要講的東西

1:09:37.640,1:09:40.300
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

