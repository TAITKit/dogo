<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:07.060<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:07.860,0:00:15.280<br>
這個是我們上一次做失敗的 code 這樣子<br>
<br>
0:00:15.740,0:00:20.480<br>
我們疊了一些，我們要做手寫數字辨識<br>
<br>
0:00:20.485,0:00:23.535<br>
那我們疊的 network 呢，它的這個<br>
<br>
0:00:23.535,0:00:26.335<br>
hidden layer 的 size，就是 689<br>
<br>
0:00:26.340,0:00:28.080<br>
用 sigmoid function<br>
<br>
0:00:28.080,0:00:31.160<br>
我本來要疊 10 層，疊 10 層其實也不 work<br>
<br>
0:00:31.160,0:00:32.660<br>
就把它註解掉了<br>
<br>
0:00:33.360,0:00:38.360<br>
那用的 loss function 是 MSE，然後用了 SGD<br>
<br>
0:00:38.420,0:00:41.340<br>
等等，那結果是差的<br>
<br>
0:00:41.480,0:00:42.320<br>
怎麼辦？<br>
<br>
0:00:42.320,0:00:44.840<br>
你自己在 train network 的時候才會遇到這個問題<br>
<br>
0:00:44.840,0:00:47.275<br>
很多個 network train 下去，結果是差的<br>
<br>
0:00:47.280,0:00:49.960<br>
這個時候你就會去問老師說，怎麼辦？<br>
<br>
0:00:49.960,0:00:53.280<br>
每次有人問我這個問題的時候，我第一個會問你的就是<br>
<br>
0:00:53.280,0:00:55.240<br>
你在 training set 上<br>
<br>
0:00:55.240,0:00:57.620<br>
得到多少的 performance<br>
<br>
0:00:57.620,0:00:58.640<br>
你可能會問說<br>
<br>
0:00:58.640,0:01:01.420<br>
你怎麼不是問 testing set 上得到多少的 performance<br>
<br>
0:01:01.420,0:01:04.580<br>
而是問 training set 上，得到多少的 performance<br>
<br>
0:01:04.580,0:01:06.920<br>
因為如果你有看<br>
<br>
0:01:07.000,0:01:09.460<br>
錄影的話，你會知道說<br>
<br>
0:01:09.460,0:01:11.740<br>
今天 deep learning 在 training 的時候<br>
<br>
0:01:11.740,0:01:13.780<br>
你非常容易 train 壞掉<br>
<br>
0:01:13.780,0:01:16.720<br>
它跟其他方法不一樣，它跟 SVM 不一樣<br>
<br>
0:01:16.720,0:01:19.120<br>
SVM 是解一個 convex optimization problem<br>
<br>
0:01:19.125,0:01:20.980<br>
所以，你每次找的時候<br>
<br>
0:01:20.980,0:01:23.560<br>
它都可以找到一個 optimal 的 solution<br>
<br>
0:01:23.560,0:01:25.165<br>
也就是當你用 SVM 的時候<br>
<br>
0:01:25.165,0:01:26.860<br>
SVM 會竭盡全力<br>
<br>
0:01:26.860,0:01:29.000<br>
給你它可以得到最好的結果<br>
<br>
0:01:29.100,0:01:30.880<br>
那 deep learning 不是這樣的方法<br>
<br>
0:01:30.880,0:01:32.320<br>
它雖然很 powerful<br>
<br>
0:01:32.320,0:01:35.620<br>
但它其實就跟噴火龍一樣，你不見得能夠叫得動它<br>
<br>
0:01:35.620,0:01:38.800<br>
所以，你需要看一下你的 training set<br>
<br>
0:01:38.860,0:01:41.820<br>
來看看說你到底有沒有把它<br>
<br>
0:01:41.820,0:01:44.540<br>
的能力做起來<br>
<br>
0:01:44.540,0:01:47.340<br>
那你可能會想說，它在 training set 上 performance 好<br>
<br>
0:01:47.380,0:01:49.380<br>
可能只是 overfitting 阿<br>
<br>
0:01:49.380,0:01:51.220<br>
沒錯，它可能只是 overfitting <br>
<br>
0:01:51.460,0:01:54.975<br>
但是，如果你連在 training set 上 overfitting 都做不到<br>
<br>
0:01:54.980,0:01:59.740<br>
你更遑論去做在 testing set 上舉一反三了<br>
<br>
0:01:59.740,0:02:01.300<br>
所以，我們先讓它至少<br>
<br>
0:02:01.300,0:02:03.620<br>
在 training set 上得到好的結果<br>
<br>
0:02:03.620,0:02:05.860<br>
那 training set 上得到好的結果有可能<br>
<br>
0:02:05.860,0:02:07.560<br>
可以舉一反三到 testing set 上<br>
<br>
0:02:07.560,0:02:09.180<br>
也有可能 overfitting<br>
<br>
0:02:09.180,0:02:11.040<br>
我們不知道，但是<br>
<br>
0:02:11.040,0:02:13.320<br>
如果你在 training set 上都沒有好的結果<br>
<br>
0:02:13.320,0:02:15.580<br>
那你其實在 testing set 上<br>
<br>
0:02:15.660,0:02:18.320<br>
可以得到好的結果的機會是微乎其微的<br>
<br>
0:02:18.320,0:02:20.180<br>
所以，如果你在 testing set 上結果不好<br>
<br>
0:02:20.180,0:02:22.020<br>
你應該先看看你的 training set<br>
<br>
0:02:22.020,0:02:24.675<br>
結果是怎麼樣，然後才看說<br>
<br>
0:02:24.675,0:02:26.860<br>
現在是不是 overfitting<br>
<br>
0:02:26.860,0:02:29.520<br>
所以，我們來看一下在 training set 上的結果吧<br>
<br>
0:02:29.520,0:02:31.240<br>
其實，Keras 在訓練的過程中<br>
<br>
0:02:31.240,0:02:33.960<br>
就已經會告訴你它在 training set 上得到的 performance<br>
<br>
0:02:33.960,0:02:36.520<br>
不過，我們今天特別再把 training set 的結果<br>
<br>
0:02:36.520,0:02:39.720<br>
再 print 出來<br>
<br>
0:02:46.140,0:02:49.195<br>
那我們來 print 一下 training set 的結果<br>
<br>
0:02:49.200,0:02:51.000<br>
只要把 x_test 改成 x_train<br>
<br>
0:02:51.080,0:02:54.000<br>
y 改成 y_train 就好<br>
<br>
0:02:56.580,0:02:59.540<br>
我們實際上來跑一下<br>
<br>
0:03:04.540,0:03:06.780<br>
那其實在 training 的過程中<br>
<br>
0:03:06.940,0:03:08.740<br>
Keras 就會告訴你說<br>
<br>
0:03:08.740,0:03:11.320<br>
它現在算出來的 accuracy 是多少<br>
<br>
0:03:11.320,0:03:13.000<br>
在每一個 η 後面<br>
<br>
0:03:13.000,0:03:14.380<br>
它都會告訴你說<br>
<br>
0:03:14.380,0:03:17.640<br>
這個 η 結束的時候，它算出來的 accuracy 是多少<br>
<br>
0:03:17.680,0:03:21.475<br>
那如果我們今天看這個實驗結果，你就會發現說<br>
<br>
0:03:21.480,0:03:24.800<br>
其實，啊！我這邊忘記把 test 改成 train<br>
<br>
0:03:24.860,0:03:26.480<br>
不過大家知道我的意思就好<br>
<br>
0:03:26.480,0:03:30.880<br>
上面這個 accuracy 是 training set 上的 accuracy<br>
<br>
0:03:30.880,0:03:33.960<br>
下面這個 accuracy 是 testing set 上的 accuracy<br>
<br>
0:03:34.020,0:03:36.040<br>
如果你只有看 testing set 上的 accuracy<br>
<br>
0:03:36.040,0:03:38.440<br>
你並不知道你現在是不是 overfitting<br>
<br>
0:03:38.500,0:03:40.780<br>
有人看到 testing set 上的 accuracy 就會說<br>
<br>
0:03:40.780,0:03:42.820<br>
看 testing set 上的 accuracy，它 performance 很差<br>
<br>
0:03:42.820,0:03:44.820<br>
就會胡亂得到一個結論說<br>
<br>
0:03:44.820,0:03:46.560<br>
所以，deep learning 很容易 overfitting<br>
<br>
0:03:46.560,0:03:48.060<br>
所以，deep learning 很不 work<br>
<br>
0:03:48.060,0:03:48.840<br>
那其實不是這樣<br>
<br>
0:03:48.840,0:03:50.080<br>
今天在這個 task 裡面<br>
<br>
0:03:50.080,0:03:52.500<br>
如果我們看 training set 的 accuracy 的話<br>
<br>
0:03:52.500,0:03:55.460<br>
你會發現 training set 的 accuracy 也是差的<br>
<br>
0:03:55.460,0:03:58.260<br>
這告訴我們什麼，這告訴我們 network 在 train 的時候<br>
<br>
0:03:58.260,0:04:00.340<br>
它就沒 train 好<br>
<br>
0:04:00.340,0:04:02.400<br>
它可能卡在一個 local minimum<br>
<br>
0:04:02.400,0:04:04.420<br>
它可能卡在一個很差的 saddle point<br>
<br>
0:04:04.420,0:04:06.940<br>
總之，它在 training set 上的 performance 就沒做好<br>
<br>
0:04:06.940,0:04:07.820<br>
所以，這個時候<br>
<br>
0:04:07.820,0:04:09.160<br>
你遇到的問題並不是overfitting<br>
<br>
0:04:09.160,0:04:10.520<br>
而是 training 沒有 train 好<br>
<br>
0:04:10.520,0:04:12.920<br>
你要想個辦法，先在 training set 上<br>
<br>
0:04:12.920,0:04:14.620<br>
得到比較好的 performance<br>
<br>
0:04:14.620,0:04:16.660<br>
那把這個 test 改成 train<br>
<br>
0:04:20.100,0:04:24.020<br>
那這邊到底少了甚麼東西呢？<br>
<br>
0:04:24.020,0:04:27.480<br>
其實這邊少的是 loss function<br>
<br>
0:04:27.480,0:04:31.080<br>
設得不對，其實我們已經有跟大家解釋過說<br>
<br>
0:04:31.080,0:04:33.820<br>
其實，用 mean square error 看分類的問題<br>
<br>
0:04:33.820,0:04:35.560<br>
你其實不會得到好的結果<br>
<br>
0:04:35.560,0:04:37.220<br>
我們在講 Logistic Regression 的時候<br>
<br>
0:04:37.220,0:04:38.340<br>
已經講過這件事了<br>
<br>
0:04:38.340,0:04:40.140<br>
我們現在實際來示範一下<br>
<br>
0:04:43.460,0:04:47.600<br>
我們就只是單純地把 mean square error 換成<br>
<br>
0:04:48.060,0:04:50.100<br>
這個 cross entropy<br>
<br>
0:04:52.120,0:04:54.880<br>
在 Keras 裡面，categorical 的 cross entropy<br>
<br>
0:04:54.880,0:04:57.200<br>
就是我們上課講的 cross entropy 啦<br>
<br>
0:04:57.200,0:04:59.760<br>
那從 mean square error 換成 cross entropy<br>
<br>
0:04:59.760,0:05:02.360<br>
這個你可不會覺得有甚麼特別厲害的地方<br>
<br>
0:05:02.360,0:05:05.120<br>
paper 也不會跟你 emphasize 這件事<br>
<br>
0:05:05.125,0:05:07.415<br>
但是，我們看看它有甚麼樣的差別<br>
<br>
0:05:10.305,0:05:12.215<br>
那我們剛才是做不起來的<br>
<br>
0:05:17.740,0:05:19.180<br>
我們來看一下，你看<br>
<br>
0:05:19.360,0:05:21.740<br>
當我們換成 cross entropy 以後<br>
<br>
0:05:21.740,0:05:24.100<br>
在 training set 上的 accuracy<br>
<br>
0:05:24.100,0:05:25.240<br>
就起飛了<br>
<br>
0:05:25.240,0:05:26.720<br>
現在 training set 就得到<br>
<br>
0:05:26.720,0:05:29.320<br>
87% 的正確率阿<br>
<br>
0:05:31.100,0:05:34.760<br>
這其實是個巧合，我沒有辦法特別設成這個結果<br>
<br>
0:05:35.220,0:05:38.340<br>
testing set 上得到 85% 的正確率，所以現在就<br>
<br>
0:05:38.340,0:05:40.560<br>
比較有 train 起來了<br>
<br>
0:05:44.320,0:05:47.660<br>
現在試一下，batch_size 會對結果造成的影響<br>
<br>
0:05:47.660,0:05:51.280<br>
現在，你看我們 batch_size 設 100<br>
<br>
0:05:51.280,0:05:55.740<br>
那我們現在呢，把 batch_size 改成 10000<br>
<br>
0:05:56.320,0:05:59.740<br>
把 batch_size 改成 10000，再跑跑看<br>
<br>
0:06:00.480,0:06:02.680<br>
剛才我們可以得到<br>
<br>
0:06:02.760,0:06:04.700<br>
training set 上 87%<br>
<br>
0:06:04.700,0:06:06.980<br>
testing set 上 85% 的正確率<br>
<br>
0:06:06.980,0:06:08.920<br>
你看 batch_size 設 10000<br>
<br>
0:06:08.920,0:06:11.460<br>
跑超快，因為你是用 GPU 平行運算<br>
<br>
0:06:11.460,0:06:14.280<br>
所以，在 GPU 可以平行運算的能力<br>
<br>
0:06:14.395,0:06:16.220<br>
它可以承受的前提之下<br>
<br>
0:06:16.220,0:06:18.300<br>
batch_size 越大，它其實跑得越快<br>
<br>
0:06:18.300,0:06:19.740<br>
但是，batch_size 一開大<br>
<br>
0:06:20.225,0:06:21.225<br>
performance<br>
<br>
0:06:21.580,0:06:23.700<br>
開太大的時候，performance 就壞掉了<br>
<br>
0:06:23.700,0:06:25.940<br>
那至於為什麼，我們上課有解釋過了<br>
<br>
0:06:25.940,0:06:29.360<br>
就會發現說，一樣的 network 架構，batch_size 一開大<br>
<br>
0:06:29.360,0:06:30.300<br>
結果就壞掉了<br>
<br>
0:06:30.300,0:06:32.200<br>
那我們試著把 batch_size 弄小一點<br>
<br>
0:06:32.200,0:06:35.040<br>
剛才是從 100 改到 10000<br>
<br>
0:06:35.040,0:06:37.840<br>
現在改回 1，現在改成 1<br>
<br>
0:06:38.180,0:06:40.300<br>
那改成 1 你會發現怎麼樣呢？<br>
<br>
0:06:40.440,0:06:42.300<br>
今天如果 batch_size 只有 1 的時候<br>
<br>
0:06:42.300,0:06:45.260<br>
GPU 就沒有辦法發揮它平行運算的效能<br>
<br>
0:06:45.260,0:06:47.680<br>
就會發現說，變得很慢這樣子<br>
<br>
0:06:47.680,0:06:50.980<br>
變得很慢，所以，有人他不知道說<br>
<br>
0:06:50.980,0:06:53.860<br>
你要能夠用 GPU 加速<br>
<br>
0:06:53.860,0:06:58.420<br>
前提是你在 training 的時候，batch 開大一點<br>
<br>
0:06:58.420,0:06:59.720<br>
GPU 才能夠真的加速<br>
<br>
0:06:59.720,0:07:01.920<br>
如果，你今天 batch_size 設 1<br>
<br>
0:07:01.920,0:07:03.600<br>
你就做 Stochastic Gradient Descent<br>
<br>
0:07:03.600,0:07:05.840<br>
GPU 可以對你帶來的幫助<br>
<br>
0:07:05.840,0:07:08.200<br>
其實就不會很大，所以你看現在<br>
<br>
0:07:08.200,0:07:10.840<br>
跑得非常非常慢，每一個 η 要 20 秒<br>
<br>
0:07:10.840,0:07:13.500<br>
我相信大家應該不會有興趣看我把它跑完<br>
<br>
0:07:13.500,0:07:14.920<br>
所以，我們就把它停下來<br>
<br>
0:07:18.540,0:07:20.260<br>
有些人可能想說<br>
<br>
0:07:20.260,0:07:22.140<br>
那就應該要用 deep 了吧<br>
<br>
0:07:22.760,0:07:24.900<br>
再加 10 層<br>
<br>
0:07:30.200,0:07:32.920<br>
現在改成用 10 層<br>
<br>
0:07:35.520,0:07:38.220<br>
我們看一下 testing 的 accuracy<br>
<br>
0:07:38.220,0:07:39.840<br>
先來看一下 training 的 accuracy<br>
<br>
0:07:39.840,0:07:41.580<br>
看一下 training 的 accuracy，就會發現說<br>
<br>
0:07:41.580,0:07:42.860<br>
沒做起來<br>
<br>
0:07:42.860,0:07:45.200<br>
卡住了<br>
<br>
0:07:45.200,0:07:48.860<br>
那我們在錄影裡面有解釋過為甚麼會這樣<br>
<br>
0:07:48.860,0:07:50.280<br>
疊 10 層的時候<br>
<br>
0:07:50.285,0:07:52.380<br>
會有 gradient vanishing 的問題，所以卡住了<br>
<br>
0:07:52.380,0:07:54.520<br>
所以，你看 testing set 的 performance<br>
<br>
0:07:54.520,0:07:56.120<br>
大概是 11% 的正確率<br>
<br>
0:07:56.120,0:07:58.940<br>
那如果你沒有 training deep learning 的概念<br>
<br>
0:07:58.940,0:08:01.580<br>
你可能會說，所以 10 層 overfitting<br>
<br>
0:08:01.580,0:08:02.980<br>
所以 performance 這麼差<br>
<br>
0:08:02.980,0:08:04.820<br>
但是，如果你仔細看一下 training set<br>
<br>
0:08:04.820,0:08:07.740<br>
它的 performance 其實也是差的<br>
<br>
0:08:07.740,0:08:09.920<br>
所以，這個不是 overfitting，這個是沒 train 起來<br>
<br>
0:08:10.340,0:08:12.100<br>
那怎麼辦？<br>
<br>
0:08:15.780,0:08:18.340<br>
那現在要怎麼辦呢？<br>
<br>
0:08:18.360,0:08:21.040<br>
我們來改一下 activation function<br>
<br>
0:08:21.040,0:08:25.380<br>
我們把 sigmoid 都改成 ReLU<br>
<br>
0:08:25.420,0:08:27.960<br>
sigmoid 現在通通改成 ReLU<br>
<br>
0:08:27.960,0:08:29.380<br>
再 train 一次<br>
<br>
0:08:36.820,0:08:39.820<br>
你會發現說，現在 training 的 accuracy 呢<br>
<br>
0:08:39.900,0:08:43.700<br>
它就爬起來了<br>
<br>
0:08:45.940,0:08:48.420<br>
現在已經跑到 98、99 這樣<br>
<br>
0:08:50.120,0:08:52.780<br>
你會發現說，現在 training 的 accuracy<br>
<br>
0:08:52.780,0:08:54.380<br>
已經將近 100% 的 testing<br>
<br>
0:08:54.380,0:08:58.380<br>
可以得到 95.6% 的正確率<br>
<br>
0:09:01.400,0:09:05.500<br>
這邊有個有趣的地方可以跟大家分享一下<br>
<br>
0:09:05.500,0:09:08.100<br>
現在我們的 image 阿<br>
<br>
0:09:09.220,0:09:11.020<br>
它是有 normalize 的<br>
<br>
0:09:11.020,0:09:12.880<br>
所謂有 normalize 的意思是說<br>
<br>
0:09:12.880,0:09:16.880<br>
每一個 pixel，我們用一個 0~1 之間的值來表示它<br>
<br>
0:09:17.000,0:09:21.135<br>
1 代表最黑，0 代表沒有塗黑<br>
<br>
0:09:21.140,0:09:23.940<br>
其實，你剛拿到一個 image 的時候<br>
<br>
0:09:23.940,0:09:27.140<br>
通常我們是用灰階來表示它的<br>
<br>
0:09:27.140,0:09:31.040<br>
也就是每一個 pixel 的值，是用 0~255 來表示它<br>
<br>
0:09:31.040,0:09:34.020<br>
所以我這邊特別除上 255，做 normalize 這件事<br>
<br>
0:09:34.020,0:09:36.720<br>
如果今天我們把 255 拿掉<br>
<br>
0:09:36.720,0:09:38.260<br>
會發甚麼事呢？<br>
<br>
0:09:47.640,0:09:50.020<br>
你會發現說，你又做不起來了<br>
<br>
0:09:50.160,0:09:52.600<br>
所以，這種小小的地方<br>
<br>
0:09:52.640,0:09:54.560<br>
只是有沒有做 normalization 的地方<br>
<br>
0:09:54.560,0:09:57.600<br>
對你的結果會有關鍵的影響<br>
<br>
0:09:57.600,0:09:59.820<br>
而這些事情，是很多人都忽略的<br>
<br>
0:09:59.820,0:10:02.720<br>
因為我們知道說，現在 AI 非常地潮<br>
<br>
0:10:02.720,0:10:06.440<br>
現在多數人的心力都集中在 AI 會不會統治世界這件事情<br>
<br>
0:10:06.440,0:10:10.460<br>
或講一些奇奇怪怪不符合實際的話<br>
<br>
0:10:10.600,0:10:13.500<br>
這個東西，像這個小小的 normalization<br>
<br>
0:10:13.500,0:10:16.640<br>
一點都不潮，不會統治世界的東西<br>
<br>
0:10:16.640,0:10:18.820<br>
但對結果其實有非常大的影響<br>
<br>
0:10:20.795,0:10:23.725<br>
我們把它改回去<br>
<br>
0:10:23.760,0:10:26.980<br>
改回去，那接下來<br>
<br>
0:10:26.980,0:10:30.140<br>
我想示範的一個東西是<br>
<br>
0:10:31.040,0:10:33.180<br>
我們把<br>
<br>
0:10:35.280,0:10:38.100<br>
這個時程註解起來<br>
<br>
0:10:38.100,0:10:42.260<br>
然後，我們再跑一次<br>
<br>
0:10:42.260,0:10:45.100<br>
那你會發現說，今天<br>
<br>
0:10:46.720,0:10:48.620<br>
在 training 的時候阿<br>
<br>
0:10:49.660,0:10:52.000<br>
跑得很快，我們就讓它跑完<br>
<br>
0:10:53.985,0:10:56.220<br>
那今天在 training 的時候<br>
<br>
0:10:56.220,0:10:59.480<br>
大概在第一個 epoch 的時候得到 77% 的正確率<br>
<br>
0:10:59.480,0:11:02.580<br>
在第二個 epoch 的時候得到 90% 的正確率<br>
<br>
0:11:02.580,0:11:04.480<br>
那我們現在換一下<br>
<br>
0:11:04.480,0:11:08.200<br>
training 的，Gradient Descent 的 strategy<br>
<br>
0:11:08.200,0:11:09.760<br>
把它從 SGD<br>
<br>
0:11:09.760,0:11:11.340<br>
改成 Adam<br>
<br>
0:11:11.340,0:11:15.080<br>
我上課有講過 Aden，把它改成 Adam<br>
<br>
0:11:20.325,0:11:22.215<br>
然後，再跑一次<br>
<br>
0:11:24.740,0:11:27.820<br>
你會發現說，當我們用 Adam 的時候<br>
<br>
0:11:27.820,0:11:30.400<br>
它可能最後收斂的地方差不多<br>
<br>
0:11:30.400,0:11:33.980<br>
但是，你會發現它上升的速度是變快的<br>
<br>
0:11:35.680,0:11:37.760<br>
我們剛才在<br>
<br>
0:11:37.760,0:11:39.320<br>
第一個 epoch<br>
<br>
0:11:39.320,0:11:40.980<br>
在還沒有用 Adam 的時候，第一個 epoch<br>
<br>
0:11:40.980,0:11:43.100<br>
它的正確率是 7 開頭<br>
<br>
0:11:43.100,0:11:45.840<br>
如果現在，有加上了<br>
<br>
0:11:45.840,0:11:48.000<br>
用 Adam 的話呢<br>
<br>
0:11:48.000,0:11:51.000<br>
在第一個 epoch，它的正確率就有 85%<br>
<br>
0:11:51.000,0:11:53.300<br>
第二個 epoch 就有 95%<br>
<br>
0:11:53.300,0:11:56.580<br>
那今天在這個 test，因為一個 epoch 跑得非常非常快<br>
<br>
0:11:56.580,0:11:59.135<br>
所以，你可能沒有甚麼特別的感覺<br>
<br>
0:11:59.140,0:12:01.680<br>
但是，如果今天一個 epoch 要跑一天<br>
<br>
0:12:01.680,0:12:05.000<br>
你就會覺得說有 Adam 真是好這樣子<br>
<br>
0:12:09.080,0:12:10.780<br>
我在 testing set 上呢<br>
<br>
0:12:10.780,0:12:12.600<br>
故意加上了 noise<br>
<br>
0:12:12.600,0:12:14.380<br>
training set 沒有 noise<br>
<br>
0:12:14.380,0:12:15.980<br>
testing set 的每一個 image<br>
<br>
0:12:15.980,0:12:20.380<br>
每一個 pixel 都故意給它加上 random 的 noise<br>
<br>
0:12:20.580,0:12:22.180<br>
然後我們實際來<br>
<br>
0:12:22.180,0:12:25.220<br>
操作一下，看看結果會掉多少<br>
<br>
0:12:27.940,0:12:31.860<br>
我們本來在 testing set 上已經可以得到 96% 的正確率<br>
<br>
0:12:31.860,0:12:33.680<br>
但現在 training 和 testing 呢<br>
<br>
0:12:33.680,0:12:36.280<br>
是不 match 的<br>
<br>
0:12:38.680,0:12:41.840<br>
所以，一做下去，結果就爛掉了<br>
<br>
0:12:41.840,0:12:46.200<br>
結果就爛掉了，現在 testing 只有不到 50% 的正確率<br>
<br>
0:12:46.200,0:12:47.880<br>
那怎麼辦呢？<br>
<br>
0:12:47.880,0:12:51.260<br>
我們來試一下 dropout 可以帶給我們甚麼樣的結果<br>
<br>
0:12:51.260,0:12:53.380<br>
我們來加一下 dropout，怎麼加 dropout 呢？<br>
<br>
0:12:53.380,0:12:56.820<br>
你就打 model.add<br>
<br>
0:12:58.015,0:12:59.100<br>
Dropout<br>
<br>
0:12:59.100,0:13:01.060<br>
然後，你要設一個 dropout rate<br>
<br>
0:13:01.080,0:13:03.200<br>
這個 dropout rate 其實就是你自己設的啦<br>
<br>
0:13:03.200,0:13:06.020<br>
就像是 network hidden layer 的 size 一樣<br>
<br>
0:13:06.020,0:13:07.740<br>
你要設多少是你自己決定的<br>
<br>
0:13:07.740,0:13:09.600<br>
常見的是設 0.5，不過<br>
<br>
0:13:09.600,0:13:11.540<br>
因為今天在這個 task 裡面<br>
<br>
0:13:11.540,0:13:13.760<br>
training 跟 testing 非常的 mismatch<br>
<br>
0:13:13.760,0:13:15.800<br>
所以，我覺得 dropout rate 可以設大一點<br>
<br>
0:13:15.800,0:13:18.500<br>
比如說，我設 0.7 試試看<br>
<br>
0:13:25.960,0:13:30.300<br>
每一個 dropout 就是加在每一個 hidden layer 後面<br>
<br>
0:13:30.300,0:13:33.380<br>
那這邊有一件事情，大家要注意的就是說<br>
<br>
0:13:33.380,0:13:35.300<br>
今天當你加了 dropout 以後<br>
<br>
0:13:35.300,0:13:37.960<br>
其實，training 上的 performance 是會變差的<br>
<br>
0:13:37.960,0:13:39.080<br>
這個很合理嘛<br>
<br>
0:13:39.080,0:13:41.180<br>
因為加 dropout 就是去綁住 network 的手腳<br>
<br>
0:13:41.240,0:13:43.420<br>
在 training 的時候，它的 performance會變差<br>
<br>
0:13:43.420,0:13:44.660<br>
所以，如果你今天<br>
<br>
0:13:44.660,0:13:46.320<br>
你的 performance 不好是來自於<br>
<br>
0:13:46.320,0:13:47.820<br>
你在 training set 上的 performance 不好<br>
<br>
0:13:47.820,0:13:50.180<br>
你不要再加 dropout，你只會越弄越差而已<br>
<br>
0:13:50.180,0:13:52.320<br>
你今天在 training 上已經跑得太好<br>
<br>
0:13:52.320,0:13:54.860<br>
它 overfitting，你才加上 dropout<br>
<br>
0:13:54.860,0:13:57.340<br>
我們看一下，其實剛才阿<br>
<br>
0:13:57.340,0:13:59.400<br>
我們的正確率都可以做到<br>
<br>
0:13:59.400,0:14:01.020<br>
100% 的正確率這樣<br>
<br>
0:14:01.020,0:14:02.840<br>
看到沒有，100% 的正確率<br>
<br>
0:14:02.840,0:14:05.160<br>
這個才是真正的 overfitting<br>
<br>
0:14:05.480,0:14:08.440<br>
所以，我們現在加了 dropout 以後<br>
<br>
0:14:08.440,0:14:10.780<br>
應該就跑不到 100% 的正確率了<br>
<br>
0:14:10.780,0:14:12.100<br>
你看剛才在 training 的時候<br>
<br>
0:14:12.100,0:14:14.500<br>
在最後幾個 epoch 的正確率都已經是 100%<br>
<br>
0:14:14.700,0:14:16.040<br>
現在加上 dropout<br>
<br>
0:14:16.040,0:14:18.940<br>
你就會發現說，network 就 train 不到那個 performance<br>
<br>
0:14:22.140,0:14:24.820<br>
在 training 的時候，就等於是綁住 network 的手腳<br>
<br>
0:14:24.820,0:14:27.840<br>
你就會發現說，它現在有點被卡住了<br>
<br>
0:14:27.840,0:14:28.820<br>
它在 training 的時候<br>
<br>
0:14:28.820,0:14:32.060<br>
它的正確率現在在 94, 95 中間徘徊<br>
<br>
0:14:32.060,0:14:34.720<br>
那今天在這個<br>
<br>
0:14:34.720,0:14:36.620<br>
testing training data 的時候呢<br>
<br>
0:14:36.620,0:14:39.020<br>
其實就不會加上 dropout 啦<br>
<br>
0:14:39.020,0:14:40.120<br>
所以，你會發現說<br>
<br>
0:14:40.120,0:14:41.640<br>
在 testing 的時候<br>
<br>
0:14:41.700,0:14:43.120<br>
用 testing data 的時候<br>
<br>
0:14:43.120,0:14:44.820<br>
它 performance 是遠比 training 的時候<br>
<br>
0:14:44.820,0:14:47.240<br>
所呈現的 performance 要好得多<br>
<br>
0:14:47.240,0:14:48.580<br>
那有加 dropout 的時候<br>
<br>
0:14:48.580,0:14:49.780<br>
network 的 performance 呢<br>
<br>
0:14:49.780,0:14:51.580<br>
network train 的時候會綁住手腳<br>
<br>
0:14:51.580,0:14:52.840<br>
所以它的 performance 會差一點<br>
<br>
0:14:52.860,0:14:54.660<br>
那你會發現說，在 testing 的時候<br>
<br>
0:14:54.660,0:14:57.100<br>
剛才，正確率連 50% 都不到<br>
<br>
0:14:57.100,0:14:58.335<br>
但加了 dropout 以後<br>
<br>
0:14:58.335,0:15:01.425<br>
現在就有 60% 的正確率了<br>
<br>
0:15:01.425,0:15:02.720<br>
那這邊就是<br>
<br>
0:15:02.720,0:15:05.425<br>
實際示範一下，這個<br>
<br>
0:15:05.580,0:15:08.040<br>
如果我們把上課教的種種 tip<br>
<br>
0:15:08.040,0:15:10.460<br>
真的拿來實做在 MNIST 的時候<br>
<br>
0:15:10.460,0:15:11.920<br>
會有甚麼樣的不同<br>
<br>
0:15:11.920,0:15:13.600<br>
那其實還有很多東西沒有講的<br>
<br>
0:15:13.600,0:15:15.500<br>
那你可以自己回去試試看<br>
<br>
0:15:15.500,0:15:16.900<br>
或在作業三的時候<br>
<br>
0:15:16.900,0:15:18.740<br>
試試看不同的 tip 對 network 會有怎麼樣的影響<br>
<br>
0:15:18.800,0:15:21.480<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
