<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:03.640<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:03.640,0:00:07.500<br>
我要講的是幾個 deep learning 的 tip<br>
<br>
0:00:08.000,0:00:11.860<br>
其實，這一段本來是要在 CNN 之前講的啦<br>
<br>
0:00:11.860,0:00:15.520<br>
那在 CNN 那段裡面，我們留下了兩個問題<br>
<br>
0:00:15.520,0:00:20.120<br>
第一個是，在 CNN 裡面<br>
<br>
0:00:20.120,0:00:22.420<br>
有 max pooling 這樣的架構<br>
<br>
0:00:22.420,0:00:26.360<br>
但是，max pooling 這樣的架構顯然不能微分阿<br>
<br>
0:00:26.360,0:00:27.800<br>
你把它放在一個 network 裡面<br>
<br>
0:00:27.800,0:00:31.020<br>
你在做 Gradient Descent，你在微分它的時候<br>
<br>
0:00:31.020,0:00:33.040<br>
你到底是怎麼處理？<br>
<br>
0:00:33.040,0:00:36.500<br>
那第二個問題，是我們剛才看到的<br>
<br>
0:00:36.500,0:00:38.140<br>
L1 的 Regularization<br>
<br>
0:00:38.140,0:00:41.000<br>
但是，我們還沒有解釋它是甚麼東西<br>
<br>
0:00:41.000,0:00:44.580<br>
那這個，我們都會在這份投影片裡面解釋<br>
<br>
0:00:46.780,0:00:50.720<br>
那本來是要先講這份投影片，再講 CNN 的啦<br>
<br>
0:00:50.720,0:00:53.240<br>
但是，因為要講作業三的關係<br>
<br>
0:00:53.240,0:00:55.400<br>
所以，就先講了 CNN<br>
<br>
0:00:55.400,0:00:59.280<br>
講完這份投影片以後，之前的一些問題就可以被解決<br>
<br>
0:00:59.280,0:01:02.600<br>
首先，這邊呢，最重要的一個觀念是<br>
<br>
0:01:02.600,0:01:05.660<br>
Deep learning 的 recipe<br>
<br>
0:01:05.660,0:01:08.640<br>
如果你在訓練一個 deep learning 2的 network 的時候<br>
<br>
0:01:08.640,0:01:10.100<br>
你在做 deep learning 的時候<br>
<br>
0:01:10.100,0:01:13.200<br>
它的流程，應該是甚麼樣子的<br>
<br>
0:01:13.200,0:01:17.080<br>
那我們都知道說，deep learning 是 3 個 step<br>
<br>
0:01:17.080,0:01:21.640<br>
define function 、define 你的 function set<br>
<br>
0:01:21.640,0:01:23.740<br>
define 你的 network 的 structure<br>
<br>
0:01:23.740,0:01:25.700<br>
決定你的 loss function<br>
<br>
0:01:25.700,0:01:29.160<br>
接下來，你就可以用 <br>
Gradient Descent 去做 optimization<br>
<br>
0:01:29.160,0:01:31.180<br>
做完這些事情以後<br>
<br>
0:01:31.180,0:01:33.700<br>
你會得到一個 neural network<br>
<br>
0:01:33.700,0:01:36.140<br>
得到一個好的 neural network<br>
<br>
0:01:36.140,0:01:39.920<br>
接下來，你要做甚麼樣的事情呢？<br>
<br>
0:01:39.920,0:01:42.000<br>
接下來，你要做甚麼樣的事情呢？<br>
<br>
0:01:42.580,0:01:45.120<br>
其實，你第一件要檢查的事情是<br>
<br>
0:01:45.140,0:01:48.420<br>
這個 neural network 在你的 training set 上<br>
<br>
0:01:48.420,0:01:50.680<br>
有沒有得到好的結果<br>
<br>
0:01:50.680,0:01:54.340<br>
不是 testing set 哦，你要先檢查這個 neural network<br>
<br>
0:01:54.340,0:01:57.700<br>
在你的 training set 上，有沒有得到好的結果<br>
<br>
0:01:57.700,0:01:59.780<br>
如果，沒有的話<br>
<br>
0:01:59.780,0:02:02.500<br>
你就回頭去看看說，在這 3 個 step<br>
<br>
0:02:02.500,0:02:04.600<br>
這裡面，是不是哪邊出了問題<br>
<br>
0:02:04.600,0:02:07.640<br>
你可以做甚麼樣的修改，讓你在 training set 上<br>
<br>
0:02:07.640,0:02:09.520<br>
能夠得到好的結果<br>
<br>
0:02:09.520,0:02:13.460<br>
那這邊這個先檢查 training set 的 performance<br>
<br>
0:02:13.460,0:02:16.640<br>
其實是 deep learning 一個非常 unique 的地方<br>
<br>
0:02:16.640,0:02:19.140<br>
如果你想想看其他的方法，比如說<br>
<br>
0:02:19.140,0:02:20.280<br>
你今天如果用的是<br>
<br>
0:02:20.280,0:02:23.500<br>
雖然這些方法我們都還沒講過，但你或多或少都知道<br>
<br>
0:02:23.500,0:02:25.580<br>
比如說，k nearest neighbor<br>
<br>
0:02:26.120,0:02:27.780<br>
或者是 decision tree<br>
<br>
0:02:27.780,0:02:30.600<br>
其實像 k nearest neighbor 或 decision tree 這種方法<br>
<br>
0:02:30.600,0:02:34.720<br>
你做完以後，你其實會不太想檢查你 training set 的結果<br>
<br>
0:02:34.720,0:02:36.820<br>
因為，在 training set 上的 performance 正確率就是 100<br>
<br>
0:02:36.820,0:02:39.740<br>
你做完 decision tree 或做完 k nearest neighbor<br>
<br>
0:02:39.740,0:02:42.620<br>
得到正確率就是 100，沒有甚麼好檢查的<br>
<br>
0:02:42.620,0:02:45.020<br>
所以，有人說 deep learning<br>
<br>
0:02:45.020,0:02:47.380<br>
看這個 model 裡面這麼多參數<br>
<br>
0:02:47.380,0:02:49.160<br>
感覺一臉很容易 overfitting 的樣子<br>
<br>
0:02:49.160,0:02:51.000<br>
我跟你講，這個 deep learning 的方法<br>
<br>
0:02:51.000,0:02:52.140<br>
它才不容易 overfitting<br>
<br>
0:02:52.140,0:02:54.160<br>
我們說的 overfitting 就是在 training set 上<br>
<br>
0:02:54.160,0:02:56.400<br>
performance 很好，但 testing set 上 performance<br>
<br>
0:02:56.400,0:02:57.300<br>
沒有那麼好嘛<br>
<br>
0:02:57.300,0:03:00.380<br>
像這 k nearest neighbor, decision tree，它們一做下去<br>
<br>
0:03:00.380,0:03:02.020<br>
在 training set 上正確率都是 100<br>
<br>
0:03:02.020,0:03:03.740<br>
在 training set 上正確率都是 100<br>
<br>
0:03:03.740,0:03:06.100<br>
這個才是非常容易 overfitting<br>
<br>
0:03:06.100,0:03:07.960<br>
而對 deep learning 來說<br>
<br>
0:03:07.960,0:03:09.920<br>
overfitting 往往不是你會最<br>
<br>
0:03:09.920,0:03:12.060<br>
不是說，deep learning 沒有 overfitting 的問題<br>
<br>
0:03:12.060,0:03:14.600<br>
而是說，在 deep learning 裡面，overfitting<br>
<br>
0:03:14.600,0:03:16.460<br>
不是第一個你會遇到的問題<br>
<br>
0:03:16.460,0:03:19.020<br>
你第一個會遇到的問題，是你在 training 的時候<br>
<br>
0:03:19.020,0:03:21.760<br>
它並不是像 k nearest neighbor 這種方法一樣<br>
<br>
0:03:21.760,0:03:24.200<br>
你一 train 就可以得到非常好的正確率<br>
<br>
0:03:24.200,0:03:26.120<br>
它有可能在 training set 上<br>
<br>
0:03:26.120,0:03:28.640<br>
根本沒有辦法給你一個好的正確率<br>
<br>
0:03:28.640,0:03:31.100<br>
所以，這個時候你要回頭去檢查說<br>
<br>
0:03:31.100,0:03:33.020<br>
在前面的 step 裡面<br>
<br>
0:03:33.020,0:03:34.780<br>
要做什麼樣的修改<br>
<br>
0:03:34.780,0:03:37.820<br>
好讓你在 training set 上可以得到好的正確率<br>
<br>
0:03:38.640,0:03:41.660<br>
假設現在，幸運的是你已經<br>
<br>
0:03:41.660,0:03:44.520<br>
在 training set 上得到好的 performance 了<br>
<br>
0:03:44.520,0:03:46.860<br>
你要用 deep learning 在 training set 上<br>
<br>
0:03:46.860,0:03:49.240<br>
得到 100% 的正確率，是沒那麼容易的<br>
<br>
0:03:49.240,0:03:53.160<br>
但可能你在 MNIST 上得到一個 99.8% 的正確率<br>
<br>
0:03:53.480,0:03:57.760<br>
接下來，你就把你的 network apply 到 testing set 上<br>
<br>
0:03:57.760,0:03:59.600<br>
testing set 上的 performance 才是我們<br>
<br>
0:03:59.600,0:04:02.260<br>
最後真正關心的 performance<br>
<br>
0:04:02.260,0:04:06.660<br>
那你現在把你的結果 apply 到 testing set 上<br>
<br>
0:04:07.060,0:04:09.560<br>
那在 testing set 上 performance 怎麼樣呢？<br>
<br>
0:04:09.560,0:04:12.900<br>
如果現在得到的結果是 NO 的話<br>
<br>
0:04:12.900,0:04:15.820<br>
那就是 Overfitting<br>
<br>
0:04:15.820,0:04:17.680<br>
這個情況才是 Overfitting<br>
<br>
0:04:17.680,0:04:20.560<br>
你在 training set 上得到好的結果<br>
<br>
0:04:20.560,0:04:24.820<br>
但是，在 testing set 上得到的是不好的結果<br>
<br>
0:04:24.820,0:04:28.100<br>
這個時候，這個情況呢<br>
<br>
0:04:28.100,0:04:29.600<br>
才叫做 Overfitting<br>
<br>
0:04:29.840,0:04:32.880<br>
那你要回過頭去，做某一些事情<br>
<br>
0:04:32.880,0:04:35.640<br>
然後，試著去解決 overfitting 這個 problem<br>
<br>
0:04:35.640,0:04:37.980<br>
但有時候，你加了新的 technique<br>
<br>
0:04:37.980,0:04:40.700<br>
去想要 overcome overfitting 這個 problem 的時候<br>
<br>
0:04:40.700,0:04:44.640<br>
你其實反而會讓 training set 上的結果變壞<br>
<br>
0:04:44.640,0:04:47.000<br>
所以，你在做這一步的修改以後<br>
<br>
0:04:47.000,0:04:50.580<br>
你要先回頭去檢查說，training set 上的結果<br>
<br>
0:04:50.580,0:04:53.740<br>
是怎麼樣的，如果 training set 上的結果變壞的話<br>
<br>
0:04:53.740,0:04:54.880<br>
你要從頭呢<br>
<br>
0:04:54.880,0:04:59.680<br>
去對你的 network training 的 process 做一些調整<br>
<br>
0:05:00.240,0:05:03.160<br>
那如果你同時在 training set 還有你手上的 testing set<br>
<br>
0:05:03.160,0:05:04.440<br>
都得到好的結果的話<br>
<br>
0:05:04.440,0:05:08.020<br>
最後，你就可以把你的系統真正用在 application 上面<br>
<br>
0:05:08.020,0:05:09.440<br>
你就成功了<br>
<br>
0:05:10.380,0:05:12.160<br>
那這邊有一個重點就是<br>
<br>
0:05:12.160,0:05:15.760<br>
不要看到所有不好的 performance<br>
<br>
0:05:15.760,0:05:18.140<br>
就說是 overfitting<br>
<br>
0:05:18.600,0:05:21.040<br>
舉例來說，這個是文獻上的圖<br>
<br>
0:05:21.040,0:05:24.720<br>
但我在現實生活中，也常常看到這樣子的狀況<br>
<br>
0:05:24.720,0:05:26.440<br>
在 testing set 上面<br>
<br>
0:05:26.440,0:05:28.680<br>
這個是 testing data 的結果<br>
<br>
0:05:28.680,0:05:32.860<br>
橫坐標，是 model 參數 update 的次數<br>
<br>
0:05:32.860,0:05:34.280<br>
所以，你做 Gradient Descent 的時候<br>
<br>
0:05:34.280,0:05:36.040<br>
你 update 幾次參數<br>
<br>
0:05:36.040,0:05:39.120<br>
縱座標，是 error rate，所以越低越好<br>
<br>
0:05:39.280,0:05:42.400<br>
那如果我們現在表示一個 20 層的 network<br>
<br>
0:05:42.400,0:05:43.640<br>
它是黃線<br>
<br>
0:05:43.640,0:05:47.260<br>
這個 56 層的 neural network，它是紅線<br>
<br>
0:05:47.260,0:05:50.660<br>
那你會發現說，這個 56 層的 network<br>
<br>
0:05:50.660,0:05:53.380<br>
它的 error rate 比較高，它的 performance 比較差<br>
<br>
0:05:53.380,0:05:56.840<br>
20 層的 neural network，它的 performance 是比較好的<br>
<br>
0:05:56.840,0:05:59.980<br>
那有些人看到這個圖，就會馬上得到一個結論<br>
<br>
0:05:59.980,0:06:03.580<br>
說 56 層太多了，參數太多了<br>
<br>
0:06:03.580,0:06:06.760<br>
56 層果然沒有必要，這個是 overfitting<br>
<br>
0:06:06.760,0:06:08.520<br>
但是，真的是這樣子嗎？<br>
<br>
0:06:08.520,0:06:12.100<br>
你在說，現在得到的結果是 overfitting 之前<br>
<br>
0:06:12.100,0:06:15.480<br>
你要先檢查一下你在 training set 上的結果<br>
<br>
0:06:15.480,0:06:17.860<br>
對某些方法來說，你不用檢查這件事<br>
<br>
0:06:17.860,0:06:20.140<br>
比如說 k nearest neighbor 或 decision tree<br>
<br>
0:06:20.140,0:06:21.980<br>
你不用檢查這件事<br>
<br>
0:06:21.980,0:06:23.560<br>
但是，對 neural network 來說<br>
<br>
0:06:23.560,0:06:25.560<br>
你是需要檢查這件事情的<br>
<br>
0:06:25.560,0:06:29.080<br>
為甚麼呢？因為有可能你在 training set 上得到的結果<br>
<br>
0:06:29.080,0:06:30.700<br>
是這個樣子<br>
<br>
0:06:30.700,0:06:32.480<br>
是這個樣子的<br>
<br>
0:06:32.480,0:06:34.740<br>
橫軸一樣是參數 update 的次數<br>
<br>
0:06:34.740,0:06:36.680<br>
縱軸是 error rate<br>
<br>
0:06:36.680,0:06:38.980<br>
如果我們比較 20 層的 neural network 跟<br>
<br>
0:06:38.980,0:06:40.600<br>
56 層的 neural network 的話<br>
<br>
0:06:40.600,0:06:41.840<br>
你會發現說<br>
<br>
0:06:41.840,0:06:45.640<br>
在 training set 上 ，20 層的 neural network<br>
<br>
0:06:45.640,0:06:48.280<br>
它的 performance 本來就比 56 層好<br>
<br>
0:06:48.280,0:06:50.820<br>
在 training set 上 ，56 層的 neural network<br>
<br>
0:06:50.820,0:06:54.620<br>
它的 performance 是比較差的<br>
<br>
0:06:54.620,0:06:56.020<br>
是比較差的<br>
<br>
0:06:56.020,0:06:58.120<br>
那為甚麼會這樣子呢？<br>
<br>
0:06:58.120,0:07:00.480<br>
你想想看你在做 neural network training 的時候<br>
<br>
0:07:00.480,0:07:02.720<br>
有太多太多的問題<br>
<br>
0:07:02.720,0:07:05.020<br>
可以讓你的 training 的結果是不好的<br>
<br>
0:07:05.020,0:07:08.460<br>
比如說，我們有 local minimum 的問題<br>
<br>
0:07:08.460,0:07:10.600<br>
有 saddle point 的問題，有 plateau 的問題<br>
<br>
0:07:10.600,0:07:11.840<br>
有種種的問題<br>
<br>
0:07:11.840,0:07:14.720<br>
所以，有可能這個 56 層的 neural network<br>
<br>
0:07:14.720,0:07:18.480<br>
你 train 的時候，它就卡在一個 local minimum 的地方<br>
<br>
0:07:18.480,0:07:21.420<br>
所以，它得到了一個差的參數<br>
<br>
0:07:21.420,0:07:23.500<br>
所以，這個並不是 overfitting<br>
<br>
0:07:23.500,0:07:27.020<br>
是在 training 的時候，就沒有 train 好<br>
<br>
0:07:27.020,0:07:31.280<br>
那有人會說，這個叫做 underfitting <br>
<br>
0:07:31.280,0:07:33.940<br>
我覺得這個可能不叫做 underfitting <br>
<br>
0:07:33.940,0:07:37.160<br>
但是這個只是名詞定義的問題啦，你要怎麼說都行<br>
<br>
0:07:37.160,0:07:40.660<br>
但是，在我的心裡面，underfitting 的意思是說<br>
<br>
0:07:40.660,0:07:45.000<br>
這個 model 的 complexity<br>
<br>
0:07:45.000,0:07:47.680<br>
這個 model 的參數不夠多，所以<br>
<br>
0:07:47.680,0:07:50.380<br>
它的能力不足以解出這個問題<br>
<br>
0:07:50.380,0:07:53.020<br>
但對這個 56 層的 neural network 來說<br>
<br>
0:07:53.020,0:07:54.960<br>
雖然它得到比較差的 performance<br>
<br>
0:07:54.960,0:07:57.180<br>
但假如這個 56 層的 network<br>
<br>
0:07:57.180,0:08:00.000<br>
它其實是在這個 20 層的 network 後面<br>
<br>
0:08:00.000,0:08:03.100<br>
後面再另外堆 36 層的 network<br>
<br>
0:08:03.100,0:08:07.100<br>
那它的參數，其實是比 20 層的 network 還多的<br>
<br>
0:08:07.100,0:08:09.960<br>
理論上，20 層的 network 可以做到的事情<br>
<br>
0:08:09.960,0:08:12.080<br>
56 層的 network 一定可以做到<br>
<br>
0:08:12.080,0:08:13.420<br>
你前面已經有那 20 層<br>
<br>
0:08:13.420,0:08:17.500<br>
你前面那 20 層就做跟 20 層 network 一樣的事情<br>
<br>
0:08:17.500,0:08:20.740<br>
後面那 36 層就甚麼事都不幹，就都是 identity 就好了<br>
<br>
0:08:20.740,0:08:24.140<br>
你明明可以做到跟 20 層一樣的事情<br>
<br>
0:08:24.140,0:08:25.860<br>
你為甚麼做不到呢？<br>
<br>
0:08:25.860,0:08:27.880<br>
但是，因為會有很多的問題就是<br>
<br>
0:08:27.880,0:08:30.300<br>
讓你沒有辦法做到<br>
<br>
0:08:30.300,0:08:32.860<br>
所以，這個 56 層的 network 呢<br>
<br>
0:08:32.860,0:08:36.280<br>
它比 20 層差，並不是因為它能力不夠<br>
<br>
0:08:36.280,0:08:41.860<br>
它只要前 20 層都跟它一樣，後面都是 identity<br>
<br>
0:08:42.560,0:08:45.000<br>
明明就可以跟 20 層一樣好<br>
<br>
0:08:45.000,0:08:47.000<br>
但它卻沒有得到這樣的結果<br>
<br>
0:08:47.000,0:08:49.660<br>
所以，它能力是夠的，所以我覺得這不是 underfitting<br>
<br>
0:08:49.660,0:08:53.040<br>
它這個就是沒有 train 好這樣子<br>
<br>
0:08:53.040,0:08:58.300<br>
那我還不知道有沒有什麼名詞，專門指稱這個問題<br>
<br>
0:08:58.300,0:09:02.060<br>
所以，它其實就是像這個小智的噴火龍一樣<br>
<br>
0:09:02.060,0:09:04.460<br>
它等級是夠的，但它就不想要打這樣子<br>
<br>
0:09:07.660,0:09:10.800<br>
所以，在 deep learning 的文獻上<br>
<br>
0:09:10.800,0:09:13.860<br>
如果，當你讀到一個方法的時候<br>
<br>
0:09:13.860,0:09:16.820<br>
你永遠要想一下說，這個方法<br>
<br>
0:09:16.820,0:09:19.580<br>
它是要解什麼樣的問題<br>
<br>
0:09:19.580,0:09:23.040<br>
因為在 deep learning 裡面，有兩個問題<br>
<br>
0:09:23.040,0:09:25.040<br>
一個是 training set 上的 performance 不好<br>
<br>
0:09:25.040,0:09:27.440<br>
一個是 testing set 上的 performance 不好<br>
<br>
0:09:27.440,0:09:29.140<br>
當只有一個方法 propose 的時候<br>
<br>
0:09:29.140,0:09:33.360<br>
它往往就是針對這兩個問題的其中一個<br>
<br>
0:09:33.360,0:09:35.000<br>
來做處理<br>
<br>
0:09:35.000,0:09:38.540<br>
舉例來說，你等一下能會聽到一個方法叫做 dropout<br>
<br>
0:09:38.640,0:09:41.240<br>
dropout 或許大家或多或少都會知道，它是一個<br>
<br>
0:09:41.240,0:09:44.300<br>
很有 deep learning 特色，很潮的一個方法<br>
<br>
0:09:46.120,0:09:48.520<br>
那很多人就會說，哦，這麼潮的方法，所以<br>
<br>
0:09:48.520,0:09:51.720<br>
我今天只要看到 performance 不好，我就很快 dropout<br>
<br>
0:09:51.720,0:09:54.220<br>
但是，你只要仔細想一下 dropout 是甚麼時候用的<br>
<br>
0:09:54.220,0:09:57.660<br>
dropout 是你在 testing 的結果不好的時候<br>
<br>
0:09:57.660,0:10:00.080<br>
你才會 apply dropout<br>
<br>
0:10:00.080,0:10:02.020<br>
你的 testing data 結果好的時候<br>
<br>
0:10:02.020,0:10:03.580<br>
你是不會 apply dropout<br>
<br>
0:10:03.580,0:10:05.580<br>
就是說，dropout 是<br>
<br>
0:10:05.580,0:10:07.780<br>
你在 testing 結果不好的時候，才 apply dropout<br>
<br>
0:10:07.780,0:10:10.440<br>
如果你今天的問題是你 training 的結果不好<br>
<br>
0:10:10.440,0:10:13.100<br>
你 apply dropout，你只會越 train 越差而已<br>
<br>
0:10:13.100,0:10:16.660<br>
所以，不同的方法，對治甚麼樣不同的症狀<br>
<br>
0:10:16.660,0:10:18.760<br>
你是必須要在心裡想清楚的<br>
<br>
0:10:18.760,0:10:22.260<br>
那我們這邊就休息 10 分鐘，等一下再繼續講，謝謝<br>
<br>
0:10:22.300,0:10:27.340<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:10:27.400,0:10:29.280<br>
各位同學大家好<br>
<br>
0:10:29.280,0:10:30.860<br>
我們來上課吧<br>
<br>
0:10:31.320,0:10:33.000<br>
來上課吧<br>
<br>
0:10:33.440,0:10:36.560<br>
等一下呢，我們剛才講說<br>
<br>
0:10:36.560,0:10:39.400<br>
這個在 deep learning 的 recipe 裡面<br>
<br>
0:10:39.400,0:10:41.540<br>
在 train deep learning 的時候有兩個問題<br>
<br>
0:10:41.540,0:10:44.020<br>
所以，等一下呢，我們就是要<br>
<br>
0:10:44.020,0:10:47.380<br>
這兩個問題，分開來討論<br>
<br>
0:10:47.380,0:10:49.800<br>
看看當你遇到這兩個問題的時候呢<br>
<br>
0:10:49.800,0:10:51.940<br>
有甚麼樣解決的方法<br>
<br>
0:10:54.360,0:10:57.160<br>
首先，如果你今天的<br>
<br>
0:10:57.160,0:10:59.720<br>
training 在 training 的結果上不好的時候<br>
<br>
0:10:59.720,0:11:01.120<br>
你可能可以看看說<br>
<br>
0:11:01.120,0:11:04.360<br>
是不是你在做 network 架構設計的時候<br>
<br>
0:11:04.360,0:11:06.100<br>
是不是設計不好<br>
<br>
0:11:06.100,0:11:09.520<br>
舉例來說，你可能用的 activation function<br>
<br>
0:11:09.520,0:11:12.080<br>
是比較不好的 activation function<br>
<br>
0:11:12.080,0:11:14.460<br>
是對 training 比較不利的 activation function<br>
<br>
0:11:14.460,0:11:16.940<br>
你可能會換一些新的 activation function<br>
<br>
0:11:16.940,0:11:19.040<br>
它可以給你比較好的結果<br>
<br>
0:11:19.700,0:11:21.260<br>
那我們知道說<br>
<br>
0:11:21.260,0:11:23.280<br>
在 1980 年代的時候<br>
<br>
0:11:23.280,0:11:25.900<br>
比較常用的 activation function<br>
<br>
0:11:25.900,0:11:28.400<br>
是一個 sigmoid function<br>
<br>
0:11:28.400,0:11:32.300<br>
那我們之前有稍微試著 reason 一下<br>
<br>
0:11:32.300,0:11:34.880<br>
為甚麼要用 sigmoid function<br>
<br>
0:11:36.320,0:11:39.620<br>
今天如果我們用的是 sigmoid function 的時候<br>
<br>
0:11:39.620,0:11:42.520<br>
在過去，其實你會發現說<br>
<br>
0:11:42.780,0:11:46.020<br>
deeper 並不一定 imply better<br>
<br>
0:11:46.020,0:11:49.460<br>
這個是在 MNIST 上面的結果啦<br>
<br>
0:11:49.460,0:11:51.220<br>
在手寫數字辨識上的結果<br>
<br>
0:11:51.220,0:11:53.560<br>
當你 layer 越來越多的時候呢<br>
<br>
0:11:53.560,0:11:57.700<br>
你的 accuracy，一開始持平，後來就掉下去了<br>
<br>
0:11:57.700,0:12:01.060<br>
在你的 layer 是 9 層、10 層的時候，整個結果就崩潰啦<br>
<br>
0:12:01.060,0:12:02.980<br>
那有人看到這個圖，就會覺得說<br>
<br>
0:12:02.980,0:12:06.280<br>
9 層、10 層參數太多了，overfitting<br>
<br>
0:12:07.080,0:12:09.060<br>
這個，不是 overfitting<br>
<br>
0:12:09.060,0:12:12.060<br>
為甚麼呢？首先呢，我們說你要<br>
<br>
0:12:12.060,0:12:15.360<br>
檢查你現在 performance 不好是不是來自於 overfitting<br>
<br>
0:12:15.360,0:12:18.260<br>
你要看你 training set 的結果嘛，對不對？<br>
<br>
0:12:18.260,0:12:21.180<br>
那這個線，是 training set 的結果<br>
<br>
0:12:21.180,0:12:23.440<br>
所以，這個不是 overfitting<br>
<br>
0:12:23.440,0:12:25.720<br>
這個是 training 的時候，就 train 壞掉了<br>
<br>
0:12:25.720,0:12:29.140<br>
不信的話，我們實際來用 Keras 實做一下<br>
<br>
0:12:31.140,0:12:33.360<br>
一個原因是這樣子<br>
<br>
0:12:33.960,0:12:35.640<br>
一個原因是<br>
<br>
0:12:35.640,0:12:38.660<br>
這個原因叫做 Vanishing 的 Gradient<br>
<br>
0:12:38.660,0:12:42.760<br>
這個原因是這樣，當你把 network 疊得很深的時候<br>
<br>
0:12:42.760,0:12:44.740<br>
在 input 的幾個 layer<br>
<br>
0:12:44.740,0:12:47.380<br>
在最靠近 input 的地方呢<br>
<br>
0:12:47.380,0:12:50.300<br>
你的這個 Gradient<br>
<br>
0:12:50.300,0:12:54.560<br>
你的這些參數，對最後 loss function 的微分<br>
<br>
0:12:54.560,0:12:56.160<br>
會是很小的<br>
<br>
0:12:56.160,0:12:58.740<br>
而在比較靠近 output 的地方呢<br>
<br>
0:12:58.740,0:13:01.460<br>
它的微分值，會是很大的<br>
<br>
0:13:01.460,0:13:06.080<br>
因此，當你設定同樣的 learning rate 的時候<br>
<br>
0:13:06.080,0:13:08.460<br>
你會發現說，靠近 input 的地方<br>
<br>
0:13:08.460,0:13:10.600<br>
它參數的 update，是很慢的<br>
<br>
0:13:10.600,0:13:14.720<br>
靠近 output 的地方，它參數的 update 是很快的<br>
<br>
0:13:14.940,0:13:16.660<br>
所以，你會發現說呢<br>
<br>
0:13:16.660,0:13:19.320<br>
在 input 幾乎還是 random 的時候<br>
<br>
0:13:19.320,0:13:22.100<br>
output 就已經 converge 了<br>
<br>
0:13:22.480,0:13:25.480<br>
在 input layer<br>
<br>
0:13:25.480,0:13:27.580<br>
在靠近 input 地方的這些參數<br>
<br>
0:13:27.580,0:13:29.120<br>
它還是 random 的時候<br>
<br>
0:13:29.120,0:13:32.360<br>
output 的地方，就已經根據這些 random 的<br>
<br>
0:13:32.360,0:13:33.620<br>
random 的結果呢<br>
<br>
0:13:33.620,0:13:35.640<br>
找到了一個 local minimum<br>
<br>
0:13:35.640,0:13:38.220<br>
然後，它就 converge 了<br>
<br>
0:13:38.220,0:13:39.700<br>
這個時候，你會發現說<br>
<br>
0:13:39.700,0:13:44.300<br>
你這個參數的 loss 下降的速度呢<br>
<br>
0:13:44.300,0:13:46.280<br>
變得很慢，你就覺得說<br>
<br>
0:13:46.280,0:13:48.460<br>
卡在 local minimum 什麼之類的<br>
<br>
0:13:48.460,0:13:51.260<br>
就傷心地把程式停掉了<br>
<br>
0:13:51.260,0:13:53.700<br>
這個時候，你得到的結果是很差的，為什麼呢？<br>
<br>
0:13:53.700,0:13:57.220<br>
因為這個 converge，是幾乎 base on random 的參數<br>
<br>
0:13:57.220,0:13:59.380<br>
那幾乎 base on random 的 output<br>
<br>
0:13:59.380,0:14:02.780<br>
然後去 converge，所以得到的結果，是很差的<br>
<br>
0:14:02.780,0:14:05.560<br>
那為甚麼會有這個現象發生呢？<br>
<br>
0:14:05.560,0:14:07.640<br>
為甚麼會有這個現象發生呢？<br>
<br>
0:14:07.640,0:14:11.760<br>
如果你自己把 Backpropagation 的式子寫出來的話<br>
<br>
0:14:11.760,0:14:15.120<br>
你可以很輕易地發現說，用 sigmoid function<br>
<br>
0:14:15.120,0:14:17.180<br>
會導致這件事情的發生<br>
<br>
0:14:17.180,0:14:19.700<br>
但是，我們今天不看 Backpropagation 的式子<br>
<br>
0:14:19.700,0:14:21.560<br>
我們其實從直覺上來想<br>
<br>
0:14:21.560,0:14:24.400<br>
你也可以了解為什麼這件事情發生<br>
<br>
0:14:24.400,0:14:26.320<br>
怎麼用直覺來想<br>
<br>
0:14:26.320,0:14:28.940<br>
一個參數的 Gradient 的值應該是多少呢？<br>
<br>
0:14:28.940,0:14:30.520<br>
我們知道說<br>
<br>
0:14:30.520,0:14:36.260<br>
某一個參數 w 對 total cost C 的偏微分阿<br>
<br>
0:14:36.260,0:14:39.660<br>
意思就是說，它的直覺的意思就是說<br>
<br>
0:14:39.660,0:14:43.220<br>
當我今天把某一個參數做小小的變化的時後<br>
<br>
0:14:43.220,0:14:47.400<br>
它對這個 cost 的影響是怎麼樣<br>
<br>
0:14:47.400,0:14:50.140<br>
了解嗎？就是我們可以把一個參數<br>
<br>
0:14:50.140,0:14:53.700<br>
做小小的變化，然後觀察它對 cost 的變化<br>
<br>
0:14:53.700,0:14:56.120<br>
以此來決定說，這個參數<br>
<br>
0:14:56.120,0:14:58.440<br>
它的 Gradient 的值有多大<br>
<br>
0:14:59.260,0:15:02.180<br>
所以，怎麼做呢？我們就把<br>
<br>
0:15:02.320,0:15:05.760<br>
第一個 layer 裡面的某一個參數<br>
<br>
0:15:05.760,0:15:11.760<br>
加上 △w，看看對 network 的 output<br>
<br>
0:15:11.760,0:15:15.700<br>
和它的 target 之間的 loss<br>
<br>
0:15:15.700,0:15:17.240<br>
有甚麼樣的影響<br>
<br>
0:15:17.240,0:15:19.020<br>
那你會發現說<br>
<br>
0:15:19.660,0:15:22.260<br>
如果我今天這個 △w 很大<br>
<br>
0:15:22.260,0:15:24.600<br>
通過 sigmoid function 的時候<br>
<br>
0:15:24.600,0:15:27.320<br>
這個 output 呢，是會變小的<br>
<br>
0:15:27.320,0:15:31.740<br>
也就是說，改變了某一個參數的 weight<br>
<br>
0:15:31.740,0:15:36.600<br>
對某一個 neuron 的 output 的值會有影響<br>
<br>
0:15:36.600,0:15:40.380<br>
但是，這個影響是會衰減的<br>
<br>
0:15:40.380,0:15:42.080<br>
為甚麼這麼說呢？<br>
<br>
0:15:42.080,0:15:45.120<br>
因為，假設你用的是 sigmoid function<br>
<br>
0:15:45.120,0:15:48.200<br>
我們知道 sigmoid function 形狀就長這樣<br>
<br>
0:15:48.200,0:15:50.340<br>
那 sigmoid function 它會把<br>
<br>
0:15:50.340,0:15:52.940<br>
負無窮大到正無窮大之間的值<br>
<br>
0:15:52.940,0:15:56.960<br>
都硬壓到 0~1 之間<br>
<br>
0:15:57.920,0:16:02.860<br>
也就是說，如果你有很大的 input 的變化<br>
<br>
0:16:03.300,0:16:05.640<br>
通過 sigmoid function 以後<br>
<br>
0:16:05.640,0:16:08.520<br>
它 output 的變化，會是很小的<br>
<br>
0:16:08.520,0:16:13.220<br>
所以，就算今天你這個 △w 有很大的變化<br>
<br>
0:16:13.220,0:16:16.200<br>
造成 sigmoid function 的 input 有很大的變化<br>
<br>
0:16:16.200,0:16:18.880<br>
對 sigmoid function 來說，它的 output 的變化<br>
<br>
0:16:18.880,0:16:20.780<br>
是會衰減的<br>
<br>
0:16:20.780,0:16:23.480<br>
而每通過一次 sigmoid function<br>
<br>
0:16:23.480,0:16:25.740<br>
變化就衰減一次<br>
<br>
0:16:25.740,0:16:27.900<br>
所以，當你的 network 越深<br>
<br>
0:16:27.900,0:16:29.980<br>
它衰減的次數就越多<br>
<br>
0:16:29.980,0:16:33.660<br>
直到最後，它對 output 的影響是非常小的<br>
<br>
0:16:33.660,0:16:35.760<br>
也就是說，你在 input 的地方<br>
<br>
0:16:35.760,0:16:38.200<br>
改一下你的參數<br>
<br>
0:16:38.200,0:16:39.560<br>
對 output 的地方<br>
<br>
0:16:39.560,0:16:42.480<br>
它最後 output 的變化，其實是很小的<br>
<br>
0:16:42.480,0:16:45.520<br>
因此，最後對 cost 的影響也很小<br>
<br>
0:16:45.520,0:16:49.360<br>
因此，就造成說，在靠近 input 的那些 weight<br>
<br>
0:16:49.360,0:16:54.280<br>
它對它這個 Gradient 的值是小的<br>
<br>
0:16:54.280,0:16:57.780<br>
那怎麼解決這個問題呢？<br>
<br>
0:16:57.780,0:16:59.380<br>
有人就說<br>
<br>
0:16:59.380,0:17:03.360<br>
原來比較早年的做法是去 train RBM<br>
<br>
0:17:03.360,0:17:07.120<br>
去做這個 layer-wise 的 training<br>
<br>
0:17:07.120,0:17:09.560<br>
也就是說，你先認好一個 layer<br>
<br>
0:17:10.340,0:17:13.040<br>
就因為我們現在說，如果你把所有的這個<br>
<br>
0:17:13.040,0:17:16.220<br>
network 兜起來，那你做 Backpropagation 的時候<br>
<br>
0:17:16.220,0:17:18.540<br>
第一個 layer 你幾乎沒有辦法被挑到嘛<br>
<br>
0:17:18.540,0:17:22.300<br>
所以，用 RBM 做 training 的時候，它的精神就是<br>
<br>
0:17:22.300,0:17:24.100<br>
我先把一個 layer train 好<br>
<br>
0:17:24.100,0:17:26.080<br>
再 train 第二個，再 train 第三個<br>
<br>
0:17:26.080,0:17:28.060<br>
最後，你在做 Backpropagation 的時候<br>
<br>
0:17:28.060,0:17:30.900<br>
雖然說，第一個 layer 幾乎沒有被 train 到<br>
<br>
0:17:30.900,0:17:31.580<br>
那無所謂<br>
<br>
0:17:31.580,0:17:34.040<br>
一開始在 pre-train 的時候，就把它 pre-train 好了<br>
<br>
0:17:34.040,0:17:38.760<br>
所以，這就是 RBM 為什麼做 pre-train 可能有用的原因<br>
<br>
0:17:38.760,0:17:41.280<br>
那後來有人說，其實<br>
<br>
0:17:41.280,0:17:42.960<br>
後來有人發現說<br>
<br>
0:17:42.960,0:17:46.260<br>
其實，我記得 Hinton 跟 Pengel 都<br>
<br>
0:17:46.260,0:17:49.480<br>
幾乎在同樣的時間，不約而同地提出同樣的想法<br>
<br>
0:17:49.480,0:17:51.480<br>
改一下 activation function<br>
<br>
0:17:51.480,0:17:54.700<br>
可能就可以 handle 這個問題了<br>
<br>
0:17:54.700,0:17:58.180<br>
所以，現在比較常用的 activation function<br>
<br>
0:17:58.180,0:18:02.520<br>
叫做 Rectified Linear Unit，它的縮寫是 ReLU<br>
<br>
0:18:02.520,0:18:04.640<br>
會常看到有人叫它 ReLU<br>
<br>
0:18:04.640,0:18:06.740<br>
那這個 activation function 它長這樣子<br>
<br>
0:18:07.060,0:18:09.940<br>
這個 z 是 activation function 的 input<br>
<br>
0:18:09.940,0:18:12.460<br>
a 是 activation function 的 output<br>
<br>
0:18:12.460,0:18:16.060<br>
如果今天 activation function 的 input 大於 0 的時候<br>
<br>
0:18:16.060,0:18:17.260<br>
input = output<br>
<br>
0:18:17.260,0:18:20.340<br>
如果 activation function 的 input 小於 0 的時候<br>
<br>
0:18:20.340,0:18:21.980<br>
output 就是 0<br>
<br>
0:18:21.980,0:18:25.320<br>
那選擇這樣的 activation function 有甚麼好處呢？<br>
<br>
0:18:25.320,0:18:28.420<br>
有以下幾個理由，第一個理由是<br>
<br>
0:18:28.420,0:18:31.780<br>
它比較快，跟 sigmoid function 比起來<br>
<br>
0:18:31.780,0:18:33.400<br>
它的運算是快很多的<br>
<br>
0:18:33.400,0:18:35.420<br>
sigmoid function 裡面還有 exponential<br>
<br>
0:18:35.420,0:18:37.240<br>
那個是很慢的，那如果你是<br>
<br>
0:18:37.240,0:18:39.640<br>
用這個方法的話，它是快得多的<br>
<br>
0:18:39.640,0:18:41.820<br>
如果你看這個<br>
<br>
0:18:41.820,0:18:44.900<br>
我記得是 Pengel 寫得原始的 paper 的話呢<br>
<br>
0:18:44.900,0:18:46.140<br>
裡面會告訴你說<br>
<br>
0:18:46.140,0:18:48.680<br>
這個 activation function 的想法其實<br>
<br>
0:18:48.680,0:18:50.700<br>
是有一些生命上的理由的<br>
<br>
0:18:50.700,0:18:55.300<br>
那它把這樣的 activation 跟一些生物上的觀察呢<br>
<br>
0:18:55.300,0:18:57.060<br>
結合在一起<br>
<br>
0:18:57.060,0:18:59.000<br>
那 Hinton 有說過說<br>
<br>
0:18:59.000,0:19:01.220<br>
ReLU 這樣的 activation function<br>
<br>
0:19:01.220,0:19:05.340<br>
其實，等同於是無窮多的 sigmoid function<br>
<br>
0:19:05.340,0:19:06.900<br>
疊加的結果<br>
<br>
0:19:06.900,0:19:08.080<br>
無窮多的 sigmoid function<br>
<br>
0:19:08.080,0:19:10.720<br>
它們的 bias 都不一樣，疊加的結果會變成 ReLU<br>
<br>
0:19:10.720,0:19:11.980<br>
的 activation function<br>
<br>
0:19:11.980,0:19:14.100<br>
但它最重要的理由是<br>
<br>
0:19:14.100,0:19:17.980<br>
它可以 handle Vanishing gradient 的這個問題<br>
<br>
0:19:17.980,0:19:20.440<br>
它怎麼  handle Vanishing gradient 這個問題呢？<br>
<br>
0:19:20.440,0:19:26.160<br>
我們來看一下，這個是一個 ReLU 的 neural network<br>
<br>
0:19:26.200,0:19:28.960<br>
這是一個 ReLU 的 neural network<br>
<br>
0:19:28.960,0:19:30.740<br>
它裡面的每一個 activation function<br>
<br>
0:19:30.740,0:19:32.960<br>
都是 ReLU 的 activation function<br>
<br>
0:19:32.960,0:19:34.120<br>
那我們知道說<br>
<br>
0:19:34.120,0:19:35.380<br>
ReLU 的 activation function<br>
<br>
0:19:35.380,0:19:37.580<br>
它作用在兩個不同的 region<br>
<br>
0:19:37.580,0:19:41.600<br>
一個 region 是當 activation function 的 input<br>
<br>
0:19:41.600,0:19:43.680<br>
大於 0 的時候，input = output<br>
<br>
0:19:43.680,0:19:44.980<br>
另外一個 region 是<br>
<br>
0:19:44.980,0:19:46.880<br>
activation function 的 input 小於 0<br>
<br>
0:19:46.880,0:19:48.380<br>
所以，output 就是 0<br>
<br>
0:19:48.380,0:19:51.900<br>
所以，現在每一個 ReLU 的 activation function<br>
<br>
0:19:51.900,0:19:54.040<br>
它作用在兩個不同的 region<br>
<br>
0:19:54.040,0:19:56.140<br>
一個 region 是<br>
<br>
0:19:56.140,0:19:57.860<br>
每一個 activation function 的<br>
<br>
0:19:57.860,0:20:02.640<br>
一個可能是 activation function 的 output 就是 0<br>
<br>
0:20:02.640,0:20:03.700<br>
另外一個可能是<br>
<br>
0:20:03.700,0:20:05.700<br>
activation function 的 input = output<br>
<br>
0:20:05.700,0:20:07.080<br>
當 input = output 的時候<br>
<br>
0:20:07.080,0:20:09.100<br>
其實，這個 activation function 就是 linear 的<br>
<br>
0:20:09.100,0:20:10.480<br>
就是 linear 的<br>
<br>
0:20:11.280,0:20:15.400<br>
那對那些 output 是 0 的 neuron 來說<br>
<br>
0:20:15.400,0:20:18.540<br>
它其實對整個 network 是一點影響都沒有的阿<br>
<br>
0:20:18.540,0:20:20.460<br>
它 output 是 0，所以<br>
<br>
0:20:20.460,0:20:22.420<br>
它根本就不會影響最後 output 的值<br>
<br>
0:20:22.420,0:20:24.940<br>
所以，假如有一個 neuron 它 output 是 0 的話<br>
<br>
0:20:24.940,0:20:27.420<br>
你根本可以把它從 network 裡面整個拿掉<br>
<br>
0:20:27.420,0:20:29.680<br>
把它從 network 裡面整個拿掉<br>
<br>
0:20:29.680,0:20:33.940<br>
當你把這些 output 是 0 的 network 拿掉<br>
<br>
0:20:33.940,0:20:37.680<br>
剩下的 neuron，就都是 input = output，是 linear 的時候<br>
<br>
0:20:37.680,0:20:41.760<br>
你整個 network，不就是一個很瘦長的<br>
<br>
0:20:41.760,0:20:43.320<br>
linear network 嗎？<br>
<br>
0:20:43.320,0:20:46.680<br>
你整個 network，其實就變成是 linear 的 network<br>
<br>
0:20:47.800,0:20:49.440<br>
那這個時候<br>
<br>
0:20:49.440,0:20:52.220<br>
我們剛才說<br>
<br>
0:20:52.220,0:20:53.980<br>
我們的 Gradient 會遞減<br>
<br>
0:20:54.000,0:20:57.380<br>
是因為通過 sigmoid function 的關係<br>
<br>
0:20:57.380,0:21:00.260<br>
sigmoid function 會把比較大的 input<br>
<br>
0:21:00.260,0:21:01.600<br>
變成比較小的 output<br>
<br>
0:21:01.600,0:21:03.440<br>
但是，如果你是 linear 的話<br>
<br>
0:21:03.440,0:21:04.660<br>
input = output<br>
<br>
0:21:04.660,0:21:08.160<br>
你就不會有那個 activation function 遞減的問題了<br>
<br>
0:21:09.220,0:21:12.080<br>
講到這邊，有沒有人有問題呢？<br>
<br>
0:21:13.260,0:21:15.780<br>
講到這邊，我有一個問題<br>
<br>
0:21:18.280,0:21:22.380<br>
現在如果我用 ReLU 的時候，整個 network<br>
<br>
0:21:22.380,0:21:24.020<br>
都變成 linear 的阿<br>
<br>
0:21:24.020,0:21:27.440<br>
可是，我們要的不是一個 linear 的 network 阿<br>
<br>
0:21:27.440,0:21:30.260<br>
我們之所以用 deep learning，就是因為我們<br>
<br>
0:21:30.260,0:21:32.820<br>
不想要我們的 function 是 linear 的<br>
<br>
0:21:32.820,0:21:35.820<br>
我們希望它是一個 non-linear，一個比較複雜的 function<br>
<br>
0:21:35.820,0:21:37.520<br>
所以，我們用 deep learning<br>
<br>
0:21:37.520,0:21:39.500<br>
當我們用 ReLU 的時候<br>
<br>
0:21:41.220,0:21:43.380<br>
它不就變成一個 linear 的 function 了嗎？<br>
<br>
0:21:43.380,0:21:44.980<br>
這樣都不會有問題嗎？<br>
<br>
0:21:44.980,0:21:46.840<br>
這樣不是變得很弱嗎？<br>
<br>
0:21:49.040,0:21:51.560<br>
其實是這樣子的，這整個 network 呢<br>
<br>
0:21:51.560,0:21:54.540<br>
整體來說，它還是 non-linear 的<br>
<br>
0:21:54.540,0:21:55.700<br>
大家聽得懂嗎？<br>
<br>
0:21:55.700,0:22:00.560<br>
當你的每一個 neuron 做 operation<br>
<br>
0:22:00.560,0:22:04.420<br>
當每一個 neuron，它 operation 的 region 是一樣的時候<br>
<br>
0:22:04.420,0:22:05.600<br>
它是 linear 的<br>
<br>
0:22:05.600,0:22:08.020<br>
但是<br>
<br>
0:22:08.020,0:22:11.540<br>
也就是說，如果你對 input 做小小的改變<br>
<br>
0:22:11.540,0:22:14.140<br>
不改變 neuron 的 operation 的 region<br>
<br>
0:22:14.140,0:22:16.060<br>
它是一個 linear 的 function<br>
<br>
0:22:16.060,0:22:18.360<br>
但是，如果你對 input 做比較大的改變<br>
<br>
0:22:18.480,0:22:22.900<br>
你改變了 neuron 的 operation region 的話<br>
<br>
0:22:22.900,0:22:25.920<br>
它就變成是 non-linear 的<br>
<br>
0:22:26.460,0:22:28.840<br>
這樣大家可以接受嗎？<br>
<br>
0:22:32.780,0:22:34.860<br>
好那有另外一個問題<br>
<br>
0:22:35.080,0:22:37.580<br>
這個也是我常常被問到的問題<br>
<br>
0:22:37.580,0:22:40.040<br>
這個不能微分阿<br>
<br>
0:22:40.040,0:22:41.260<br>
這不能微分<br>
<br>
0:22:41.260,0:22:43.560<br>
這樣你不覺得很苦惱嗎？<br>
<br>
0:22:43.560,0:22:48.200<br>
我們之前說，我們做 Gradient Descent 的時候<br>
<br>
0:22:48.200,0:22:51.100<br>
你需要對你的 loss function 做微分<br>
<br>
0:22:51.100,0:22:52.100<br>
意思就是說<br>
<br>
0:22:52.100,0:22:54.040<br>
你要對你的 neural network 是可以做微分的<br>
<br>
0:22:54.040,0:22:55.940<br>
你的 neural network 要是一個可微的 function<br>
<br>
0:22:56.460,0:22:58.520<br>
ReLU 不可微阿<br>
<br>
0:22:58.520,0:23:00.220<br>
至少這個點是不可微的<br>
<br>
0:23:00.220,0:23:01.380<br>
那怎麼辦呢？<br>
<br>
0:23:01.380,0:23:03.580<br>
其實，實作上你就這個樣子啦<br>
<br>
0:23:04.360,0:23:06.720<br>
當你的 region 在這個地方的時候<br>
<br>
0:23:06.720,0:23:08.500<br>
gradient 微分就是 1<br>
<br>
0:23:08.500,0:23:10.480<br>
region 在這個地方的時候，微分就是 0<br>
<br>
0:23:11.340,0:23:14.880<br>
反正不可能 input 正好是 0 嘛，就不要管它<br>
<br>
0:23:14.880,0:23:16.640<br>
結束這樣<br>
<br>
0:23:18.260,0:23:20.460<br>
那我們來實際試一下<br>
<br>
0:23:20.460,0:23:23.260<br>
如果我們把 activation function 換成 ReLU 的時候<br>
<br>
0:23:23.260,0:23:24.800<br>
會得到甚麼樣的結果<br>
<br>
0:23:24.800,0:23:27.180<br>
比如說，我們就這樣子<br>
<br>
0:23:27.180,0:23:29.780<br>
把 sigmoid 換成 ReLU<br>
<br>
0:23:34.580,0:23:38.040<br>
把 sigmoid 換成 ReLU，那我們剛才用 sigmoid 的時候<br>
<br>
0:23:38.040,0:23:40.620<br>
training 和 testing 的 accuracy 都很差<br>
<br>
0:23:40.620,0:23:42.940<br>
我們就簡單的把它換成 ReLU<br>
<br>
0:23:42.940,0:23:44.460<br>
甚麼其他事都沒做<br>
<br>
0:23:46.960,0:23:51.000<br>
沒換嗎？等我一下<br>
<br>
0:24:02.280,0:24:05.520<br>
那 ReLU 其實還有種種的變數<br>
<br>
0:24:05.520,0:24:09.420<br>
那有人覺得說，如果是 ReLU 的時候<br>
<br>
0:24:09.420,0:24:11.280<br>
如果是原來的 ReLU<br>
<br>
0:24:11.280,0:24:13.140<br>
它在 input 小於 0 的時候<br>
<br>
0:24:13.140,0:24:15.320<br>
output 會是 0，這個時候微分是 0<br>
<br>
0:24:15.320,0:24:17.920<br>
你就沒有辦法 update 你的參數了<br>
<br>
0:24:17.920,0:24:19.460<br>
所以，我們應該讓<br>
<br>
0:24:19.460,0:24:23.940<br>
在 input 小於 0 的時候，output 還是有一點點的值<br>
<br>
0:24:23.940,0:24:26.380<br>
也就是 input 小於 0 的時候，output 是<br>
<br>
0:24:26.380,0:24:28.560<br>
input 乘上 0.01<br>
<br>
0:24:28.560,0:24:31.860<br>
這個東西叫做 Leaky ReLU<br>
<br>
0:24:32.420,0:24:34.400<br>
那這個時候，有人就會問說<br>
<br>
0:24:34.400,0:24:39.460<br>
為甚麼是 0.01，為甚麼不是 0.07, 0.08 之類的呢？<br>
<br>
0:24:39.460,0:24:42.800<br>
所以，就有人提出了 Parametric ReLU<br>
<br>
0:24:42.800,0:24:45.440<br>
他說，在負的這邊呢<br>
<br>
0:24:45.440,0:24:52.160<br>
(output) a = (input) z*α<br>
<br>
0:24:52.160,0:24:54.680<br>
α 是一個 network 的參數<br>
<br>
0:24:54.680,0:24:57.780<br>
它可以透過 training data 被學出來<br>
<br>
0:24:57.780,0:25:01.320<br>
甚至每一個 neuron 都可以有不同的 α 的值<br>
<br>
0:25:02.760,0:25:07.000<br>
那又會有人問說，為甚麼一定要是 ReLU 這個樣子呢？<br>
<br>
0:25:07.000,0:25:08.560<br>
可不可以是別的樣子<br>
<br>
0:25:08.560,0:25:12.660<br>
所以，後來又有一個更進階的想法，<br>
叫做 Maxout network<br>
<br>
0:25:12.660,0:25:14.580<br>
那在 Maxout network 裡面呢<br>
<br>
0:25:14.580,0:25:20.600<br>
你就是讓你的 network 自動學它的 activation function<br>
<br>
0:25:20.600,0:25:23.980<br>
那因為現在 activation function 是自動學出來的<br>
<br>
0:25:23.980,0:25:29.520<br>
所以 ReLU 就只是 Maxout network 的一個 special case<br>
<br>
0:25:29.520,0:25:33.060<br>
Maxout network 它可以學出 <br>
ReLU 這樣的 activation function<br>
<br>
0:25:33.060,0:25:36.440<br>
但是，它也可以是其他的 activation function<br>
<br>
0:25:36.440,0:25:37.960<br>
用 training data 來決定說<br>
<br>
0:25:37.960,0:25:40.860<br>
現在的 activation function 應該要長甚麼樣子<br>
<br>
0:25:41.580,0:25:43.620<br>
Maxout network 長甚麼樣子呢？<br>
<br>
0:25:43.620,0:25:45.720<br>
假設現在有 input<br>
<br>
0:25:45.720,0:25:48.260<br>
一個 2 dimension 的 vector，[x1, x2]<br>
<br>
0:25:48.260,0:25:50.100<br>
然後，我們就把<br>
<br>
0:25:50.100,0:25:52.800<br>
x1, x2 乘上不同的 weight<br>
<br>
0:25:52.800,0:25:55.300<br>
變成一個 value, 5<br>
<br>
0:25:55.300,0:25:57.780<br>
然後，再乘上不同 weight 得到 7<br>
<br>
0:25:57.780,0:26:00.760<br>
再乘上不同 weight 得到 -1，再乘上不同 weight 得到 1<br>
<br>
0:26:00.760,0:26:02.120<br>
那本來這些值呢<br>
<br>
0:26:02.120,0:26:03.920<br>
應該要通過 activation function<br>
<br>
0:26:03.920,0:26:07.100<br>
不管是 sigmoid function 還是 ReLU<br>
<br>
0:26:07.100,0:26:08.920<br>
得到另外一個 value<br>
<br>
0:26:08.920,0:26:11.680<br>
但是，現在在 Maxout network 裡面<br>
<br>
0:26:11.680,0:26:13.980<br>
現在在 Maxout network 裡面呢<br>
<br>
0:26:13.980,0:26:15.600<br>
我們做的事情是這樣子<br>
<br>
0:26:16.600,0:26:18.920<br>
你把這些 value<br>
<br>
0:26:18.920,0:26:21.360<br>
group 起來，你把這些 value group 起來<br>
<br>
0:26:21.360,0:26:24.820<br>
哪些 value 應該被 group 起來這件事情是<br>
<br>
0:26:24.820,0:26:26.380<br>
事先決定的<br>
<br>
0:26:26.380,0:26:29.940<br>
比如說，現在，這兩個 value 是一組<br>
<br>
0:26:29.940,0:26:31.440<br>
這兩個 value 是一組<br>
<br>
0:26:31.440,0:26:34.100<br>
那你在同一個組裡面<br>
<br>
0:26:34.100,0:26:36.040<br>
選一個值最大的當作 output<br>
<br>
0:26:36.040,0:26:38.040<br>
比如說，這個組就選 7<br>
<br>
0:26:38.040,0:26:39.660<br>
這個組就選 1<br>
<br>
0:26:39.660,0:26:41.400<br>
那這件事情呢<br>
<br>
0:26:42.060,0:26:44.940<br>
其實就跟 Max Pooling 一樣對不對<br>
<br>
0:26:44.940,0:26:47.660<br>
只是我們現在不是在 image 上做 Max Pooling <br>
<br>
0:26:47.660,0:26:50.620<br>
我們是在在一個 layer 上做 Max Pooling <br>
<br>
0:26:50.620,0:26:52.060<br>
我們把 layer 裡面的<br>
<br>
0:26:52.540,0:26:54.980<br>
本來要放到 neuron 裡面的<br>
<br>
0:26:54.980,0:26:57.900<br>
這個 activation function<br>
<br>
0:26:57.900,0:27:01.220<br>
我們本來要把它放到 neuron 的 activation function<br>
<br>
0:27:01.220,0:27:03.000<br>
的這個 input 的值 group 起來<br>
<br>
0:27:03.000,0:27:05.740<br>
然後，只選 max 當作 output<br>
<br>
0:27:05.740,0:27:07.580<br>
然後，就不用 activation function 了<br>
<br>
0:27:07.580,0:27:09.460<br>
就不加 activation function<br>
<br>
0:27:09.460,0:27:11.200<br>
得到的值是 7 跟 1<br>
<br>
0:27:11.200,0:27:13.620<br>
那你可以想說，這個東西呢<br>
<br>
0:27:13.620,0:27:15.580<br>
就是一個 neuron<br>
<br>
0:27:15.580,0:27:18.280<br>
只是它的 output 是一個 vector，而不是一個值<br>
<br>
0:27:20.340,0:27:22.680<br>
那接下來這個 7 跟 1 呢<br>
<br>
0:27:22.680,0:27:24.260<br>
就乘上不同的 weight<br>
<br>
0:27:24.260,0:27:26.600<br>
就得到另外一排不同的值<br>
<br>
0:27:26.940,0:27:30.440<br>
然後，你一樣把它們做 grouping<br>
<br>
0:27:30.440,0:27:34.040<br>
你一樣從每個 group 裡面選最大的值<br>
<br>
0:27:34.040,0:27:36.880<br>
1 跟2 就選 2，4 跟3 就選 4<br>
<br>
0:27:38.580,0:27:40.660<br>
其實，在實作上<br>
<br>
0:27:40.660,0:27:44.440<br>
幾個 element 要不要放在同一個 group 裡面，這件事情<br>
<br>
0:27:44.440,0:27:45.640<br>
是你可以自己決定的<br>
<br>
0:27:45.640,0:27:49.240<br>
就跟 network structure 一樣，是你自己需要調的參數<br>
<br>
0:27:49.240,0:27:51.720<br>
所以，你可以不是兩個 element 放一組<br>
<br>
0:27:51.720,0:27:53.880<br>
你可以是 3 個、4 個、5 個都可以<br>
<br>
0:27:53.880,0:27:55.240<br>
這個是你自己決定的<br>
<br>
0:27:57.920,0:28:01.020<br>
我們現在先說，Maxout network<br>
<br>
0:28:01.020,0:28:05.720<br>
它是有辦法做到跟 ReLU 一模一樣的事情<br>
<br>
0:28:05.720,0:28:09.360<br>
它可以模仿 ReLU 這個 activation function<br>
<br>
0:28:09.360,0:28:10.820<br>
怎麼做呢？<br>
<br>
0:28:10.820,0:28:14.560<br>
我們知道說，假設我們這邊有一個 ReLU 的 neuron<br>
<br>
0:28:14.560,0:28:16.780<br>
它的 input 就一個 value x<br>
<br>
0:28:17.980,0:28:22.200<br>
你會把 x 乘上這個 neuron 的 weight, w<br>
<br>
0:28:22.200,0:28:24.520<br>
再加上 bias, b<br>
<br>
0:28:24.520,0:28:29.260<br>
然後，通過 activation function, ReLU 得到 a<br>
<br>
0:28:29.260,0:28:34.100<br>
所以，現在如果我們看 x 跟 a 的關係<br>
<br>
0:28:34.100,0:28:35.700<br>
是什麼樣子呢？<br>
<br>
0:28:36.120,0:28:38.060<br>
假設 x 是橫軸<br>
<br>
0:28:38.060,0:28:41.320<br>
那這個 x 是橫軸<br>
<br>
0:28:41.320,0:28:43.440<br>
假設 y 軸是這個 z 的話<br>
<br>
0:28:43.440,0:28:46.000<br>
它就是 w*x + b<br>
<br>
0:28:46.000,0:28:49.900<br>
z 跟 x 之間的關係是 linear 的<br>
<br>
0:28:49.900,0:28:51.140<br>
是 linear 的，是這個樣子<br>
<br>
0:28:51.900,0:28:53.340<br>
那如果你選 a 呢<br>
<br>
0:28:53.340,0:28:56.580<br>
a 跟 z有甚麼樣的關係呢？<br>
<br>
0:28:56.580,0:29:00.240<br>
因為現在通過的是 ReLU 的 activation function<br>
<br>
0:29:00.240,0:29:04.320<br>
所以，如果你今天 z 的值大過 0 的時候<br>
<br>
0:29:04.320,0:29:05.500<br>
a = z<br>
<br>
0:29:05.500,0:29:07.100<br>
z 的值小於 0 的時候<br>
<br>
0:29:07.100,0:29:08.400<br>
a 就是 0<br>
<br>
0:29:08.400,0:29:11.980<br>
所以，a 跟 x 的關係是這個樣子<br>
<br>
0:29:11.980,0:29:16.200<br>
在這個地方，a = z；在這個地方，a = 0<br>
<br>
0:29:16.200,0:29:19.480<br>
所以，我們今天用 ReLU 的 activation function<br>
<br>
0:29:19.480,0:29:22.980<br>
它 input 和 output，x 和 a 之間的關係是長這樣子<br>
<br>
0:29:23.680,0:29:26.320<br>
如果我們今天用 Maxout network<br>
<br>
0:29:26.320,0:29:29.080<br>
用 Maxout network，你把 w<br>
<br>
0:29:29.080,0:29:34.660<br>
你把 input, x 乘上 weight, w 再加上 bias，得到 z1<br>
<br>
0:29:35.140,0:29:38.940<br>
你再把 x 乘上另外一組 weight<br>
<br>
0:29:39.420,0:29:40.760<br>
加上另外一個 bias<br>
<br>
0:29:40.760,0:29:43.180<br>
得到 z2，那我今天假設說<br>
<br>
0:29:43.180,0:29:46.920<br>
另外一個 weight 跟另外一個 bias 都是 0，所以 z2 = 0<br>
<br>
0:29:46.920,0:29:50.420<br>
然後，你做 Max Pooling <br>
<br>
0:29:50.420,0:29:53.240<br>
你就可以選 z1, z2 其中一個比較大的呢<br>
<br>
0:29:53.240,0:29:54.900<br>
當作 a<br>
<br>
0:29:54.900,0:29:59.380<br>
現在，如果我們看 z1 跟 x 之間的關係<br>
<br>
0:29:59.380,0:30:02.400<br>
我們得到的是藍色這條線<br>
<br>
0:30:02.900,0:30:07.240<br>
如果我們看 x 跟 z2 之間的關係<br>
<br>
0:30:07.240,0:30:11.720<br>
我們得到的是水平這條線，因為 z2 總是 0<br>
<br>
0:30:11.720,0:30:14.480<br>
如果 z2 前面接的 weight 跟 bias 都是 0<br>
<br>
0:30:14.480,0:30:17.380<br>
z2 總是 0，所以它是紅色的這條線<br>
<br>
0:30:17.680,0:30:20.440<br>
那我們現在做的是 Maxout<br>
<br>
0:30:20.440,0:30:25.040<br>
我們是在 z1, z2 裡面選一個大的當作 output a<br>
<br>
0:30:25.040,0:30:27.860<br>
所以，如果今天 x 是在這個 region 的時候<br>
<br>
0:30:27.860,0:30:30.240<br>
你的 a 就會等於 z1，是這個 region<br>
<br>
0:30:30.920,0:30:33.340<br>
如果今天 x 是在這個 region 的時候<br>
<br>
0:30:33.340,0:30:37.160<br>
你的 a 就會等於比較大的 z2，所以是這個 region<br>
<br>
0:30:37.700,0:30:40.700<br>
那今天你只要把這個 w 跟這個 b<br>
<br>
0:30:40.700,0:30:42.520<br>
等於這個 w 跟這個 b<br>
<br>
0:30:42.520,0:30:44.080<br>
你就可以讓<br>
<br>
0:30:44.080,0:30:46.740<br>
這個 ReLU 的 input 和 output 的關係<br>
<br>
0:30:46.740,0:30:49.540<br>
等於這個 Maxout network 的 input 和 output 的關係<br>
<br>
0:30:49.540,0:30:51.420<br>
所以，由此可知， 就是<br>
<br>
0:30:51.420,0:30:55.140<br>
ReLU 是 Maxout network 可以做到的事情<br>
<br>
0:30:55.140,0:30:58.400<br>
只要它設定出正確的參數<br>
<br>
0:30:58.400,0:31:01.240<br>
但是，Maxout network 它也可以做出<br>
<br>
0:31:01.240,0:31:04.100<br>
更多的、不同的 activation function<br>
<br>
0:31:04.100,0:31:08.020<br>
比如說，現在假設這兩個 weight 不是 0，而是 w', b'<br>
<br>
0:31:08.020,0:31:09.200<br>
那會怎樣呢？<br>
<br>
0:31:09.200,0:31:11.960<br>
就得到藍色這條線 z1<br>
<br>
0:31:11.960,0:31:14.660<br>
跟紅色這條線 z2<br>
<br>
0:31:14.660,0:31:16.780<br>
因為 w', b' 是不一樣的值，所以<br>
<br>
0:31:16.780,0:31:19.280<br>
它可能是另外一條斜直線，長的是這樣子<br>
<br>
0:31:19.880,0:31:21.800<br>
接下來，你做 Max Pooling 的時候<br>
<br>
0:31:21.800,0:31:24.040<br>
你會在 z1, z2 裡面選一個大的<br>
<br>
0:31:24.040,0:31:27.260<br>
所以，在這個範圍內，你選了<br>
<br>
0:31:28.120,0:31:29.320<br>
你選了 z1<br>
<br>
0:31:29.320,0:31:32.020<br>
在這個範圍內，你選了 z2<br>
<br>
0:31:32.020,0:31:35.100<br>
所以你就得到了一個不一樣的 activation function<br>
<br>
0:31:35.100,0:31:37.340<br>
而這個 activation function 長甚麼樣子<br>
<br>
0:31:37.340,0:31:41.280<br>
是由 network 的參數 w, b, w', b' 決定的<br>
<br>
0:31:41.280,0:31:43.840<br>
所以，這個 activation function 它是一個<br>
<br>
0:31:43.840,0:31:45.760<br>
Learnable 的 activation function<br>
<br>
0:31:45.760,0:31:48.680<br>
它是一個可以根據 data 去 <br>
generate 出來的 activation function<br>
<br>
0:31:48.680,0:31:50.880<br>
每一個 neuron 根據不同的 weight<br>
<br>
0:31:50.880,0:31:52.780<br>
它可以有不同的 activation function<br>
<br>
0:31:53.760,0:31:55.520<br>
那 ReLU 是這樣子<br>
<br>
0:31:55.520,0:31:58.640<br>
它可以做出任何的<br>
<br>
0:31:58.640,0:32:03.340<br>
piecewise 的 linear 的 convex activation function<br>
<br>
0:32:03.340,0:32:06.980<br>
如果你看一下它的性質，你就不難理解這件事情<br>
<br>
0:32:06.980,0:32:10.200<br>
那至於這個 piecewise 的 linear function 裡面<br>
<br>
0:32:10.200,0:32:12.300<br>
有多少個 piece<br>
<br>
0:32:12.300,0:32:16.180<br>
這決定於你現在把多少個 element 放在一個 group<br>
<br>
0:32:16.180,0:32:17.920<br>
假如說兩個 element 一個 group<br>
<br>
0:32:17.920,0:32:20.400<br>
那你可以有長這樣子的 activation function<br>
<br>
0:32:20.400,0:32:21.340<br>
是 ReLU<br>
<br>
0:32:21.340,0:32:24.460<br>
你可以有一個 activation function 它的作用就是取<br>
<br>
0:32:24.460,0:32:25.720<br>
絕對值<br>
<br>
0:32:25.720,0:32:28.980<br>
假設你是 3 個 element 一個 group<br>
<br>
0:32:28.980,0:32:31.440<br>
你可以有長這樣子的 activation function<br>
<br>
0:32:31.440,0:32:33.980<br>
你也可以有長這樣子的 activation function 等等<br>
<br>
0:32:34.920,0:32:37.940<br>
那接下來我們要面對另外一個問題，就是<br>
<br>
0:32:37.940,0:32:39.580<br>
這個東西怎麼 train<br>
<br>
0:32:39.580,0:32:41.280<br>
這個東西怎麼 train<br>
<br>
0:32:41.280,0:32:42.760<br>
這裡面有個 Max 阿<br>
<br>
0:32:42.760,0:32:45.520<br>
它不能微分阿<br>
<br>
0:32:45.520,0:32:47.980<br>
這個東西怎麼 train<br>
<br>
0:32:47.980,0:32:50.280<br>
這個做法是這樣子的<br>
<br>
0:32:50.940,0:32:54.600<br>
假設現在這個 z1 跟<br>
<br>
0:32:54.600,0:32:58.460<br>
假設這邊這兩個值，比較大的是<br>
<br>
0:32:58.460,0:32:59.960<br>
上面這個值<br>
<br>
0:32:59.960,0:33:04.220<br>
我們現在把這個 group 裡面比較大的值，用框框框起來<br>
<br>
0:33:04.220,0:33:05.560<br>
用框框框起來<br>
<br>
0:33:05.560,0:33:08.100<br>
那比較大的值<br>
<br>
0:33:08.100,0:33:11.160<br>
就會等於這個 max operation 的 output<br>
<br>
0:33:11.160,0:33:13.040<br>
就會等於 max operation 的 output<br>
<br>
0:33:13.040,0:33:16.680<br>
所以，這個值等於這個值，這個值等於這個值<br>
<br>
0:33:16.680,0:33:19.000<br>
這個值等於這個值，這個值等於這個值<br>
<br>
0:33:19.000,0:33:23.980<br>
所以，max operation 其實<br>
在這邊就是一個 linear 的 operation<br>
<br>
0:33:23.980,0:33:27.340<br>
這是 linear，這是 linear，只是它<br>
<br>
0:33:27.340,0:33:29.940<br>
會選擇在前面這個 group 裡面的<br>
<br>
0:33:29.940,0:33:34.720<br>
它只接給前面這個 group 裡面的某一個 element<br>
<br>
0:33:36.600,0:33:38.900<br>
也就是說，也就是說其實呢<br>
<br>
0:33:38.900,0:33:43.040<br>
那這些沒有被接到的 element，它就沒用啦<br>
<br>
0:33:43.040,0:33:44.760<br>
它就不會影響 network 的 output 啦<br>
<br>
0:33:44.760,0:33:47.300<br>
所以，你就可以把它拿掉，你就可以把它拿掉<br>
<br>
0:33:49.340,0:33:53.700<br>
所以，其實當我們在做 Maxout 的時候<br>
<br>
0:33:53.700,0:33:55.880<br>
當你給它一個 input 的時候<br>
<br>
0:33:55.880,0:34:01.260<br>
你其實也是得到一個比較細長的 linear network<br>
<br>
0:34:01.260,0:34:03.860<br>
所以，你在 train 的時候，你 train 的就是<br>
<br>
0:34:03.860,0:34:07.140<br>
這個比較細長的 linear network 裡面的參數<br>
<br>
0:34:07.140,0:34:11.140<br>
你就是去 train 這些連到這一個 element 的這些參數<br>
<br>
0:34:11.140,0:34:15.520<br>
連到這個 element 的這些參數<br>
<br>
0:34:15.520,0:34:19.700<br>
假設我給你一個這樣子的 linear network，<br>
你當然知道它是怎麼 train 的<br>
<br>
0:34:19.700,0:34:22.880<br>
用 Backpropagation train 就好，你知道它是怎麼 train 的<br>
<br>
0:34:22.880,0:34:25.620<br>
但這個時候呢，你就會有一個問題<br>
<br>
0:34:29.040,0:34:31.760<br>
沒被 train 到的 element 怎麼辦呢？<br>
<br>
0:34:31.760,0:34:34.960<br>
如果某一個這個 element，它不是最大的值<br>
<br>
0:34:34.960,0:34:37.640<br>
那它連接的那些 weight<br>
<br>
0:34:37.640,0:34:39.320<br>
就不會被 train 到了嗎？<br>
<br>
0:34:39.320,0:34:40.900<br>
你做 Backpropagation 的時候<br>
<br>
0:34:40.900,0:34:43.900<br>
你只會 train 在這個圖上的<br>
<br>
0:34:43.900,0:34:46.240<br>
比較深顏色的這些實線<br>
<br>
0:34:46.240,0:34:48.180<br>
你不會 train 到這個 weight 阿<br>
<br>
0:34:48.340,0:34:50.680<br>
這個 weight 不就沒被 train 到了嗎？<br>
<br>
0:34:50.680,0:34:52.500<br>
怎麼辦呢？<br>
<br>
0:34:52.500,0:34:54.620<br>
這看起來，表面上是一個問題<br>
<br>
0:34:54.620,0:34:57.180<br>
但實作上，它不是一個問題<br>
<br>
0:34:57.180,0:34:58.220<br>
為甚麼呢？<br>
<br>
0:34:58.220,0:35:01.260<br>
因為當你 input 不同的<br>
<br>
0:35:01.260,0:35:04.020<br>
當你給它不同的 input 的時候<br>
<br>
0:35:04.020,0:35:06.660<br>
你得到的這些 z 的值是不一樣的<br>
<br>
0:35:06.660,0:35:08.680<br>
你給它不同 input 的時候<br>
<br>
0:35:08.680,0:35:11.220<br>
max 的值，是不一樣的<br>
<br>
0:35:11.220,0:35:14.480<br>
所以，每一次你給它不同的 input 的時候<br>
<br>
0:35:14.480,0:35:17.320<br>
這個 network 的 structure 都是不一樣的<br>
<br>
0:35:17.320,0:35:19.900<br>
因為我們有很多很多筆 training data<br>
<br>
0:35:19.900,0:35:21.780<br>
而 network 的 structure 不斷地變換<br>
<br>
0:35:21.780,0:35:25.640<br>
所以，最後每一個 weight 在實際上都會被 train 到<br>
<br>
0:35:25.640,0:35:27.440<br>
Maxout 就是這麼做<br>
<br>
0:35:27.440,0:35:29.240<br>
Maxout network 就是這麼做<br>
<br>
0:35:29.240,0:35:31.300<br>
所以，如果我們回到 Max Pooling <br>
<br>
0:35:31.300,0:35:34.100<br>
Max Pooling跟 Maxout 是一模一樣的 operation 阿<br>
<br>
0:35:34.100,0:35:35.900<br>
只是換一個說法而已，對不對<br>
<br>
0:35:35.900,0:35:38.040<br>
所以，你會 train Maxout<br>
<br>
0:35:38.040,0:35:40.620<br>
你就會 train Max Pooling，這是一模一樣的作法<br>
<br>
0:35:40.620,0:35:43.700<br>
講到這邊大家有沒有甚麼問題呢？<br>
<br>
0:35:44.940,0:35:47.160<br>
沒有的話，那<br>
<br>
0:35:48.040,0:35:50.360<br>
另外一個我們要講的是這個<br>
<br>
0:35:50.360,0:35:52.260<br>
adaptive 的 learning rate<br>
<br>
0:35:52.260,0:35:55.860<br>
其實 adaptive 的 learning rate，我們之前已經有講過了<br>
<br>
0:35:55.860,0:35:59.080<br>
我們之前有講過這個 Adagrad<br>
<br>
0:35:59.280,0:36:00.960<br>
我們之前有講過 Adagrad<br>
<br>
0:36:00.960,0:36:03.280<br>
我們說 Adagrad 的做法就是<br>
<br>
0:36:03.280,0:36:07.780<br>
我們現在每一個 parameter 都要有不同的 learning rate<br>
<br>
0:36:07.780,0:36:11.220<br>
而這個 learning rate 是怎麼<br>
給它這麼 adaptive 的 learning rate 呢？<br>
<br>
0:36:11.220,0:36:13.160<br>
我們就把一個固定的 learning rate η<br>
<br>
0:36:13.160,0:36:19.900<br>
除掉這一個參數過去所有 gradient 值的平方和，開根號<br>
<br>
0:36:19.900,0:36:23.200<br>
把這項除以平方和開根號<br>
<br>
0:36:23.200,0:36:26.020<br>
就得到新的 parameter<br>
<br>
0:36:26.020,0:36:28.460<br>
那這個 Adagrad 它的精神就是說<br>
<br>
0:36:28.460,0:36:32.140<br>
如果我們今天考慮兩個參數，w1, w2<br>
<br>
0:36:32.140,0:36:35.640<br>
如果 w1 是在<br>
<br>
0:36:35.640,0:36:36.960<br>
這個方向上<br>
<br>
0:36:36.960,0:36:41.480<br>
如果 w1 在這個方向上，它平常 gradient 都比較小<br>
<br>
0:36:41.480,0:36:44.840<br>
那它是比較平坦的，給它比較大的 learning rate<br>
<br>
0:36:44.840,0:36:46.860<br>
反過來說，在這個方向上<br>
<br>
0:36:46.860,0:36:48.380<br>
平常 gradient 都是比較大的<br>
<br>
0:36:48.380,0:36:51.940<br>
所以，它是比較陡峭的，所以給它比較小的 learning rate<br>
<br>
0:36:55.800,0:36:58.660<br>
但是，實際上呢，我們面對的問題<br>
<br>
0:36:58.660,0:37:04.320<br>
有可能是比 Adagrad 可以處理的問題更加複雜的<br>
<br>
0:37:04.320,0:37:10.720<br>
也就是說，我們之前在做這個 Linear Regression 的時候<br>
<br>
0:37:10.720,0:37:13.680<br>
我們看到的這個 optimization 的 function<br>
<br>
0:37:13.680,0:37:15.660<br>
loss function 是這樣子 convex的形狀<br>
<br>
0:37:15.660,0:37:18.240<br>
但實際上，當我們在做 deep learning 的時候<br>
<br>
0:37:18.240,0:37:22.440<br>
這個 loss function 它可以是任何形狀，你知道嗎？<br>
<br>
0:37:22.440,0:37:24.800<br>
它可以是任何形狀<br>
<br>
0:37:24.800,0:37:28.420<br>
比如說，它可以是這樣，怪異的月形的形狀<br>
<br>
0:37:28.420,0:37:34.420<br>
如果當今天你的 error surface 是這個形狀的時候<br>
<br>
0:37:34.420,0:37:38.160<br>
那你會遇到的問題是，就算是同一個方向上<br>
<br>
0:37:38.160,0:37:42.720<br>
你的 learning rate 也比需要能夠快速地變動<br>
<br>
0:37:42.720,0:37:45.120<br>
就我們剛才在做 convex function 的時候<br>
<br>
0:37:45.120,0:37:46.400<br>
在每個方向上<br>
<br>
0:37:46.400,0:37:48.080<br>
這個方向很平坦，就一直很平坦<br>
<br>
0:37:48.080,0:37:50.260<br>
這個方向很陡峭，就一直很陡峭<br>
<br>
0:37:50.260,0:37:52.880<br>
但是，如果今天在更複雜的問題的時候<br>
<br>
0:37:52.880,0:37:56.040<br>
有可能，你考慮 w1 改變是在這個方向<br>
<br>
0:37:56.040,0:37:57.960<br>
在某個區域<br>
<br>
0:37:57.960,0:38:01.020<br>
它很平坦，所以它需要比較小的 learning rate<br>
<br>
0:38:01.020,0:38:03.200<br>
但是，到了另外一個區域<br>
<br>
0:38:03.200,0:38:04.740<br>
它又突然變得很陡峭<br>
<br>
0:38:04.740,0:38:07.520<br>
這個時候，它需要比較大的 learning rate<br>
<br>
0:38:07.520,0:38:11.680<br>
所以，真正要處理 deep learning 的問題，用 Adagrad<br>
<br>
0:38:11.680,0:38:16.420<br>
可能是不夠的，你需要更 dynamic 的調整<br>
<br>
0:38:16.420,0:38:18.100<br>
這個 learning rate 的方法<br>
<br>
0:38:18.100,0:38:21.820<br>
所以，這邊有一個 Adagrad 的進階膽，叫 RMSProp<br>
<br>
0:38:21.820,0:38:24.600<br>
RMSProp，我覺得是一個滿神奇的方法<br>
<br>
0:38:24.600,0:38:26.720<br>
因為你好像，找不到它的 paper<br>
<br>
0:38:26.720,0:38:28.420<br>
因為這個在 Hinton 的那個<br>
<br>
0:38:28.420,0:38:30.580<br>
MOOC 的 course 裡面，他提出來<br>
<br>
0:38:30.580,0:38:34.300<br>
他在他的線上課程裡面提出一個方法<br>
<br>
0:38:34.300,0:38:39.020<br>
大家要 cite 的時候，要 cite 那個線上課程的連結<br>
<br>
0:38:40.660,0:38:42.000<br>
這招還真的有用<br>
<br>
0:38:42.000,0:38:44.380<br>
這個 RMSProp 是這樣子做的<br>
<br>
0:38:44.900,0:38:48.080<br>
我們現在把<br>
<br>
0:38:48.080,0:38:50.580<br>
這個固定的 learning rate<br>
<br>
0:38:50.580,0:38:53.540<br>
除掉一個值，我們稱之為 σ<br>
<br>
0:38:53.540,0:38:55.900<br>
這個 σ 是甚麼呢？<br>
<br>
0:38:55.900,0:38:58.380<br>
在第一個時間點<br>
<br>
0:38:58.380,0:39:02.800<br>
這個 σ 就是你第一個算出來的 gradient 的值 g^0<br>
<br>
0:39:04.340,0:39:06.820<br>
那在第二個時間點呢？<br>
<br>
0:39:06.820,0:39:10.780<br>
在第二個時間點，你算出一個新的 gradient, g^1<br>
<br>
0:39:10.780,0:39:13.300<br>
這個時候<br>
<br>
0:39:13.300,0:39:15.740<br>
你的 σ 的值<br>
<br>
0:39:15.740,0:39:18.300<br>
新的 σ 的值，σ^1 呢<br>
<br>
0:39:18.300,0:39:25.400<br>
就是原來的 σ 值的平方，乘上 α<br>
<br>
0:39:25.400,0:39:29.160<br>
再加上新的 g 的值，(g^1)^2<br>
<br>
0:39:29.160,0:39:31.600<br>
再乘上 (1 - α)<br>
<br>
0:39:31.600,0:39:34.640<br>
而這個 α 的值是<br>
<br>
0:39:34.640,0:39:36.820<br>
你可以自由去調的<br>
<br>
0:39:36.820,0:39:39.680<br>
也就是我們原來在<br>
<br>
0:39:39.680,0:39:42.260<br>
或是我們再來看下一個例子<br>
<br>
0:39:42.260,0:39:43.360<br>
我們現在有一個<br>
<br>
0:39:43.360,0:39:45.400<br>
在下一個時間點，我們又算出 g^2<br>
<br>
0:39:45.400,0:39:47.400<br>
我們得到 σ^2<br>
<br>
0:39:47.400,0:39:50.500<br>
σ^2 怎麼算的呢？它是把原來的 σ^1<br>
<br>
0:39:50.500,0:39:52.600<br>
取平方乘上 α<br>
<br>
0:39:52.600,0:39:55.720<br>
再加上 (1 - α) 乘上 (g^2)^2<br>
<br>
0:39:55.720,0:39:58.080<br>
再開根號，得到這個 σ^2<br>
<br>
0:39:58.080,0:40:00.680<br>
那跟原來的 Adagrad 不一樣的地方是<br>
<br>
0:40:00.680,0:40:04.300<br>
原來的 Adagrad 你在這邊分母放的值<br>
<br>
0:40:04.300,0:40:08.140<br>
就是把 g^0, g^1, g^2 都取平方和開根號<br>
<br>
0:40:08.140,0:40:11.960<br>
但是，在這邊的時候<br>
<br>
0:40:11.960,0:40:14.740<br>
在 RMSProp 裡面呢，這個 σ^1<br>
<br>
0:40:14.740,0:40:17.260<br>
它裡面包含了 g^0 跟 g^1<br>
<br>
0:40:17.260,0:40:18.900<br>
那這邊也包含了 g^2<br>
<br>
0:40:18.900,0:40:21.620<br>
所以，它根號裡面也同樣包含了 g^0, g^1, g^2<br>
<br>
0:40:21.620,0:40:22.700<br>
就跟 Adagrad 一樣<br>
<br>
0:40:22.700,0:40:25.520<br>
但是，你現在可以給它乘上 weight, α<br>
<br>
0:40:25.520,0:40:27.900<br>
或者是 (1 - α)<br>
<br>
0:40:27.900,0:40:29.900<br>
所以，你可以調整說<br>
<br>
0:40:29.900,0:40:31.580<br>
我比較傾向<br>
<br>
0:40:31.580,0:40:32.860<br>
你可以調整這個 α 的值<br>
<br>
0:40:32.860,0:40:34.800<br>
這個 α 的值就也是像 learning rate 阿<br>
<br>
0:40:34.800,0:40:38.220<br>
也是你要手動設的值，當然你就設個 0.9 之類的<br>
<br>
0:40:38.220,0:40:40.780<br>
你可以手動去調這個<br>
<br>
0:40:40.780,0:40:43.600<br>
α 的值，讓它說<br>
<br>
0:40:43.600,0:40:47.740<br>
如果你把這個 α 的值設的小一點<br>
<br>
0:40:47.740,0:40:52.040<br>
那意思就是說，你傾向於相信新的 gradient<br>
<br>
0:40:52.040,0:40:54.500<br>
所告訴你的，這個 error surface<br>
<br>
0:40:54.500,0:40:56.860<br>
平滑或陡峭的程度<br>
<br>
0:40:56.860,0:40:59.200<br>
傾向於相信新的 gradient<br>
<br>
0:40:59.200,0:41:02.140<br>
比較無視於舊的 gradient 提供給你的 information<br>
<br>
0:41:02.140,0:41:06.060<br>
這樣大家應該可以瞭解這個結果<br>
<br>
0:41:06.060,0:41:09.300<br>
所以，在第 t 個時間點，你算出來的 σ<br>
<br>
0:41:09.300,0:41:11.740<br>
就是把 (σ^(t-1))^2 乘上 α<br>
<br>
0:41:11.740,0:41:15.040<br>
加上 (1 - α) 乘上在第 t 個時間點算出來的<br>
<br>
0:41:15.040,0:41:17.380<br>
gradient 的平方<br>
<br>
0:41:17.380,0:41:20.300<br>
所以，當你做 RMSProp 的時候<br>
<br>
0:41:20.300,0:41:23.320<br>
你一樣是在這算 gradient 的 zooming square<br>
<br>
0:41:23.320,0:41:26.200<br>
但是，你可以給<br>
<br>
0:41:26.200,0:41:28.400<br>
現在已經看到的 gradient 比較大的 weight<br>
<br>
0:41:28.400,0:41:30.920<br>
給過去看到的 gradient 比較小的 weight<br>
<br>
0:41:33.120,0:41:35.920<br>
除了 learning rate 的問題以外<br>
<br>
0:41:35.920,0:41:38.040<br>
我們知道說在做 deep learning 的時候<br>
<br>
0:41:38.040,0:41:40.700<br>
大家都會說，我們會卡在 local minimum<br>
<br>
0:41:40.700,0:41:43.500<br>
那我之前也有講過說，我們不見得是卡在 local minimum<br>
<br>
0:41:43.500,0:41:45.000<br>
也有可能卡在 saddle point<br>
<br>
0:41:45.000,0:41:48.420<br>
甚至，你有可能卡在 plateau 的地方<br>
<br>
0:41:48.420,0:41:51.360<br>
大家聽到這個問題都非常的擔心<br>
<br>
0:41:51.360,0:41:54.080<br>
覺得說，哇！這個做 deep learning 呢<br>
<br>
0:41:54.080,0:41:55.340<br>
是非常困難的<br>
<br>
0:41:55.340,0:41:57.580<br>
因為你可能胡亂做一下就一大推的問題<br>
<br>
0:41:57.780,0:41:59.060<br>
那其實呢<br>
<br>
0:41:59.060,0:42:01.340<br>
Yann LeCun 他在 07 年的時候<br>
<br>
0:42:01.340,0:42:04.100<br>
他有一個滿特別的說法<br>
<br>
0:42:04.100,0:42:05.880<br>
07 年的時候就講過這件事情<br>
<br>
0:42:05.880,0:42:10.200<br>
它說你不用擔心 local minimum 的問題<br>
<br>
0:42:10.200,0:42:14.080<br>
我不知道這件事情有多確切<br>
<br>
0:42:14.080,0:42:15.140<br>
我沒有 verify 過<br>
<br>
0:42:15.140,0:42:18.540<br>
但是，如果你有甚麼 verify 的結果的話<br>
<br>
0:42:18.540,0:42:20.120<br>
你可以跟我分享一下<br>
<br>
0:42:20.120,0:42:22.820<br>
Yann LeCun 的說法，他是這樣說的，他說<br>
<br>
0:42:22.820,0:42:25.920<br>
其實，在這個 error surface 上<br>
<br>
0:42:25.920,0:42:27.200<br>
沒有太多 local minimum<br>
<br>
0:42:27.200,0:42:28.940<br>
所以，你不用太擔心<br>
<br>
0:42:28.940,0:42:31.080<br>
為甚麼呢？他說<br>
<br>
0:42:31.080,0:42:33.660<br>
你要是一個 local minimum<br>
<br>
0:42:33.660,0:42:34.980<br>
你在每一個 dimension<br>
<br>
0:42:34.980,0:42:37.540<br>
都必須要是這樣子的形狀對不對？<br>
<br>
0:42:37.540,0:42:40.740<br>
都要是一個山谷的谷底<br>
<br>
0:42:40.740,0:42:42.720<br>
每一個 dimension 都要是山谷的谷底<br>
<br>
0:42:42.720,0:42:46.380<br>
我們假設這個山谷的谷底出現的機率是 p 好了<br>
<br>
0:42:46.380,0:42:47.860<br>
山谷的谷底出現的機率是 p<br>
<br>
0:42:47.860,0:42:51.060<br>
因為我們的 network 有非常非常多的參數<br>
<br>
0:42:51.060,0:42:53.320<br>
所以，假設有 1000 個參數<br>
<br>
0:42:53.320,0:42:55.380<br>
你每一個參數都要是山谷的谷底<br>
<br>
0:42:55.380,0:42:57.440<br>
那機率就是 p^1000<br>
<br>
0:42:57.440,0:42:59.460<br>
你的 network 越大<br>
<br>
0:42:59.460,0:43:02.100<br>
參數越多，這個出現的機率就越低<br>
<br>
0:43:02.100,0:43:03.920<br>
所以呢，local minimum<br>
<br>
0:43:03.920,0:43:05.740<br>
在一個很大的 neural network 裡面<br>
<br>
0:43:05.740,0:43:07.600<br>
其實沒有你想像的那麼多<br>
<br>
0:43:07.600,0:43:10.020<br>
一個很大的 neural network，它看起來其實是<br>
<br>
0:43:10.020,0:43:13.260<br>
其實搞不好是很平滑的，根本沒有太多 local minimum<br>
<br>
0:43:13.260,0:43:16.480<br>
所以，當你走走走，走到一個你覺得是<br>
<br>
0:43:16.480,0:43:18.480<br>
local minimum 的地方，卡住的時候<br>
<br>
0:43:18.480,0:43:22.260<br>
它八成就是 local minimum，或是很接近 local minimum<br>
<br>
0:43:22.260,0:43:25.160<br>
給大家參考<br>
<br>
0:43:25.160,0:43:29.120<br>
那你有甚麼特別的想法，再告訴我<br>
<br>
0:43:31.500,0:43:34.820<br>
有一個 heuristic 的方法<br>
<br>
0:43:34.820,0:43:38.000<br>
可以稍微處理一下，我們上述說的<br>
<br>
0:43:38.000,0:43:41.360<br>
我們剛才講的 local minimum 還有 plateau 的問題<br>
<br>
0:43:41.360,0:43:46.120<br>
這個方法，你可以說是從真實的世界，得到一些靈感<br>
<br>
0:43:46.120,0:43:48.160<br>
我們知道在真實的世界裡面<br>
<br>
0:43:48.160,0:43:51.780<br>
如果這個是一個地形，是一個山坡<br>
<br>
0:43:51.780,0:43:56.080<br>
你把一個球從左上角丟下來，把它滾下來<br>
<br>
0:43:56.080,0:43:58.840<br>
然後，它滾滾滾，它滾到 plateau 的地方呢<br>
<br>
0:43:58.840,0:44:02.880<br>
它不會停下來阿，因為有慣性嘛，它還會繼續往前<br>
<br>
0:44:02.880,0:44:06.620<br>
它就算是走到上坡的地方，假設這個波沒有很陡<br>
<br>
0:44:06.620,0:44:07.880<br>
因為慣性的關係<br>
<br>
0:44:07.880,0:44:10.500<br>
它搞不好走走走，還是可以翻過這個山坡<br>
<br>
0:44:10.500,0:44:12.180<br>
結果它就可以走到了<br>
<br>
0:44:12.660,0:44:15.800<br>
比這個 local minimum 還要好的地方<br>
<br>
0:44:15.800,0:44:17.580<br>
那所以我們<br>
<br>
0:44:17.580,0:44:19.040<br>
要做的事情就是把<br>
<br>
0:44:19.040,0:44:21.480<br>
這個慣性這個特性呢<br>
<br>
0:44:21.480,0:44:23.580<br>
塞到 Gradient Descent 裡面去<br>
<br>
0:44:23.580,0:44:28.800<br>
那這件事情，就叫做 momentum<br>
<br>
0:44:29.540,0:44:32.880<br>
這個東西怎麼做呢？我們先很快地秒複習一下<br>
<br>
0:44:32.880,0:44:34.680<br>
一般的 Gradient Descent<br>
<br>
0:44:34.680,0:44:37.520<br>
一般的 Gradient Descent 是怎麼做的呢？<br>
<br>
0:44:37.520,0:44:38.800<br>
我們是這樣子做的<br>
<br>
0:44:38.800,0:44:40.840<br>
這個是選一個初始的值<br>
<br>
0:44:40.840,0:44:43.100<br>
然後，計算一下它的 gradient<br>
<br>
0:44:43.100,0:44:44.620<br>
它的 gradient 是這個方向<br>
<br>
0:44:44.620,0:44:46.920<br>
那我們就走 gradient 的反方向<br>
<br>
0:44:46.920,0:44:49.180<br>
乘上一個 learning rate η<br>
<br>
0:44:49.180,0:44:51.340<br>
得到 θ^1，再算 gradient<br>
<br>
0:44:51.340,0:44:52.820<br>
再走一個新的方向<br>
<br>
0:44:52.820,0:44:55.140<br>
再算 gradient、再走一個方向；<br>
再算 gradient、再走一個方向<br>
<br>
0:44:55.140,0:44:55.860<br>
以此類推<br>
<br>
0:44:55.860,0:44:58.760<br>
一直到 gradient = 0 的時候，或 gradient 趨近 0 的時候<br>
<br>
0:44:58.760,0:45:00.100<br>
我們就停止<br>
<br>
0:45:00.100,0:45:03.020<br>
當我們加上 momentum 的時候我們是怎麼做的呢？<br>
<br>
0:45:03.020,0:45:05.780<br>
當我們加上 momentum 的時候<br>
<br>
0:45:05.780,0:45:09.880<br>
我們每一次移動的方向<br>
<br>
0:45:09.880,0:45:13.300<br>
不再是只有考慮 gradient<br>
<br>
0:45:14.000,0:45:19.520<br>
而是我們現在的 gradient<br>
<br>
0:45:19.520,0:45:22.900<br>
加上在前一個時間點<br>
<br>
0:45:22.900,0:45:23.960<br>
移動的方向<br>
<br>
0:45:23.960,0:45:26.160<br>
這樣聽起來可能很抽象，所以<br>
<br>
0:45:26.160,0:45:28.600<br>
我們實際地來看一下，它是怎麼運作的<br>
<br>
0:45:28.600,0:45:31.140<br>
一樣選一個初始值 θ^0<br>
<br>
0:45:31.140,0:45:33.120<br>
一樣選一個初始值 θ^0<br>
<br>
0:45:33.540,0:45:39.420<br>
然後，我們用一個值 v 去記錄<br>
<br>
0:45:39.420,0:45:41.380<br>
我們在前一個時間點<br>
<br>
0:45:41.380,0:45:42.800<br>
移動的方向<br>
<br>
0:45:42.800,0:45:45.500<br>
v 記錄我們前一個時間點移動的方向<br>
<br>
0:45:45.500,0:45:48.340<br>
因為現在是初始值，之前沒有移動過，所以<br>
<br>
0:45:48.340,0:45:50.900<br>
前一個時間點移動的方向是 0<br>
<br>
0:45:50.900,0:45:53.980<br>
接下來計算在 θ^0 地方的 gradient<br>
<br>
0:45:53.980,0:45:56.420<br>
現在算出 θ^0 的 gradient<br>
<br>
0:45:56.420,0:45:58.380<br>
算出來是紅色這個箭頭<br>
<br>
0:45:59.620,0:46:02.880<br>
然後，我們現在要移動的方向<br>
<br>
0:46:02.880,0:46:05.100<br>
並不是紅色箭頭告訴我們的方向<br>
<br>
0:46:05.100,0:46:07.820<br>
而是，前一個時間點的<br>
<br>
0:46:07.820,0:46:09.280<br>
movement v^0<br>
<br>
0:46:09.280,0:46:13.340<br>
再加上 negative 的 gradient<br>
<br>
0:46:13.600,0:46:17.500<br>
然後，我們得到現在要移動的方向 v^1<br>
<br>
0:46:17.500,0:46:19.620<br>
所以，到這邊就好像是慣性一樣<br>
<br>
0:46:19.620,0:46:24.540<br>
如果我們之前走的方向是 v^0<br>
<br>
0:46:25.200,0:46:28.140<br>
那今天有一個新的 gradient，並不會<br>
<br>
0:46:28.140,0:46:31.380<br>
讓你參數 update 的方向完全轉向<br>
<br>
0:46:31.380,0:46:33.960<br>
它會改變你的方向<br>
<br>
0:46:33.960,0:46:36.200<br>
但是，因為有慣性的關係，所以<br>
<br>
0:46:36.200,0:46:39.520<br>
原來走的方向還是有一定程度的影響<br>
<br>
0:46:39.520,0:46:43.100<br>
那我們或許看下一個例子，會比較清楚<br>
<br>
0:46:43.100,0:46:45.800<br>
我們現在在上一個時間點移動的方向呢<br>
<br>
0:46:45.800,0:46:47.940<br>
是 v^1<br>
<br>
0:46:49.180,0:46:52.720<br>
接下來，再計算一下 gradient<br>
<br>
0:46:52.720,0:46:55.860<br>
計算一下 gradient，就是紅色的箭頭<br>
<br>
0:46:55.860,0:46:59.380<br>
接下來要決定說，在第二個時間點<br>
<br>
0:46:59.380,0:47:01.180<br>
我們要走的方向是甚麼樣？<br>
<br>
0:47:01.180,0:47:03.260<br>
第二個時間點要走的方向是<br>
<br>
0:47:03.260,0:47:05.360<br>
過去走的方向 v^1<br>
<br>
0:47:05.360,0:47:08.240<br>
減掉 leaning rate 乘上 gradient<br>
<br>
0:47:08.240,0:47:11.300<br>
如果我們看這個圖上的話，gradient 會告訴我們說<br>
<br>
0:47:12.140,0:47:13.840<br>
要走這個方向<br>
<br>
0:47:13.840,0:47:17.040<br>
負的 η 乘上 gradient，要走這個方向<br>
<br>
0:47:17.040,0:47:19.300<br>
但是，前面的 movement<br>
<br>
0:47:19.300,0:47:21.840<br>
是綠色的箭頭，它是這個方向<br>
<br>
0:47:21.840,0:47:22.940<br>
這個方向<br>
<br>
0:47:22.940,0:47:25.000<br>
我們會把這個 movement 乘上一個 λ<br>
<br>
0:47:25.000,0:47:26.360<br>
那這個 λ 其實也是一個<br>
<br>
0:47:26.360,0:47:28.940<br>
跟 learning rate 一樣，是手要調的參數<br>
<br>
0:47:28.940,0:47:32.660<br>
它告訴你說，現在這個慣性這件事情，影響力有多大<br>
<br>
0:47:32.660,0:47:34.560<br>
λ 大的話，慣性影響力就大<br>
<br>
0:47:34.560,0:47:36.220<br>
λ 小的話，慣性影響力就大<br>
<br>
0:47:36.220,0:47:38.620<br>
總之，慣性告訴我們走這邊<br>
<br>
0:47:38.620,0:47:40.280<br>
gradient 告訴我們走這邊<br>
<br>
0:47:40.280,0:47:43.980<br>
這兩個合起來呢，就走了一個新的方向，就是這邊<br>
<br>
0:47:43.980,0:47:46.620<br>
這個就是 v2，所以，以此類推<br>
<br>
0:47:46.620,0:47:49.020<br>
新的 gradient 告訴我們走這邊<br>
<br>
0:47:49.020,0:47:50.580<br>
慣性告訴我們<br>
<br>
0:47:50.580,0:47:53.820<br>
新的 gradient 告訴我們走紅色這個虛線的方向<br>
<br>
0:47:53.820,0:47:56.460<br>
慣性告訴我們走綠色虛線的方向<br>
<br>
0:47:56.460,0:47:58.740<br>
合起來最後就是走藍色的方向<br>
<br>
0:47:59.360,0:48:01.080<br>
然後，update 參數以後<br>
<br>
0:48:01.080,0:48:03.040<br>
gradient 告訴我們走紅色這個虛線的方向<br>
<br>
0:48:03.040,0:48:06.360<br>
慣性告訴我們走綠色虛線的方向<br>
<br>
0:48:06.360,0:48:08.780<br>
合起來就是走藍色的方向<br>
<br>
0:48:10.060,0:48:14.020<br>
那你可以用另外一個方法來理解這件事情<br>
<br>
0:48:14.020,0:48:16.280<br>
其實，v^i<br>
<br>
0:48:16.280,0:48:20.040<br>
你在每一個時間點移動的 movement<br>
<br>
0:48:20.040,0:48:25.320<br>
你在第 i 個時間點移動的步伐 v^i 呢<br>
<br>
0:48:25.320,0:48:28.760<br>
移動的量、方向，v^i 呢<br>
<br>
0:48:28.760,0:48:33.480<br>
其實就是過去所有算出來的 gradient 的總和<br>
<br>
0:48:33.480,0:48:35.840<br>
為甚麼這麼說呢？<br>
<br>
0:48:35.840,0:48:38.480<br>
我們知道 v^0 = 0<br>
<br>
0:48:39.140,0:48:41.440<br>
v^1 呢，v^1 在這邊<br>
<br>
0:48:41.440,0:48:45.820<br>
v^1 是 λ*(v^0) - η*(θ^0) 的 gradient<br>
<br>
0:48:45.820,0:48:47.260<br>
而 v^0 = 0<br>
<br>
0:48:47.260,0:48:51.760<br>
所以，v^1 = -η * gradient<br>
<br>
0:48:51.760,0:48:52.920<br>
所以是這個樣子<br>
<br>
0:48:52.920,0:48:56.520<br>
那 v^2 呢，v^2 我們就把<br>
<br>
0:48:56.520,0:48:58.900<br>
v^1 是負的 gradient<br>
<br>
0:48:58.900,0:49:04.800<br>
v^1 是負的 η * gradient 代進去<br>
<br>
0:49:04.800,0:49:07.980<br>
我們把 v^1 代到這邊，再乘上 λ<br>
<br>
0:49:07.980,0:49:10.320<br>
再減掉 η，乘以 θ^1 的 gradient<br>
<br>
0:49:10.320,0:49:11.560<br>
得到結果就是這樣<br>
<br>
0:49:11.560,0:49:14.920<br>
你得到的結果就是你把 θ^0 的地方的 gradient<br>
<br>
0:49:14.920,0:49:16.660<br>
減掉 λ * η<br>
<br>
0:49:16.660,0:49:20.580<br>
再減掉 η * θ^1 的 gradient<br>
<br>
0:49:20.580,0:49:23.160<br>
你得到 v^2<br>
<br>
0:49:23.160,0:49:27.020<br>
所以，v^2 裡面同時有在 θ^0 算出來的 gradient<br>
<br>
0:49:27.020,0:49:29.100<br>
同時有在 θ^1 的地方算出來的 gradient<br>
<br>
0:49:29.100,0:49:32.700<br>
只是這兩個 gradient，它的 weight 是不一樣的<br>
<br>
0:49:32.700,0:49:35.620<br>
如果你 λ 都設小於 0 的值的話呢<br>
<br>
0:49:35.620,0:49:40.340<br>
越之前的 gradient，它的 weight 就越小<br>
<br>
0:49:40.340,0:49:42.720<br>
越之前的 gradient，就越不去理它<br>
<br>
0:49:42.720,0:49:45.880<br>
你越在意現在的 gradient，但是過去的 gradient<br>
<br>
0:49:45.880,0:49:50.260<br>
也會對你現在要 update 的方向有一定程度的影響力<br>
<br>
0:49:50.260,0:49:53.240<br>
這個，就是 momentum<br>
<br>
0:49:53.240,0:49:56.880<br>
如果你看數學式子不太喜歡的話<br>
<br>
0:49:56.880,0:49:58.800<br>
那我們就從直覺上來看一下<br>
<br>
0:49:58.800,0:50:01.760<br>
到底加入 Momentum 以後，是怎麼運作的<br>
<br>
0:50:02.060,0:50:03.880<br>
在加入 Momentum 以後呢<br>
<br>
0:50:03.880,0:50:06.220<br>
每一次移動的方向<br>
<br>
0:50:06.220,0:50:13.000<br>
是 negative 的 gradient 加上 momentum<br>
<br>
0:50:13.000,0:50:14.720<br>
建議我們要走的方向<br>
<br>
0:50:14.720,0:50:18.440<br>
Momentum 其實就是上一個時間點的 movement<br>
<br>
0:50:19.880,0:50:22.780<br>
所以，現在假設我們有一個<br>
<br>
0:50:22.780,0:50:25.720<br>
假設我們初始的參數是在這個位置<br>
<br>
0:50:25.720,0:50:28.280<br>
那 gradient 建議我們往右走<br>
<br>
0:50:28.280,0:50:30.800<br>
所以，最後就往右移動<br>
<br>
0:50:31.300,0:50:34.000<br>
那如果說之後移到這個位置<br>
<br>
0:50:34.000,0:50:37.620<br>
gradient 建議我們往右走<br>
<br>
0:50:37.900,0:50:41.780<br>
而 momentum 也會建議我們往右走<br>
<br>
0:50:41.780,0:50:44.600<br>
因為我們是從左邊這邊移過來的嘛<br>
<br>
0:50:44.600,0:50:47.580<br>
所以，前一個步伐我們是向右的<br>
<br>
0:50:47.580,0:50:49.700<br>
如果你考慮 momentum 的時候呢<br>
<br>
0:50:49.700,0:50:50.780<br>
我們也會向右<br>
<br>
0:50:50.780,0:50:53.320<br>
所以，你把 gradient 建議我們走的方向<br>
<br>
0:50:53.320,0:50:55.220<br>
跟 momentum 建議我們走的方向合起來<br>
<br>
0:50:55.220,0:50:57.260<br>
你就得到這個藍色的線<br>
<br>
0:50:58.000,0:51:00.900<br>
所以你會繼續向右，如果我們今天走到<br>
<br>
0:51:00.900,0:51:02.740<br>
local minimum 的地方<br>
<br>
0:51:02.740,0:51:05.040<br>
走到 local minimum 的地方，gradient 是 0<br>
<br>
0:51:05.040,0:51:07.860<br>
所以，gradient 會告訴你說就停在這裡吧<br>
<br>
0:51:07.860,0:51:11.480<br>
但是，momentum 會告訴你說，之前是從右邊走過來的<br>
<br>
0:51:11.480,0:51:13.580<br>
所以，你仍然應該要繼續往右走<br>
<br>
0:51:13.580,0:51:15.800<br>
也就是綠色箭頭的方向<br>
<br>
0:51:15.800,0:51:17.960<br>
所以，最後你參數 update 的方向<br>
<br>
0:51:17.960,0:51:19.520<br>
仍然會繼續向右<br>
<br>
0:51:19.520,0:51:22.040<br>
甚至你可以樂觀地期待說<br>
<br>
0:51:22.040,0:51:25.420<br>
如果今天在往右的時候，走到這個地方<br>
<br>
0:51:25.420,0:51:28.220<br>
gradient 要求我們向左走<br>
<br>
0:51:28.220,0:51:31.500<br>
現在左邊如果是算微分的話呢<br>
<br>
0:51:31.500,0:51:32.980<br>
如果考慮 gradient 的話呢<br>
<br>
0:51:32.980,0:51:35.600<br>
參數應該往左移動<br>
<br>
0:51:35.600,0:51:37.600<br>
但是，momentum 建議我們<br>
<br>
0:51:37.600,0:51:39.120<br>
繼續向右走<br>
<br>
0:51:39.120,0:51:41.700<br>
因為你是從左邊過來的<br>
<br>
0:51:41.700,0:51:43.200<br>
因為你是從左向右過來的<br>
<br>
0:51:43.200,0:51:45.140<br>
所以 momentum 建議你繼續向右走<br>
<br>
0:51:45.140,0:51:47.360<br>
如果今天 momentum 其實比較強的話<br>
<br>
0:51:47.360,0:51:50.560<br>
你最後，就還是會向右走<br>
<br>
0:51:50.560,0:51:52.880<br>
所以，你有一定的可能性，你可以<br>
<br>
0:51:52.880,0:51:55.420<br>
有可能你可以跳出<br>
<br>
0:51:55.420,0:51:57.760<br>
local minimum，如果這個 local minimum 不深的話<br>
<br>
0:51:57.760,0:52:01.720<br>
你有可能藉由慣性的力量呢<br>
<br>
0:52:01.720,0:52:06.280<br>
跳出 local minimum，然後走到比較好的 global minimum<br>
<br>
0:52:06.280,0:52:08.360<br>
比較低的 local minimum<br>
<br>
0:52:08.820,0:52:13.060<br>
那如果你今天把 RMSProp 加上 momentum 的話<br>
<br>
0:52:13.060,0:52:15.860<br>
其實，你就得到 Adam 這樣<br>
<br>
0:52:15.860,0:52:17.560<br>
現在如果你沒有甚麼 prefer 的話<br>
<br>
0:52:17.560,0:52:20.020<br>
你就先學 Adam 就是了，那我發現在<br>
<br>
0:52:20.020,0:52:23.140<br>
作業二裡面，其實還滿多人 implement Adam 的<br>
<br>
0:52:23.140,0:52:26.400<br>
大家太強了，都自己 implement Adam 在作業二<br>
<br>
0:52:26.400,0:52:28.640<br>
我想這些你們都很熟了<br>
<br>
0:52:28.640,0:52:30.320<br>
沒什麼特別好講的<br>
<br>
0:52:30.320,0:52:33.400<br>
你可能看這個 algorithm<br>
<br>
0:52:33.400,0:52:35.780<br>
哇！感覺好像有點複雜<br>
<br>
0:52:35.780,0:52:39.380<br>
但是，好多人 implement 這個東西<br>
<br>
0:52:39.380,0:52:46.140<br>
其實，Adam 就是 RMSProp 加上 momentum<br>
<br>
0:52:46.140,0:52:48.940<br>
這兩個東西綜合起來，就是 Adam<br>
<br>
0:52:48.940,0:52:52.960<br>
我們非常非常快地來看一下這個式子<br>
<br>
0:52:52.960,0:52:54.740<br>
在這個式子裡面呢<br>
<br>
0:52:54.740,0:52:57.640<br>
在這個式子裡面，一開始要先初始一個東西叫做 m0<br>
<br>
0:52:57.640,0:53:00.280<br>
m0 就是 momentum<br>
<br>
0:53:00.280,0:53:02.420<br>
就是前一個時間點的 movement<br>
<br>
0:53:02.420,0:53:05.860<br>
那這邊有另外一個值叫做 v0<br>
<br>
0:53:05.860,0:53:09.800<br>
v0 就是我們剛才在 RMSProp 裡面看到的那個 σ<br>
<br>
0:53:09.800,0:53:13.600<br>
這個東西就是之前的 gradient 的 zooming square<br>
<br>
0:53:13.600,0:53:17.720<br>
之前算出來的 gradient 的平方和<br>
<br>
0:53:17.720,0:53:19.460<br>
就是 v0<br>
<br>
0:53:19.460,0:53:23.800<br>
你看，它先算一下 gradient，就是 gt<br>
<br>
0:53:24.380,0:53:27.080<br>
然後，根據 gt 呢<br>
<br>
0:53:27.080,0:53:32.380<br>
你就可以算出 mt，也就是現在要走的方向<br>
<br>
0:53:32.380,0:53:35.420<br>
現在要走的方向，是考慮過去要走的方向<br>
<br>
0:53:35.420,0:53:36.920<br>
再加上 gradient<br>
<br>
0:53:36.920,0:53:42.640<br>
接下來，算一下要放在分母的地方的 vt<br>
<br>
0:53:42.640,0:53:46.520<br>
這個 vt 是過去、前一個時間點的 vt<br>
<br>
0:53:46.520,0:53:49.780<br>
加上 gradient 的平方，等一下要開根號<br>
<br>
0:53:49.780,0:53:51.260<br>
這邊呢，它做了一個<br>
<br>
0:53:51.260,0:53:55.160<br>
跟原來 RMSProp 跟 momentum 裡面沒有的東西<br>
<br>
0:53:55.160,0:53:58.160<br>
叫做 bias correction<br>
<br>
0:53:58.160,0:54:01.520<br>
它會把 mt 跟 vt 都除上一個值<br>
<br>
0:54:01.520,0:54:04.000<br>
都除上一個值，那這個值本來比較大<br>
<br>
0:54:04.000,0:54:05.260<br>
那後來呢<br>
<br>
0:54:05.260,0:54:09.560<br>
這個值本來比較小，那後來呢<br>
<br>
0:54:09.560,0:54:11.820<br>
會越來越接近 1<br>
<br>
0:54:11.820,0:54:15.620<br>
至於為甚麼要這麼做，<br>
他的 paper 裡面會告訴你他的理由<br>
<br>
0:54:16.100,0:54:18.180<br>
最後，你在 update 的時候<br>
<br>
0:54:18.180,0:54:22.760<br>
你把 momentum 建議你的方向，mt\head<br>
<br>
0:54:22.760,0:54:29.400<br>
去乘上 learning rate α，再除掉 RMSProp<br>
<br>
0:54:29.400,0:54:34.080<br>
就是 RMSProp normalize 以後，建議的 learning rate<br>
<br>
0:54:34.080,0:54:35.940<br>
最後，得到你 update 的方向<br>
<br>
0:54:35.940,0:54:37.680<br>
這個就是 Adam 這樣<br>
<br>
0:54:37.680,0:54:39.680<br>
那我猜你應該沒有聽得太懂<br>
<br>
0:54:39.680,0:54:42.240<br>
不過沒有關係，因為在 toolkit 裡面，只是打幾個<br>
<br>
0:54:42.240,0:54:45.000<br>
指令而已，我們就先在這邊休息 10 分鐘<br>
<br>
0:54:45.000,0:54:46.740<br>
等一下再繼續，謝謝<br>
<br>
0:54:56.680,0:54:59.220<br>
我們來上課吧<br>
<br>
0:54:59.220,0:55:02.800<br>
我們剛才講的，就是說<br>
<br>
0:55:02.800,0:55:08.120<br>
如果今天你在 training data 上的結果不好的話怎麼辦<br>
<br>
0:55:08.120,0:55:10.000<br>
那等一下我要講的呢？<br>
<br>
0:55:10.000,0:55:14.340<br>
如果今天你已經在 training data 上得到夠好的結果<br>
<br>
0:55:14.340,0:55:17.740<br>
但是，你在 testing data 上的結果仍然不好<br>
<br>
0:55:17.740,0:55:19.780<br>
那你有甚麼可行的方法<br>
<br>
0:55:19.780,0:55:22.640<br>
等一下會很快介紹 3 個方法<br>
<br>
0:55:22.640,0:55:24.060<br>
一個是 Early Stopping<br>
<br>
0:55:24.060,0:55:25.380<br>
Regularization 跟 Dropout<br>
<br>
0:55:25.380,0:55:29.040<br>
Early Stopping 跟 Regularization 是很 typical 的作法<br>
<br>
0:55:29.040,0:55:32.380<br>
他們不是 specific design for deep learning 的<br>
<br>
0:55:32.380,0:55:35.000<br>
這是一個很傳統、typical 的作法<br>
<br>
0:55:35.000,0:55:38.380<br>
那 Dropout 是一個滿有 deep learning 特色的做法<br>
<br>
0:55:38.380,0:55:41.060<br>
那在講 deep learning 的時候，需要講一下<br>
<br>
0:55:41.620,0:55:45.120<br>
我們來講一下 Early Stopping，<br>
Early Stopping 是甚麼意思呢？<br>
<br>
0:55:45.220,0:55:48.780<br>
我們知道說，隨這你的 training<br>
<br>
0:55:48.780,0:55:53.180<br>
你的 total loss，如果你今天的 learning rate 調的對的話<br>
<br>
0:55:53.180,0:55:54.940<br>
你的 total loss 通常會越來越小<br>
<br>
0:55:54.940,0:55:58.640<br>
那有可能你 rate 沒有設好，loss 變大也是有可能的<br>
<br>
0:55:58.640,0:56:01.180<br>
那你想像 learning rate 調的很好的話<br>
<br>
0:56:01.180,0:56:03.940<br>
那你在 training set 上的 loss 應該是逐漸變小的<br>
<br>
0:56:04.220,0:56:05.600<br>
但是，因為我們知道說<br>
<br>
0:56:05.600,0:56:07.760<br>
training set 跟 testing set 他們的 distribution<br>
<br>
0:56:07.760,0:56:09.020<br>
並不完全一樣<br>
<br>
0:56:09.020,0:56:13.280<br>
所以，有可能當你的 training 的 loss 逐漸減小的時候<br>
<br>
0:56:13.280,0:56:17.620<br>
你的 testing data 的 loss 卻反而上升了<br>
<br>
0:56:17.620,0:56:18.640<br>
這是有可能的<br>
<br>
0:56:18.640,0:56:23.520<br>
所以，理想上，假如你知道 testing data 的 loss 的變化<br>
<br>
0:56:23.520,0:56:27.960<br>
你應該停在不是 training set 的 loss 最小<br>
<br>
0:56:27.960,0:56:31.100<br>
而是 testing set 的 loss 最小的地方<br>
<br>
0:56:31.100,0:56:33.380<br>
你在 train 的時候，你不要一直 train 下去<br>
<br>
0:56:33.380,0:56:36.380<br>
你可能 train 到這個地方的時候，就停下來了<br>
<br>
0:56:36.380,0:56:39.740<br>
但是實際上，我們不知道 testing set 阿<br>
<br>
0:56:39.740,0:56:42.780<br>
你根本不知道你 testing set 的 error 是甚麼阿<br>
<br>
0:56:42.780,0:56:46.160<br>
所以，我們其實會用 validation set<br>
<br>
0:56:46.160,0:56:48.740<br>
來 verify 這件事情<br>
<br>
0:56:49.580,0:56:52.280<br>
所以，有的地方我可能需要稍微講得清楚一點<br>
<br>
0:56:52.280,0:56:55.500<br>
就是這邊的 testing set 阿，並不是指<br>
<br>
0:56:55.500,0:56:57.160<br>
真正的 testing set<br>
<br>
0:56:57.160,0:56:59.960<br>
它指的是你有 label data 的 testing set<br>
<br>
0:56:59.960,0:57:02.520<br>
比如說，如果你今天是在做作業的時候<br>
<br>
0:57:02.520,0:57:04.780<br>
這邊的 testing set 可能指的是<br>
<br>
0:57:04.780,0:57:06.120<br>
Kaggle 上的 public set<br>
<br>
0:57:06.120,0:57:10.760<br>
或者是，你自己切出來的 validation set<br>
<br>
0:57:10.760,0:57:13.760<br>
希望大家可以知道我的意思<br>
<br>
0:57:13.760,0:57:15.580<br>
這個只是名詞用的不同而已<br>
<br>
0:57:17.420,0:57:22.340<br>
但是，你不會知道真正的 testing set 的變化<br>
<br>
0:57:22.340,0:57:25.220<br>
所以，其實我們會切一個 validation set<br>
<br>
0:57:25.220,0:57:26.320<br>
來 verify 說<br>
<br>
0:57:26.320,0:57:29.380<br>
甚麼時候用 validation set 模擬 testing set<br>
<br>
0:57:29.380,0:57:30.360<br>
來看說甚麼時候呢<br>
<br>
0:57:30.360,0:57:32.560<br>
這個 validation set 的 loss 最小的時候<br>
<br>
0:57:32.560,0:57:34.360<br>
你的 training 就停下來<br>
<br>
0:57:34.360,0:57:37.360<br>
那其實在 Kaggle 裡面，就可以支援你做這件事啊<br>
<br>
0:57:37.360,0:57:40.820<br>
所以，你就自己看一下 documentation<br>
<br>
0:57:40.820,0:57:45.140<br>
那 Regularization 是甚麼呢？<br>
<br>
0:57:45.140,0:57:51.400<br>
我們重新定義了那個我們要去 minimize 的 loss function<br>
<br>
0:57:51.400,0:57:55.620<br>
我們原來有一個 loss function<br>
<br>
0:57:55.620,0:57:58.020<br>
我們要 minimize 的 loss function<br>
<br>
0:57:58.020,0:58:00.540<br>
是 define 在你的 training data 上的<br>
<br>
0:58:00.540,0:58:04.560<br>
比如說要 minimize square error <br>
或 minimize cross entropy<br>
<br>
0:58:04.560,0:58:06.480<br>
那在做 Regularization 的時候呢<br>
<br>
0:58:06.480,0:58:09.820<br>
我們會加另外一個 Regularization 的 term<br>
<br>
0:58:09.820,0:58:13.360<br>
這個 Regularization 的 term 呢<br>
<br>
0:58:13.360,0:58:15.380<br>
這個 Regularization 的 term 呢<br>
<br>
0:58:15.380,0:58:18.620<br>
比如說，它可以是你的參數的 L2 norm<br>
<br>
0:58:18.620,0:58:19.880<br>
甚麼意思呢？<br>
<br>
0:58:19.880,0:58:22.960<br>
假設現在我們的參數 θ 裡面，它是<br>
<br>
0:58:22.960,0:58:26.080<br>
一群參數，w1, w2 等等有一大堆的參數<br>
<br>
0:58:26.080,0:58:29.340<br>
那這個 θ 的 L2 norm 呢，就是<br>
<br>
0:58:29.340,0:58:33.700<br>
你把你的 model 裡面的每一個參數都取平方<br>
<br>
0:58:33.700,0:58:37.640<br>
然後加起來，就是這個 θ2<br>
<br>
0:58:37.640,0:58:41.300<br>
那因為我們現在用 L2 norm 來做 Regularization<br>
<br>
0:58:41.300,0:58:44.640<br>
所以，這件事稱之為 L2 的 Regularization<br>
<br>
0:58:44.640,0:58:48.140<br>
那我們之前有講過說，在做 Regularization 的時候呢<br>
<br>
0:58:48.140,0:58:52.500<br>
一般我們是不會考慮 bias 這項<br>
<br>
0:58:52.500,0:58:56.180<br>
因為我們之前有講過說，加 Regularization  的目的<br>
<br>
0:58:56.180,0:58:59.840<br>
是為了要讓我們的 function 更平滑<br>
<br>
0:58:59.840,0:59:01.320<br>
而 bias 這件事情<br>
<br>
0:59:01.320,0:59:04.260<br>
通常跟 function 的平滑程度是沒有關係的<br>
<br>
0:59:04.260,0:59:07.060<br>
所以，通常我們在算 Regularization 的時候<br>
<br>
0:59:07.060,0:59:09.060<br>
不會把 bias 考慮進來<br>
<br>
0:59:10.580,0:59:13.340<br>
那如果我們把<br>
<br>
0:59:13.340,0:59:15.980<br>
L2 的 Regularization 放在這邊<br>
<br>
0:59:15.980,0:59:18.780<br>
我們會得到怎麼樣的結果呢<br>
<br>
0:59:18.780,0:59:21.060<br>
如果做微分的話，會得到怎麼樣的結果呢<br>
<br>
0:59:21.060,0:59:23.840<br>
如果我們把這個新的 objective function<br>
<br>
0:59:23.840,0:59:26.160<br>
我們把新的這個 loss function，也就是 L'<br>
<br>
0:59:26.160,0:59:31.240<br>
等於 L 加上 parameter 的 2 norm<br>
<br>
0:59:31.240,0:59:33.180<br>
做 gradient 的話呢<br>
<br>
0:59:33.180,0:59:37.000<br>
我們會得到 L' 對某一個參數 w 的偏微分<br>
<br>
0:59:37.000,0:59:39.240<br>
等於 L 對某個參數的偏微分<br>
<br>
0:59:39.240,0:59:42.660<br>
加上 λ 乘上某一個參數<br>
<br>
0:59:42.660,0:59:45.200<br>
因為這一項是<br>
<br>
0:59:45.200,0:59:47.080<br>
所有參數的平方和<br>
<br>
0:59:47.080,0:59:50.660<br>
所以，把這項對某個參數 w 做偏微分<br>
<br>
0:59:50.660,0:59:53.680<br>
你得到的結果就是 w<br>
<br>
0:59:53.680,0:59:57.640<br>
所以，你現在 update 參數的式子<br>
<br>
0:59:57.640,0:59:58.940<br>
會變成這樣<br>
<br>
0:59:58.940,1:00:02.580<br>
本來我們 update 的式子是把原來的參數<br>
<br>
1:00:02.580,1:00:07.880<br>
減掉 η 乘上 w 對<br>
<br>
1:00:07.880,1:00:11.640<br>
loss function 的偏微分 就得到新的參數<br>
<br>
1:00:11.640,1:00:13.900<br>
那現在這個 loss function 呢<br>
<br>
1:00:13.900,1:00:16.740<br>
這個 L'，這個 ∂L'/∂w<br>
<br>
1:00:16.740,1:00:18.780<br>
你可以換成，這個樣子<br>
<br>
1:00:18.780,1:00:21.220<br>
那你把這一項塞到這個地方<br>
<br>
1:00:21.220,1:00:23.980<br>
你把這一項塞到這個地方<br>
<br>
1:00:24.240,1:00:25.840<br>
那你會發現說呢<br>
<br>
1:00:25.840,1:00:28.500<br>
這邊有出現原來的參數<br>
<br>
1:00:28.500,1:00:31.800<br>
這邊也有出現原來的參數<br>
<br>
1:00:31.800,1:00:35.160<br>
所以，你可以把這幾項整理在一起<br>
<br>
1:00:35.160,1:00:36.780<br>
就變成這樣<br>
<br>
1:00:36.780,1:00:39.640<br>
你把這一項、這一項提出來<br>
<br>
1:00:39.640,1:00:42.460<br>
變成 (1 - η*λ) * w^t<br>
<br>
1:00:42.460,1:00:49.300<br>
再減掉你的參數對原來的 loss function 的 gradient<br>
<br>
1:00:49.300,1:00:52.360<br>
所以，如果根據這個式子，你就會發現說<br>
<br>
1:00:52.360,1:00:55.000<br>
其實在 update 參數的時候<br>
<br>
1:00:55.000,1:00:57.020<br>
每一次在 update 之前<br>
<br>
1:00:57.020,1:01:01.180<br>
你就把參數先乘個 (1 - η*λ)<br>
<br>
1:01:01.180,1:01:04.900<br>
也就是，每次你在 update 你的參數之前<br>
<br>
1:01:04.900,1:01:07.560<br>
通常你這邊的 η 就是你的 learning rate<br>
<br>
1:01:07.560,1:01:09.720<br>
它是一個很小的值<br>
<br>
1:01:09.720,1:01:12.500<br>
那這個 λ 通常會設一個很小的值，比如說<br>
<br>
1:01:12.500,1:01:15.220<br>
0.001 之類的<br>
<br>
1:01:15.220,1:01:17.800<br>
所以，η*λ 就是一個很小的值<br>
<br>
1:01:17.800,1:01:20.660<br>
(1 - η*λ) 通常是一個接近 1 的值<br>
<br>
1:01:20.660,1:01:25.240<br>
比如說 0.99，所以，今天你看這個 update 的式子的話<br>
<br>
1:01:25.240,1:01:29.320<br>
如果我們不管原來的 loss function 怎麼寫<br>
<br>
1:01:29.320,1:01:30.720<br>
只看這個 update 式子的話<br>
<br>
1:01:30.720,1:01:33.860<br>
等於你在 update 參數的時候，你做的事情是<br>
<br>
1:01:33.860,1:01:38.620<br>
每次 update 參數之前，就不分青紅皂白，先乘個 0.99<br>
<br>
1:01:39.240,1:01:41.680<br>
也就是說，你每次都會讓你的參數<br>
<br>
1:01:41.680,1:01:42.980<br>
越來越接近 0<br>
<br>
1:01:42.980,1:01:46.240<br>
不一定是越來越小，因為如果今天 w 是負的<br>
<br>
1:01:46.240,1:01:49.320<br>
w 是負的，負的乘上 0.99<br>
<br>
1:01:49.320,1:01:52.360<br>
它就變大了，它就接近 0，對不對<br>
<br>
1:01:52.360,1:01:55.420<br>
所以，今天每一個參數在 update 之前<br>
<br>
1:01:55.420,1:01:59.040<br>
都乘上一個小於 1 的值，所以它每次都越來越靠近 0<br>
<br>
1:01:59.040,1:02:00.340<br>
越來越靠近 0<br>
<br>
1:02:00.480,1:02:02.060<br>
那有人就會想說<br>
<br>
1:02:02.060,1:02:04.960<br>
越來越靠近 0，最後不就通通變 0 嗎？<br>
<br>
1:02:04.960,1:02:07.140<br>
這很崩潰阿，聽起來就不 make sense<br>
<br>
1:02:07.140,1:02:09.280<br>
那不會最後所有的參數都變 0<br>
<br>
1:02:09.280,1:02:11.700<br>
為甚麼？因為你還有後面這一項阿<br>
<br>
1:02:11.700,1:02:13.340<br>
沒有後面這一項<br>
<br>
1:02:13.340,1:02:16.240<br>
每一次 update 參數就越來越小，最後通通變 0<br>
<br>
1:02:16.240,1:02:18.300<br>
但是，問題就是後面還有這個<br>
<br>
1:02:18.300,1:02:20.360<br>
從微分那邊得到這一項<br>
<br>
1:02:20.380,1:02:25.220<br>
那這一項，會跟前面這一項，最後取得平衡<br>
<br>
1:02:25.220,1:02:28.460<br>
所以，並不會最後所有的參數都變成 0<br>
<br>
1:02:29.200,1:02:31.540<br>
因誤，如果我們使用<br>
<br>
1:02:31.540,1:02:33.560<br>
L2 的 Regularization 的時候<br>
<br>
1:02:33.560,1:02:37.300<br>
我們每次都會讓 weight 小一點、小一點、小一點<br>
<br>
1:02:37.300,1:02:40.260<br>
所以，這招叫做 Weight Decay<br>
<br>
1:02:41.180,1:02:42.860<br>
那其實 是這樣子<br>
<br>
1:02:42.860,1:02:45.500<br>
在 deep learning 裡面<br>
<br>
1:02:45.500,1:02:49.960<br>
Regularization 雖然有幫助，但是它的重要性<br>
<br>
1:02:49.960,1:02:54.140<br>
跟其他方法，比如說 SVM 比起來，並沒有那麼高<br>
<br>
1:02:54.140,1:02:57.360<br>
Regularization 幫助往往沒有那麼顯著<br>
<br>
1:02:57.360,1:02:59.100<br>
我覺得有一個可能的原因是<br>
<br>
1:02:59.100,1:03:01.780<br>
如果你看前面的 Early Stopping<br>
<br>
1:03:01.780,1:03:06.020<br>
我們可以決定說，甚麼時候 training 應該要被停下來<br>
<br>
1:03:06.020,1:03:09.620<br>
因為，我們現在在做這個 neural network 的時候<br>
<br>
1:03:09.620,1:03:11.640<br>
通常初始參數的時候，我們都是從<br>
<br>
1:03:11.640,1:03:15.360<br>
一個很小的、接近 0 的值開始初始參數<br>
<br>
1:03:15.360,1:03:18.320<br>
初始的時候，都是給它一個很小的、接近 0 的值<br>
<br>
1:03:18.320,1:03:20.380<br>
那你在做 update 的時候<br>
<br>
1:03:20.380,1:03:24.360<br>
通常就是讓參數離 0 越來越遠、越來越遠<br>
<br>
1:03:24.360,1:03:27.780<br>
而做 Regularization 這件事情<br>
<br>
1:03:27.780,1:03:30.940<br>
它要達到的目的，就是希望我們的參數<br>
<br>
1:03:30.940,1:03:32.800<br>
不要離 0 太遠<br>
<br>
1:03:32.800,1:03:35.580<br>
那我們參數不要離 0 太遠<br>
<br>
1:03:35.580,1:03:38.220<br>
加上 Regularization 所造成的效果<br>
<br>
1:03:38.220,1:03:40.800<br>
跟減少 update 次數<br>
<br>
1:03:40.800,1:03:42.220<br>
所造成的效果<br>
<br>
1:03:42.220,1:03:43.720<br>
其實，可能是很像的<br>
<br>
1:03:43.720,1:03:46.200<br>
但你今天做 Early Stopping，減少 update 次數<br>
<br>
1:03:46.200,1:03:47.920<br>
其實也會避免你的參數<br>
<br>
1:03:47.920,1:03:50.800<br>
離那些接近 0 的值太遠<br>
<br>
1:03:50.800,1:03:54.280<br>
那跟 Regularization 做的事情可能是很接近的<br>
<br>
1:03:54.280,1:03:55.440<br>
所以在 neural network 裡面<br>
<br>
1:03:55.440,1:03:57.780<br>
Regularization 雖然有幫助，但沒有那麼重要<br>
<br>
1:03:57.780,1:04:00.040<br>
沒有重要到說，比如說你看像 SVM<br>
<br>
1:04:00.040,1:04:03.440<br>
它是 explicitly 把 Regularization 這件事情<br>
<br>
1:04:03.440,1:04:05.820<br>
寫在它的 objective function 裡面<br>
<br>
1:04:05.820,1:04:08.360<br>
對不對，因為在做 SVM 的時候<br>
<br>
1:04:08.360,1:04:11.480<br>
它其實是要解一個 compass optimization problem<br>
<br>
1:04:11.480,1:04:12.400<br>
所以，實際上<br>
<br>
1:04:12.400,1:04:15.280<br>
它解的時候，並不一定會有 iteration 的過程<br>
<br>
1:04:15.280,1:04:17.780<br>
它一步就解出那個最好的結果了<br>
<br>
1:04:17.780,1:04:24.160<br>
它不像 deep learning 裡面有 Early Stopping 這件事<br>
<br>
1:04:24.160,1:04:25.840<br>
SVM 裡面，沒有 Early Stopping 這件事<br>
<br>
1:04:25.840,1:04:27.060<br>
一步就走到結果了<br>
<br>
1:04:27.060,1:04:28.600<br>
所以，你沒有辦法<br>
<br>
1:04:28.600,1:04:31.320<br>
用 Early Stopping 防止它離你太遠<br>
<br>
1:04:31.320,1:04:33.320<br>
所以你必須要把 Regularization<br>
<br>
1:04:33.320,1:04:35.280<br>
explicitly 加到你的 loss function 裡面去<br>
<br>
1:04:38.060,1:04:43.160<br>
那如果我們看 L1 的 Regularization<br>
<br>
1:04:43.160,1:04:46.220<br>
有人就會問說，為甚麼一定是平方，能不能用別的<br>
<br>
1:04:46.220,1:04:48.400<br>
當然可以用別的，比如說，你可以做<br>
<br>
1:04:48.400,1:04:51.340<br>
L1 的 Regularization，你可以把你的<br>
<br>
1:04:51.340,1:04:54.740<br>
Regularization 換成你的參數的 1 norm<br>
<br>
1:04:54.740,1:04:56.980<br>
也就是換成你參數裡面<br>
<br>
1:04:56.980,1:04:58.880<br>
換成你這個參數的集合裡面<br>
<br>
1:04:58.880,1:05:01.000<br>
每一個參數的絕對值的和<br>
<br>
1:05:01.000,1:05:05.260<br>
所以，如果我們把這一項換掉的話<br>
<br>
1:05:05.260,1:05:07.840<br>
如果我們把這一項從 2 norm 換成 1 norm 的話<br>
<br>
1:05:07.840,1:05:09.380<br>
會得到麼事呢？<br>
<br>
1:05:09.940,1:05:12.220<br>
你的第一個問題可能就是<br>
<br>
1:05:12.220,1:05:14.960<br>
絕對值不能微分阿<br>
<br>
1:05:14.960,1:05:16.020<br>
不能微分阿<br>
<br>
1:05:16.020,1:05:18.480<br>
給你一個最簡單的回答就是<br>
<br>
1:05:18.480,1:05:21.360<br>
這個東西 implement 在 Keras, TensorFlow 都沒有問題<br>
<br>
1:05:21.360,1:05:23.880<br>
所以，顯然是可以微分這樣<br>
<br>
1:05:23.880,1:05:26.040<br>
那實際的回答是這個樣子的<br>
<br>
1:05:26.040,1:05:29.900<br>
這個東西阿<br>
<br>
1:05:29.900,1:05:32.780<br>
它是取絕對值對不對？<br>
<br>
1:05:32.780,1:05:36.180<br>
那取絕對值，input 和 output 的關係不就長這樣子嗎？<br>
<br>
1:05:36.180,1:05:38.960<br>
不就是一個 V 的形狀嗎？<br>
<br>
1:05:38.960,1:05:41.500<br>
然後，在 V 的一邊<br>
<br>
1:05:41.500,1:05:45.140<br>
微分值是 1，在另外一邊微分值是 -1<br>
<br>
1:05:45.140,1:05:47.940<br>
那不能微的地方，其實只有在 0 的地方而已<br>
<br>
1:05:47.940,1:05:50.060<br>
就不要管它這樣子<br>
<br>
1:05:51.240,1:05:54.240<br>
真的出現、真的走到 0 的時候<br>
<br>
1:05:54.240,1:05:56.520<br>
你就胡亂給它一個值，比如說，給它 0 就好了<br>
<br>
1:05:59.300,1:06:00.660<br>
所以說<br>
<br>
1:06:00.660,1:06:02.560<br>
如果你把這一項<br>
<br>
1:06:02.560,1:06:04.720<br>
對 w 做微分的時候<br>
<br>
1:06:04.720,1:06:06.340<br>
你得到的結果是怎麼樣呢？<br>
<br>
1:06:06.340,1:06:10.120<br>
如果今天 w 是正的<br>
<br>
1:06:10.120,1:06:12.240<br>
那微分出來就是 +1<br>
<br>
1:06:12.240,1:06:15.780<br>
w 是負數，微分出來就是 -1<br>
<br>
1:06:15.780,1:06:20.740<br>
所以，我們這邊寫了一個 w 的 sign function<br>
<br>
1:06:20.740,1:06:23.620<br>
w 的 sign function 意思就是說，如果 w 是正數的話<br>
<br>
1:06:23.620,1:06:25.260<br>
這個 function output 就是 +1<br>
<br>
1:06:25.260,1:06:29.080<br>
w 是負數的話，這個 function output 就是 -1<br>
<br>
1:06:29.700,1:06:32.420<br>
如果我們把這一項<br>
<br>
1:06:32.420,1:06:36.360<br>
塞到參數 update 的式子裡面，會有甚麼結果呢？<br>
<br>
1:06:36.360,1:06:37.840<br>
就變成這樣<br>
<br>
1:06:37.840,1:06:41.040<br>
那我們可以把這個展開來<br>
<br>
1:06:41.040,1:06:42.180<br>
就變成這樣<br>
<br>
1:06:42.180,1:06:44.240<br>
那這個式子告訴我們甚麼？<br>
<br>
1:06:44.240,1:06:48.140<br>
這個式子告訴我們說，我們每一次在 update 參數的時候<br>
<br>
1:06:48.140,1:06:50.520<br>
我們每一次 update 參數的時候<br>
<br>
1:06:50.520,1:06:51.980<br>
我們就不管三七二十一<br>
<br>
1:06:51.980,1:06:56.000<br>
都一定要去減一個 η*λ，再乘一個<br>
<br>
1:06:56.000,1:06:57.440<br>
w 的 sign<br>
<br>
1:06:57.500,1:07:00.780<br>
也就是說，如果今天 w 是正的<br>
<br>
1:07:00.780,1:07:03.300<br>
w 是正的，這項就是 +1<br>
<br>
1:07:03.300,1:07:05.740<br>
所以，就變成是減一個 positive 的值<br>
<br>
1:07:05.740,1:07:07.640<br>
就會讓你的參數變小<br>
<br>
1:07:07.640,1:07:09.980<br>
如果 w 是負的，這一項是 -1<br>
<br>
1:07:09.980,1:07:13.980<br>
那就變成是加一個值，就會讓你的參數變大<br>
<br>
1:07:13.980,1:07:16.200<br>
也就是說，只要你的參數是正的<br>
<br>
1:07:16.200,1:07:19.980<br>
就減掉一些，只要你的參數是負的<br>
<br>
1:07:19.980,1:07:21.020<br>
就加上一些<br>
<br>
1:07:21.020,1:07:23.300<br>
那不管那個參數原來的值是多少<br>
<br>
1:07:23.300,1:07:25.780<br>
所以，如果你把這個 L1<br>
<br>
1:07:25.780,1:07:29.240<br>
跟 L2 做一下比較的話<br>
<br>
1:07:29.240,1:07:32.560<br>
他們同樣是讓參數變小，但是他們做的事情<br>
<br>
1:07:32.560,1:07:34.600<br>
是略有不同的<br>
<br>
1:07:34.600,1:07:37.880<br>
因為如果你是用 L1 的時候，每次都減掉固定的值<br>
<br>
1:07:37.880,1:07:39.280<br>
你用 L2 的時候<br>
<br>
1:07:39.280,1:07:42.660<br>
你是每一次都乘上一個小於 1 固定的值<br>
<br>
1:07:42.660,1:07:46.320<br>
所以，比如說，今天 w 是一個很正的值的話<br>
<br>
1:07:46.320,1:07:47.960<br>
比如說，它是一百萬<br>
<br>
1:07:47.960,1:07:53.440<br>
那你乘上一個 0.99，你其實把 w 減掉一個很大的值<br>
<br>
1:07:53.440,1:07:55.820<br>
但是，對 L1 來說<br>
<br>
1:07:55.820,1:07:57.220<br>
它每次減掉的值都是固定的<br>
<br>
1:07:57.220,1:08:01.700<br>
不管 w 是一百萬還是 0.1，w 減掉的值都是固定的<br>
<br>
1:08:01.700,1:08:04.640<br>
所以，對 L1 來說，對 L2 來說<br>
<br>
1:08:04.640,1:08:06.860<br>
只要 w 有出現很大的值<br>
<br>
1:08:06.860,1:08:10.460<br>
這個很大的 w<br>
<br>
1:08:10.460,1:08:12.660<br>
它下降很快，它很快就會變得很小<br>
<br>
1:08:12.660,1:08:14.160<br>
在 learning 的 process 中<br>
<br>
1:08:14.160,1:08:16.460<br>
但是，如果你今天是<br>
<br>
1:08:16.460,1:08:18.420<br>
L1 的話，那就不一樣了<br>
<br>
1:08:18.420,1:08:21.540<br>
如果 w 有很大的值，它的下降速度跟其他<br>
<br>
1:08:21.540,1:08:22.840<br>
很小的 w 是一樣的<br>
<br>
1:08:22.840,1:08:24.620<br>
所以，透過 L1 的 training 以後<br>
<br>
1:08:24.620,1:08:26.920<br>
你有可能認出來的 model 裡面<br>
<br>
1:08:26.920,1:08:29.580<br>
還是有一些很大很大的值<br>
<br>
1:08:29.580,1:08:32.540<br>
但是，如果我們考慮很小的值的話<br>
<br>
1:08:32.540,1:08:34.760<br>
對 L2 來說，很小的值<br>
<br>
1:08:34.760,1:08:40.520<br>
比如說 0.1, 0.01 阿，它的下降速度就很慢<br>
<br>
1:08:40.520,1:08:42.980<br>
所以，在 L 裡面，它會<br>
<br>
1:08:42.980,1:08:47.280<br>
train 出來的結果，它會保留很多接近 0 的值<br>
<br>
1:08:47.280,1:08:51.880<br>
那 L1 呢，它每次到下降一個固定的 value<br>
<br>
1:08:51.880,1:08:53.200<br>
那在 L1 裡面呢<br>
<br>
1:08:53.200,1:08:56.020<br>
它不會保留很多很小的值<br>
<br>
1:08:56.020,1:09:00.260<br>
所以，如果你用 L1 做 training 的時候呢，你得到的結果<br>
<br>
1:09:00.260,1:09:01.580<br>
就是會比較 sparse<br>
<br>
1:09:01.580,1:09:04.420<br>
那比較 sparse 的意思是說你 train 出來的參數裡面<br>
<br>
1:09:04.420,1:09:06.300<br>
有很多接近 0 的值<br>
<br>
1:09:06.300,1:09:08.380<br>
但是，也有很大的值<br>
<br>
1:09:08.380,1:09:11.500<br>
不像如果是 L2 的話，你 train 出來的結果<br>
<br>
1:09:11.500,1:09:14.760<br>
你的值是平均的都比較小<br>
<br>
1:09:14.760,1:09:16.420<br>
所以，他們 train 出來的結果是<br>
<br>
1:09:16.420,1:09:18.740<br>
略有差異的，那我們剛才<br>
<br>
1:09:18.740,1:09:21.660<br>
我們剛才在講 cn 的時候，有講過<br>
<br>
1:09:21.660,1:09:25.500<br>
L1 就是要產生一個 image 的時候，有產生 L1<br>
<br>
1:09:25.500,1:09:27.240<br>
那在剛才那個 task 裡面呢<br>
<br>
1:09:27.240,1:09:29.680<br>
L1 是比較適合的<br>
<br>
1:09:29.680,1:09:32.320<br>
因為我想要看到 sparse 的結果<br>
<br>
1:09:32.320,1:09:37.040<br>
我有試過用 L2，但是結果就沒有 L1 看起來那麼明顯<br>
<br>
1:09:37.040,1:09:38.960<br>
雖然 L1 看起來也沒有很明顯啦<br>
<br>
1:09:38.960,1:09:40.180<br>
但 L1 看起來的結果<br>
<br>
1:09:40.180,1:09:42.620<br>
是還比較像是一個 digit<br>
<br>
1:09:43.660,1:09:47.480<br>
那這邊就胡亂講一個東西，Weight Decay<br>
<br>
1:09:47.480,1:09:50.320<br>
我們在人腦裡面也會做 Weight Decay，對不對<br>
<br>
1:09:50.320,1:09:53.140<br>
這個是從、我記得龍騰的生物課本上有這個圖<br>
<br>
1:09:53.980,1:09:57.840<br>
這個是剛出生的時候，嬰兒的神經是這樣<br>
<br>
1:09:57.840,1:10:00.560<br>
6 歲的時候，有很多很多的神經<br>
<br>
1:10:00.560,1:10:03.880<br>
但是，到 14 歲的時候，神經間的連結<br>
<br>
1:10:03.880,1:10:06.360<br>
又減少了，所以<br>
<br>
1:10:06.360,1:10:08.620<br>
neural network 也會跟我們人<br>
<br>
1:10:08.620,1:10:10.820<br>
有一些很類似的事情，如果有一些 weight<br>
<br>
1:10:10.820,1:10:12.620<br>
你都沒有去 update 它<br>
<br>
1:10:12.620,1:10:14.340<br>
那它每次都會越來越小<br>
<br>
1:10:14.340,1:10:16.340<br>
最後就接近 0 就不見了<br>
<br>
1:10:16.340,1:10:19.540<br>
這跟人腦的運作，是有異曲同工之妙<br>
<br>
1:10:20.640,1:10:22.900<br>
那最後我們要講一下 dropout<br>
<br>
1:10:22.900,1:10:25.400<br>
我們先講 dropout 是怎麼做的<br>
<br>
1:10:25.400,1:10:29.180<br>
然後，才講為甚麼這樣做<br>
<br>
1:10:29.420,1:10:31.040<br>
dropout 是怎麼做的呢？<br>
<br>
1:10:31.040,1:10:34.440<br>
它是這樣，在 training 的時候<br>
<br>
1:10:34.440,1:10:37.740<br>
training 的時候，每一次我們要 update 參數之前<br>
<br>
1:10:37.740,1:10:42.060<br>
我們都對每一個 neuron，其實也包括 input 的地方<br>
<br>
1:10:42.060,1:10:45.000<br>
input 的 input layer 裡面的每一個 element<br>
<br>
1:10:45.000,1:10:46.300<br>
也算是一個 neuron<br>
<br>
1:10:46.300,1:10:48.980<br>
我們對 network 裡面的每一個 neuron<br>
<br>
1:10:48.980,1:10:53.760<br>
做 sampling，那這個 sampling 是要決定說<br>
<br>
1:10:53.760,1:10:58.500<br>
這個 neuron 要不要被丟掉，每個 neuron 有 p% 的機率<br>
<br>
1:10:58.500,1:11:00.300<br>
會被丟掉<br>
<br>
1:11:01.380,1:11:04.440<br>
那如果一個 neuron 被 sample 到要丟掉的時候<br>
<br>
1:11:04.440,1:11:07.540<br>
那你知道這個 neuron 要被丟掉了，那跟它相連的 weight<br>
<br>
1:11:07.780,1:11:11.520<br>
也失去作用，也都被丟掉，所以就變這樣<br>
<br>
1:11:11.520,1:11:12.980<br>
所以，做完這個 sample 以後<br>
<br>
1:11:12.980,1:11:17.220<br>
你的 network 的 structure 就變瘦了，變得比較細長<br>
<br>
1:11:17.960,1:11:22.780<br>
然後，你再去 train 這個比較細長的 network<br>
<br>
1:11:22.780,1:11:26.100<br>
而要注意一下，這個 sampling<br>
<br>
1:11:26.100,1:11:29.580<br>
是每次 update 參數之前，都要做一次<br>
<br>
1:11:29.580,1:11:31.320<br>
所以，每一次 update 參數的時候<br>
<br>
1:11:31.320,1:11:35.160<br>
你拿來 training 的那個 network structure 是不一樣的<br>
<br>
1:11:35.160,1:11:38.440<br>
每一次你都要重新做一次 sample，所以，你每一次<br>
<br>
1:11:38.440,1:11:40.460<br>
在做重新 sample 的時候<br>
<br>
1:11:40.460,1:11:43.360<br>
你得到的這個結果，會是不一樣的<br>
<br>
1:11:45.360,1:11:47.180<br>
那 testing 的時候呢<br>
<br>
1:11:47.180,1:11:52.300<br>
當你在 training 的時候，使用 dropout 的時候<br>
<br>
1:11:52.300,1:11:54.480<br>
你的 performance 是會變差的<br>
<br>
1:11:54.480,1:11:56.100<br>
了解我意思嗎？就是<br>
<br>
1:11:56.180,1:11:59.260<br>
因為本來如果你不要 dropout 的話<br>
<br>
1:11:59.260,1:12:01.140<br>
本來好好的做，不要 dropout 的話<br>
<br>
1:12:01.140,1:12:06.620<br>
你在 MNIST 上，剛剛可以把正確率做個 100% 這樣<br>
<br>
1:12:06.620,1:12:08.240<br>
但是，如果你加 dropout 的時後<br>
<br>
1:12:08.240,1:12:09.640<br>
因為你的神經元在 train 的時候<br>
<br>
1:12:09.640,1:12:11.180<br>
有時候莫名其妙就會不見<br>
<br>
1:12:11.180,1:12:13.600<br>
所以，你在 training 的時候，<br>
有時候 performance 是會變差的<br>
<br>
1:12:13.600,1:12:14.940<br>
本來可以 train 到 100%<br>
<br>
1:12:14.940,1:12:17.520<br>
它就會變成只剩下 98%<br>
<br>
1:12:17.520,1:12:18.680<br>
有一些 neuron 不見了嘛<br>
<br>
1:12:18.680,1:12:21.500<br>
所以，當你加了 dropout 的時候<br>
<br>
1:12:21.500,1:12:23.020<br>
你在 training 上會看到的結果變差<br>
<br>
1:12:23.020,1:12:25.420<br>
但是，dropout 它真正要做的事情是<br>
<br>
1:12:25.420,1:12:28.140<br>
它就是要讓你 training 的結果變差<br>
<br>
1:12:28.140,1:12:30.080<br>
但是 testing 的結果是變好的<br>
<br>
1:12:30.080,1:12:32.760<br>
也就是，如果你今天遇到的問題是你 training 做得不夠好<br>
<br>
1:12:32.760,1:12:34.660<br>
你再加 dropout，你就是越做越差這樣子<br>
<br>
1:12:35.320,1:12:36.880<br>
那在 testing 的時候怎麼做呢？<br>
<br>
1:12:36.880,1:12:39.380<br>
在 testing 的時候要注意兩件事<br>
<br>
1:12:39.380,1:12:42.400<br>
第一件事情就是 testing 的時候不加 dropout<br>
<br>
1:12:42.400,1:12:44.940<br>
testing 的時候就是所有的 neuron 都要用<br>
<br>
1:12:44.940,1:12:46.140<br>
不做 dropout<br>
<br>
1:12:46.340,1:12:49.720<br>
另外一個地方是<br>
<br>
1:12:49.720,1:12:51.460<br>
在 testing 的時候<br>
<br>
1:12:51.460,1:12:54.140<br>
假設你的 dropout rate<br>
<br>
1:12:54.140,1:12:56.280<br>
在 training 的時候，dropout rate 是 p%<br>
<br>
1:12:56.280,1:13:00.320<br>
那在 testing 的時候，所有 weight 都要乘 (1 - p%)<br>
<br>
1:13:01.080,1:13:04.740<br>
也就是說，假設現在 dropout rate 是 50%<br>
<br>
1:13:04.740,1:13:07.280<br>
那我們在 training 的時候，learn 出來的 weight<br>
<br>
1:13:07.280,1:13:08.500<br>
等於 1<br>
<br>
1:13:08.500,1:13:11.080<br>
那 testing 的時候，你要把那個 weight<br>
<br>
1:13:11.080,1:13:12.440<br>
設 0.5<br>
<br>
1:13:12.440,1:13:17.060<br>
那有沒有很奇怪，我看很多人都皺眉頭這樣子<br>
<br>
1:13:17.060,1:13:21.680<br>
這個步驟非常地神妙<br>
<br>
1:13:21.680,1:13:22.940<br>
我覺得第一個想出來這個人<br>
<br>
1:13:22.940,1:13:27.280<br>
這個 Hinton，若憑空想出來這個想法真的非常神妙<br>
<br>
1:13:27.280,1:13:29.900<br>
那你自己在 implement dropout 的時候阿<br>
<br>
1:13:29.900,1:13:32.460<br>
過去，在還沒有那麼多 toolkit 的時候，常常有人說<br>
<br>
1:13:32.840,1:13:35.500<br>
拿給我看一個程式，說我做 dropout 了<br>
<br>
1:13:35.500,1:13:37.400<br>
它都沒有進步，老師你看看怎麼辦<br>
<br>
1:13:37.400,1:13:41.740<br>
我看就說，你忘了除這個 0.5，難怪沒有進步<br>
<br>
1:13:41.740,1:13:43.500<br>
不過現在 toolkit 都會自動幫你除 0.5<br>
<br>
1:13:43.500,1:13:45.500<br>
所以，就不用再擔心這件事情了<br>
<br>
1:13:46.480,1:13:51.140<br>
那為甚麼 dropout 有用，直覺的想法是這樣子<br>
<br>
1:13:51.720,1:13:54.780<br>
training 的時候，會丟掉一些 neuron<br>
<br>
1:13:54.780,1:13:58.820<br>
就好像是你要練輕功的時候，你會在腳上綁一些重物<br>
<br>
1:13:59.320,1:14:01.500<br>
然後，你實際上戰鬥的時候<br>
<br>
1:14:01.500,1:14:03.500<br>
實際上 testing 的時候，是沒有 dropout 的<br>
<br>
1:14:03.500,1:14:04.560<br>
實際上 test 的時候<br>
<br>
1:14:04.560,1:14:06.880<br>
你就把重物拿下來，所以就會變得很強<br>
<br>
1:14:07.220,1:14:09.660<br>
這個是小李，他平常都綁一個重物<br>
<br>
1:14:09.660,1:14:12.680<br>
只有在，我記得是要貫徹自己的忍道的時候<br>
<br>
1:14:12.680,1:14:14.680<br>
他才會拿下來<br>
<br>
1:14:14.680,1:14:16.460<br>
還是打輸我愛羅就是了<br>
<br>
1:14:19.760,1:14:23.000<br>
另外一個直覺的理由是這樣子<br>
<br>
1:14:24.380,1:14:28.920<br>
一個 neural network 裡面的每一個 neuron 就是一個學生<br>
<br>
1:14:28.920,1:14:31.380<br>
那大家被連結在一起<br>
<br>
1:14:31.380,1:14:34.260<br>
就是大家聽到要做 final project<br>
<br>
1:14:34.260,1:14:37.080<br>
那你知道說，在一個團隊裡面<br>
<br>
1:14:37.080,1:14:40.540<br>
總是有人會擺爛，就是它是會 dropout 的<br>
<br>
1:14:40.540,1:14:43.500<br>
所以，假設說你覺得你的隊友<br>
<br>
1:14:43.500,1:14:45.060<br>
其實是會擺爛的<br>
<br>
1:14:45.060,1:14:47.440<br>
所以，這個時候你就會想要好好做<br>
<br>
1:14:47.440,1:14:50.000<br>
實際上，你就會想要去 carry 他<br>
<br>
1:14:50.000,1:14:51.700<br>
但是，實際上最後在 testing 的時候<br>
<br>
1:14:51.700,1:14:53.840<br>
大家都有好好做，沒有人需要被 carry<br>
<br>
1:14:53.840,1:14:57.120<br>
那因為每個人都做是更有利，所以，結果是更好的<br>
<br>
1:14:59.360,1:15:01.800<br>
所以，在 testing 的時候，不用 dropout<br>
<br>
1:15:01.800,1:15:04.360<br>
另外，我想解釋的就是<br>
<br>
1:15:04.360,1:15:07.520<br>
直覺的需要解釋的就是<br>
<br>
1:15:07.520,1:15:11.000<br>
為甚麼 dropout rate 50% 的時候，就要乘 0.5<br>
<br>
1:15:11.220,1:15:15.160<br>
為甚麼 training 跟 testing 的 weight 是不一樣的呢？<br>
<br>
1:15:15.160,1:15:17.960<br>
照理說 training 用甚麼 weight 就要用在 testing 上阿<br>
<br>
1:15:17.960,1:15:20.520<br>
你這樣 training 跟 testing 的時候居然是用不同的 weight<br>
<br>
1:15:20.520,1:15:21.620<br>
為甚麼這樣呢？<br>
<br>
1:15:21.620,1:15:23.220<br>
直覺的理由是這樣<br>
<br>
1:15:24.980,1:15:27.440<br>
假設現在 dropout rate 是 50%<br>
<br>
1:15:27.780,1:15:32.920<br>
那在 training 的時候，你的期望總是會<br>
<br>
1:15:32.920,1:15:34.840<br>
丟掉一半的 neuron<br>
<br>
1:15:34.840,1:15:37.060<br>
對每一個 neuron 來說<br>
<br>
1:15:37.060,1:15:39.800<br>
總是期望說它有一半的 neuron<br>
<br>
1:15:39.800,1:15:41.620<br>
是不見的，是沒有 input 的<br>
<br>
1:15:41.620,1:15:44.660<br>
所以，你現在如果認好一組 weight<br>
<br>
1:15:44.660,1:15:47.340<br>
假設你在這個情況下，認好一組 weight<br>
<br>
1:15:47.340,1:15:50.480<br>
但是，在 testing 的時候，是沒有 dropout 的阿<br>
<br>
1:15:50.480,1:15:53.000<br>
所以，對同一組 weight 來說<br>
<br>
1:15:53.000,1:15:56.000<br>
假如你在這邊用這組 weight 得到 z<br>
<br>
1:15:56.000,1:15:58.280<br>
跟在這邊用這組 weight 得到 z'<br>
<br>
1:15:58.280,1:16:00.920<br>
它們得到的值，其實是會差兩倍的，對不對<br>
<br>
1:16:00.920,1:16:04.040<br>
因為在這個情況下，你總是會有一半的 input 不見<br>
<br>
1:16:04.040,1:16:06.520<br>
在這個情況下，你所有的 input 都會在<br>
<br>
1:16:06.520,1:16:10.820<br>
而你用同一組 weight 的話，變成 z' 就是 z 的兩倍了<br>
<br>
1:16:10.820,1:16:12.400<br>
這樣變成 training跟 testing 不 match<br>
<br>
1:16:12.400,1:16:13.960<br>
你 performance反而會變差<br>
<br>
1:16:13.960,1:16:15.400<br>
所以，怎麼辦？<br>
<br>
1:16:15.400,1:16:17.780<br>
把所有 weight 都乘 0.5 阿<br>
<br>
1:16:17.940,1:16:24.500<br>
乘 0.5  以後，做一下 normalization<br>
<br>
1:16:26.080,1:16:30.220<br>
把所有 weight 都乘 0.5，這樣 z 就會等於 z'<br>
<br>
1:16:30.220,1:16:32.540<br>
就是這麼回事<br>
<br>
1:16:32.540,1:16:35.120<br>
把這個 weight 乘上一個值以後，<br>
<br>
1:16:35.120,1:16:38.040<br>
反而會讓 training 跟 testing 是比較 match 的<br>
<br>
1:16:38.040,1:16:41.600<br>
這個是比較直觀上的結果，如果你要<br>
<br>
1:16:41.600,1:16:44.200<br>
更正式講的話，其實 dropout 有很多理由<br>
<br>
1:16:44.200,1:16:47.640<br>
這個東西還是一個可以探討的問題<br>
<br>
1:16:47.700,1:16:51.720<br>
在文獻上找到很多不同的觀點來解釋<br>
<br>
1:16:51.720,1:16:53.820<br>
為甚麼 dropout 會 work<br>
<br>
1:16:53.820,1:16:56.160<br>
那我覺得我比較能接受的是<br>
<br>
1:16:56.160,1:16:59.840<br>
dropout 是一種終極的 ensemble 的方法<br>
<br>
1:16:59.840,1:17:01.800<br>
甚麼是 ensemble 的方法呢？<br>
<br>
1:17:01.800,1:17:03.680<br>
ensemble 的方法在比賽的時候常用<br>
<br>
1:17:03.680,1:17:05.200<br>
ensemble 的方法意思是說<br>
<br>
1:17:05.200,1:17:08.500<br>
我們有一個很大的 training set<br>
<br>
1:17:08.500,1:17:10.640<br>
那你每次從 training set 裡面<br>
<br>
1:17:10.640,1:17:13.640<br>
只 sample 一部分的 data 出來<br>
<br>
1:17:13.640,1:17:15.760<br>
只 sample 一部分的 data 出來<br>
<br>
1:17:15.760,1:17:19.700<br>
記得我們在講 bias 跟 variance 的 trade off 的時候<br>
<br>
1:17:19.700,1:17:24.340<br>
我們不是以講過說，打靶有兩種狀況，一種是<br>
<br>
1:17:24.340,1:17:27.420<br>
你的 bias 大，所以你打不準<br>
<br>
1:17:27.420,1:17:30.380<br>
一種是你的 variance 很大，所以你打得準<br>
<br>
1:17:30.380,1:17:32.500<br>
如果你今天有一個很複雜的 model<br>
<br>
1:17:32.500,1:17:34.340<br>
很笨重、很大的 model 的時候<br>
<br>
1:17:34.340,1:17:38.020<br>
它往往是 bias 準，但 variance 很大<br>
<br>
1:17:38.020,1:17:40.520<br>
但是，如果你這個笨重的 model 有很多個<br>
<br>
1:17:40.520,1:17:42.500<br>
雖然它 variance 很大，最後平均起來<br>
<br>
1:17:42.500,1:17:43.900<br>
結果就很準<br>
<br>
1:17:43.900,1:17:46.960<br>
對不對，所以今天只 ensemble 做的事情<br>
<br>
1:17:46.960,1:17:49.620<br>
其實，就是要利用這個特性<br>
<br>
1:17:49.620,1:17:54.380<br>
我們 train 很多個 model<br>
<br>
1:17:54.380,1:17:57.480<br>
我們把原來的 training data 裡面 sample 出很多 subset<br>
<br>
1:17:57.480,1:17:59.240<br>
然後，train 很多個 model<br>
<br>
1:17:59.240,1:18:01.680<br>
然後，每一個 model 你甚至可以是 structure 不一樣<br>
<br>
1:18:01.680,1:18:04.880<br>
雖然說，每一個 model 他們可能<br>
<br>
1:18:04.880,1:18:07.320<br>
variance 很大，但是<br>
<br>
1:18:07.320,1:18:10.500<br>
如果他們都是很複雜的 model 的時候，平均起來<br>
<br>
1:18:11.360,1:18:13.020<br>
這個 bias 就很小<br>
<br>
1:18:13.020,1:18:15.740<br>
所以，你真正在 testing 的時候<br>
<br>
1:18:15.740,1:18:18.640<br>
train 了一把 model，然後在 testing 的時候<br>
<br>
1:18:18.640,1:18:22.440<br>
丟一筆 training data 近來，它通過所有的 model<br>
<br>
1:18:22.440,1:18:25.640<br>
得到一大堆的結果，再把這一大堆的結果平均起來<br>
<br>
1:18:25.640,1:18:27.200<br>
當作我們最後的結果<br>
<br>
1:18:27.200,1:18:30.740<br>
那如果你的 model 很複雜的畫，這一招往往有用<br>
<br>
1:18:30.740,1:18:33.040<br>
那 random forest 也是實踐這個方法的<br>
<br>
1:18:33.040,1:18:36.360<br>
一個實踐這個精神的一個方法<br>
<br>
1:18:36.360,1:18:38.400<br>
如果你用一個 decision tree，它就很弱<br>
<br>
1:18:38.400,1:18:41.180<br>
胡亂做它就會 overfitting，那如果你用 random forest<br>
<br>
1:18:41.180,1:18:43.100<br>
它就沒有那麼容易 overfitting<br>
<br>
1:18:44.660,1:18:48.080<br>
那為甚麼說 dropout 是一個終極的 ensemble 的方法呢<br>
<br>
1:18:48.400,1:18:51.000<br>
我們知道在做 dropout 的時候，我們每次<br>
<br>
1:18:51.000,1:18:54.140<br>
我們每一次要 update 參數的時候<br>
<br>
1:18:54.140,1:18:56.540<br>
就你拿一個 minibatch 出來，要 update 參數的時候<br>
<br>
1:18:56.540,1:18:58.740<br>
你都會做一次 sample<br>
<br>
1:18:58.740,1:19:01.980<br>
所以，你拿第一個 minibatch 的時候<br>
<br>
1:19:01.980,1:19:03.480<br>
你 train 的 network 長這樣子<br>
<br>
1:19:03.480,1:19:04.860<br>
你拿第二個 minibatch 的時候<br>
<br>
1:19:04.860,1:19:06.020<br>
你 train 的 network 可能長這樣<br>
<br>
1:19:06.020,1:19:08.460<br>
你拿第三個長這樣，你拿第四個長這樣<br>
<br>
1:19:08.460,1:19:10.200<br>
所以，在做 dropout 的時候<br>
<br>
1:19:10.200,1:19:13.660<br>
你等於是一個終極的 ensemble 的方式<br>
<br>
1:19:13.660,1:19:16.380<br>
你是在 train，假設你有 M 個 neuron<br>
<br>
1:19:16.380,1:19:18.440<br>
每一個 neuron 可以 drop 或不 drop<br>
<br>
1:19:18.440,1:19:22.020<br>
所以你可能的 neuron 的數目有 2^M 個<br>
<br>
1:19:22.020,1:19:23.540<br>
當你在做 dropout 的時候<br>
<br>
1:19:23.540,1:19:28.240<br>
你等於是在 train 這 2^M 個 neuron<br>
<br>
1:19:28.240,1:19:32.920<br>
你每次都只用一個 minibatch 的 data<br>
<br>
1:19:32.920,1:19:35.980<br>
去 train 一個 network，你用這個 minibatch 裡面的 data<br>
<br>
1:19:35.980,1:19:38.100<br>
用 minibatch 可能就 100 筆 data 嘛<br>
<br>
1:19:38.100,1:19:43.200<br>
你用這些 data 去 train 這些 network<br>
<br>
1:19:43.200,1:19:47.000<br>
那總共有 2^M 個可能的 network<br>
<br>
1:19:47.000,1:19:50.680<br>
當然因為你最後 update 的次數是有限的<br>
<br>
1:19:50.680,1:19:54.180<br>
你可能沒有辦法把 2^M 個 network 每個都 train 一遍<br>
<br>
1:19:54.180,1:19:57.080<br>
但是，你可能就 train 了好多好多的參數<br>
<br>
1:19:57.080,1:19:58.220<br>
好多好多的 network<br>
<br>
1:19:58.220,1:20:01.000<br>
你有做幾次 update 參數，你就 train 幾次 network<br>
<br>
1:20:01.000,1:20:04.240<br>
但是，每個 network 就只用一個 batch 來 train<br>
<br>
1:20:04.240,1:20:06.020<br>
那每一個 network 用一個 batch 來 train<br>
<br>
1:20:06.020,1:20:08.200<br>
可能會讓人覺得很不安<br>
<br>
1:20:08.200,1:20:11.700<br>
一個 batch 才 100 筆 data，怎麼 train 一個 network 呢<br>
<br>
1:20:11.700,1:20:14.780<br>
那沒有關係，因為這些不同的 network 之間的參數<br>
<br>
1:20:14.780,1:20:16.980<br>
是 shared，也就是說<br>
<br>
1:20:16.980,1:20:19.820<br>
這一個 network 的這一個參數<br>
<br>
1:20:19.820,1:20:21.920<br>
就是這個 network 的這個參數<br>
<br>
1:20:21.920,1:20:25.800<br>
就是它的這個參數，這 4 個參數其實是同一個參數<br>
<br>
1:20:25.800,1:20:28.160<br>
所以，雖然說一個 network<br>
<br>
1:20:28.160,1:20:31.020<br>
的 structure，它只用一個 batch train<br>
<br>
1:20:31.020,1:20:34.980<br>
但是一個 weight，它可能用好多個 batch 來 train<br>
<br>
1:20:34.980,1:20:37.840<br>
比如說，這個 weight，它在這 4 個 batch<br>
<br>
1:20:37.840,1:20:40.760<br>
裡面，在這 4 個 batch 做 dropout 的時候<br>
<br>
1:20:40.760,1:20:42.440<br>
都沒有把這個 weight 丟掉<br>
<br>
1:20:42.440,1:20:45.960<br>
那這個 weight，就是拿這 4 個 batch 合起來 train 的結果<br>
<br>
1:20:47.540,1:20:49.900<br>
所以，當你做 dropout 的時候<br>
<br>
1:20:49.900,1:20:54.280<br>
你就是 train 了一大把的 network structure<br>
<br>
1:20:54.280,1:20:57.060<br>
理論上，每一次 update 參數的時候<br>
<br>
1:20:57.060,1:20:58.320<br>
你都 train 了一個 network 出來<br>
<br>
1:20:59.020,1:21:00.620<br>
那 testing 的時候呢？<br>
<br>
1:21:00.620,1:21:03.540<br>
按照 ensemble 這個方法的邏輯應該就是<br>
<br>
1:21:03.540,1:21:06.180<br>
你把那一大把的 network 通通拿出來<br>
<br>
1:21:06.180,1:21:09.920<br>
然後，你把你的 testing data 丟到那一把<br>
<br>
1:21:09.920,1:21:11.100<br>
network 裡面去<br>
<br>
1:21:11.100,1:21:13.400<br>
每一個 network 都給你吐一個結果<br>
<br>
1:21:13.400,1:21:15.440<br>
然後，把所有的結果平均起來<br>
<br>
1:21:15.440,1:21:16.880<br>
就是最終的結果<br>
<br>
1:21:16.880,1:21:19.620<br>
但是，在實作上你沒辦法這麼做，因為<br>
<br>
1:21:19.620,1:21:21.880<br>
這一把 network 實在太多了<br>
<br>
1:21:21.880,1:21:23.300<br>
這一把 network 實在太多了<br>
<br>
1:21:23.300,1:21:25.260<br>
你沒有辦法把它都通通都拿出來<br>
<br>
1:21:25.260,1:21:28.200<br>
你沒有辦法每一個都丟一個 input 進去<br>
<br>
1:21:28.200,1:21:29.900<br>
去看看它 output 是什麼，再平均起來<br>
<br>
1:21:29.900,1:21:31.020<br>
這樣運算量太大<br>
<br>
1:21:31.020,1:21:34.440<br>
所以，dropout 最神奇的地方是它告訴你說<br>
<br>
1:21:35.280,1:21:38.060<br>
當你把一個完整的 network 不做 dropout<br>
<br>
1:21:38.280,1:21:42.220<br>
但是，把它的 weight 乘上 (1 - p%)<br>
<br>
1:21:42.220,1:21:47.340<br>
然後，你把這個東西，把你的 training data 丟進去<br>
<br>
1:21:47.340,1:21:49.560<br>
然後，得到它的 output 的時候<br>
<br>
1:21:49.560,1:21:51.080<br>
神奇的就是<br>
<br>
1:21:51.080,1:21:53.180<br>
這個 ensemble 的結果<br>
<br>
1:21:53.180,1:21:56.480<br>
跟這一個，把 weight 乘上 (1 - p%) 的結果<br>
<br>
1:21:56.480,1:21:58.660<br>
是可以 approximate 的<br>
<br>
1:21:58.660,1:21:59.920<br>
是可以 approximate 的<br>
<br>
1:21:59.920,1:22:04.480<br>
那你可能會想說，何以見得呢？<br>
<br>
1:22:04.480,1:22:08.000<br>
我們來舉一個例子<br>
<br>
1:22:08.000,1:22:12.220<br>
我們來 train 一個很簡單的 network，它就只有一個 neuron<br>
<br>
1:22:12.220,1:22:15.000<br>
它的 activation function 是 linear 的<br>
<br>
1:22:15.000,1:22:18.840<br>
我這邊就不考慮 bias<br>
<br>
1:22:18.840,1:22:21.440<br>
我們這邊有一個 neuron<br>
<br>
1:22:21.440,1:22:24.780<br>
然後，它的 input 是 x1, x2<br>
<br>
1:22:24.780,1:22:27.040<br>
然後，x1, x2 分別乘上<br>
<br>
1:22:27.040,1:22:28.520<br>
經過 training 以後<br>
<br>
1:22:28.520,1:22:30.560<br>
經過 dropout training 以後<br>
<br>
1:22:30.560,1:22:31.900<br>
你算出來的 weight 是 w1, w2<br>
<br>
1:22:31.900,1:22:35.860<br>
所以，它的 output 就是 w1*x1 + w2*x2<br>
<br>
1:22:35.860,1:22:39.480<br>
這個 neuron 沒有 activation function<br>
<br>
1:22:39.480,1:22:41.220<br>
或 activation function 是 linear 的<br>
<br>
1:22:41.220,1:22:44.620<br>
如果我們今天要做 ensemble 的話<br>
<br>
1:22:44.620,1:22:48.000<br>
theoretically 就是這麼做，對不對<br>
<br>
1:22:48.000,1:22:51.620<br>
每一個 neuron，我們做 dropout 的時候<br>
<br>
1:22:51.620,1:22:53.880<br>
你不會 drop 那個 output 的 neuron<br>
<br>
1:22:53.880,1:22:56.420<br>
你只會 drop hidden layer 跟 input 的 neuron<br>
<br>
1:22:56.420,1:22:58.940<br>
那這邊每一個 neuron，它可以<br>
<br>
1:22:59.020,1:23:02.280<br>
它可以被 drop，或不被 drop<br>
<br>
1:23:02.280,1:23:04.940<br>
對不對，所以我們總共有 4 種 structure<br>
<br>
1:23:04.940,1:23:07.020<br>
一個是通通沒被 drop<br>
<br>
1:23:07.020,1:23:11.320<br>
一個是 drop x1、一個是 drop x2,<br>
一個是 x1, x2 都被 drop 掉<br>
<br>
1:23:11.320,1:23:14.180<br>
那你最後得到的 output 呢，這個 network<br>
<br>
1:23:14.180,1:23:17.340<br>
假設你 input x1, x2，這個 network 給我們的<br>
<br>
1:23:17.340,1:23:19.820<br>
就是 w1*x1 + w2*x2<br>
<br>
1:23:19.820,1:23:22.340<br>
同樣的 input，但是 x1 被 drop 掉<br>
<br>
1:23:22.340,1:23:24.920<br>
你得到的 output 是 w2*x2，這邊是 w1*x1<br>
<br>
1:23:24.920,1:23:26.600<br>
這邊給我們的 output 是 0<br>
<br>
1:23:26.600,1:23:28.360<br>
我們要做 ensemble<br>
<br>
1:23:28.360,1:23:31.340<br>
所以你要把這 4 個 network <br>
它的 output 通通都 average 起來<br>
<br>
1:23:31.340,1:23:33.740<br>
通通都 average 起來，那你 average 起來的結果<br>
<br>
1:23:33.740,1:23:37.120<br>
是不是就是，這邊有 4 個值嘛<br>
<br>
1:23:37.120,1:23:39.520<br>
然後，w1*x1 出現兩次<br>
<br>
1:23:39.520,1:23:40.780<br>
w2*x2 出現兩次<br>
<br>
1:23:40.780,1:23:44.420<br>
所以，得到的結果是 1/2* w1*x1 + 1/2*w2*x2，對不對<br>
<br>
1:23:45.720,1:23:48.900<br>
那我們現在做的事情是把<br>
<br>
1:23:48.900,1:23:52.200<br>
這兩個 weight 都乘 1/2<br>
<br>
1:23:52.200,1:23:55.860<br>
我可以把 w1*1/2, w2*1/2<br>
<br>
1:23:55.860,1:23:57.520<br>
同樣 input x1, x2<br>
<br>
1:23:57.520,1:24:03.320<br>
那得到的 output 也同樣是 1/2*w1*x1 + 1/2*w2*x2<br>
<br>
1:24:03.320,1:24:08.400<br>
所以，這邊想要呈現的是說，在這個最簡單的 case 裡面<br>
<br>
1:24:09.220,1:24:11.540<br>
ensemble 這件事情<br>
<br>
1:24:11.540,1:24:14.840<br>
用不同的 network structure 做 ensemble 這件事情<br>
<br>
1:24:14.840,1:24:19.300<br>
跟我們把 weight multiply 一個值<br>
<br>
1:24:19.300,1:24:22.220<br>
而不做 ensemble 所得到的 output<br>
<br>
1:24:22.220,1:24:23.620<br>
其實是一樣的<br>
<br>
1:24:23.620,1:24:27.180<br>
那你可能會說，這個例子這麼簡單<br>
<br>
1:24:27.180,1:24:30.080<br>
所以，這個例子上會 work，我想也是很直覺的阿<br>
<br>
1:24:30.080,1:24:32.680<br>
大概小學生都知道說，這個是 equivalent<br>
<br>
1:24:32.680,1:24:36.460<br>
但是，比如說，這邊是 sigmoid function<br>
<br>
1:24:36.460,1:24:38.760<br>
或是它是很多個 layer，它會 work 嗎？<br>
<br>
1:24:38.760,1:24:41.680<br>
結論就是不會 equivalent<br>
<br>
1:24:41.680,1:24:43.120<br>
就是不會 equivalent<br>
<br>
1:24:43.120,1:24:45.140<br>
只有是 linear 的 network<br>
<br>
1:24:45.140,1:24:47.900<br>
ensemble 才會等於 multiply 一個 weight<br>
<br>
1:24:47.900,1:24:51.300<br>
左邊跟右邊要相等的前提是你的 network 要是 linear 的<br>
<br>
1:24:51.300,1:24:53.320<br>
但是，network 不是 linear 的阿<br>
<br>
1:24:53.320,1:24:54.600<br>
所以，他們其實不 equivalent<br>
<br>
1:24:54.600,1:24:58.380<br>
這個就是 dropout 最後一個很神奇的地方<br>
<br>
1:24:58.380,1:25:01.480<br>
雖然不 equivalent，但是最後結果還是會 work 這樣<br>
<br>
1:25:01.480,1:25:07.480<br>
所以，根據這個結論，有人有一個想法是說<br>
<br>
1:25:07.480,1:25:10.820<br>
既然 dropout 在 linear 的時候<br>
<br>
1:25:10.820,1:25:13.600<br>
linear 的 network 上，ensemble 才會<br>
<br>
1:25:13.600,1:25:15.700<br>
等於前一個 weight<br>
<br>
1:25:15.700,1:25:19.420<br>
所以，今天如果我的 network 很接近 linear 的話<br>
<br>
1:25:19.420,1:25:23.600<br>
應該 dropout performance 會比較好，比如說<br>
<br>
1:25:23.600,1:25:26.580<br>
怎麼做 network 會比較接近 linear，比如說你用 ReLU<br>
<br>
1:25:26.580,1:25:28.760<br>
比如說，你用 Maxout network<br>
<br>
1:25:28.760,1:25:29.740<br>
他們是很接近 linear 的<br>
<br>
1:25:29.740,1:25:32.400<br>
相對於 sigmoid，它們是比較接近 linear 的<br>
<br>
1:25:32.400,1:25:36.720<br>
所以 dropout 確實在用 ReLU 或 Maxout network 的時候<br>
<br>
1:25:36.720,1:25:39.100<br>
它的 performance 是確實比較好的<br>
<br>
1:25:39.100,1:25:42.860<br>
比如說，你去看 Maxout network 的 paper 的話<br>
<br>
1:25:42.860,1:25:45.260<br>
它裡面也有 point 這一點，它的<br>
<br>
1:25:45.260,1:25:47.960<br>
Maxout 跟 dropout 加起來的記錄量<br>
<br>
1:25:47.960,1:25:50.400<br>
是比 sigmoid function 還要大的，那這個是<br>
<br>
1:25:50.400,1:25:53.340<br>
作者相當自豪的一點<br>
<br>
1:25:53.340,1:25:56.780<br>
這邊我要講的其實<br>
<br>
1:25:56.780,1:25:59.380<br>
就是這樣啦<br>
<br>
1:25:59.380,1:26:02.800<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
