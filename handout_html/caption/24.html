<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.080<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:02.080,0:00:08.100<br>
接下來我們要來講這個 Deep Auto-encoder<br>
<br>
0:00:08.200,0:00:09.920<br>
那什麽是 Auto-encoder 呢？<br>
<br>
0:00:09.920,0:00:11.920<br>
Auto-encoder 的想法是這樣<br>
<br>
0:00:11.920,0:00:16.660<br>
我們首先去找一個 encoder<br>
<br>
0:00:16.660,0:00:19.240<br>
那這個 encoder 你可以 input 一個東西<br>
<br>
0:00:19.240,0:00:21.620<br>
比如說我們現在這邊要做的是影像辨識的話<br>
<br>
0:00:21.620,0:00:23.620<br>
你要做是跟影像有關的東西的話<br>
<br>
0:00:23.620,0:00:26.360<br>
就 input 一張，假設我們要做 MNIST 的話<br>
<br>
0:00:26.420,0:00:28.820<br>
就 input 一張 digit 他是 784 維的 vector<br>
<br>
0:00:29.560,0:00:33.380<br>
那接下來呢，這一個 encoder 他可能就是一個 neural network<br>
<br>
0:00:33.860,0:00:36.220<br>
他的 output 呢，就是一個 code<br>
<br>
0:00:36.220,0:00:39.880<br>
這個 code 通常是遠比 784 維還要小的<br>
<br>
0:00:39.880,0:00:42.480<br>
所以他會有類似壓縮的效果<br>
<br>
0:00:43.000,0:00:46.460<br>
這個 code 代表了原來 input 這張 image<br>
<br>
0:00:46.460,0:00:54.040<br>
的某種 compact，某種精簡的有效的 representation<br>
<br>
0:00:54.860,0:00:59.540<br>
但是現在問題是這樣，我們現在做的是 unsupervised learning<br>
<br>
0:00:59.540,0:01:01.540<br>
你可以找到一大堆的 image<br>
<br>
0:01:01.540,0:01:03.540<br>
當作這個 NN encoder 的 input<br>
<br>
0:01:03.920,0:01:06.240<br>
但是我們沒有任何的 output<br>
<br>
0:01:06.300,0:01:09.880<br>
並不知道一個 image，如果把它變成一個 code<br>
<br>
0:01:09.880,0:01:12.400<br>
這個 code 到底應該要長什麼樣子<br>
<br>
0:01:12.420,0:01:14.740<br>
那你要 learn 一個 network 你要有 input output<br>
<br>
0:01:14.740,0:01:16.740<br>
只有 input 你沒辦法 learn 他<br>
<br>
0:01:17.880,0:01:20.800<br>
那沒有關係，我們先想說我們要做另外一件事情<br>
<br>
0:01:20.800,0:01:22.800<br>
我們想要 learn 一個 decoder<br>
<br>
0:01:23.860,0:01:25.860<br>
所謂 decoder 他做的事情是說<br>
<br>
0:01:25.860,0:01:29.620<br>
input 一個 vector 他就通過這個 decoder<br>
<br>
0:01:29.620,0:01:33.420<br>
他也是個 NN，他的 output 就是一張 image<br>
<br>
0:01:33.420,0:01:36.020<br>
input 給他一個 code，他 output<br>
<br>
0:01:36.020,0:01:38.020<br>
就根據這個 code 裡面的 information<br>
<br>
0:01:38.020,0:01:40.020<br>
output 就是一張 image<br>
<br>
0:01:40.520,0:01:43.700<br>
接下來呢，你也沒辦法 train NN 的 decoder<br>
<br>
0:01:43.700,0:01:47.220<br>
你也沒辦法 train 他，因為你只有 network 的 output<br>
<br>
0:01:47.220,0:01:48.560<br>
你沒有他的 input<br>
<br>
0:01:48.560,0:01:53.440<br>
這兩個 network，encoder 和 decoder，單獨一個人你都無法 train 它<br>
<br>
0:01:53.440,0:01:56.920<br>
但是，我們可以把它接起來<br>
<br>
0:01:56.920,0:01:58.340<br>
然後一起 train<br>
<br>
0:01:58.340,0:02:01.000<br>
也就是說，我們接一個 neural network<br>
<br>
0:02:01.000,0:02:03.500<br>
input 一個 image，中間變成 code<br>
<br>
0:02:03.500,0:02:06.480<br>
再把 code 通過 decoder 變成原來的 image<br>
<br>
0:02:07.000,0:02:10.640<br>
你就可以把這個 encoder 和 decoder 一起學<br>
<br>
0:02:10.640,0:02:14.540<br>
那你就可以同時把 encoder 和 decoder 學出來了<br>
<br>
0:02:17.480,0:02:22.200<br>
我們剛才在 PCA 裡面其實看過非常類似的概念<br>
<br>
0:02:22.880,0:02:25.880<br>
我們來看一下，先從剛才講過的 PCA 開始講起<br>
<br>
0:02:26.740,0:02:27.760<br>
我們剛才講過說<br>
<br>
0:02:28.960,0:02:33.300<br>
PCA 實際上他在做的事情是這樣，input 一張 image x<br>
<br>
0:02:33.300,0:02:36.880<br>
那在剛才我們的例子裡面，我們會把 x 減掉<br>
<br>
0:02:37.180,0:02:39.320<br>
他的平均 x bar 當作 input<br>
<br>
0:02:39.680,0:02:41.980<br>
但這個減掉 x bar 並不總是<br>
<br>
0:02:42.600,0:02:43.840<br>
這邊我們把他省略掉<br>
<br>
0:02:44.560,0:02:48.000<br>
把他省略掉並不會太奇怪，因為通常你在做 NN 的時候<br>
<br>
0:02:48.000,0:02:50.500<br>
你拿到 data 起手式你就先做 normalize<br>
<br>
0:02:50.800,0:02:52.620<br>
先把它變成 mean 是零，variance 是一<br>
<br>
0:02:52.880,0:02:56.354<br>
所以你的 data 通常 mean 其實就是零了<br>
<br>
0:02:56.354,0:02:58.380<br>
所以你就不用再減掉 mean<br>
<br>
0:02:59.060,0:03:05.320<br>
你把 x 乘上一個 weight，通過 NN 的一個 layer<br>
<br>
0:03:05.880,0:03:10.700<br>
得到你的 component 的 weight，這邊寫成 c<br>
<br>
0:03:12.200,0:03:18.860<br>
那這個 component 的 weight 再乘上一個 matrix W 的 transpose<br>
<br>
0:03:19.340,0:03:22.840<br>
得到 x 的 hat<br>
<br>
0:03:23.820,0:03:29.440<br>
這個 x 的 hat 是根據這些 component 的 reconstruction<br>
<br>
0:03:29.440,0:03:31.360<br>
根據這些 component 的 weight<br>
<br>
0:03:31.520,0:03:33.960<br>
component 就放在這個 weight 裡面<br>
<br>
0:03:34.140,0:03:38.360<br>
根據 component 的 weight 和 component 做 reconstruction 的結果<br>
<br>
0:03:38.880,0:03:40.740<br>
我們說，在 PCA 裡面<br>
<br>
0:03:40.900,0:03:44.040<br>
我們要做的事情就是 minimize input<br>
<br>
0:03:44.980,0:03:47.280<br>
跟 reconstruction 的結果<br>
<br>
0:03:47.400,0:03:49.720<br>
我們要讓 x 跟 x hat<br>
<br>
0:03:50.120,0:03:53.640<br>
他的 Euclidean distance 越近越好<br>
<br>
0:03:53.640,0:03:56.020<br>
然後 x 跟 x hat 他們越接近越好<br>
<br>
0:03:56.140,0:04:00.040<br>
這個是 PCA 做的事，如果把它當成 neural network 來看的話<br>
<br>
0:04:00.540,0:04:02.560<br>
那input 的 x 就是 input layer<br>
<br>
0:04:02.880,0:04:05.140<br>
output 的 x hat 就是 output layer<br>
<br>
0:04:05.720,0:04:07.700<br>
中間 component 的 weight<br>
<br>
0:04:07.700,0:04:11.500<br>
就是 hidden layer，在 PCA 裡面他是 linear<br>
<br>
0:04:12.380,0:04:15.660<br>
那中間這個 hidden layer 我們通常又叫他<br>
<br>
0:04:15.760,0:04:16.520<br>
bottleneck layer<br>
<br>
0:04:16.940,0:04:18.260<br>
為什麼叫 bottleneck layer 呢？<br>
<br>
0:04:18.640,0:04:20.640<br>
因為你這個 code 的數目<br>
<br>
0:04:20.640,0:04:22.680<br>
因為你現在就是要做 dimension reduction<br>
<br>
0:04:22.860,0:04:25.460<br>
所以你這 component 的數目<br>
<br>
0:04:26.300,0:04:28.680<br>
通常會比你的 input 的 dimension 還要小得多<br>
<br>
0:04:28.680,0:04:30.720<br>
這樣才會達到 dimension reduction 的效果<br>
<br>
0:04:30.960,0:04:33.780<br>
如果把它當作一個 layer 來看的話<br>
<br>
0:04:33.960,0:04:36.500<br>
他是個特別窄的 layer<br>
<br>
0:04:36.500,0:04:40.420<br>
所以我們叫他 bottleneck layer，他就是一個瓶頸的意思<br>
<br>
0:04:43.440,0:04:46.740<br>
前面這個部分就是在做 encode<br>
<br>
0:04:46.740,0:04:48.640<br>
你把 input 變成一組 code<br>
<br>
0:04:49.260,0:04:51.260<br>
後面這個部份就是在做 decode<br>
<br>
0:04:51.560,0:04:54.100<br>
你如果把 component 的 weight 想成 code 的話<br>
<br>
0:04:54.500,0:04:58.100<br>
後面這件事就是把 code 變回原來的 image<br>
<br>
0:05:00.180,0:05:03.600<br>
hidden layer 的 output，就是我們要找的那些code<br>
<br>
0:05:05.040,0:05:06.740<br>
那 PCA 是這樣做的<br>
<br>
0:05:07.160,0:05:09.020<br>
你其實也可用 gradient decent 來做 PCA<br>
<br>
0:05:09.620,0:05:11.200<br>
PCA 只有一個 hidden layer<br>
<br>
0:05:11.740,0:05:14.460<br>
你就可以想說，要不要把它變更多 hidden layer<br>
<br>
0:05:15.000,0:05:16.600<br>
當然可以把它變更多 hidden layer<br>
<br>
0:05:17.340,0:05:21.160<br>
你就兜一個很深的 neural network，它有很多很多層<br>
<br>
0:05:22.060,0:05:24.400<br>
然後在這個很多很多層的 hidden layer 裡面<br>
<br>
0:05:25.100,0:05:28.360<br>
你 input 一個 x<br>
<br>
0:05:28.880,0:05:31.380<br>
他最後得到的 output 是 x\head<br>
<br>
0:05:31.960,0:05:38.060<br>
你會希望 training 的 target，希望這個 x 跟 x\head<br>
<br>
0:05:38.160,0:05:39.580<br>
越接近越好<br>
<br>
0:05:40.540,0:05:42.540<br>
training 的方法 完全沒有什麼特別的<br>
<br>
0:05:43.080,0:05:45.080<br>
就是 back propagation<br>
<br>
0:05:46.260,0:05:48.260<br>
就跟我們之前在 neural network 的時候講的呢<br>
<br>
0:05:48.260,0:05:50.920<br>
是完全一模一樣的東西<br>
<br>
0:05:51.920,0:05:54.920<br>
那中間你會有個特別窄的 layer<br>
<br>
0:05:55.580,0:05:58.020<br>
這個特別窄的 layer 他有特別少的 neural<br>
<br>
0:05:58.320,0:06:00.900<br>
那些 neural 的 output，這個 layer 的 output<br>
<br>
0:06:00.900,0:06:02.720<br>
就代表了一組 code<br>
<br>
0:06:03.300,0:06:07.840<br>
那從 input 到 bottleneck layer 的部分就是 encoder<br>
<br>
0:06:08.400,0:06:13.480<br>
那從 bottleneck layer 的 output 到最後的 x\head<br>
<br>
0:06:13.660,0:06:16.560<br>
到最後整個 network 的 output 就是你的 decoder<br>
<br>
0:06:16.760,0:06:20.140<br>
所以你把 input 做 encode 變 bottleneck layer 的output<br>
<br>
0:06:20.840,0:06:23.860<br>
再把 bottleneck layer 的 output，做decode 變成原來的 image<br>
<br>
0:06:23.860,0:06:25.200<br>
這個就是 decode<br>
<br>
0:06:29.500,0:06:30.720<br>
這個 deep 的 auto encoder<br>
<br>
0:06:31.380,0:06:34.600<br>
最早是出現在 06 年的 Hinton  的 science 的 paper<br>
<br>
0:06:35.140,0:06:36.240<br>
那個時候呢<br>
<br>
0:06:36.320,0:06:39.240<br>
其實 deep auto-encoder 沒有那麼好 train<br>
<br>
0:06:39.560,0:06:41.500<br>
你有可能 train 一 train 以後就壞掉了<br>
<br>
0:06:41.640,0:06:44.400<br>
那個時候需要用 RBM<br>
<br>
0:06:44.580,0:06:48.320<br>
做 layer wise 的 initialization，然後才可能<br>
<br>
0:06:48.480,0:06:52.840<br>
把 deep auto-encoder train 得比較好一點<br>
<br>
0:06:54.740,0:06:58.300<br>
如果是按照我們剛才在 PCA 裡面看到的<br>
<br>
0:06:59.560,0:07:03.360<br>
這個從 input 到第一個 hidden layer 的 weight w1<br>
<br>
0:07:04.160,0:07:06.800<br>
好像應該要跟最後一個 hidden layer 的 output<br>
<br>
0:07:07.720,0:07:10.920<br>
跟 output layer 中間的 weight 互為 transpose<br>
<br>
0:07:11.940,0:07:15.940<br>
這個 layer 是 weight w1，這個 layer 好像應該是 w1 transpose<br>
<br>
0:07:16.160,0:07:19.380<br>
這個是 w2，這個好像應該是 w2 transpose<br>
<br>
0:07:20.300,0:07:23.980<br>
你在 training 的時候，你可以做到這件事情<br>
<br>
0:07:23.980,0:07:27.920<br>
你可以把這邊的 weight，跟這邊的 weight tight 起來<br>
<br>
0:07:27.920,0:07:32.540<br>
讓他們在做 training 的時候永遠保持他們的值是一樣的<br>
<br>
0:07:32.680,0:07:37.940<br>
你可以做到這件事情，做這件事的好處呢<br>
<br>
0:07:37.940,0:07:40.280<br>
是你現在 auto-encoder 的參數<br>
<br>
0:07:41.000,0:07:44.860<br>
就少一半，比較不會有 overfitting 的情形<br>
<br>
0:07:45.700,0:07:50.900<br>
但這件事情並不是必要的，沒有什麼理由說<br>
<br>
0:07:51.500,0:07:55.580<br>
這邊的 weight，跟這邊的 weight，一定要互為 transpose<br>
<br>
0:07:56.200,0:07:59.540<br>
現在常見的作法就是兜個 neural network<br>
<br>
0:07:59.540,0:08:02.140<br>
然後用 back propagation 直接 train 下去<br>
<br>
0:08:02.140,0:08:05.820<br>
管他 train 出來的 weight 是什麼，就是你得到的結果<br>
<br>
0:08:07.700,0:08:12.220<br>
那這邊是 Hinton 06年的 Nature 的 paper<br>
<br>
0:08:12.220,0:08:14.060<br>
Science paper 的上面呢<br>
<br>
0:08:14.060,0:08:17.840<br>
截出一些，那時候看起來，相當驚人的結果<br>
<br>
0:08:17.840,0:08:20.420<br>
現在是覺得還好，因為現在誰都<br>
<br>
0:08:20.420,0:08:23.100<br>
順手就可以 reproduce 這些結果<br>
<br>
0:08:23.100,0:08:25.080<br>
那時候是覺得相當驚人<br>
<br>
0:08:25.920,0:08:28.840<br>
那這是這樣子的，如果我們今天，這是原來的 image<br>
<br>
0:08:28.840,0:08:30.680<br>
那在 MNIST 上面是長這個樣子<br>
<br>
0:08:31.500,0:08:33.500<br>
如果你做 PCA<br>
<br>
0:08:33.900,0:08:36.740<br>
把他從 784 維降到 30 維<br>
<br>
0:08:37.120,0:08:41.120<br>
然後再從 30 維 reconstruct 回 784 維<br>
<br>
0:08:42.040,0:08:44.440<br>
那你得到的 image 是這個樣子<br>
<br>
0:08:44.860,0:08:47.040<br>
你可以看出說，他是比較模糊的<br>
<br>
0:08:47.260,0:08:50.920<br>
他是有一點霧霧的感覺<br>
<br>
0:08:52.040,0:08:55.020<br>
那如果你今天是用 deep auto-encoder 的話<br>
<br>
0:08:55.740,0:08:57.740<br>
在 Hinton 的 paper 裡他是這麼做的<br>
<br>
0:08:58.640,0:09:01.480<br>
把 784 維，先擴展成 1000 維<br>
<br>
0:09:01.480,0:09:04.660<br>
再把 1000 維降到 500 維再降到 250 維再降到 30 維<br>
<br>
0:09:04.660,0:09:07.000<br>
你很難知道說為什麼他設計成這樣子<br>
<br>
0:09:07.600,0:09:11.640<br>
然後再把 30 維變成 250 維再變成 500 維 1000 維<br>
<br>
0:09:12.520,0:09:14.920<br>
然後再把他解回來<br>
<br>
0:09:15.780,0:09:18.920<br>
你會發現說，如果你今天用的是 deep auto-encoder 的話呢<br>
<br>
0:09:19.660,0:09:24.820<br>
他的結果就看起來非常的好<br>
<br>
0:09:27.560,0:09:32.380<br>
那如果你今天，不是把它降到 30 維，而是把他降到 2 維的話<br>
<br>
0:09:32.380,0:09:35.420<br>
把它降到 2 維再去 visualize 他的話<br>
<br>
0:09:35.800,0:09:37.980<br>
你會發現，如果你是做 PCA 的話<br>
<br>
0:09:38.480,0:09:41.700<br>
在這二維的平面上，所有的 digit 是被<br>
<br>
0:09:41.700,0:09:47.420<br>
混在一起，這邊不同顏色就代表了不同的數字<br>
<br>
0:09:47.840,0:09:51.500<br>
如果今天是用 PCA 把它變成二維<br>
<br>
0:09:51.500,0:09:53.940<br>
所有的數字他是混在一起<br>
<br>
0:09:55.060,0:09:58.340<br>
如果你是用 deep auto-encoder 的話，你會發現說<br>
<br>
0:10:00.220,0:10:03.680<br>
這些數字是分開的<br>
<br>
0:10:03.680,0:10:07.240<br>
你可以輕易地看到說，不同的數字會變一群一群<br>
<br>
0:10:07.900,0:10:12.240<br>
現在你其實可以很輕易的 reproduce 這樣的結果<br>
<br>
0:10:16.420,0:10:19.980<br>
我後來又做一個無聊的東西不過沒看到什麼特別的東西<br>
<br>
0:10:19.980,0:10:23.140<br>
就是把寶可夢的 data 又做了一下<br>
<br>
0:10:23.140,0:10:27.440<br>
用一個 hidden layer 的 auto-encoder，硬做了一下<br>
<br>
0:10:27.440,0:10:30.000<br>
但是沒有看到什麼特別的東西<br>
<br>
0:10:32.520,0:10:34.000<br>
那這個 auto-encoder 呢<br>
<br>
0:10:34.000,0:10:37.000<br>
也可以把它用在文字處理上<br>
<br>
0:10:37.240,0:10:40.020<br>
如果把它用在文字處理上像是怎樣呢<br>
<br>
0:10:40.020,0:10:42.720<br>
比如說我們會想要把一篇文章<br>
<br>
0:10:42.720,0:10:46.140<br>
壓成一個 vector，壓成一個code<br>
<br>
0:10:46.660,0:10:49.900<br>
那為什麼我們會想把一篇文章壓成一個 code 呢？<br>
<br>
0:10:50.560,0:10:54.960<br>
舉例來說假設我們現在要做文字的搜尋<br>
<br>
0:10:54.960,0:10:57.580<br>
假設我們現在要做文字的搜尋<br>
<br>
0:10:58.600,0:11:02.900<br>
那在文字搜尋裡面有一招，你可能都聽過叫 vector space model<br>
<br>
0:11:03.780,0:11:06.200<br>
那這個 vector space model 他非常單純，他就是說<br>
<br>
0:11:06.760,0:11:12.680<br>
我們現在把每一篇文章都表示成空間中的一個 vector<br>
<br>
0:11:12.980,0:11:14.636<br>
都是空間中的一個點<br>
<br>
0:11:14.640,0:11:18.200<br>
圖上每一個藍色的圈圈就是一篇文章<br>
<br>
0:11:18.440,0:11:19.760<br>
那接下來呢<br>
<br>
0:11:22.540,0:11:26.380<br>
我們也把，假設使用者輸入一個查詢的詞彙<br>
<br>
0:11:26.920,0:11:30.360<br>
那我們把查詢的詞彙也變成空間中的一個點<br>
<br>
0:11:31.200,0:11:34.040<br>
接下來就是計算這個輸入的查詢詞彙<br>
<br>
0:11:34.680,0:11:38.540<br>
跟每一篇 document 之間的 inner product<br>
<br>
0:11:39.520,0:11:42.040<br>
或是 cosine similarity 等等<br>
<br>
0:11:42.040,0:11:45.600<br>
cosine similarity 會有 normalize 的效果<br>
<br>
0:11:45.600,0:11:47.460<br>
可能會得到比較好的結果<br>
<br>
0:11:49.260,0:11:51.900<br>
如果距離最近 cosine similarity 的結果最大<br>
<br>
0:11:52.020,0:11:54.780<br>
你就會 retrieve 這些 document<br>
<br>
0:11:55.340,0:11:57.340<br>
比如說你輸入紅色的 query<br>
<br>
0:11:57.480,0:11:59.760<br>
可能會 retrieve 這篇  document 跟這篇 document<br>
<br>
0:11:59.760,0:12:03.740<br>
因為他跟紅色這個 query 的 cosine similarity 是比較大<br>
<br>
0:12:05.320,0:12:08.080<br>
這個模型要 work，depend on 你現在把一個 document<br>
<br>
0:12:08.080,0:12:12.240<br>
變成 vector 表示的好還是不好<br>
<br>
0:12:12.960,0:12:14.400<br>
假設你今天做的是<br>
<br>
0:12:15.220,0:12:17.220<br>
怎麼把一個 document 變成一個 vector呢<br>
<br>
0:12:17.920,0:12:19.960<br>
最 trivial 的方法叫做 bag-of-word<br>
<br>
0:12:20.680,0:12:22.180<br>
這個 bag-of-word 的想法是說<br>
<br>
0:12:22.760,0:12:24.120<br>
我們現在就開一個 vector<br>
<br>
0:12:24.940,0:12:27.060<br>
這個 vector 的 size 就是<br>
<br>
0:12:28.060,0:12:30.200<br>
lexicon 的 size，就是 lexicon 的 size<br>
<br>
0:12:31.180,0:12:34.160<br>
假設今天世界上有十萬個詞彙<br>
<br>
0:12:34.940,0:12:39.340<br>
這個 vector 的 size 就是十萬維<br>
<br>
0:12:40.220,0:12:44.440<br>
假設現在有一篇 document 他只有一個句子就是 this is an apple<br>
<br>
0:12:45.220,0:12:48.840<br>
那這篇 document 如果把他表示成一個 vector 的話<br>
<br>
0:12:49.420,0:12:53.440<br>
就是在 this 那維是一，is 那維是一<br>
<br>
0:12:53.440,0:12:56.580<br>
an 那維是一，apple 那維是一，其他都是 0<br>
<br>
0:12:57.180,0:13:00.540<br>
就是這樣子，沒什麼特別好講<br>
<br>
0:13:01.820,0:13:06.280<br>
那有時候你想把它做得更好，你會把它乘上 inverse document frequency<br>
<br>
0:13:06.960,0:13:11.880<br>
你每一維不只會用詞彙在 document 出現的次數<br>
<br>
0:13:12.460,0:13:17.180<br>
你會再乘上一個 weight，代表那個詞彙的重要性<br>
<br>
0:13:17.180,0:13:20.200<br>
那這個重要性你可以用不同的方法衡量他<br>
<br>
0:13:20.200,0:13:22.920<br>
比如說用 inverse document frequency 來衡量<br>
<br>
0:13:23.800,0:13:28.380<br>
舉例來說，is 它可能在每個 document 都有出現，所以重要性很低<br>
<br>
0:13:28.380,0:13:30.240<br>
那 1 就會乘上比較小的值<br>
<br>
0:13:31.020,0:13:36.060<br>
apple 只有在某些 document 出現，重要性比較高，所以乘上比較高的值，等等<br>
<br>
0:13:36.960,0:13:40.720<br>
但是用這個模型他很 weak<br>
<br>
0:13:41.000,0:13:44.660<br>
他沒辦法考慮任何 semantic 相關的東西<br>
<br>
0:13:45.600,0:13:48.020<br>
他沒辦法考慮任何語意相關的東西<br>
<br>
0:13:48.020,0:13:51.440<br>
比如說，他不知道台灣大學的指的就是台大<br>
<br>
0:13:52.120,0:13:56.080<br>
他不知道 apple 跟 orange 都是水果，他沒辦法知道這些事情<br>
<br>
0:13:56.420,0:14:00.100<br>
對他來說每一個詞彙都是 independent<br>
<br>
0:14:00.860,0:14:05.440<br>
對他來說 apple 跟 pen，apple 跟 an，apple 跟 is 就是不同詞彙<br>
<br>
0:14:05.440,0:14:08.660<br>
他們中間是完全沒有任何相關性<br>
<br>
0:14:10.680,0:14:15.860<br>
那我們可以用 auto-encoder 讓語意這件事情被考慮進來<br>
<br>
0:14:16.520,0:14:19.380<br>
舉例來說，你 learn 一個 auto-encoder<br>
<br>
0:14:20.140,0:14:22.960<br>
他的 input 就是一個 document<br>
<br>
0:14:23.560,0:14:25.560<br>
或是 query 就是一段文字<br>
<br>
0:14:27.240,0:14:29.940<br>
這個在 Hinton science paper 上面是有實驗的<br>
<br>
0:14:30.380,0:14:33.920<br>
他只有用比較小的 lexicon size，只有 2000 個詞彙<br>
<br>
0:14:33.920,0:14:37.520<br>
一個 document 就把它變成一個 vector<br>
<br>
0:14:38.240,0:14:44.640<br>
把這個 vector 通過一個 encoder，把他壓成二維<br>
<br>
0:14:45.420,0:14:49.840<br>
那在 Hinton 的 paper 上你會看到結果是這個樣子<br>
<br>
0:14:50.140,0:14:53.060<br>
我記得他做的是 20 Newsgroups 的那個 corpus<br>
<br>
0:14:53.260,0:14:59.460<br>
在這個 corpus 裡面 ，document 會標示說他是屬於哪一類<br>
<br>
0:15:00.180,0:15:01.560<br>
那這邊不同顏色的點呢<br>
<br>
0:15:02.140,0:15:06.540<br>
每一個點代表一個 document，不同顏色就代表說，這篇 document 屬於哪一類<br>
<br>
0:15:06.540,0:15:09.240<br>
那其實我們在作業四就是要做類似的事情<br>
<br>
0:15:09.240,0:15:11.760<br>
希望你可以得到一樣漂亮的圖<br>
<br>
0:15:12.560,0:15:17.240<br>
你會發現同一類的 document，就都集中在一起，散佈像一朵花一樣<br>
<br>
0:15:17.620,0:15:19.660<br>
所以如果你今天要做搜尋的時候<br>
<br>
0:15:20.200,0:15:23.080<br>
今天輸入一個詞彙、查詢詞<br>
<br>
0:15:23.080,0:15:27.980<br>
那就把 query 也通過這個 encoder，把他變成一個二維的 vector<br>
<br>
0:15:28.580,0:15:31.060<br>
假設那個 query 落在這邊，你就可以知道說<br>
<br>
0:15:31.060,0:15:35.920<br>
這個 query 是跟 energy marketing<br>
<br>
0:15:36.900,0:15:38.500<br>
跟 energy market 有關的<br>
<br>
0:15:38.500,0:15:41.680<br>
就把這邊的 document retrieve 出來<br>
<br>
0:15:43.240,0:15:46.520<br>
那這個 auto-encoder 在這邊看起來結果是相當驚人<br>
<br>
0:15:47.380,0:15:50.580<br>
如果你用 LSA 的話，你得不到類似的結果<br>
<br>
0:15:50.720,0:15:53.660<br>
如果你用 LSA 的話你會發現說<br>
<br>
0:15:55.380,0:15:57.380<br>
我們剛剛有講 LSA<br>
<br>
0:15:57.380,0:15:59.820<br>
就是建一個 matrix，然後你可以找每個詞彙<br>
<br>
0:15:59.820,0:16:02.940<br>
跟每一個 document 它背後 latent 的 vector<br>
<br>
0:16:03.100,0:16:05.960<br>
一樣假設我們用 LSA，然後每個 document<br>
<br>
0:16:05.960,0:16:07.860<br>
用二維的 latent vector 來表示他<br>
<br>
0:16:07.860,0:16:13.760<br>
那你看到的結果看起來像是，像是這個樣子<br>
<br>
0:16:19.460,0:16:26.600<br>
Auto-encoder 也可以用在 image 的搜尋上面<br>
<br>
0:16:28.660,0:16:31.980<br>
你可以用在以圖找圖上面<br>
<br>
0:16:31.980,0:16:35.140<br>
那怎麼做以圖找圖呢，最簡單的方式就是<br>
<br>
0:16:35.620,0:16:40.340<br>
你就拿一張，假設這是你要找的對象<br>
<br>
0:16:40.340,0:16:42.760<br>
假設這是你 image 的 query<br>
<br>
0:16:42.760,0:16:47.480<br>
你去計算這個 image 的 query，跟其他你的 database 裡面的 image<br>
<br>
0:16:47.480,0:16:51.780<br>
他的相似程度，比如說你可以算他們在 pixel 上面的相似程度<br>
<br>
0:16:52.480,0:16:57.060<br>
然後，你再看說最像的幾張就是要 retrieve 的結果<br>
<br>
0:16:57.180,0:17:01.200<br>
如果你只是這麼做的話呢，你其實得不到太好的結果<br>
<br>
0:17:02.200,0:17:05.440<br>
假設這是你的 query，這個是 Michael Jackson<br>
<br>
0:17:06.140,0:17:08.120<br>
如果你拿這張 image<br>
<br>
0:17:08.120,0:17:10.760<br>
去跟其他 database 的 images 算相似度的話<br>
<br>
0:17:10.760,0:17:13.560<br>
你找出來最像的會是這幾張<br>
<br>
0:17:14.080,0:17:19.240<br>
你會發現 Michael Jackson 跟這個馬蹄鐵是很像的<br>
<br>
0:17:22.440,0:17:24.720<br>
這個是很像，說實在也是很像<br>
<br>
0:17:24.720,0:17:28.260<br>
所以如果只是這麼做，在 pixel wise 上做比較<br>
<br>
0:17:28.740,0:17:30.740<br>
你找不到好的結果的<br>
<br>
0:17:31.820,0:17:34.680<br>
那怎麼辦，你要用<br>
<br>
0:17:35.560,0:17:41.040<br>
你可以用 deep auto-encoder 把每一張 image 變成一個 code<br>
<br>
0:17:41.600,0:17:44.340<br>
然後在 code 上面再去做搜尋<br>
<br>
0:17:46.560,0:17:49.800<br>
而且因為今天做這件事情是 unsupervised<br>
<br>
0:17:50.340,0:17:54.920<br>
learn 一個 auto-encoder 是 unsupervised，所以你要 collect 多少 data 都行<br>
<br>
0:17:54.920,0:17:58.400<br>
你要 collect 很多很多的 data，要 collect 多少 data都行<br>
<br>
0:17:58.900,0:18:01.880<br>
train 這種 auto-encoder 的 data 是永遠不缺的<br>
<br>
0:18:02.060,0:18:08.000<br>
不像你 train supervised learning 很缺 data，做 unsupervised learning 是不缺 data<br>
<br>
0:18:09.180,0:18:12.940<br>
那現在怎麼把它變成一個 code 呢<br>
<br>
0:18:14.100,0:18:19.800<br>
這也是從 Hinton paper 上來的，input 一個 32*32 image<br>
<br>
0:18:20.760,0:18:24.740<br>
每一個 pixel 用 RGB 表示，所以 32*32*3<br>
<br>
0:18:26.640,0:18:31.780<br>
然後把這個 image 變成 8192 維，再變成 4096<br>
<br>
0:18:31.780,0:18:34.880<br>
再變 2048 維，1024 維，到256 維<br>
<br>
0:18:34.880,0:18:39.360<br>
也就是說這張 image 用 256 維的 vector 來描述他<br>
<br>
0:18:39.360,0:18:44.060<br>
你再把這個 code 通過另一個 decoder<br>
<br>
0:18:44.460,0:18:47.460<br>
它的形狀跟這個一樣，只是反過來的<br>
<br>
0:18:47.460,0:18:50.000<br>
再變回原來的 image<br>
<br>
0:18:50.280,0:18:56.460<br>
你得到 reconstruction 的結果會是這樣<br>
<br>
0:18:57.120,0:18:58.520<br>
他是可以被 construct 回來的<br>
<br>
0:18:59.080,0:19:01.860<br>
如果你不是在 pixel 上算相似度<br>
<br>
0:19:03.140,0:19:05.640<br>
而是在這種 code 上面算相似度的話<br>
<br>
0:19:05.640,0:19:08.460<br>
你就會得到比較好的結果<br>
<br>
0:19:09.300,0:19:13.180<br>
比如來說，如果是用 Michael Jackson 做 input<br>
<br>
0:19:13.180,0:19:17.840<br>
你找到的都是人臉<br>
<br>
0:19:17.840,0:19:22.000<br>
雖然這些 images 在 pixel level 上看起來是不像的<br>
<br>
0:19:22.940,0:19:27.100<br>
但是你透過很多 hidden layer ，把它轉成 code 的時候<br>
<br>
0:19:27.100,0:19:29.880<br>
在 256 維的空間上，他們是很像的<br>
<br>
0:19:31.560,0:19:33.560<br>
看起來是像的，原來是不像的<br>
<br>
0:19:33.560,0:19:36.460<br>
比如說，這是黑頭髮，這是金頭髮<br>
<br>
0:19:37.160,0:19:40.900<br>
看起來不是很像，但通過很多轉換以後<br>
<br>
0:19:41.380,0:19:45.520<br>
可能，在那個 256 維裡面，有一個 dimension 就代表人臉<br>
<br>
0:19:46.180,0:19:51.180<br>
所以他們都知道說，這些 images 都對應到人臉那個 class<br>
<br>
0:19:53.120,0:19:55.120<br>
auto-encoder 在過去有一個<br>
<br>
0:19:58.080,0:20:02.920<br>
很好的 application 是可以用在 pre-training 上面<br>
<br>
0:20:03.500,0:20:08.320<br>
我們都知道說，在 train 一個 neural network 的時候<br>
<br>
0:20:08.760,0:20:12.620<br>
你有時候在煩惱怎麼做參數的 initialization<br>
<br>
0:20:13.340,0:20:18.060<br>
有沒有一些方法讓你找到一組比較好的 initialization<br>
<br>
0:20:18.620,0:20:22.060<br>
這種找比較好的 initialization 方法，就叫做 pre-training<br>
<br>
0:20:22.060,0:20:25.760<br>
那你可以用 auto-encoder 來做 pre-training<br>
<br>
0:20:26.840,0:20:31.180<br>
怎麼做呢，假設我現在要做，比如說<br>
<br>
0:20:32.680,0:20:34.040<br>
MNIST 的 recognition<br>
<br>
0:20:34.760,0:20:38.300<br>
我可以兜個 network 他input是 784 維<br>
<br>
0:20:38.300,0:20:42.100<br>
第一個 hidden layer 1000 維，第二個 hidden layer 1000 維，然後 500 到 10 維<br>
<br>
0:20:43.240,0:20:46.480<br>
我在做 pre-train 的時候，我就先 train 一個 auto-encoder<br>
<br>
0:20:47.120,0:20:49.120<br>
這個 auto-encoder 他是這樣子<br>
<br>
0:20:50.440,0:20:54.880<br>
他 input 784 維，然後中間有個 1000 維的 vector<br>
<br>
0:20:54.880,0:20:56.880<br>
然後再把它變回 784 維<br>
<br>
0:20:57.640,0:21:03.020<br>
那我希望 input 跟 output 越接近越好<br>
<br>
0:21:03.920,0:21:07.160<br>
那在做這件事，其實你需要稍微小心一點<br>
<br>
0:21:07.600,0:21:09.620<br>
因為我們一般在做 auto-encoder 的時候<br>
<br>
0:21:09.620,0:21:14.420<br>
你會希望你的 code 比 dimension  還要小<br>
<br>
0:21:15.060,0:21:17.880<br>
那如果比 dimension 還要大會遇到什麼問題呢<br>
<br>
0:21:18.360,0:21:20.000<br>
你有可能會遇到說他就不 learn 了<br>
<br>
0:21:20.260,0:21:22.760<br>
他要 reconstruct 他就只要把 784 維<br>
<br>
0:21:22.760,0:21:26.060<br>
放到這 1000 維裡面去，然後再解回來，就結束了<br>
<br>
0:21:26.060,0:21:30.300<br>
他為啥都沒 learn 到，learn 一個接近 identity 的 matrix<br>
<br>
0:21:30.300,0:21:37.420<br>
所以你要很小心，如果你今天發現你的 hidden layer 是比 input 還要大的時候<br>
<br>
0:21:37.420,0:21:41.320<br>
code 是比 input 還要大的時候，要加一個很強的regularization<br>
<br>
0:21:41.900,0:21:45.100<br>
在這 1000 維上，所謂很強的 regularization 是說<br>
<br>
0:21:45.600,0:21:49.100<br>
你可以對這 1000 維的 output 做<br>
<br>
0:21:49.820,0:21:51.680<br>
L1 的 regularization<br>
<br>
0:21:52.160,0:21:55.920<br>
你會希望說，這 1000 維的 output 是 sparse<br>
<br>
0:21:56.760,0:22:01.460<br>
這 1000 維裡面可能只有某幾維是可以有值的，其他維都必須要是零<br>
<br>
0:22:01.980,0:22:07.840<br>
這樣你就可以避免，auto-encoder 直接把 input 硬背起來，再輸出的問題<br>
<br>
0:22:08.500,0:22:14.900<br>
總之如果你的 code 是比 input 還要大的，你要注意這種問題<br>
<br>
0:22:15.480,0:22:19.920<br>
我們現在先 learn 一個 auto-encoder，learn 好以後<br>
<br>
0:22:21.100,0:22:25.200<br>
我們把從 784 維到 1000 維的這個 weight w1<br>
<br>
0:22:25.620,0:22:29.280<br>
把它保留下來然後 fit 住<br>
<br>
0:22:30.100,0:22:39.780<br>
接下來你就把所有 database 裡面的 digit 通通變成 1000 維的 vector<br>
<br>
0:22:40.400,0:22:42.800<br>
接下來，你再 learn 另一個 auto-encoder<br>
<br>
0:22:43.440,0:22:47.920<br>
他把 1000 維的 vector 變成 1000 維的 code<br>
<br>
0:22:48.740,0:22:51.500<br>
再把 1000 維的 code 轉回 1000 維的 vector<br>
<br>
0:22:52.380,0:22:55.480<br>
你再 learn 一個這樣的 auto-encoder<br>
<br>
0:22:55.980,0:22:59.560<br>
他會讓 input 跟 output 越接近越好<br>
<br>
0:22:59.960,0:23:04.520<br>
然後你再把 w2 保存下來<br>
<br>
0:23:05.440,0:23:10.160<br>
接下來你 fix 住 w2 的值，再 learn 第三個 auto-encoder<br>
<br>
0:23:10.160,0:23:14.180<br>
第三個 auto-encoder input 1000 維，code 500維， output 1000 維<br>
<br>
0:23:14.620,0:23:20.340<br>
learn 好這個 auto-encoder，得到他的 weight w3，再把 w3 保留下來<br>
<br>
0:23:21.120,0:23:29.700<br>
這個 w1 w2 w3 就等於是你在 learn 你整個 neural network 的時候的 initialization<br>
<br>
0:23:30.220,0:23:33.940<br>
然後你最後再 random initialize 最後 500 到 100 的 weight<br>
<br>
0:23:34.640,0:23:40.880<br>
再用 back propagation 去調一遍，我們稱之為 fine tune<br>
<br>
0:23:40.880,0:23:45.140<br>
因為 w1 w2 w3 都已經是很好的 weight 了<br>
<br>
0:23:45.140,0:23:47.800<br>
我們只是微調他，所以把它叫做 fine tune<br>
<br>
0:23:47.900,0:23:53.340<br>
用 back propagation 把 w1 到 w4 的值調一下<br>
<br>
0:23:58.540,0:24:02.100<br>
你把他的值調一下，就可以 learn 好一個 neural network<br>
<br>
0:24:04.740,0:24:08.560<br>
這招 pre-training 在過去呢<br>
<br>
0:24:10.560,0:24:13.660<br>
如果你要 learn 一個很 deep 的 neural network 可能是很需要的<br>
<br>
0:24:13.740,0:24:17.760<br>
不過現在基本上 network 不用 pre-training<br>
<br>
0:24:18.460,0:24:22.080<br>
現在 training 技術進步以後，不用 pretraining 也 train 得起來<br>
<br>
0:24:22.600,0:24:24.600<br>
但 pre-training 有個妙用就是<br>
<br>
0:24:24.600,0:24:28.500<br>
如果你今天有很多 unlabeled data 只有少量 labeled data<br>
<br>
0:24:29.080,0:24:33.200<br>
你可以用大量的 unlabeled data 去把 w1 w2 w3<br>
<br>
0:24:33.480,0:24:35.480<br>
先 learn 好，先 initialize 好<br>
<br>
0:24:35.740,0:24:40.740<br>
那最後的 labeled data 就只需要稍微調整 weight 就好<br>
<br>
0:24:40.940,0:24:46.420<br>
所以 pre-training 這招在你有大量 unlabeled data 的時候還是有用的<br>
<br>
0:24:48.760,0:24:53.500<br>
有個辦法可以讓 auto-encoder 做得更好，叫做 de-noising auto-encoder<br>
<br>
0:24:53.500,0:24:56.400<br>
把 reference 列在下面給大家參考<br>
<br>
0:24:57.120,0:25:00.500<br>
他的概念其實很簡單，你把原來的 input x<br>
<br>
0:25:01.360,0:25:05.260<br>
加上一些 noise 變成 x'<br>
<br>
0:25:06.160,0:25:11.380<br>
然後你把 x' encode 以後變成 code c<br>
<br>
0:25:11.660,0:25:14.340<br>
再把 c decode 回來變成 y<br>
<br>
0:25:15.160,0:25:17.300<br>
但是要注意一下，我們現在的 y<br>
<br>
0:25:17.300,0:25:21.520<br>
本來在做 auto-encoder 是讓 input 跟 output 越接近越好<br>
<br>
0:25:22.340,0:25:26.260<br>
但現在 de-noising auto-encoder 你是要讓 output<br>
<br>
0:25:26.960,0:25:33.860<br>
跟原來的 input，再加 noise 之前的 input 越接近越好<br>
<br>
0:25:34.840,0:25:39.920<br>
如果你有做這件事，learn 出來的結果會比較 robust<br>
<br>
0:25:42.600,0:25:47.440<br>
直覺解釋就是 encoder 現在不只 learn 到 encode 這件事<br>
<br>
0:25:47.440,0:25:52.020<br>
他還可以 learn 到把雜訊濾掉這件事<br>
<br>
0:25:55.460,0:25:58.580<br>
那還有另一招叫 constrictive auto-encoder<br>
<br>
0:25:59.140,0:26:03.560<br>
這邊就沒有要細講，那 constrictive  auto-encoder 他做的事情是這樣<br>
<br>
0:26:04.220,0:26:07.620<br>
他會希望說，我們在 learn 這個 code 的時候<br>
<br>
0:26:07.760,0:26:09.760<br>
我們加上一個 constraint<br>
<br>
0:26:11.980,0:26:21.040<br>
這個 constraint 是說，當 input 有變化的時候對這個 code 的影響是被 minimize 的<br>
<br>
0:26:22.040,0:26:27.540<br>
這件事其實很像 de-noising auto-encoder 只是從不同角度來看<br>
<br>
0:26:28.000,0:26:33.320<br>
de-noising auto-encoder 是說，我加了 noise 以後，還要 reconstruct 回原來沒有 noise 的結果<br>
<br>
0:26:34.660,0:26:36.920<br>
那 constrictive auto-encoder 是說<br>
<br>
0:26:37.480,0:26:40.920<br>
我們希望說，當 input 變了，也就是加了 noise 以後<br>
<br>
0:26:40.920,0:26:43.440<br>
對這個 code 的影響是小的<br>
<br>
0:26:43.800,0:26:45.800<br>
他們做的事其實蠻類似的<br>
<br>
0:26:47.880,0:26:51.240<br>
那其實還有很多 non-linear dimension reduction 的方法<br>
<br>
0:26:51.240,0:26:54.440<br>
比如說 Restricted Boltzmann machine<br>
<br>
0:26:54.440,0:26:57.740<br>
我們這邊就沒有打算講 Restricted Boltzmann machine<br>
<br>
0:26:58.120,0:27:00.640<br>
Restricted Boltzmann machine 看起來很像 neural network，但其實不是，<br>
<br>
0:27:00.640,0:27:06.820<br>
有些人就用 neural network 來想，但文獻你怎麼看都看一頭霧水，因為他就不是 neural network<br>
<br>
0:27:08.080,0:27:11.400<br>
還有一個東西叫 deep belief  network<br>
<br>
0:27:13.280,0:27:15.960<br>
deep belief  network 聽起來很像一個 deep neural network<br>
<br>
0:27:15.960,0:27:23.400<br>
這要問一下大家意見，你覺得 deep belief  network 跟 deep neural network 是一樣的東西嗎<br>
<br>
0:27:24.300,0:27:27.180<br>
你覺得是一樣東西的同學請舉手<br>
<br>
0:27:28.020,0:27:30.020<br>
你覺得是不一樣東西的請舉手<br>
<br>
0:27:31.220,0:27:34.200<br>
大家都知道說他是不一樣的東西，這是很正確的概念<br>
<br>
0:27:34.200,0:27:35.680<br>
他們只是名字像而已<br>
<br>
0:27:35.680,0:27:40.160<br>
看 paper 架構，乍看下你會覺得是不是一樣的東西<br>
<br>
0:27:40.820,0:27:43.220<br>
但實際上他們是不一樣的東西<br>
<br>
0:27:43.480,0:27:46.240<br>
這個 deep belief  network 跟前面的 Restricted Boltzmann machine<br>
<br>
0:27:46.240,0:27:48.720<br>
他們是 graph co-model<br>
<br>
0:27:49.260,0:27:51.080<br>
他們只是看起來很像 neural network<br>
<br>
0:27:51.080,0:27:54.440<br>
你把它的概念直接套在 neural network上 ，你讀文獻會卡到不行<br>
<br>
0:27:54.940,0:27:59.500<br>
但我們還沒有打算要講這個 graph co-model<br>
<br>
0:28:01.780,0:28:06.120<br>
這個部分沒辦法細講，就留一些 reference 給大家參考<br>
<br>
0:28:07.100,0:28:10.860<br>
那接下來我們講一下 CNN 的 auto-encoder<br>
<br>
0:28:10.860,0:28:17.200<br>
如果我們今天要處理的對象是 image 的話，我們都知道你會用 CNN<br>
<br>
0:28:18.020,0:28:20.540<br>
那在 CNN 處理 image 的時候<br>
<br>
0:28:21.560,0:28:25.940<br>
你會有一些 convolution layers，pooling layers<br>
<br>
0:28:26.540,0:28:29.280<br>
用 convolution 和 pooling 交替，然後<br>
<br>
0:28:29.280,0:28:32.860<br>
讓 image 變得越來越小，最後去做 flatten<br>
<br>
0:28:33.540,0:28:36.080<br>
那今天如果是做 Auto-encoder 的話<br>
<br>
0:28:36.080,0:28:39.920<br>
你不只要有個 encoder，還要有個 decoder<br>
<br>
0:28:40.480,0:28:44.880<br>
如果 encoder 的部分是做  convolution 再做pooling，convolution 再做pooling<br>
<br>
0:28:45.320,0:28:50.420<br>
理論上 decoder 應該就是做跟 encode 相反的事情<br>
<br>
0:28:52.760,0:28:54.760<br>
decode 的時候你應該就是做<br>
<br>
0:28:54.760,0:28:59.340<br>
就是你做這些 process 得到的 code 再做反過來的事情<br>
<br>
0:28:59.940,0:29:04.260<br>
本來有 pooling 就做 unpooling，本來有 convolution 就做 deconvolution<br>
<br>
0:29:05.020,0:29:09.960<br>
但是這個 unpooling 跟 deconvolution 到底是什麼呢<br>
<br>
0:29:09.960,0:29:14.540<br>
那 training 的 criteria 就一樣，就是讓 input  和 output 越接近越好<br>
<br>
0:29:15.280,0:29:23.760<br>
但是這個 convolution 跟 deconvolution 他們是什麼呢<br>
<br>
0:29:24.400,0:29:32.640<br>
那我們現在來看一下這個 unpooling 的部分<br>
<br>
0:29:33.680,0:29:36.240<br>
我們知道在做 pooling 的時候你就是<br>
<br>
0:29:36.240,0:29:40.040<br>
現在得到一個 4X4 的 matrix<br>
<br>
0:29:40.560,0:29:47.080<br>
接下來把 matrix 裡面的 pixel 分組，四個一組<br>
<br>
0:29:47.240,0:29:49.520<br>
接下來從每一組裡面挑一個最大的<br>
<br>
0:29:49.660,0:29:55.360<br>
比如說，在這個例子裡面，你挑了這個東西<br>
<br>
0:29:55.480,0:29:58.100<br>
然後你的 image 就變成原來的四分之一<br>
<br>
0:29:58.400,0:30:04.980<br>
但是如果今天做的事情，如果你今天要做的是 unpooling<br>
<br>
0:30:04.980,0:30:08.600<br>
pooling 是這麼做，但如果你要等一下要做 unpooling<br>
<br>
0:30:08.600,0:30:14.660<br>
你要做另一件事，要先記得說我剛剛在做 pooling 是從哪裡取值<br>
<br>
0:30:15.300,0:30:21.620<br>
我在這個地址是從左上角，這四個方塊從左上角取值<br>
<br>
0:30:22.000,0:30:23.640<br>
所以這邊就是白的<br>
<br>
0:30:24.340,0:30:28.540<br>
這四個是從右下角取值，所以右下角就留下紀錄<br>
<br>
0:30:28.800,0:30:34.720<br>
從這個地方取值，這邊有個紀錄，你要記得是從哪裡取值<br>
<br>
0:30:35.660,0:30:40.300<br>
接下來如果你要做 unpooling 的時候<br>
<br>
0:30:40.780,0:30:45.100<br>
你要把原來比較小的 matrix 擴大<br>
<br>
0:30:45.540,0:30:50.200<br>
比如說原來做 pooling，原來比較大的 matrix 變成四分之一<br>
<br>
0:30:50.200,0:30:53.900<br>
現在要把比較小的 matrix 變成原來的四倍<br>
<br>
0:30:54.040,0:30:59.580<br>
那怎麼做呢<br>
這個時候你之前紀錄的 pooling 的位子就可以派上用場<br>
<br>
0:31:00.340,0:31:05.780<br>
之前記得說你在 pooling 的時候是從左上角 pool 值<br>
<br>
0:31:06.300,0:31:11.940<br>
那現在你在做 unpooling 的時候就把值放到左上角，其他補零<br>
<br>
0:31:12.200,0:31:16.200<br>
記得在這裡取值，就把這個值放到右下角其他都補零<br>
<br>
0:31:16.640,0:31:19.760<br>
這裡取值，就把值放到這裡其他補零<br>
<br>
0:31:20.100,0:31:22.740<br>
這裡取值，就把值放到這邊其他補零<br>
<br>
0:31:23.300,0:31:25.780<br>
這就是 unpooling 的其中一種方式<br>
<br>
0:31:26.020,0:31:29.880<br>
所以做完 unpooling 以後本來一張比較小的 image 會變得比較大<br>
<br>
0:31:30.420,0:31:36.340<br>
原來14X14 的 image 會變成 28X28 的 image<br>
<br>
0:31:37.800,0:31:43.060<br>
你會發現它就是把原來 14*14 的 image 做一下擴散<br>
<br>
0:31:43.060,0:31:46.520<br>
有些藍色的地方就是補零<br>
<br>
0:31:47.640,0:31:50.020<br>
每一個原來在 14*14 image 裡面的值<br>
<br>
0:31:50.520,0:31:54.260<br>
擴散到 28*28 裡面，他都會加上三個零<br>
<br>
0:31:55.260,0:31:59.220<br>
其實這不是 unpooling 唯一的做法，他在 Keras 裡面作法是不一樣的<br>
<br>
0:31:59.580,0:32:01.580<br>
就我所知 Keras 裡面的做法<br>
<br>
0:32:01.880,0:32:03.880<br>
他是直接 repeat 那些 value<br>
<br>
0:32:04.300,0:32:08.660<br>
也就是說不用去記你之前 pooling 的位子<br>
<br>
0:32:09.120,0:32:17.100<br>
你就直接把這個值複製四份，就行了<br>
<br>
0:32:17.200,0:32:19.200<br>
就我所知 Keras 裡面是這麼做<br>
<br>
0:32:20.800,0:32:25.040<br>
接下來比較難理解的地方叫做 Deconvolution<br>
<br>
0:32:25.880,0:32:31.400<br>
原來 convolution 已經很難懂了，deconvolution 到底是什麼呢<br>
<br>
0:32:31.680,0:32:34.060<br>
大家很難搞清楚他是什麼<br>
<br>
0:32:34.680,0:32:37.880<br>
事實上 deconvolution 就是 convolution<br>
<br>
0:32:39.100,0:32:45.900<br>
這樣大家知道我的意思嗎？如果你看 Keras 的 code，作業三不是有個連結教你怎麼做 auto-encoder<br>
<br>
0:32:46.140,0:32:48.140<br>
不知道怎麼回事，大家都好像沒有做一樣<br>
<br>
0:32:52.540,0:32:56.700<br>
你會發現你看那個 code，根本就沒有什麼 deconvolution 這種東西<br>
<br>
0:32:56.780,0:33:01.440<br>
他就是做 convolution，他在做 decode 的地方再做 convolution<br>
<br>
0:33:01.900,0:33:05.900<br>
這是怎麼一回事，我們不是本來應該是做一個 convolution 的相反<br>
<br>
0:33:05.980,0:33:09.120<br>
叫做 deconvolution 嗎，怎麼又是在做 convolution<br>
<br>
0:33:09.580,0:33:13.220<br>
其實 deconvolution 就是 convolution 我們來解釋一下<br>
<br>
0:33:13.220,0:33:17.560<br>
所以有人說 deconvolution 名字取的不好會讓大家困惑<br>
<br>
0:33:18.580,0:33:22.100<br>
我們舉一維的 convolution<br>
<br>
0:33:23.380,0:33:28.440<br>
我們平常做 image 是做二維的，但那個圖有點累，我們取一維的當做例子<br>
<br>
0:33:29.160,0:33:33.620<br>
一維的 convolution 是怎樣呢，我們假設 input 有五個 dimension<br>
<br>
0:33:34.200,0:33:36.200<br>
然後我們的 filter size 是三<br>
<br>
0:33:37.340,0:33:46.360<br>
那我們就把 input 的這三個 value 分別乘上紅色藍色綠色的 weight，得到一個 output<br>
<br>
0:33:46.540,0:33:52.640<br>
再把這個 filter shift 一格，把這三個 value 分別乘上紅色藍色綠色的 weight 得到下一個 output<br>
<br>
0:33:52.640,0:33:56.340<br>
再 shift 一格，乘上紅色藍色綠色的 weight 再得到一個 output<br>
<br>
0:33:56.340,0:34:00.440<br>
這是 convolution，deconvolution 應該是怎樣呢<br>
<br>
0:34:00.920,0:34:05.480<br>
你的想像可能會是這樣子，deconvolution 就是 convolution 的相反<br>
<br>
0:34:06.060,0:34:09.400<br>
所以本來是三個值變成一個值<br>
<br>
0:34:09.400,0:34:14.960<br>
在做 deconvolution 的時候，就應該是一個值變三個值<br>
<br>
0:34:14.960,0:34:21.620<br>
然後發現這邊動畫有一個錯，這個圈圈不應該出現，請忽視他<br>
<br>
0:34:22.280,0:34:27.580<br>
所以你現在應該是一個值變成三個值，所以怎麼做呢<br>
<br>
0:34:29.000,0:34:34.260<br>
你的想像是這樣，一個值分別乘上紅色綠色藍色的 weight 變成三個值<br>
<br>
0:34:35.480,0:34:38.760<br>
這個值也乘上紅色綠色藍色的 weight 變成三個值<br>
<br>
0:34:39.600,0:34:44.440<br>
但是他已經有在這邊貢獻一些值了，他也要在這邊貢獻一些值怎麼辦呢<br>
<br>
0:34:44.800,0:34:52.800<br>
就加起來，他產生三個值，他也產生三個值，重疊的地方加起來<br>
<br>
0:34:53.760,0:34:58.580<br>
然後，他也產生三個值，重疊的地方加起來<br>
<br>
0:34:59.580,0:35:07.020<br>
但事實上這件事情等同於，你看看我這麼說對不對<br>
<br>
0:35:07.020,0:35:10.020<br>
等同於是在做 convolution，為什麼呢<br>
<br>
0:35:10.020,0:35:13.180<br>
他等同於是把，我們 input 就是三個 value<br>
<br>
0:35:13.180,0:35:16.380<br>
然後我們會做這個<br>
<br>
0:35:16.660,0:35:20.060<br>
把它做 padding，在旁邊補零<br>
<br>
0:35:20.620,0:35:25.420<br>
接下來我們一樣做 convolution<br>
<br>
0:35:25.420,0:35:31.520<br>
做 convolution 的時候三個 input 乘上綠色藍色紅色的 weight 得到一個值<br>
<br>
0:35:32.080,0:35:36.100<br>
三個 input 乘上綠色藍色紅色的 weight 得到一個值，以此類推<br>
<br>
0:35:36.560,0:35:43.840<br>
你會發現說，這個框框裡面做的事情，跟這個框框裡面做的事情是一模一樣的<br>
<br>
0:35:44.400,0:35:51.060<br>
怎麼說呢，我們檢查中間這個值，他是三個 value 加起來<br>
<br>
0:35:51.760,0:35:58.420<br>
這三個 value 分別是他乘上綠色，他乘上藍色，他乘上紅色再加起來<br>
<br>
0:35:58.420,0:36:02.860<br>
那這邊這個 value 的值也是他乘上綠色，他乘上藍色，他乘上紅色再加起來<br>
<br>
0:36:03.400,0:36:06.400<br>
所以這件事情跟這件事情是一樣的<br>
<br>
0:36:06.400,0:36:10.000<br>
你檢查這邊的1 2 3 4 5，5 個值<br>
<br>
0:36:10.280,0:36:13.860<br>
跟這邊的 1 2 3 4 5，5 個值，他們是一樣的<br>
<br>
0:36:14.900,0:36:20.440<br>
如果你把 deconvolution 跟 convolution 做比較，他們不同點在哪裡？<br>
<br>
0:36:20.800,0:36:24.000<br>
不同點是在他們的 weight 是相反<br>
<br>
0:36:24.400,0:36:31.420<br>
他這邊是紅藍綠，這邊是綠藍紅，正好是相反<br>
<br>
0:36:31.940,0:36:36.080<br>
但他做的 operation 一樣也就是 convolution 這件事<br>
<br>
0:36:36.080,0:36:41.480<br>
所以你把這個 input 做 *** ，再做 convolution其實就等於是 deconvolution<br>
<br>
0:36:41.620,0:36:46.300<br>
你會發現在 Karas 裡面，你根本就不需要再另外寫一個 deconvolution 的 layer<br>
<br>
0:36:46.300,0:36:49.520<br>
你直接 call 一個 convolution 的 layer 就可以了<br>
<br>
0:36:52.120,0:36:55.820<br>
我本來想要講 sequence 的 auto-encoder<br>
<br>
0:36:55.820,0:36:59.140<br>
不過可以等到 RNN 的時候再講<br>
<br>
0:36:59.140,0:37:03.060<br>
我們知道說，剛才我們看到的 auto-encoder 他的 input<br>
<br>
0:37:03.380,0:37:07.040<br>
通通都是 fix-length 的 vector，但很多東西<br>
<br>
0:37:08.520,0:37:12.380<br>
有很多東西你本質上不該把他表示成 vector，比如說語音<br>
<br>
0:37:12.380,0:37:14.960<br>
一段聲音訊號有長有短他不是一個 vector<br>
<br>
0:37:14.960,0:37:18.980<br>
一段文章有長有短他不是一個 vector<br>
<br>
0:37:18.980,0:37:20.780<br>
你雖然可以用 bag-of-word 把它變成一個 vector<br>
<br>
0:37:21.360,0:37:26.840<br>
但這個方法你會失去詞彙和詞彙間的前後關係<br>
<br>
0:37:31.300,0:37:37.440<br>
所以我們剛才已經講了 dimension reduction<br>
<br>
0:37:40.140,0:37:44.260<br>
我們剛都是用 encoder 來把原來的 image 變成小的 dimension<br>
<br>
0:37:44.960,0:37:48.260<br>
但是我們同時也 train 一個 decoder 不是嗎<br>
<br>
0:37:48.700,0:37:55.620<br>
那個 decoder 其實是有妙用的，你可以拿 decoder 來產生新的 image<br>
<br>
0:37:56.080,0:37:58.680<br>
也就是說我們把 learn 好的 decoder 拿出來<br>
<br>
0:37:59.020,0:38:04.120<br>
然後給他一個 random 的 input number，他的 output 希望就是一張圖<br>
<br>
0:38:04.560,0:38:08.340<br>
這件事可以做到嗎，其實這件事做起來相當容易<br>
<br>
0:38:08.760,0:38:12.160<br>
我就胡亂拿MNIST 來秒 train 一下<br>
<br>
0:38:12.740,0:38:20.260<br>
然後我把每一張圖，784 維的 image 通過一個 hidden layer 然後 project 到二維<br>
<br>
0:38:20.360,0:38:23.720<br>
再把二維通過一個 hidden layer 解回原來的 image<br>
<br>
0:38:23.800,0:38:28.560<br>
那在 encoder 的部分，那個二維的 vector 畫出來長這樣<br>
<br>
0:38:28.560,0:38:30.280<br>
跟 Hinton 那個圖其實是有些像<br>
<br>
0:38:30.760,0:38:35.760<br>
那不同顏色的點代表不同數字，畫出來是這個樣子<br>
<br>
0:38:36.980,0:38:40.360<br>
接下來我在紅色這個框框裡面<br>
<br>
0:38:40.940,0:38:45.220<br>
等間隔的去 sample 一個二維的 vector 出來<br>
<br>
0:38:45.220,0:38:48.200<br>
然後把那個二維的 vector 丟到 NN decoder 裡面<br>
<br>
0:38:48.200,0:38:50.920<br>
然後叫他 output 一個 image 出來<br>
<br>
0:38:51.760,0:38:55.600<br>
這些二維的 vector 不見得是某個 image compress 以後的結果<br>
<br>
0:38:55.820,0:38:59.480<br>
他不見得原來有對應的 image，他就是某個二維的 vector<br>
<br>
0:38:59.480,0:39:02.920<br>
然後丟到 decoder 裡面，看看他可以產生什麼<br>
<br>
0:39:03.040,0:39:10.540<br>
你會發現我們在紅色框框內等距離的做 sample<br>
<br>
0:39:10.540,0:39:13.440<br>
得到的結果就是這樣<br>
<br>
0:39:14.380,0:39:18.300<br>
你就可以發現很多有趣的現象<br>
<br>
0:39:18.540,0:39:25.820<br>
從下到上，感覺是圓圈然後慢慢的就垮了<br>
<br>
0:39:26.360,0:39:30.540<br>
這邊本來是不知道是四還是九，然後變八<br>
<br>
0:39:30.540,0:39:34.080<br>
然後越來越細，變成 1<br>
<br>
0:39:34.160,0:39:38.560<br>
最後不知道為什麼變成 2，還蠻有趣的<br>
<br>
0:39:39.880,0:39:43.500<br>
你會發現這邊感覺比較差，為什麼呢<br>
<br>
0:39:43.560,0:39:45.940<br>
因為在這邊其實是沒有 image<br>
<br>
0:39:45.940,0:39:48.780<br>
你在 input image 的時候其實不會對到這邊<br>
<br>
0:39:48.980,0:39:55.060<br>
這個區域的 vector sample 出來，通過 decoder 他解回來不是 image<br>
<br>
0:39:55.060,0:39:57.360<br>
他看起來是一個有點怪怪的東西<br>
<br>
0:39:58.440,0:40:06.080<br>
有人可能會說，你怎麼知道要 sample 在這個地方<br>
<br>
0:40:06.220,0:40:09.940<br>
因為我必須要先觀察一下二維 vector 的分佈<br>
<br>
0:40:10.480,0:40:12.700<br>
才能知道哪邊是有值的<br>
<br>
0:40:12.700,0:40:15.600<br>
才知道從那個地方 sample 出來比較有可能是一個 image<br>
<br>
0:40:16.140,0:40:20.580<br>
如果我 sample 在這個地方，我相信我得到東西看起來就不像是 image 了<br>
<br>
0:40:21.560,0:40:27.360<br>
可是這樣你要先分析二維的 code 感覺有點麻煩<br>
<br>
0:40:27.720,0:40:33.940<br>
那怎麼辦，怎麼確保說，在我們希望的 region 裡面都是 image<br>
<br>
0:40:34.320,0:40:37.700<br>
有個很簡單的做法就是在你的 code 上面加 regularization<br>
<br>
0:40:38.120,0:40:42.280<br>
在你的 code 直接加上 L2 的 regularization，讓所有的 code 都比較接近零<br>
<br>
0:40:42.720,0:40:44.480<br>
接下來就在零附近 sample<br>
<br>
0:40:44.560,0:40:49.240<br>
比較有可能你 sample 出來的 vector 都可以對應到數字<br>
<br>
0:40:49.460,0:40:54.400<br>
所以我就做這樣的事情，我在 train 的時候加上 L2 的 regularization 在 code 上面<br>
<br>
0:40:55.020,0:40:59.000<br>
train 出來你的 code 都會集中在接近零的地方<br>
<br>
0:40:59.360,0:41:02.920<br>
他就會以零為核心，然後分佈在接近零的地方<br>
<br>
0:41:03.600,0:41:09.360<br>
接下來我就以零為中心，然後等距的在這個紅框內 sample image<br>
<br>
0:41:09.860,0:41:12.960<br>
sample 出來就這個樣子<br>
<br>
0:41:13.440,0:41:17.040<br>
從這邊你就可以觀察到很多有趣的現象，你會發現說<br>
<br>
0:41:17.520,0:41:21.060<br>
他的這個 dimension 跟這個 dimension 是有意義的<br>
<br>
0:41:21.380,0:41:25.340<br>
從左到右橫軸代表的是有沒有圈圈<br>
<br>
0:41:25.840,0:41:31.660<br>
本來是很圓的圈圈， 然後接下來就慢慢變成一<br>
<br>
0:41:32.320,0:41:38.560<br>
那如果是縱的呢，我覺得本來是正的，然後慢慢就倒過來<br>
<br>
0:41:38.940,0:41:43.640<br>
這邊也是本來是正的，然後往下面慢慢就倒過來<br>
<br>
0:41:44.660,0:41:50.820<br>
所以你可以不只是做 encode，還可以用 code 來畫<br>
<br>
0:41:50.820,0:41:57.280<br>
這個 image 並不是從原來 image database sample 出來的，他是 machine 自己畫出來的<br>
<br>
0:41:57.280,0:42:02.420<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
