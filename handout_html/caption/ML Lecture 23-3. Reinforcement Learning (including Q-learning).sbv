0:00:00.000,0:00:04.560
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心

0:00:04.560,0:00:12.460
Lecture 4，我們想要講的就是，
如何讓 machine 學會和環境作互動

0:00:12.460,0:00:16.120
那主要要講的就是 Reinforcement learning

0:00:16.460,0:00:22.560
那機器學習和環境作互動，
跟一般 machine learning 的問題，有什麼不一樣呢

0:00:22.660,0:00:33.180
我覺得最大的差異就是，機器所採取的行為，
會影響到它未來發生的事情

0:00:36.200,0:00:38.820
比如我們假設以影像分類的問題為例

0:00:38.840,0:00:41.980
來對照讓機器學習和環境作互動

0:00:43.020,0:00:45.420
那如果你今天是做影像的分類

0:00:45.420,0:00:49.180
給機器看一張圖片，
讓它決定這張圖片是貓還是狗

0:00:49.320,0:00:52.400
那一個非常類似的 task 是要讓機器和環境互動

0:00:52.400,0:00:56.580
可能是給他一張圖片，假設那機器其實是自駕車好了

0:00:56.860,0:01:02.020
給它一張圖片，一個影像，它決定它要採取怎樣的行為

0:01:02.080,0:01:04.620
這跟一般的分類問題有什麼不同呢？

0:01:04.720,0:01:09.540
在一般的分類問題裡面，你得到輸出的結果以後

0:01:09.580,0:01:13.600
就沒事了，我給他一張圖片，
覺得是貓就是貓，覺得是狗就是狗

0:01:14.000,0:01:16.000
但是在互動的問題裡面

0:01:16.020,0:01:18.800
假設是一個自駕車，給他看一個圖片

0:01:18.800,0:01:21.320
要決定現在要往左轉與往右轉

0:01:21.440,0:01:25.900
它的決策，會影響接下來它看到的 data

0:01:25.900,0:01:30.680
也就是說它如果決定往左轉，
接下來它看到的就是往左轉街道的樣子

0:01:30.760,0:01:33.360
決定往右轉，
它看到的就是往右轉街道的樣子

0:01:33.880,0:01:37.320
所以今天讓機器學習和環境作互動

0:01:37.520,0:01:41.440
跟一般的 learning problem，是不太一樣的

0:01:41.440,0:01:44.200
所以會需要另外來討論它

0:01:44.200,0:01:45.700
它最不一樣的地方就是

0:01:45.700,0:01:47.960
今天機器和環境作互動的時候

0:01:47.960,0:01:53.340
它會影響環境，它會影響它接下來的 input

0:01:55.080,0:01:59.880
好，那有什麼樣的例子，
是機器要學習和環境作互動的呢？

0:02:00.040,0:02:02.820
大家都知道的就是 Alpha Go

0:02:02.920,0:02:07.720
在 Alpha Go 裡面，你有一個 machine 在下圍棋

0:02:08.780,0:02:10.780
Alpha Go 的輸入是什麼？

0:02:10.800,0:02:13.940
Alpha Go 的輸入，就是棋盤的盤勢

0:02:14.020,0:02:15.860
黑子和白子的位置

0:02:15.860,0:02:20.000
它的輸出就是接下來應該要落子的位置

0:02:20.280,0:02:25.260
那我們剛才講說，
今天在機器學習和環境作互動的過程中

0:02:25.280,0:02:30.460
最重要的特色就是，你所採取的行為，
會影響往後的發展

0:02:30.700,0:02:33.260
在圍棋裡面，顯然是這樣子

0:02:33.380,0:02:36.740
你落子的位置，當然會影響你對手落子的位置

0:02:36.960,0:02:43.620
你今天出手下在天元，對方一定守，跟你出手下在 5-5，對方一定守，顯然是會不一樣的

0:02:46.080,0:02:50.060
在做這種和環境互動的，你通常會訂一個目標

0:02:50.060,0:02:52.720
舉例來說在下圍棋裡面，
要達成的目的，當然就是要贏棋

0:02:52.720,0:02:58.060
舉例來說在下圍棋裡面，
要達成的目的，當然就是要贏棋

0:02:58.640,0:03:03.900
那在機器和環境作互動的文獻上，
你常常看到一個字眼叫做 state

0:03:04.040,0:03:11.640
這是什麼意思，在過去，
我們通常覺得說外界來的資訊太過複雜

0:03:11.700,0:03:16.460
我們會需要有另一個 model 幫我們把外接資訊做摘要

0:03:16.540,0:03:19.720
做完摘要以後，再丟給 machine

0:03:19.940,0:03:23.220
外界輸入資訊沒有處理的，就做 observation

0:03:23.220,0:03:27.520
但是今天隨著 deep learning 技術的發展，
model 的能力已經越來越強

0:03:27.520,0:03:32.380
但是今天隨著 deep learning 技術的發展，
model 的能力已經越來越強

0:03:32.380,0:03:37.080
我們不太再需要幫 model 另外再去做
資訊的 summarization

0:03:37.080,0:03:41.600
反正它自己有那麼多個 layer，
它自己會決定說 input 這個 raw 的 feature

0:03:41.600,0:03:45.000
observation 哪些資訊是可以用的，哪些是它不要的

0:03:45.060,0:03:49.560
所以 observation 跟 state 
現在這兩個詞彙基本上是混著用的

0:03:49.760,0:03:54.760
所以當我講 state 跟 observation 
其實我指的是一樣的東西

0:03:55.200,0:03:58.240
讓機器下圍棋，是一個機器和環境互動的例子

0:03:58.500,0:04:01.740
舉例來說，讓機器學會玩電玩

0:04:01.900,0:04:04.700
也是一個和環境作互動的例子

0:04:04.700,0:04:06.740
在機器學習玩電玩的時候

0:04:06.740,0:04:10.480
它有另外一個 AI，另外一個主機當作對手

0:04:10.480,0:04:13.820
機器採取的行為，當然會影響對方的回應

0:04:13.880,0:04:16.500
如果你想做一些 video game 的 playing 的話

0:04:16.740,0:04:19.040
你可以參考下面這兩個連結

0:04:19.040,0:04:26.580
OpenAI 它們提供了一些平台，
讓你的機器可以練習去玩一些遊戲，比如說 GTA

0:04:27.280,0:04:29.280
有人可能會問說，讓機器學習玩遊戲，
好像沒有什麼特別的

0:04:32.560,0:04:37.860
有人可能會問說，讓機器學習玩遊戲，
好像沒有什麼特別的

0:04:38.860,0:04:41.600
遊戲主機裡面就有一個 AI，它也會玩遊戲嘛

0:04:41.620,0:04:45.500
當機器學習玩遊戲後，它看到的遊戲畫面，
跟人看到的遊戲畫面，是一模一樣的

0:04:46.080,0:04:54.260
再來呢，它並沒有 handcrafted 的 rule

0:04:54.440,0:04:56.940
告訴它什麼樣的行為是好的，
什麼樣的行為是不好的

0:04:56.940,0:05:00.580
告訴它什麼樣的行為是好的，
什麼樣的行為是不好的

0:05:00.720,0:05:04.020
它是人告訴他說看到這樣的畫面，
看到這樣的狀況，你就採取這樣的行為

0:05:04.100,0:05:09.920
它是人告訴他說看到這樣的畫面，
看到這樣的狀況，你就採取這樣的行為

0:05:10.160,0:05:12.360
但是當我們讓機器自己學習的時候，
沒有人告訴機器說，該採取什麼樣的行為

0:05:12.360,0:05:15.040
沒有人告訴機器說應該採取什麼樣的行為

0:05:15.040,0:05:19.680
它必須要自己 dig out 什麼樣的行為是好的，
什麼樣的行為是不好的

0:05:20.600,0:05:24.840
但和環境作互動，還有很多例子，
比如說自駕車也是一個例子

0:05:24.840,0:05:27.720
如果你今天要做一台自駕車的話

0:05:27.740,0:05:31.800
比如說看到一個紅燈，
然後它就決定說，現在要踩個煞車

0:05:31.800,0:05:36.240
那當然，我們剛才用自駕車的例子有講過說

0:05:36.240,0:05:41.520
比如說看到一個紅燈，
然後它就決定說，現在要踩個煞車

0:05:41.520,0:05:44.840
那當然，我們剛才用自駕車的例子有講過說

0:05:44.840,0:05:48.980
機器採取不同的行為，
就會影響它接下來看到的畫面

0:05:49.020,0:05:53.780
隨著採取的行為不同，
接下來它看到的 observation，就會不一樣

0:05:54.060,0:05:58.540
或者是說，假設你想要做一個 chat-bot，
也是一樣

0:05:58.700,0:06:00.960
然後 machine 回答說，你希望從哪裡出發

0:06:00.960,0:06:03.740
你說，你想要訂 11/5 到台北的機票

0:06:03.800,0:06:07.760
然後 machine 回答說，你希望從哪裡出發

0:06:08.720,0:06:11.760
這個輸入，和輸出

0:06:12.060,0:06:15.300
這個東西，你可以想成，你有一個巨大的 network

0:06:15.420,0:06:18.800
輸入一個 input，然後你就可以回一個輸出

0:06:18.940,0:06:24.480
但是今天你的 Dialog system 它的輸出，
會影響它接下來看到的 input

0:06:24.620,0:06:28.560
但是假如今天 dialog system 的輸出
是比如說你要幾點出發呢？

0:06:28.760,0:06:33.080
那它接下來看到的 input，可能就是，
我要從 Boston 出發

0:06:33.360,0:06:38.120
但是假如今天 dialog system 的輸出
是比如說你要幾點出發呢？

0:06:38.140,0:06:40.780
它看到的輸入，可能就是我要 6 點出發

0:06:40.800,0:06:43.820
所以跟其他互動的 task 一樣

0:06:43.860,0:06:46.180
今天你要訓練一個對話系統的時候

0:06:46.180,0:06:48.420
環境就是指你的客人

0:06:49.160,0:06:51.160
但你今天採取的行為，會影響你客人的回應

0:06:51.240,0:06:53.860
環境就是指你的客人

0:06:53.860,0:06:58.140
但你今天採取的行為，會影響你客人的回應

0:06:59.120,0:07:02.680
好，要怎麼解這種和環境互動的問題呢？

0:07:02.860,0:07:05.080
你當然可以說，和環境互動的問題

0:07:05.080,0:07:10.920
我們就直接把它想成是一個分類的問題

0:07:10.920,0:07:14.260
直接把它當成是一個
 supervised learning 的問題

0:07:14.260,0:07:18.400
用 supervised learning 的方法，去學一個 network

0:07:18.400,0:07:20.240
怎麼說，假設是圍棋的話

0:07:20.240,0:07:24.720
我們就教 machine，看到這個盤勢，你就下 3 3

0:07:24.800,0:07:27.460
那你就跟人做的一樣，
那這不過是一個 supervised model

0:07:27.460,0:07:29.860
人看到這個盤勢，就會下 3 3

0:07:29.860,0:07:33.480
那你就跟人做的一樣，
那這不過是一個 supervised model

0:07:33.480,0:07:37.380
看到這個盤勢，輸出 3 3 的機率要越大越好

0:07:37.880,0:07:40.340
好，那如果是自駕車，你就教機器說

0:07:40.580,0:07:43.600
看到這個畫面，你就踩煞車

0:07:43.600,0:07:45.520
那你說要怎麼收集這種 data 呢？

0:07:45.660,0:07:51.840
你就 collect 人在開車的時候，
行車紀錄器的畫面，還有人的動作

0:07:51.880,0:07:56.280
你要調這個 network 的參數，讓它的 output 是踩煞車

0:07:56.500,0:07:59.140
機器要學到說，看這個畫面的時候

0:07:59.140,0:08:02.820
你要調這個 network 的參數，讓它的 output 是踩煞車

0:08:02.980,0:08:09.760
如果是對話系統的話，
你就搜集很多真人的客服，跟顧客的對話

0:08:09.760,0:08:13.420
那你知道真人的顧客說，
我想訂 11/5 到台北的機票

0:08:13.420,0:08:16.160
真人會問說，你要從哪裡出發？

0:08:16.360,0:08:19.860
那當我們用這種方式，讓機器做學習的時候

0:08:19.860,0:08:22.480
你要回答的跟真人客服，越像越好

0:08:22.480,0:08:23.700
你就這樣輸出

0:08:24.100,0:08:27.560
那當我們用這種方式，讓機器做學習的時候

0:08:27.560,0:08:34.980
這個叫做 behavior cloning，
就是複製行為，複製 expert 的行為

0:08:34.980,0:08:39.500
我們可以告訴機器說，真人駕駛，
看到這個畫面就採取怎麼樣的行為

0:08:39.500,0:08:43.500
真人的客服，聽到顧客說這句話，就這樣子回答

0:08:43.660,0:08:48.520
你就跟你的老師，你的 expert 做得一模一樣就好了

0:08:48.520,0:08:51.160
就希望你可以學得跟 expert 一樣厲害

0:08:51.240,0:08:54.220
這麼做，會有什麼樣的問題呢？

0:08:54.220,0:08:59.280
以下是一個 behavior cloning 的例子，
那其實跟 machine learning 沒什麼關係

0:09:07.940,0:09:13.880
以下部分省略

0:09:45.660,0:09:50.260
好，所以機器在學的時候，
它就跟 Shelton 一樣困惑了

0:09:50.320,0:09:55.000
當它有一個人類的老師的時候，
人類的老師，採取的一些行為

0:09:55.040,0:10:00.820
機器能夠做的事情，就是相信它的老師全然是對的，
然後完全去模仿它的老師

0:10:01.140,0:10:05.440
事實上，假設機器可以完全模仿它的老師

0:10:05.440,0:10:07.760
也許問題並沒有很大

0:10:07.760,0:10:10.880
機器只是學了一些不該學的東西而已

0:10:10.920,0:10:13.160
但真正害怕的 case 是什麼呢？

0:10:13.160,0:10:17.760
真正害怕的 case 是，
機器沒有辦法完全模仿它的老師

0:10:17.860,0:10:23.300
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的

0:10:23.300,0:10:26.920
到底哪些東西該學，那些東西不該學

0:10:27.060,0:10:29.060
就變成是一個問題了

0:10:29.400,0:10:32.160
這 behavior cloning 我覺得最大的問題就是

0:10:32.300,0:10:40.260
如果是只有 behavior cloning 的話，你沒有告訴 machine 說什麼樣的行為是重要的，什麼樣的行為是不重要的

0:10:40.260,0:10:46.740
舉例來說，剛剛 Shelton 在學中文的時候，
它不知道說語音是重要的，手勢是不重要的

0:10:46.740,0:10:52.280
對機器來說，你今天就給它一個示範，
它又不知道說到底是語音重要，還是手勢重要

0:10:52.280,0:10:55.480
就好像有一個人說，它想要成為成功的人物

0:10:55.480,0:10:57.100
那整個結果就會壞掉

0:10:57.180,0:11:01.680
所以 behavior cloning 最大的問題就是，
機器不知道說什麼是重要的，什麼是不重要的

0:11:01.680,0:11:04.380
它只能夠做照單全收這件事

0:11:04.460,0:11:07.280
就好像有一個人說，它想要成為成功的人物

0:11:07.280,0:11:11.700
比如說它想跟 Jobs 一樣，
他就列出了 Jobs 的 20 個人格特質

0:11:11.700,0:11:14.960
可能包括勤奮，創造力，還有壞脾氣

0:11:15.180,0:11:18.340
然後他就覺得說他能力很差，
三項裡面他學一樣就好了

0:11:18.340,0:11:22.200
他就決定他只學壞脾氣，然後就一無是處這樣子

0:11:22.400,0:11:24.800
所以 behavior cloning 的問題就是這樣

0:11:25.620,0:11:33.820
好，那在這種與環境互動的情況下，
有些行為是重要的，有些行為是不重要的

0:11:34.300,0:11:37.460
為什麼，有些行為是重要的，有些行為是不重要的呢？

0:11:37.460,0:11:42.120
因為你現在你的 machine 採取的行為，
會影響接下來的發展

0:11:42.120,0:11:46.940
所以有些行為非常的關鍵，
有些行為也許沒那麼關鍵

0:11:47.040,0:11:51.180
如果說，behavior cloning，機器學不到這件事

0:11:51.260,0:11:56.480
好那要怎麼樣讓機器，真的可以在和環境互動的情況下學好呢

0:11:56.680,0:12:01.140
你就不能單純的考慮每一個 step

0:12:01.140,0:12:05.460
你就不能夠只是告訴機器說，在看到這個盤勢的時候

0:12:05.480,0:12:07.240
你就下在這個位置

0:12:07.240,0:12:12.580
你不能只把他當作一個簡單的，
一般的 supervised learning 來看待

0:12:12.600,0:12:14.480
機器是沒有辦法學好的

0:12:14.480,0:12:19.180
你要讓機器把所有 actions 都當作整體來看待

0:12:19.180,0:12:23.240
那怎麼做到這一件事呢？有兩個方向

0:12:23.240,0:12:26.820
一個方向，就是大家都知道的 
reinforcement learning

0:12:27.240,0:12:30.880
那在 reinforcement learning 裡面，機器會去跟環境互動

0:12:31.000,0:12:32.820
他自己去跟環境互動

0:12:32.820,0:12:36.520
他在跟環境互動的過程中，他會得到一些 reward

0:12:36.600,0:12:40.380
告訴他說，甚麼樣的行為是好的，甚麼樣的行為是不好的

0:12:40.820,0:12:45.040
接下來機器要自己去學習說，怎麼樣可以得到比較好的reward

0:12:45.060,0:12:46.540
怎麼樣多採取好的行為

0:12:46.540,0:12:51.340
避免採取會得到 negative reward 會得到差的評價的行為

0:12:51.560,0:12:55.700
另外一個也許大家可能比較沒有哪麼熟悉的，
叫做 learning by demonstration

0:12:55.940,0:13:00.080
learning by demonstration 又叫做 imitation learning 或著是 apprenticeship learning

0:13:00.380,0:13:05.920
那在這種 task 裡面，
機器它有一些 expert 的 demo

0:13:06.180,0:13:11.120
但是它今天在學習 expert 的行為的時候，
它必須要有特別的學習方式

0:13:11.120,0:13:16.200
而不是使用照單全收，behavior cloning 的方式

0:13:16.200,0:13:19.840
所以 learning by demonstration，
它並不是一般 supervised learning 的問題

0:13:19.920,0:13:21.920
機器不是複製 expert 的行為

0:13:21.980,0:13:26.960
而是用其他方法，來讓它可以學得跟 expert 一樣好

0:13:27.220,0:13:29.940
好，那我們就先來講 reinforcement learning

0:13:29.940,0:13:34.740
但其實我們除了講 reinforcement learning，
還會講 inverse reinforcement learning 的技術

0:13:34.740,0:13:39.920
其實 inverse reinforcement learning 的技術，
就是 learning by demonstration

0:13:40.260,0:13:45.800
所以，inverse reinforcement learning 的技術，
就是 learning by demonstration 的其中一種

0:13:46.080,0:13:49.440
好，那我們先來看一下 reinforcement learning

0:13:49.460,0:13:51.460
等一下如果時間夠的話

0:13:51.500,0:13:57.540
我們就會先介紹 Actor，再介紹 Critic，
然後再介紹 Actor + Critic 的方法

0:13:59.140,0:14:03.600
那我們分別以電玩，和下圍棋為例，
來說明這3 個東西分別是什麼

0:14:03.620,0:14:05.760
第一個 component 是一個 actor

0:14:05.760,0:14:07.720
第二個 component 是一個 environment

0:14:07.720,0:14:10.780
第三個 component 是一個 reward function

0:14:10.780,0:14:16.080
那我們分別以電玩，和下圍棋為例，
來說明這3 個東西分別是什麼

0:14:16.180,0:14:22.549
如果在電玩裡面 你的actor要做的事情 就是去操控搖桿

0:14:22.549,0:14:25.380
決定要向左向右，還是開火

0:14:25.420,0:14:30.400
如果是在下圍棋的時候，
它就決定說現在要落子落在哪個位置

0:14:30.880,0:14:38.000
至於環境，在 Video game 裡面，
你的環境，就是主機

0:14:38.000,0:14:44.540
machine 現在要去玩遊戲，
至少要有主機跟遊戲玩，你的主機就是環境

0:14:44.540,0:14:48.720
那在下圍棋裡面，你的環境就是 machine 的對手

0:14:48.720,0:14:50.640
就是另外一個人類的對手

0:14:50.940,0:14:57.220
好，那 reward function 呢，在遊戲裡面，
你會先訂好比如說殺一隻怪獸得 20 分等等

0:14:57.240,0:14:58.560
這個就是 reward function

0:14:58.760,0:15:02.120
那在圍棋裡面，
reward function 是很明確的，就是圍棋的規則

0:15:02.120,0:15:04.920
下到這個地方就贏了，就得一分

0:15:04.920,0:15:07.920
下到這個地方就輸了，就得負一分

0:15:08.120,0:15:09.900
那在 reinforcement learning 的 task

0:15:09.900,0:15:14.960
你要注意 environment 跟 reward function 
是訂好的，是訂死的

0:15:14.960,0:15:21.000
你不能去動它，
你不能去動圍棋的規則，你不能去動你的對手

0:15:21.000,0:15:22.680
那都是你無法控制的

0:15:22.860,0:15:28.020
我們要做的事情是什麼，我們唯一能夠控制的，
就是 actor 它採取的行為

0:15:28.020,0:15:31.440
我們要做的事情是，調整 actor 採取的行為

0:15:31.440,0:15:34.180
使得它可以得到，最大的 reward

0:15:34.580,0:15:38.080
好，那我們以電玩為例，來說明一下

0:15:38.080,0:15:43.420
這個環境，還有 actor，
還有 reward 它們互動的情形

0:15:43.640,0:15:47.760
假設我們現在要讓機器去玩電玩，那是什麼樣的狀況呢？

0:15:47.760,0:15:54.440
首先機器會先看到一個遊戲畫面，
這個遊戲畫面，我們就叫做 s1

0:15:54.560,0:15:59.100
這個遊戲畫面，就輸給 machine，就輸入給 actor

0:15:59.100,0:16:02.680
actor 就要做一個決定，決定現在要做什麼

0:16:02.680,0:16:09.160
舉例來說，它決定說，看到這個畫面 s1，
我要採取的行為，叫做 a1

0:16:09.160,0:16:11.780
那 a1 就是向右移動

0:16:12.060,0:16:16.320
好，那它向右移動以後，它會得到一個 reward

0:16:16.320,0:16:21.500
在每一個 time set，在每一個互動的過程中

0:16:21.500,0:16:25.220
reward function 都會給 machine 一個 reward

0:16:25.220,0:16:28.300
那在這一步，只是採取向右，
所以得到的 reward 是 0

0:16:28.300,0:16:31.820
也就是採取向右，不會得到任何的分數，所以 r1 是 0

0:16:31.820,0:16:36.600
那機器採取這個行為之後，它就會看到新的遊戲畫面

0:16:36.600,0:16:38.840
至少它看到自己往右移了

0:16:38.840,0:16:42.000
這畫面是我從真實的遊戲截下來的

0:16:42.000,0:16:45.080
它本來在這個地方，然後它就往右移了

0:16:45.200,0:16:48.880
然後往右移以後，就會看到新的遊戲畫面

0:16:48.880,0:16:51.440
機器就會決定要採取新的行為

0:16:51.540,0:16:54.260
舉例來說，它可能看到新的畫面

0:16:54.260,0:16:57.960
這次他決定說，看到畫面 s2，它要採取 a2 這個行為

0:16:57.960,0:17:03.140
a2 這個行為，就是開火
它決定要開火

0:17:03.620,0:17:10.220
好，哪假設它開火以後，它殺了一個怪

0:17:10.220,0:17:13.280
根據 reward function 的定義，
就會告訴他說，你殺了一隻怪

0:17:13.280,0:17:16.920
那假設，殺那一隻怪，值 5 分
那你就得到 5 分，那 r2 就是 5

0:17:16.960,0:17:19.340
那你又看到新的遊戲畫面，就是 s3

0:17:19.340,0:17:22.900
那這互動的過程，就反覆的繼續下去

0:17:22.920,0:17:30.080
直到說，現在呢，在某一個遊戲畫面，
機器決定採取 action at，得到 reward rt

0:17:30.080,0:17:34.680
然後進入 terminal state，然後進入按照這個主機的設定

0:17:34.680,0:17:38.660
走到這邊，遊戲就結束，那互動的過程就結束了

0:17:38.660,0:17:46.740
那每一場遊戲，叫做一個 Episode，每一個互動，
從頭到尾，在文獻上，我們叫它一個 Episode

0:17:46.740,0:17:49.100
在電玩裡面，每一場遊戲叫做一個 Episode

0:17:49.100,0:17:52.440
在圍棋裡面，每一局棋叫做一個 Episode

0:17:52.680,0:17:57.380
那在一個 Episode 中，我們把每一個 step 得到的 reward

0:17:57.380,0:18:02.800
r1 r2 r3 rT 都加起來，就叫做 total reward

0:18:02.960,0:18:05.720
那 total reward 我們就寫成大 R

0:18:06.600,0:18:13.180
那現在機器要做的事情是，
我們希望大 R 就是 total reward 的值

0:18:13.180,0:18:20.820
在互動的過程中，越大越好

0:18:21.440,0:18:26.920
那我們再用另外一個簡化的圖，來說明一下 environment，actor，還有 reward function 之間的關係

0:18:27.120,0:18:31.560
Environment 輸出一個 state，其實就是遊戲的畫面

0:18:31.780,0:18:36.220
它也可以說是輸出一個 observation，我剛才講過 state 跟 observation 在我心裡面是一樣的東西

0:18:36.420,0:18:43.880
輸出一個遊戲畫面 s1，s1 輸入給 actor，
actor 就輸出說，我要執行 a1

0:18:43.880,0:18:48.220
那再把 a1 輸給 environment ，
environment 就說，我要執行 s2

0:18:48.220,0:18:51.380
s2 輸給 actor，actor 就說我要執行 a2

0:18:51.380,0:18:55.140
然後 environment 看你執行 a2 以後，
它再說現在有新的畫面 s3

0:18:55.140,0:18:57.100
就這樣反覆執行下去

0:18:57.340,0:18:59.340
那 reward 怎麼計算呢？你有一個 reward function

0:18:59.520,0:19:01.000
就是這個遊戲的規則

0:19:01.000,0:19:07.320
結果 reward function 告訴我們說，在 state s1，採取 a1，你把 state s1 a1，丟到 reward function

0:19:07.820,0:19:10.800
你得到一個東西，數值是 r1，得到分數是 r1

0:19:11.160,0:19:17.220
在 s2 a2，在 s2 這個 state，
採取 a2 這個行為，得到 reward 是 r2

0:19:17.220,0:19:19.120
以此類推

0:19:19.120,0:19:22.900
那我們如果把 state 跟 action 這個序列啊

0:19:22.900,0:19:27.360
就是在 state s1 採取 action a1，
state 2 s2 採取 action a2 的這個序列

0:19:27.360,0:19:33.640
通通記錄起來，叫做一個 Trajectory

0:19:34.940,0:19:39.800
那在這個 Trajectory 裡面呢

0:19:39.800,0:19:44.120
這個 s，a 的序列叫做 Trajectory

0:19:44.120,0:19:49.260
那你把這些 reward 通通都加起來，
就得到 total reward 大 R

0:19:49.260,0:19:54.380
或者你可以說，
你得到某一個 trajectory tau 的 reward，R of tau

0:19:54.660,0:20:06.440
那我現在說我們目標，希望可以調整這個 actor，
使得最後可以得到的 reward 越大越好

0:20:06.840,0:20:09.900
好，那接下來就是要說怎麼調整這個 actor

0:20:09.960,0:20:15.340
在講調整 actor 之前，
我們當然要先來看一下 actor 長什麼樣子

0:20:15.780,0:20:20.580
好，那 actor 長什麼樣子了，
actor 也是一個 neural network

0:20:20.580,0:20:25.780
所以其實這個 reinforcement learning 
從來都不是一個特別新的題目

0:20:25.780,0:20:32.020
現在我們講的這些技術，
其實在 80 年代就已經有相當完整的版本

0:20:32.020,0:20:36.940
近年來，deep reinforcement  learning 突然又變得很紅，
到底有什麼樣不一樣的地方

0:20:37.020,0:20:41.100
其實他最不一樣的地方就是，
我們現在使用了 neural network

0:20:41.380,0:20:46.300
過去的 actor 通常都是查表
而不是 neural network

0:20:46.400,0:20:49.700
長久以來，多數人都相信說

0:20:49.700,0:20:55.320
當我們把這個 actor 換成一個 
non-linear 的 network 的時候，是無法 trained

0:20:55.520,0:20:58.180
它沒有辦法收斂，沒有辦法證明它會收斂

0:20:58.180,0:21:00.860
那怎麼辦，就不要用它？

0:21:01.080,0:21:07.260
可是後來就是 google 他們的貢獻就是，
他們想了一大堆的 tip

0:21:07.260,0:21:10.320
讓這個 training 能夠真的 work 起來

0:21:10.320,0:21:15.020
所以現在，當我們講 actor 的時候，
它其實都是一個 neural network

0:21:16.020,0:21:20.060
好，那這個 neural network 
它的輸入輸出分別是什麼呢？

0:21:20.500,0:21:23.920
actor，這個 neural network，
它的輸入就是一個遊戲的畫面

0:21:24.180,0:21:26.520
那這個遊戲的畫面，就是由 pixel 組成的嘛

0:21:26.520,0:21:30.140
那你要處理影像，通常就是需要用到 CNN

0:21:30.140,0:21:33.420
所以這個 actor 它的前幾個 layer 
可能都是 CNN

0:21:33.420,0:21:36.480
為了要處理影像遊戲的畫面

0:21:39.320,0:21:44.580
好，那這個輸出呢，就是看你有幾個 actions

0:21:44.580,0:21:48.580
你輸出的每一個 neuron，就對應到一個 action

0:21:48.580,0:21:56.300
就假設說你現在可以採取的 actions，
就是向左，向右跟開火

0:21:56.660,0:22:01.200
那你的這個 network，
它的 output layer，就有 3 個 neuron

0:22:01.200,0:22:07.180
分別對應到向左，向右跟開火

0:22:07.380,0:22:09.840
好，哪假設現在輸入這個遊戲畫面

0:22:10.000,0:22:14.740
向左的分數是 0.7，向右的分數是 0.2，
開火的分數是 0.1

0:22:15.080,0:22:17.820
那最後 actor 會決定採取哪一個 action 呢？

0:22:17.820,0:22:24.380
通常你會做一個 sampling，你就根據這個數值，
產生一個 probability distribution

0:22:24.380,0:22:29.800
有 70% 機率向左，20%  機率向右，10% 機率開火

0:22:30.540,0:22:34.600
當然你也可以說，我不想要 stochastic 的 actor

0:22:34.620,0:22:38.000
我們剛剛有說 70% 機率向左，20% 機率向右，10% 機率開火

0:22:38.000,0:22:42.140
意味著說，你的 actor 在同一個畫面下，會採取不同行為

0:22:42.380,0:22:44.960
然後它是 stochastic，它每次採取的行為都不一樣

0:22:45.600,0:22:50.000
這樣當然有好處，
這樣的好處就是你的對手比較不容易識破你要做的事情

0:22:50.240,0:22:57.380
當然如果你不喜歡這樣的話，你也可以說，看哪一個 action 得到的分數最高，我們就採取那個 action

0:22:57.380,0:23:00.920
這樣也可以，反正就 depend on 你要怎麼設計你的 actor

0:23:00.920,0:23:02.380
都是可以的

0:23:03.660,0:23:08.920
那過去其實會用一個 lookup table，
來當作你的 actor

0:23:08.920,0:23:15.520
那用 lookup table 有什麼壞處呢？有人會說，如果用 network 參數會比較多

0:23:15.520,0:23:16.800
用 lookup table 參數會比較少

0:23:16.800,0:23:18.720
其實不是，用 lookup table 參數太多

0:23:18.720,0:23:22.440
用 lookup table 如果你 input 是遊戲畫面，
根本無法處理

0:23:23.260,0:23:28.200
因為遊戲畫面是無窮無盡的，
你根本無法窮舉所有可能發生的遊戲畫面

0:23:28.200,0:23:29.940
你用 lookup table 就不 work

0:23:29.980,0:23:33.440
但是如果你是用 neural network，
就算是從來沒有看過的遊戲畫面

0:23:33.440,0:23:36.560
你也可以把遊戲畫面丟進去看它會得到什麼樣的結果

0:23:36.900,0:23:40.320
假如你這個 network train 的夠好，
它有 generalization 的能力

0:23:40.320,0:23:43.540
那你也可能可以得到好的回應

0:23:43.680,0:23:45.680
那我們回到我們剛才要做的事情

0:23:45.760,0:23:50.480
我們說 actor/environment/reward，
他們中間的互動就是這個樣子

0:23:50.760,0:23:54.640
那我們最想要做的事情是什麼，我們想要做的是事情是

0:23:54.660,0:23:59.100
希望調整 actor 的參數，
注意一下我們都講說 actor 都是一個 neural network 嗎

0:23:59.180,0:24:02.060
neural network 就是輸入一個遊戲畫面，就是 state

0:24:02.060,0:24:04.540
output 就是現在要採取的一個 action a

0:24:04.540,0:24:07.520
所有遊戲畫面，採取一個 action a

0:24:07.620,0:24:10.840
actor 是一個 neural network

0:24:11.380,0:24:14.700
那我們希望說，現在能夠達成的目標

0:24:14.820,0:24:19.520
是希望整個 episode 所有 reward 合起來，
它得到 total reward 大 R 的值

0:24:19.520,0:24:21.140
越大越好

0:24:21.140,0:24:26.920
你，如果看這個圖，你知道怎麼解這個問題嗎？

0:24:27.040,0:24:30.980
仔細想想，這個問題應該沒有那麼難
對不對

0:24:31.020,0:24:37.040
你想想看，假設

0:24:37.660,0:24:41.280
reward function 也是一個 neural network

0:24:41.860,0:24:43.860
假設 reward function 也是一個 neural network

0:24:43.860,0:24:48.320
這個 neural network 就是輸入 
state and action 給你一個分數

0:24:48.320,0:24:50.860
假設 environment 也是一個 neural network

0:24:50.860,0:24:54.080
給它一個 action 它就輸出一個 state

0:24:54.560,0:24:58.360
這一整個畫面上，這三個neural network 串起來

0:24:58.360,0:25:01.480
不就只是一個巨大的 neural network 而已嗎？

0:25:01.860,0:25:06.440
它可能在這邊有一個類似隨機的 random seed

0:25:06.440,0:25:08.980
當作 input 去決定初始的畫面是什麼

0:25:09.340,0:25:12.560
然後最後 output 就是一個數值

0:25:12.660,0:25:17.040
然後我們現在這個目標，
就是希望這個數值，越大越好

0:25:17.720,0:25:18.920
大家可以想像啊

0:25:18.920,0:25:24.120
這是一個巨大的，假設這些東西都是 network

0:25:24.120,0:25:30.400
把它們通通串起來，你就只是有一個巨大的 network
然後你希望它的輸出，越大越好

0:25:31.100,0:25:35.240
那我們剛才有講說 
reward and environment 你是動不了的

0:25:35.240,0:25:39.560
那是別人的東西，你動不了，
你唯一能調的只有 actor 參數

0:25:39.880,0:25:45.320
在這個巨大的 network 裡面，
只有藍色的部分的參數是你是可以調的

0:25:45.360,0:25:52.360
你要調藍色的這部分的參數，
使得最終輸出的值，越大越好

0:25:52.360,0:25:56.380
這個你會做嗎？
這不就只是 Gradient ascent 而已嗎？

0:25:57.020,0:26:00.720
你想想看，你想讓這個 R 越大越好

0:26:00.900,0:26:07.780
然後你從這個地方，一路 back propagate 回來

0:26:07.840,0:26:13.160
然後你就可以調這個 actor 的參數，
希望最終讓的 output 越大越好

0:26:13.180,0:26:16.260
這樣大家可以接受這個想法嗎？

0:26:16.560,0:26:19.720
你自己想想看，
這東西聽起來跟 GAN 也蠻像的，那講 GAN 的時候

0:26:19.720,0:26:25.080
我們說有一個 generator，
然後 generator 會把它的輸出接給 discriminator

0:26:25.080,0:26:29.160
然後 discriminator 要調整它的參數，
讓輸出的值越大越好

0:26:29.240,0:26:32.340
那現在我們有 environment，有 reward，有 actor

0:26:32.340,0:26:35.700
這個 environment 跟 reward，
你可以想成是 discriminator

0:26:35.700,0:26:38.820
我們要調 actor 的參數，actor 你可以想成是 generator

0:26:38.820,0:26:41.400
我們要調 actor 參數，讓它輸出的東西

0:26:41.400,0:26:45.040
通過 reward function 以後，output 的值，越大越好

0:26:45.060,0:26:49.100
這個不是跟前面 GAN 做的事情，非常地像嗎？

0:26:50.280,0:26:52.280
那這個東西你會不會做呢？

0:26:52.440,0:26:56.100
假設 reward 是一個 network，environment 是一個 network

0:26:56.100,0:27:00.640
用 back propagation 一路 back propagate 回去，
就可以調 actor 參數，去 maximize 最終的 output

0:27:00.640,0:27:03.500
這個我假設你是會做的

0:27:03.600,0:27:07.300
那現在的難點是什麼，現在的難點是

0:27:07.300,0:27:10.380
這個 environment 跟 reward 
就不是 neural network 啊

0:27:10.380,0:27:12.780
它是個黑盒子

0:27:12.800,0:27:14.560
你根本不知道它是什麼東西

0:27:14.560,0:27:17.140
如果下圍棋的話，你的 environment 是個對手

0:27:17.140,0:27:21.100
它其實也是個 neural network 啦，
不過你沒有辦法把它剖開來看就是了

0:27:21.440,0:27:25.720
如果下圍棋的話，
你的 reward 就是圍棋的規則，那它很複雜

0:27:25.720,0:27:28.760
你可能也不知道怎麼
把它用一個 neural network 來表示它

0:27:29.800,0:27:34.520
現在真正遇到的問題是，我們想要 maximize 這個 R

0:27:34.660,0:27:42.220
但是 reward function 它的參數我們不知道
environment function 的參數，我們也不知道

0:27:42.560,0:27:44.520
怎麼辦？

0:27:44.520,0:27:49.260
本來如果這兩個東西就是 neural network，
它是我們知道怎麼調整參數去 maximize 它

0:27:49.260,0:27:50.680
但實際上不知道

0:27:50.680,0:27:53.660
那怎麼辦呢，這邊遇到記得一個口訣

0:27:53.660,0:27:59.220
那個口訣就是，
如果你今天發現你要 optimize 的 function 不能微分的話

0:27:59.220,0:28:03.320
就用 policy gradient 這個技術硬 train 一發，
就結束了

0:28:03.320,0:28:12.580
如果你想要知道 policy gradient 是什麼的話，
請參看下面這個連結

0:28:12.840,0:28:16.980
今天只要記得說，如果有叫你 maximize 一個東西

0:28:16.980,0:28:18.920
比如說我要 maximize 這個 R

0:28:19.360,0:28:24.140
但問題就是這個東西，
有這個 environment 跟 reward，導致我們無法微分

0:28:24.140,0:28:27.800
那怎麼辦，反正就是有一招，叫做 policy gradient

0:28:27.800,0:28:32.220
它可以去調這個 actor 的參數
讓我們最終可以 output 這個 R，就結束了

0:28:32.540,0:28:40.280
這個就是 reinforcement learning 做的事情

0:28:43.020,0:28:48.080
我知道這跟你一般平常聽到的
 reinforcement learning 講法有點不太一樣

0:28:48.160,0:28:55.040
通常如果你看 David Silver 的 video，
通常先從 Markov decision process 開始講

0:28:55.040,0:29:00.420
然後等你聽完 Markov decision process，以為自己聽懂了，然後就開始想睡了，然後剩下的東西，你都聽不懂了

0:29:00.420,0:29:05.160
然後再講一次，其實你也是聽不懂，
所以這邊是採取一個不太一樣的講法

0:29:05.160,0:29:09.420
告訴你，其實 reinforcement learning 做的事情，
就是這樣

0:29:10.600,0:29:31.180
Q&A 時間

0:29:31.380,0:29:35.320
好那我們就不要講 policy gradient 的部分，
你只要記得這個口訣

0:29:35.320,0:29:41.020
就是發現只要是不能微分的東西，
policy gradient 就是可以幫你 optimize 就是了

0:29:41.520,0:29:44.880
那 policy gradient，這邊其實也不是推導啦

0:29:44.880,0:29:49.880
這邊只是要講實作是怎麼做的，
推導的部分，還有實做具體怎麼做

0:29:49.880,0:29:54.760
永遠可以看底下我線上課程的錄影

0:29:55.240,0:30:01.040
這個部分，就把它跳過

0:30:01.240,0:30:05.900
好，那我們剛才講了 actor 怎麼 train

0:30:06.000,0:30:10.400
接下來我們要講另外一個東西，這個東西叫做 critic

0:30:10.700,0:30:22.920
Critic 做的事情是什麼，
Critic 本身並沒有辦法決定要採取哪一個 action

0:30:22.960,0:30:26.520
那 Critic 可以做的事情是什麼呢？Critic 可以做的事情是

0:30:26.520,0:30:32.700
給它一個 actor pi，
它可以告訴你說這個 actor pi 有多好

0:30:32.960,0:30:36.800
什麼叫做這個 actor pi 有多好呢？
這個 Critic 其實有很多種

0:30:36.800,0:30:42.620
我們今天介紹一個 state value function，寫成 V(pi) of x

0:30:42.860,0:30:49.700
它做的事情是，給它一個 actor pi，
它告訴你說，現在在給一個 actor pi 的前提下

0:30:49.700,0:30:54.580
假設我們看到一個 observation or state s

0:30:54.880,0:31:03.180
接下來，一直到遊戲結束的時候，
會得到 reward 的總和期望值有多大

0:31:03.220,0:31:09.360
注意一下，今天我們算的，
並不是看到這個 state 之後，下一秒會得到的 reward

0:31:09.360,0:31:15.960
而是看到這個 state 以後，
所有 accumulated 的 reward 的期望值

0:31:16.200,0:31:19.640
或者舉例來說，以下圍棋為例的話

0:31:19.640,0:31:24.440
這個 V(pi) of x 的意思就是說，
假設你已經有一個下圍棋的 agent

0:31:24.440,0:31:25.700
叫做 pi

0:31:25.780,0:31:29.620
那你現在給它一個 observation，就是棋盤的盤勢

0:31:29.840,0:31:33.880
比如說，出手天元

0:31:33.920,0:31:40.240
接下來 V(pi) of x 的意思就是說，
從出手下到天元，一直到遊戲結束為止

0:31:40.240,0:31:46.360
假設今天在圍棋裡面，遊戲結束，
贏了就得到分數 1，輸了就得到分數 -1

0:31:46.380,0:31:50.900
在其他的狀況下，得到的分數都是 0

0:31:50.920,0:31:55.080
那 V(pi) of x，x 是出手天元，假設 x 出手天元的話

0:31:55.080,0:32:02.600
V(pi) of x 就是 假設出手下在天元，
接下來獲勝的機率有多大，就是 V(pi) of x

0:32:02.600,0:32:08.140
或出手下在天元，你的 actor 是 pi，
那你獲勝的機率有多大

0:32:08.140,0:32:10.180
這個就是 V(pi) of x

0:32:10.500,0:32:14.900
而透過圖像化的方式來畫它的話，
就是有一個 function 叫做 V(pi)

0:32:14.900,0:32:18.740
給它一個 state，然後它就會 output 一個數值

0:32:18.740,0:32:20.260
叫做 V(pi) of x

0:32:20.260,0:32:24.840
那這個數值代表什麼？這個數值就代表了說，這個 actor

0:32:24.840,0:32:30.480
假設我們用 pi 這個 actor，
在 state s 的時候，接下來，看到 state s

0:32:30.480,0:32:34.380
接下來它會得到的 reward 期望值有多大

0:32:34.460,0:32:38.320
舉例來說，假設你有一個很強的 actor

0:32:38.320,0:32:41.500
然後它看到這個遊戲的畫面，接下來還有很多怪

0:32:41.500,0:32:44.660
因為它有很多怪可以殺，
所以接下來它可以得到很高的分數

0:32:44.660,0:32:46.020
所以 V(pi) of s 就很大

0:32:46.140,0:32:51.020
這邊舉另外一個例子，遊戲畫面剩下怪已經很少了

0:32:51.080,0:32:54.700
V(pi) of s 就會比較小，因為它到遊戲結束的時候

0:32:54.700,0:32:58.500
可以得到的分數就比較少了，
因為其他怪都已經被殺完了

0:32:58.500,0:33:00.880
剩下的怪，已經很少了

0:33:01.060,0:33:06.300
這邊有一件事你要特別注意，
當你看到這種 Critic 的時候

0:33:06.300,0:33:11.480
Critic 都是 depend on actor 的，
給不同的 actor 就算是同樣的 state

0:33:11.480,0:33:13.260
Critic 的 output 也是不一樣的

0:33:13.260,0:33:18.540
所以我們說 Critic 的工作，就是衡量一個 actor 好不好

0:33:18.540,0:33:23.040
所以給它不同的 actor，就算是同一個 state

0:33:23.040,0:33:25.560
它得到的分數也是不一樣

0:33:25.820,0:33:31.560
我們舉一個棋靈王的例子，
這例子是這樣子（只註記提到 Critic 部分）

0:34:12.780,0:34:17.420
所以如果把它對應到 Critic 的話

0:34:17.420,0:34:22.420
如果 actor 是以前的阿光，
那你不應該下，大馬步飛

0:34:22.420,0:34:26.060
以前的阿光，
如果 state 是下大馬步飛，是壞的

0:34:26.060,0:34:32.200
因為以前的阿光比較弱，下大馬步飛的話，
這個比較複雜，所以會下的不好

0:34:32.280,0:34:38.140
但是因為阿光它後來變強了，所以如果是要 evaluate 變強的阿光這個 actor 的話

0:34:38.140,0:34:40.320
下大馬步飛，就會變得好

0:34:40.320,0:34:46.900
這個所要強調的意思是說，
今天你的 Critic 其實會隨著 actor 的不同

0:34:46.900,0:34:49.560
而得到不同的分數

0:34:52.960,0:34:58.140
怎麼算這個 Critic，怎麼評估這個 Critic

0:34:58.220,0:35:03.980
有兩個方法，一個是 Monte-Carlo 的方法，
另外一個是 Temporal-Difference 的方法

0:35:04.140,0:35:08.720
那 Monte-Carlo 的方法其實非常的直觀，它就是說

0:35:08.720,0:35:13.700
今天 Critic 怎麼衡量一個 actor 好不好，
它就去看那個 actor 玩遊戲

0:35:13.700,0:35:16.620
假設我們以玩遊戲為例子，打電玩為例子

0:35:16.620,0:35:23.340
Critic 就去看那個 actor pi 玩遊戲，
看 actor pi 玩得怎麼樣

0:35:23.400,0:35:30.560
那假設現在 Critic 觀察到說，
actor pi 在經過這個 state Sa 以後

0:35:30.560,0:35:35.680
它會得到的 accumulated 的 reward，這從 state Sa 之後

0:35:35.980,0:35:42.200
這個 actor pi 它會得到的 reward 是 Ga，
那這個 Critic 就要學說

0:35:42.200,0:35:47.360
如果 input state Sa，那我的 output 要跟 Ga 越接近越好

0:35:47.360,0:35:51.080
這不過是一個 regression 的問題嘛

0:35:52.020,0:35:56.180
這個 actor 要調它的參數，
那它的 output 跟 Ga 越接近越好

0:35:56.260,0:36:00.780
那假設又觀察到說，現在 actor 跑到 state b

0:36:00.780,0:36:04.560
玩到遊戲結束的時候，會得到 accumulated reward Gb

0:36:04.560,0:36:11.180
那現在輸入 state b

0:36:11.180,0:36:16.340
那它的 output 就要跟 Gb 越接近越好

0:36:16.840,0:36:20.140
這個很直觀，這個就是 Monte-Carlo 的方法

0:36:20.400,0:36:24.480
另外一種叫做不直觀的方法，
叫做 Temporal-Difference 的方法

0:36:24.660,0:36:31.220
Temporal-Difference 方法是說，
現在一樣讓 Critic 去看 actor 玩遊戲

0:36:31.220,0:36:33.780
那 Critic 看到 actor 在做什麼呢？

0:36:33.780,0:36:42.320
它看到 actor 在 state st 採取 action at，
接下來得到 reward rt，然後跳到 state s(t+1)

0:36:42.500,0:36:48.920
光看到這樣一個 data，一筆 data，
之前我們在前一頁做 Monte-Carlo 的時候

0:36:48.920,0:36:53.960
看到某一個 state，我們必須要一直玩到遊戲結束，
才知道 accumulated reward 是多少

0:36:54.040,0:36:57.380
但是在 temporal-difference 的時候，
只要看這樣一筆 data

0:36:57.940,0:37:02.800
那個，Critic 就可以學了，
actor 只要在某一個 state 採取某一個行為

0:37:02.800,0:37:04.060
Critic 就可以學了

0:37:04.200,0:37:07.400
為什麼Critic 就可以學呢，它就是 based on 這個式子

0:37:07.660,0:37:10.820
因為我們現在 V(pi) of s(t)

0:37:10.820,0:37:14.900
是要衡量在 s(t) 這個 state 
會得到的 accumulated 的 reward

0:37:15.220,0:37:16.480
V(pi) of s(t+1)

0:37:16.480,0:37:20.620
是要衡量在 s(t+1) 這個 state 
會得到的 accumulated 的 reward

0:37:20.640,0:37:24.300
今天如果我們觀察到，在 s(t) 這個 state

0:37:24.300,0:37:27.400
會得到 reward r(t)，跳到 s(t+1)

0:37:27.400,0:37:34.860
意味著說，
在 s(t+1) 和 s(t) 中間它們差了 reward 就是 r(t)

0:37:34.860,0:37:39.040
這一項，你會得到 accumulated reward 是這一項

0:37:39.040,0:37:44.340
然後這一項，你得到 accumulated reward 是這一項

0:37:44.340,0:37:50.540
他們中間經過了得到 reward r(t) 這一件事，
所以他們中間的差異，就是 r(t)

0:37:50.880,0:37:53.840
那你在訓練 network 的時候怎麼辦呢？

0:37:53.840,0:38:03.820
訓練 network 你就說，現在把 s(t) 丟進去，你就會得到一個分數 ，把 s(t+1) 丟進去，你會得到另外一個分數

0:38:04.060,0:38:09.060
我們希望這兩個分數的差，跟 r(t) 越接近越好

0:38:09.060,0:38:14.660
所以現在的訓練目標，你不知道這個實際上的是值多少，你不知道這個實際上的是值多少，不知道

0:38:14.660,0:38:18.160
因為還沒有玩到遊戲結束嘛，
你不知道 accumulated reward 是多少

0:38:18.320,0:38:21.000
你學到一件事，你學到的事情是

0:38:21.000,0:38:25.020
雖然我不知道他們值是多少，但我知道它們差了 r(t)

0:38:25.020,0:38:28.480
所有就告訴 machine 說，看到 s(t) 你輸出的值

0:38:28.480,0:38:34.000
跟你看到 s(t+1) 你輸出的值，
中間要差了 r(t)，然後 learn 下去，就結束了

0:38:34.180,0:38:38.400
那用 temporal-difference，
有一個非常明確的好處，就是

0:38:38.400,0:38:42.200
今天當遊戲還沒有結束，玩到一半的時候

0:38:42.200,0:38:45.280
你就可以開始 update 你的 network

0:38:45.280,0:38:49.280
那有時候有些遊戲非常的長，如果你沒有辦法一邊玩遊戲

0:38:49.280,0:38:57.220
一邊 update 你的 network 的話，那你會搞太久，
所以在 temporal-difference 是有它的好處的

0:38:57.980,0:39:14.700
略過

0:39:15.900,0:39:19.760
接下來，我們剛才講了一個 critic，
這個 critic 是給它一個 state

0:39:19.760,0:39:25.300
它會衡量說，這個 state 到遊戲結束的時候
會得到多少的 reward

0:39:25.460,0:39:29.360
但那一種 critic 沒有辦法拿來決定 action

0:39:29.880,0:39:37.380
但是有另外一種 critic，它可以拿來決定 action，
這種 critic 我們叫做 Q function

0:39:37.480,0:39:45.940
這種 Q function 它的 input 就是一個 state，一個 action

0:39:45.940,0:39:52.620
那它到底在量什麼，它量的就是，給我一個 actor pi

0:39:52.620,0:39:56.060
然後量說，在給我一個 actor pi 的前提之下，

0:39:56.140,0:40:02.960
在 observation s，採取了 action a

0:40:02.960,0:40:10.920
在這個 state 採取了 action a 的話，到遊戲結束的時候，會得到多少 accumulated reward

0:40:11.020,0:40:15.920
之前第一它只量在 s 的時候，會得到多少的 reward

0:40:15.920,0:40:20.740
現在是量測，在 s 採取了 a 會得到多少 的 reward

0:40:20.740,0:40:26.420
當你採取的 action 不同，非常直觀，
你採取 action 不同，你得到的 reward 就不一樣嘛

0:40:26.520,0:40:30.400
之前它沒量這件事，它沒量說你會採取哪一個 action

0:40:30.720,0:40:36.300
今天 Q 會量說，在 state s 採取 action a 的時候，
會得到多少的 reward

0:40:37.000,0:40:41.060
那所以 Q 呢，理論上它會有兩個 input，s 跟 a

0:40:41.060,0:40:46.380
它吃 s 跟 a，決定說它要得到多少的分數

0:40:46.720,0:40:51.600
那有時候我們會改寫這個 Q function，
假設你的 a 是可以窮舉的

0:40:51.600,0:40:56.060
舉例來說，在玩遊戲的時候，
a 只有向左／向右，跟開火三個選擇

0:40:56.160,0:41:00.100
那你就可以說，我們現在呢，任一個 Q function

0:41:00.100,0:41:02.660
我們的 Q function 是 input 一個 state s

0:41:02.660,0:41:10.820
它的 output 分別就是 Q(pi) of (s, a=left)，
Q(pi) of (s, a=right)，Q(pi) of (s, a=fire)

0:41:10.940,0:41:15.020
這樣的好處就是，你只要輸入一個 state s ，
你就可以知道說

0:41:15.020,0:41:22.440
s 配上，向左的時候，分數是多少，s 配上向右的時候，分數是多少，s 配上開火的時候，分數是多少

0:41:22.620,0:41:27.120
那在這個 case 你必須要把不同的 a 一個一個帶進去，
你才可以算出它的 Q function

0:41:27.300,0:41:35.180
但在這個 case，你只要 s 帶進去，
就可以一次把不同的 action，它的分數都算出來

0:41:35.820,0:41:39.220
那有了這個 Q 有什麼用呢？

0:41:39.220,0:41:47.140
它的妙用是這個樣子，
你可以用 Q function 找出一個比較好的 actor

0:41:47.140,0:41:49.820
這一招就叫做 Q learning

0:41:50.300,0:41:58.240
所以這個 Q learning 的整個 process 是這樣，
你有一個已經初始的 actor pi

0:41:58.240,0:42:02.260
然後這個 actor pi，去玩，去跟這個環境互動

0:42:02.280,0:42:07.180
然後我們說，critic 的工作就是去觀察，
 這個 actor pi 它跟環境的互動

0:42:07.180,0:42:12.360
那它可以透過 TD or MC 的方法，去學出這個 Q function

0:42:12.360,0:42:18.380
它可以去估測說，根據這個 pi 跟環境互動的資料，
用 TD or MC 的方法

0:42:18.380,0:42:22.300
你可以估測說，現在給定這個 actor 的前提之下

0:42:22.300,0:42:26.180
在某一個 state 採取某一個 action，
得到的 Q value 是多少

0:42:26.180,0:42:35.480
假設估出這種 Q function 以後，估測出這種 critic 以後，可以保證一件事，細節我們下一頁投影片講

0:42:35.480,0:42:41.340
可以保證什麼事？可以保證我們說，
我們一定能夠找到一個新的 actor pi，

0:42:41.520,0:42:46.140
它比原來的 pi 更好，我們本來有一個 actor pi

0:42:46.140,0:42:50.960
我們就觀察它，去跟環境互動的狀況，然後我們估測出

0:42:50.960,0:42:57.320
我們認出一個 critic，它估測 actor pi，
在某一個 state 採取某一個 action 的時候

0:42:57.320,0:42:59.180
會得到的分數

0:42:59.400,0:43:02.320
然後接下來，有了這個 Q 以後

0:43:02.320,0:43:06.180
保證我們可以找到一個，另外一個新的 actor pi prime

0:43:06.180,0:43:11.080
它比原來的 pi 還要好，
這樣我們是不是就找到一個比較好的 actor 了

0:43:11.200,0:43:17.360
所以你有這個比較好的 actor pi prime 以後，
你就把這個 pi 用 pi prime 取代掉

0:43:17.360,0:43:24.080
你有新的 actor，觀察一下，
量出新的 actor 的 Q function，再找到一個更好的 actor

0:43:24.080,0:43:26.620
本來是 pi prime，那現在就變成 pi double prime，

0:43:26.660,0:43:30.020
然後再重新來一次，那你的 actor 是不是越找越好？

0:43:30.080,0:43:33.240
那這就是我們要的，你就可以找到越來越好的

0:43:33.540,0:43:38.300
所以 Q function 的精神就是這樣，
重點的地方就是這一步

0:43:38.500,0:43:42.240
就這個打問號這一步，只要量得出 Q function

0:43:42.240,0:43:46.800
接下來就一定可以找到一個更好的 actor pi prime

0:43:46.920,0:43:49.760
好，那這件事到底是怎麼說的呢？

0:43:49.880,0:43:57.340
它的理論就是這個樣子，它的理論是說，
證明在下一頁，證明我就不要講這樣子

0:43:57.520,0:44:02.720
理論是這樣，理論上是說，
什麼叫做 pi prime 一定比 pi 好

0:44:02.720,0:44:11.340
pi prime 比 pi 好的定義是說，給所有可能的 state s

0:44:11.600,0:44:16.040
如果你用 pi 去玩這個遊戲，得到的 reward

0:44:16.040,0:44:19.940
用 pi 去玩遊戲得到的 accumulated reward，
一定會小於

0:44:19.940,0:44:25.920
我們已經定義過 V 了嘛，這就是為什麼前面要講 V，
就是為了要講這一個啦，V 的定義大家已經知道了

0:44:26.180,0:44:30.540
給所有可能的 state s，如果你採取 pi 這個 actor

0:44:30.540,0:44:35.440
跟你採取 pi prime 這個 actor，
pi prime 這個 actor 得到的 accumulated reward

0:44:35.440,0:44:39.640
一定會大過 pi 這個 actor，
所以 pi prime 會得到 reward 一定會比 pi 大

0:44:39.640,0:44:44.480
不管是哪一個 state，
那就代表 pi prime 是一個比較好的 actor

0:44:44.780,0:44:49.620
那怎麼根據這個 Q 找到一個比較好的 actor pi prime 呢？

0:44:49.680,0:44:53.500
它的原理就是只有下面這條式子

0:44:53.860,0:44:56.460
這個比較好的 actor pi prime 怎麼來啊？

0:44:56.460,0:45:01.060
這個 pi prime 就是，給你一個 Q function

0:45:01.060,0:45:06.420
這個 Q function 是拿來衡量 pi 這個 actor 的 Q function

0:45:06.520,0:45:09.600
然後我們說，給某一個 state 的時候

0:45:09.600,0:45:14.280
窮舉所有可能的 action，
看哪一個 action 的 Q value 最大

0:45:14.280,0:45:17.820
把那個 action 當作新的 actor，pi prime 的輸出

0:45:17.900,0:45:22.320
就我們一個新的 actor pi prime，但它其實是空的，它其實不太會做決定，它怎麼做決定

0:45:22.320,0:45:26.960
它都聽 Q 的，它自己其實沒有參數，它都聽 Q 的

0:45:26.960,0:45:31.100
所以你說給你一個 state s，
pi prime 你想要做什麼樣的行為呢？

0:45:31.100,0:45:37.140
它就說，那我們把 Q 找出來，
Q 其實只看過 pi ，它是看 pi 的

0:45:37.280,0:45:41.440
Q 是說，它之前看過 pi 做過的事情，它知道說

0:45:41.440,0:45:45.960
pi 這個 actor 在 s 採取 a 的時候，會得到多少的 reward

0:45:45.960,0:45:50.180
然後它說窮舉所有可能的 a，
看看哪一個 a 可以讓這個 function 最大

0:45:50.180,0:45:54.740
然後這個 a，pi prime 就說這就是它的輸出了，
然後就結束了

0:45:54.880,0:45:58.020
然後這個 pi prime 呢，就一定會比 pi 還要好

0:46:00.680,0:46:05.440
這邊有一個顯然的問題是什麼問題？
就是你怎麼解這個 arg max 的 problem

0:46:05.760,0:46:10.940
如果 a 是 discrete 的，只有向左，向右，開火，
你就只需要把向左，向右，開火

0:46:10.940,0:46:14.380
分別帶到 Q function 裡面，看你會得到什麼樣的結果

0:46:14.380,0:46:18.540
但是比較慘的地方是，Q learning 好像聽起來很厲害

0:46:18.540,0:46:22.400
但是如果今天你的 action 無法窮舉

0:46:22.400,0:46:27.360
它是 continuous 的，你就爆炸了，
你就不能解這個 arg max 的 problem 了

0:46:27.620,0:46:36.020
那至於這個理論的證明，其實還蠻簡單，
一頁就可以講完，但我們就不要講這個了

0:46:36.060,0:46:40.080
那 Q 怎麼量，你就可以用 TD 來量

0:46:40.360,0:46:44.080
那其實 Q learning 有非常非常多的 trick 啦

0:46:44.080,0:46:52.540
你要怎麼找那些 trick 呢？
你就去找一篇 google paper 叫做 rainbow

0:46:52.540,0:46:58.380
然後裡面就講了，7 種不同的 DQN 的 tip

0:46:58.740,0:47:01.900
因為正好 7 種，就對應到彩虹的 7 個顏色

0:47:01.900,0:47:06.340
他在做圖的時候，每一個方法就對應到彩虹的一個顏色

0:47:06.340,0:47:11.580
所以他把它的 paper，就取做 rainbow

0:47:11.900,0:47:17.200
那我們細節，也許就不需要講，那裏面有很多的技術啦

0:47:17.260,0:47:23.320
我覺得比較好實作的是那個 double DQN 跟 Dueling DQN

0:47:23.320,0:47:27.580
那細節如果你自己要 implement DQN 的時候，
你再去看看那些 paper

0:47:27.620,0:47:37.280
總之，DQN 有很多的 tip 可以讓他做得比較好，
那這些都整理在 rainbow 那篇 paper 裡面了

0:47:37.980,0:47:42.680
那最後，我們要講 Actor+Critic

0:47:42.680,0:47:46.860
同時使用的技術，
就我們剛才有講說，怎麼 learn 一個 actor

0:47:46.860,0:47:54.580
我們也講說怎麼 learn 一個 critic，我們也講說其實 critic 也有辦法告訴我們說要採取什麼樣的 action 才是對的

0:47:54.640,0:48:01.180
那接下來我們要講的是 actor+critic 的技術

0:48:01.180,0:48:03.900
那什麼是 Actor+Critic 的技術呢？

0:48:03.900,0:48:10.500
有一個非常知名的方法，叫做 A3C 也許大家都有聽過

0:48:10.500,0:48:18.260
那 A3C 就是，的 3 個 A 分別是什麼呢？
他的前兩個 A 就是 Advantage Actor-Critic

0:48:18.260,0:48:22.380
所以這是 A2C，然後等一下再講第三個 A 是什麼

0:48:22.880,0:48:26.140
那這個 Advantage Actor-Critic 它是什麼意思呢？

0:48:26.140,0:48:33.060
他是說，這邊我們就沒有把細節說出來，
那他的概念其實很簡單

0:48:33.060,0:48:39.980
我們之前在 learn 這個 actor 的時候，
我們是看 reward function 的 output

0:48:39.980,0:48:46.780
來決定 actor 要怎麼樣 update，
才可以得到最好的 reward

0:48:47.160,0:48:54.600
但是今天實際上在這個互動的過程中

0:48:54.600,0:49:01.360
有非常大的隨機性，所以直接根據互動的過程學，
可能沒有辦法學得很好

0:49:01.680,0:49:05.840
所以 actor-critic 這種方法，
他的精神，細節我們就不要講了

0:49:05.840,0:49:13.320
他的精神是什麼？他的精神就是，
今天 actor 不要真的去看環境的 reward

0:49:13.320,0:49:17.860
因為環境的 reward 變化太大了，
因為中間有隨機性，變化太大

0:49:17.860,0:49:25.200
但是我不要跟環境的 reward 學，
我們只跟 critic 學，這個方法就叫 actor-critic

0:49:25.200,0:49:29.460
那怎麼跟 critic 學呢？其實有非常多不同的方法

0:49:29.460,0:49:34.120
Advantage Actor-Critic 只是眾多的方法的其中一種而已

0:49:34.120,0:49:38.640
那他之所以變得比較有名，
是因為他的 performance，是比較好的

0:49:38.660,0:49:43.460
那當然還有很多其他的方法，
可以讓 actor 跟著 critic 學這樣

0:49:43.460,0:49:49.220
那總之，只要是 actor 不是真的看環境的 reward，
而是看 critic 的比較來學習的

0:49:49.220,0:49:57.940
就叫做 actor-critic，那其中的某有一種方法，
叫做 Advantage Actor-Critic

0:49:59.740,0:50:04.720
好，那我們要講 A3C，
我們剛才只講了 Advantage Actor-Critic，只有兩個 A

0:50:04.720,0:50:07.440
第三個 A 是什麼呢？第三個 A 是這個 Asynchronous

0:50:07.440,0:50:12.400
所以 A3C 完整的名字，
叫做 Asynchronous Advantage Actor-Critic

0:50:12.400,0:50:14.300
Asynchronous 的意思是什麼呢？

0:50:14.300,0:50:21.040
Asynchronous 的意思是說，你有一個 global 的 network

0:50:21.040,0:50:25.880
你本來有一個 global 的 actor 跟 global 的 critic

0:50:26.040,0:50:30.100
那現在要去學習的時候，每到學習的時候呢

0:50:30.100,0:50:34.960
就去跟 global 的 actor 和 critic copy 一組參數過來

0:50:34.960,0:50:42.340
你可以開分身，
假設你要開 N 個分身的話，就是 copy N 組參數，

0:50:42.340,0:50:48.000
好，那把參數 copy 完以後呢？
就讓這個 actor 實際去跟環境互動

0:50:48.000,0:50:52.500
那有 N 個 actor，它們就 N 個 actor 各自去跟環境做互動

0:50:52.500,0:50:57.980
那互動完以後，就會知道說要怎麼樣，Critic 就會告訴 actor 說要怎麼樣 update 參數

0:50:57.980,0:51:03.080
那把這個 updated 參數，傳回去 global 的 network

0:51:03.080,0:51:11.500
所以每一個分身，都會傳一個 update 的方向，
那把所有 update 的方向合起來

0:51:11.500,0:51:16.000
可以一起做 update，那你等於就是做平行的運算

0:51:16.060,0:51:20.600
你等於就是平行的開 N個分身學習，
所以可以學得比較快

0:51:20.880,0:51:25.260
以下部分略過

0:51:41.520,0:51:48.440
這跟 Asynchronous actor critic 的方法是一模一樣的，
你開越多的分身，學習的速度就越快

0:51:52.940,0:51:59.540
不過當然在實作上，你要做 asynchronous 這一招，
前提就是，要有很多很多的 machine 這樣子

0:51:59.540,0:52:03.600
你想要開 8 個分身，就要 8 個 machine，
開一千個分身，就要一千個 machine

0:52:03.600,0:52:08.920
如果你只有一台 machine，你就只能降到 A2C，
你其實也沒有辦法做 A3C 就是了

0:52:09.280,0:52:15.460
這邊有一些同學實作做的 actor-critic 
在一些遊戲上的結果

0:52:15.780,0:52:20.340
以下部分略過

0:53:41.500,0:53:45.740
那這邊有一個在網路上找到的 Doom 的比賽

0:53:45.740,0:53:49.840
這個其實也蠻知名的，
就有一個 machine 去玩 Doom 的比賽

0:54:29.500,0:54:35.000
那還有另外一個技術，是 actor-critic 的一個變形

0:54:35.000,0:54:39.400
那我之所以要把他提出來是因為，他非常地像是 GAN

0:54:39.400,0:54:43.840
我們剛才有講說，
在做 Q-learning 的時候，我們遇到的一個問題就是

0:54:43.840,0:54:46.640
我們要解一個 arg max 的 problem

0:54:46.800,0:54:50.960
我們要找一個 action，他要讓 Q function 最大

0:54:50.960,0:54:58.040
但是你今天常常會遇到的一個問題就是，
你沒有辦法窮舉所有的 a

0:54:58.040,0:55:01.180
今天如果尤其是 a 是一個 continuous vector

0:55:01.180,0:55:04.660
舉例來說，
什麼時候你的 action output 會是 continuous？

0:55:04.660,0:55:10.080
舉例來說，你想要控制機器手臂，
那你 output 的是關節的角度，那他就是 continuous，

0:55:10.200,0:55:16.820
一般的 Q-learning 只能處理 discrete 的 case，
那如果要處理 continuous 要怎麼辦呢？

0:55:17.080,0:55:19.080
你就 learn 一個 actor

0:55:19.380,0:55:24.340
那這個 actor 做的事情就是，
給他一個 state，那它 output 的那個 a

0:55:24.340,0:55:29.720
會是讓 Q function 的值最大的那個 a

0:55:29.720,0:55:34.260
這樣大家懂嗎？
我們有一個 Q function，然後 actor 它 output 的 a

0:55:34.260,0:55:37.780
actor 它要去學習，它學習的目標就是希望它 output 的 a

0:55:37.780,0:55:41.220
會讓 Q function 的值呢，越大越好

0:55:41.340,0:55:44.680
那你如果仔細想想，這不就跟 GAN 是一樣的嗎？

0:55:44.680,0:55:50.100
如果這個東西把它當作是 Discriminator，
這個東西當作是 Generator

0:55:50.100,0:55:54.440
Generator 要做的事情，就是產生一個 image 是 Discriminator 覺得分數好的

0:55:54.440,0:55:59.760
那這邊是 actor 要產生一個 action，
這個 action 是 Q function 覺得分數好的

0:55:59.800,0:56:07.240
那這個叫做 Pathwise Derivative Policy Gradient，
那其中比較著名的方法，就是 DDPG

0:56:07.240,0:56:13.440
那這個我們就跳過，只是要告訴大家有這個技術而已

0:56:13.440,0:56:21.100
那剩下的時間，我想要講一下，
Inverse reinforcement learning ，它是什麼呢？

0:56:21.100,0:56:25.680
它是 Imitation learning 的一種，
在 inverse reinforcement learning 裡面

0:56:25.680,0:56:27.600
你只有 environment 跟 actor

0:56:27.600,0:56:32.320
我們剛才講過 environment 
跟 actor 它們互動關係是長這個樣子

0:56:32.440,0:56:37.580
但是在 Inverse reinforcement learning 裡面，
我們沒有 reward function

0:56:37.580,0:56:43.960
我們有的東西是什麼，
我們有的東西就只有這個 expert demo trajectory

0:56:43.960,0:56:48.560
如果是遊戲的話，有專家，有高手，
去把這個遊戲，玩了 N 遍

0:56:48.560,0:56:52.360
給 machine 看，
告訴 machine 說玩這個遊戲看起來是什麼樣子

0:56:52.360,0:56:55.620
但是沒有 reward function

0:56:55.840,0:56:59.400
那你可能會說，什麼樣的狀況，
會沒有 reward function 呢？

0:56:59.400,0:57:04.600
事實上多數生活中的 case，
我們都是沒有 reward function 的

0:57:04.600,0:57:09.100
今天下圍棋是用 reinforcement learning，
是因為圍棋的規則是明確的

0:57:09.100,0:57:10.940
輸就是輸，贏就是贏

0:57:11.200,0:57:17.220
今天玩電玩可以用 reinforcement learning 就玩電玩的規則是明確的，殺一隻怪得到幾分是訂好的

0:57:17.220,0:57:22.160
但在多數的 case，我們根本就不知道 reward function 是什麼，比如說，自駕車

0:57:22.160,0:57:26.920
撞到一個人，要扣 10000 分嗎？
撞到一個狗要扣多少分呢？

0:57:26.920,0:57:32.020
或者是說，今天如果你拿 reinforcement learning 
的技術去學一個 chat bot

0:57:32.020,0:57:34.760
chat bot 要做到什麼樣的事情，才能得到分數呢？

0:57:34.760,0:57:41.940
舉例來說，它把人激怒，會扣 100 分嗎？那人掛掉電話，扣 50 分嗎？這個東西你怎麼訂，都是訂不清楚的

0:57:42.060,0:57:45.780
而且有時候你用一些自己訂出來的 reward

0:57:45.780,0:57:50.180
那如果它跟現實的狀況差很多，
machine 會學出很奇怪的東西

0:57:50.400,0:57:53.680
舉例來說，其中一個例子就是機械公敵

0:57:53.680,0:57:58.320
機械公敵那部影片說，創造機器的人，它訂了 3 大法則

0:57:58.320,0:58:02.160
這 3 大法則你就可以想成是，只要違反這 3 大法則

0:58:02.160,0:58:07.140
就會得到非常 negative 的 reward，
遵守這 3 大法則，就會得到 positive 的 reward

0:58:07.140,0:58:11.160
那機器自己想辦法根據這個規則，
根據這個 reward function

0:58:11.160,0:58:13.120
去找出最好的 action

0:58:13.120,0:58:19.600
然後它就有一個神邏輯，它決定說最好的 action 就是，為了保護人類，應該把人類監禁起來

0:58:19.840,0:58:25.300
那這可能是一個比較極端的例子，
但在真實的研究上，確實有，這樣的例子

0:58:25.300,0:58:30.820
舉例來說，你想讓機器學習收盤子，
然後它就把盤子放到櫃子裡面，告訴他說

0:58:30.820,0:58:37.080
盤子放到櫃子裏面，你就可以得到一分，
機器確實可以學到，為了要得到分數

0:58:37.080,0:58:40.620
它會把盤子放到放到櫃子裏面，但是它可能都用摔的

0:58:40.620,0:58:44.480
然後盤子通通都被打破了，
因為你沒有告訴他說，盤子打破要扣分啊

0:58:44.480,0:58:47.380
那以後變成說可能盤子都打破以後，才發現

0:58:47.400,0:58:54.620
這樣不行，只好再加上新的 reward，
就是打破盤子要扣分，但盤子都已經被打破了

0:58:55.460,0:59:02.380
今天我們丟很多現實的任務，
是我們根本就不知道 reward function 長怎麼樣子

0:59:02.380,0:59:05.600
所以我們需要 inverse reinforcement learning 這個技術

0:59:05.600,0:59:09.280
inverse reinforcement learning 這個技術，
它做的事情就是

0:59:09.280,0:59:13.680
在原來的 reinforcement learning 裡面，
我們有 reward function，有 environment

0:59:13.680,0:59:18.380
根據 reward function 還有 environment，
用 reinforcement learning 技術找出最好的 actor，

0:59:18.540,0:59:22.860
inverse reinforcement learning 技術，
剛好是反過來的，我們有 actor

0:59:22.860,0:59:29.600
我們雖然不知道最好的 actor 是什麼，但是我們有專家，專家去玩了 N 場遊戲，告訴我們說

0:59:29.600,0:59:32.460
厲害的人玩這個遊戲，看起來就是怎麼樣的

0:59:32.460,0:59:36.160
根據專家的 demo，還有 environment

0:59:36.160,0:59:42.960
透過一個叫做 inverse reinforcement learning 的技術，
我們可以推出 reward function 應該長什麼樣子

0:59:43.360,0:59:48.460
把 reward function 推出來以後，
你就可以根據你推導出的 reward function

0:59:48.460,0:59:54.800
再去 apply reinforcement learning 的方法，
去找出最好的 actor

0:59:54.800,0:59:58.220
所以你是用 inverse reinforcement learning 
的方法去推出 reward function

0:59:58.220,1:00:05.060
再用 reinforcement learning 的方法去找出最好的 actor

1:00:05.160,1:00:09.080
那 inverse reinforcement learning 是怎麼做的呢？
它的原則就是

1:00:09.080,1:00:13.240
你的老師，就是那些 experts 他永遠是對的

1:00:13.240,1:00:20.460
什麼意思，就是說，現在你一開始你有一個 actor，
先隨機的找出初始化一個 actor 出來

1:00:20.660,1:00:26.460
然後去用這個 actor 去跟環境做互動，
那這個 actor 會得到很多的 trajectory

1:00:26.460,1:00:34.360
會得到很多的遊戲紀錄，然後接下來啊，
你比較 actor 的遊戲紀錄，跟老師的遊戲紀錄

1:00:34.360,1:00:41.020
然後你訂一個 reward function，一定要讓，
老師得到的分數，比 actor 得到的分數高

1:00:41.020,1:00:46.760
就是先射箭，再畫靶的概念，
就是 expert 去玩一堆遊戲，他有一堆遊戲的紀錄

1:00:46.760,1:00:50.900
然後 actor 也去玩遊戲，也有遊戲的紀錄，那我們不知道 reward function 是什麼

1:00:50.900,1:00:53.860
等他們都玩完以後，再訂一個 reward function

1:00:53.920,1:00:58.160
訂的標準就是，老師，就是這個 expert 得到的分數

1:00:58.160,1:01:02.080
一定要比學生還要高這樣子，先射箭，再畫靶的概念

1:01:02.480,1:01:07.880
好那把靶畫完以後，學生說，好吧，
雖然因為根據這個新的 reward，我是比較弱的

1:01:07.880,1:01:14.140
沒關係，那我就再去學習，
我想要 maximize 新的 reward function

1:01:14.220,1:01:20.580
actor 學到 maximize 新的 reward function 以後，
他就再去跟環境互動，他就會得到新的 trajectory

1:01:20.580,1:01:24.180
他得到新的 trajectory 以後，他本來以為，他跟老師一樣厲害了

1:01:24.180,1:01:27.420
但是不幸的就是，那個規則是會一直改的

1:01:27.420,1:01:33.780
當他變得跟老師一樣厲害以後，
我們再改一下規格，讓老師算出來的分數，還是比較高

1:01:33.780,1:01:39.120
然後 actor 就只好很生氣的，想辦法學，
想要跟老師做的一樣好

1:01:39.120,1:01:44.160
就反覆反覆進行這個 process，最後，
就可以找到一個 reward function

1:01:44.380,1:01:48.420
那這整個 process，
我們用圖示畫來表示一下，有一個 expert

1:01:48.420,1:01:54.680
他有 N 筆遊戲紀錄，然後你有一個 actor，
它也有 N 筆遊戲紀錄

1:01:54.680,1:02:00.500
然後你要訂一個 reward function，
讓 expert 得到的分數，永遠贏過 actor

1:02:00.500,1:02:08.200
然後你接下來，反正你去找一個 reward function，
老師一定是好的，它一定是不好的

1:02:08.220,1:02:10.720
接下來根據這個 reward function

1:02:10.720,1:02:16.260
你可以去學這個 actor，根據這個 reward function，
你可以去學這個 actor

1:02:16.260,1:02:21.460
讓這個 actor 根據這個 reward function，
可以得到最好的分數

1:02:21.780,1:02:26.760
但等這個 actor 做得比較好之後，
這個規則又變了，然後這個 process

1:02:26.760,1:02:31.620
又反覆的循環，這個 process，你有沒有覺得很熟悉呢？

1:02:31.620,1:02:36.300
它跟 GAN 的 process，其實是一模一樣的，怎麼說？

1:02:36.580,1:02:39.980
在 GAN 裡面，有一堆 expert 畫的圖

1:02:39.980,1:02:46.240
generator 會產生一堆圖，discriminator 說，
只要 expert 畫的就是好的，這些就是高分

1:02:46.240,1:02:49.100
這些就是低分，你 learn 出一個 discriminator

1:02:49.100,1:02:54.580
generator 要做的事情是，調整它畫出來的圖，
使得 discriminator，覺得它是高分

1:02:54.580,1:02:56.440
但是 generator 以為它畫的圖

1:02:56.440,1:02:59.520
discriminator 會給它高分，
但是 discriminator 會 update 參數

1:02:59.520,1:03:05.740
再使得 generator 畫的圖，得到低分，
然後就反覆的，不斷去畫

1:03:05.740,1:03:12.100
事實上，在 inverse reinforcement learning 裡面，
我們只是把 generator 換個名字叫做 actor

1:03:12.100,1:03:14.980
把 discriminator 換個名字，叫做 reward function，

1:03:15.280,1:03:20.700
我們說 actor 會產生一大堆遊戲的紀錄，
但是我們要訂一個 reward function

1:03:20.700,1:03:26.000
反正就是要讓 actor 輸，讓老師贏，
然後 actor 就會修改它做的事情

1:03:26.000,1:03:27.720
希望可以得到比較好的分數

1:03:27.720,1:03:31.100
在 actor 修改以後，reward function 也會跟著修改

1:03:31.100,1:03:35.520
然後就這樣反覆地進行這個 process

1:03:35.900,1:03:43.200
在這個結束之前呢，我就給大家看一個，
這個是 Berkeley 做的

1:03:43.200,1:03:49.920
用 inverse reinforcement learning
 的技術來教機器人做一些行為

1:03:50.820,1:03:59.980
以下部分省略

1:05:18.100,1:05:33.020
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
