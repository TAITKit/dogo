<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:04.080<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:04.320,0:00:06.340<br>
所謂的硬 train 的意思就是說<br>
<br>
0:00:06.340,0:00:08.280<br>
看起來好像不能 train 的東西<br>
<br>
0:00:08.280,0:00:10.620<br>
但是，我們還是用 train 的方法來做<br>
<br>
0:00:10.620,0:00:13.960<br>
這個就叫做硬 train，或是叫做硬 train 一發這樣子<br>
<br>
0:00:14.180,0:00:16.420<br>
如果你看一下這個<br>
<br>
0:00:16.620,0:00:19.540<br>
這個是一個部落格，這個部落格裡面<br>
<br>
0:00:19.540,0:00:22.860<br>
有一個 Fizz Buzz in TensorFlow 的故事<br>
<br>
0:00:22.860,0:00:24.800<br>
有一天，有一個人他去<br>
<br>
0:00:24.800,0:00:26.380<br>
這個人好像還滿知名的<br>
<br>
0:00:26.380,0:00:27.920<br>
知名的資料科學家<br>
<br>
0:00:27.960,0:00:30.820<br>
他去面試<br>
<br>
0:00:30.820,0:00:34.380<br>
然後人家說，那我們來考一下程式能力吧<br>
<br>
0:00:34.380,0:00:36.320<br>
在白板上面寫程式<br>
<br>
0:00:36.320,0:00:37.400<br>
人家就問說<br>
<br>
0:00:37.400,0:00:39.780<br>
你會不會來寫一下這個<br>
<br>
0:00:39.780,0:00:41.300<br>
Fizz Buzz 的程式<br>
<br>
0:00:41.300,0:00:44.335<br>
Fizz Buzz是甚麼意思呢？Fizz Buzz 的意思是說<br>
<br>
0:00:44.335,0:00:46.580<br>
現在讓你 print 一串數字<br>
<br>
0:00:46.580,0:00:48.080<br>
比如說，1~100<br>
<br>
0:00:48.160,0:00:50.820<br>
但是，如果這串數字裡面<br>
<br>
0:00:50.820,0:00:54.580<br>
這個數字可以被 3 整除，我們就 output Fizz<br>
<br>
0:00:54.580,0:00:56.220<br>
被 5 整除，你就 output Buzz<br>
<br>
0:00:56.280,0:00:59.960<br>
同時可以被 3 和 5 整除，你就 output Fizz Buzz<br>
<br>
0:00:59.960,0:01:02.760<br>
所以，你的程式 output 應該是 1, 2<br>
<br>
0:01:02.780,0:01:06.520<br>
然後，3 可以被 3 整除 ，你就 output Fizz<br>
<br>
0:01:06.520,0:01:09.360<br>
然後 4，然後 5 可以被 5 整除，你就 output Buzz<br>
<br>
0:01:09.360,0:01:11.080<br>
然後，output Fizz<br>
<br>
0:01:11.080,0:01:14.220<br>
然後，7, 8, Fizz, Buzz, 11, Fizz<br>
<br>
0:01:14.220,0:01:15.440<br>
然後，13, 14, Fizz Buzz 這樣<br>
<br>
0:01:15.440,0:01:17.740<br>
你應該 output 這樣一個 sequence<br>
<br>
0:01:17.740,0:01:19.120<br>
我們今天知道，對大家來說<br>
<br>
0:01:19.120,0:01:21.160<br>
這個是一個非常簡單的問題<br>
<br>
0:01:21.200,0:01:23.460<br>
那個人就說，怎麼做呢<br>
<br>
0:01:23.460,0:01:25.740<br>
怎麼做？我要用 Python 做<br>
<br>
0:01:25.740,0:01:29.160<br>
那我要先 import 一些 library，這很正常<br>
<br>
0:01:29.160,0:01:31.820<br>
直接 import TensorFlow<br>
<br>
0:01:32.620,0:01:36.460<br>
然後，接下來，我要準備一些 training data<br>
<br>
0:01:36.460,0:01:38.200<br>
你叫我 output 1~100<br>
<br>
0:01:38.200,0:01:40.460<br>
我當然不能用這個當作 training data<br>
<br>
0:01:40.460,0:01:45.280<br>
所以，我先要 label 101~1000 的 Fizz Buzz<br>
<br>
0:01:45.280,0:01:47.960<br>
怎麼做你就去 Amazon 上找<br>
<br>
0:01:47.960,0:01:49.760<br>
你去 amt 上找人 label 就好<br>
<br>
0:01:49.760,0:01:53.040<br>
label 好以後，我們就弄一個 network 給它 train 下去<br>
<br>
0:01:53.040,0:01:55.320<br>
看看結果會怎麼樣<br>
<br>
0:01:55.320,0:01:56.700<br>
然後，它後來 train 下去以後<br>
<br>
0:01:56.700,0:01:59.000<br>
哇，正確率才 80 幾這樣子<br>
<br>
0:01:59.140,0:02:01.180<br>
然後他就沒有得到那一份工作<br>
<br>
0:02:03.820,0:02:07.240<br>
我決定來自己試一下這個 Fizz Buzz 的東西<br>
<br>
0:02:07.240,0:02:09.385<br>
到底能不能夠做得起來<br>
<br>
0:02:09.385,0:02:11.360<br>
自己來實作一下<br>
<br>
0:02:11.360,0:02:16.260<br>
首先，我們先來看我們的 training data 長甚麼樣子<br>
<br>
0:02:17.740,0:02:21.100<br>
開一下我的 ipython，run 一下<br>
<br>
0:02:25.440,0:02:29.480<br>
那我現在就對數字 101~1000 <br>
<br>
0:02:29.480,0:02:32.920<br>
做了 labeling，你就不要管那個 label 是怎麼來的了<br>
<br>
0:02:32.920,0:02:35.920<br>
我們先來看一下我們的 training data<br>
<br>
0:02:35.920,0:02:38.800<br>
那 training data 的 input 每一筆呢<br>
<br>
0:02:38.800,0:02:41.340<br>
每一筆就代表了一個數字<br>
<br>
0:02:41.340,0:02:43.700<br>
我們先看一下它的 shape<br>
<br>
0:02:44.500,0:02:48.480<br>
總共有 900 筆 data，就是從 101 一直數到 1000<br>
<br>
0:02:48.480,0:02:49.480<br>
總共 900 筆 data<br>
<br>
0:02:49.480,0:02:52.700<br>
我們把第一筆 data dump 出來看看<br>
<br>
0:02:53.620,0:02:59.540<br>
那每一個數字，我們都是用二進位來表示它<br>
<br>
0:02:59.540,0:03:01.640<br>
每一個數字，我們都是用<br>
<br>
0:03:01.640,0:03:04.235<br>
二進位的數值來表示它<br>
<br>
0:03:04.240,0:03:06.400<br>
第一個數字是 101<br>
<br>
0:03:06.400,0:03:11.780<br>
101 用二進位來表示就是 1010011000<br>
<br>
0:03:11.780,0:03:15.020<br>
第一個數字，代表的是這個<br>
<br>
0:03:17.400,0:03:20.440<br>
2 的 0 次方、2 的 1 次方、2 的 2 次方<br>
<br>
0:03:20.440,0:03:22.660<br>
然後，2 的 5 次方、2 的 6 次方<br>
<br>
0:03:22.660,0:03:25.320<br>
你直接把它加起來，你會發現說，它確實就是<br>
<br>
0:03:25.540,0:03:28.940<br>
101 這樣，那如果 102 的話<br>
<br>
0:03:28.940,0:03:33.200<br>
就是這個樣子，那 103 的話，就是這個樣子<br>
<br>
0:03:33.620,0:03:35.880<br>
然後，我們看一下 label 的 data<br>
<br>
0:03:35.880,0:03:37.640<br>
label 的 data 長甚麼樣子呢？<br>
<br>
0:03:37.640,0:03:41.120<br>
比如說，101 它可以被 3 整除嗎？不行<br>
<br>
0:03:41.120,0:03:44.660<br>
它可以被 5 整除嗎？不行，所以<br>
<br>
0:03:44.660,0:03:47.080<br>
它是 output 原來自己的數字<br>
<br>
0:03:47.080,0:03:48.705<br>
那我們現在總共有 4 個 class<br>
<br>
0:03:48.705,0:03:50.400<br>
這 4 個 class 分別代表了<br>
<br>
0:03:50.400,0:03:51.660<br>
output 原來的數字<br>
<br>
0:03:51.660,0:03:53.740<br>
output Fizz、output Buzz、output Fizz Buzz<br>
<br>
0:03:53.760,0:03:55.320<br>
那如果 output 原來的數字<br>
<br>
0:03:55.320,0:03:57.680<br>
就是第一維是 1，其他維是 0<br>
<br>
0:03:57.680,0:04:00.180<br>
所以，101 的話，第一維是 1，其他都是 0<br>
<br>
0:04:00.460,0:04:03.880<br>
接下來呢，我把前面的燈稍微關一下<br>
<br>
0:04:03.980,0:04:07.220<br>
接下來我們考慮說 output 102<br>
<br>
0:04:07.560,0:04:08.560<br>
output 102<br>
<br>
0:04:08.980,0:04:12.340<br>
那如果 102 的話，它可以被 3 整除嗎？它可以被 3 整除<br>
<br>
0:04:12.340,0:04:15.500<br>
但不能被 5 整除，所以應該 output Fizz<br>
<br>
0:04:16.100,0:04:20.680<br>
所以，它的第二維是 1，其他都是 0<br>
<br>
0:04:20.680,0:04:23.760<br>
就這樣子，硬做一下<br>
<br>
0:04:25.520,0:04:27.460<br>
我這邊用的 network 架構<br>
<br>
0:04:27.460,0:04:30.800<br>
跟那個人面試時用的 network 架構是一樣的<br>
<br>
0:04:31.340,0:04:33.060<br>
input 10 維<br>
<br>
0:04:33.060,0:04:36.360<br>
然後，1 個 hidden layer 就是 100 個 neuron<br>
<br>
0:04:36.380,0:04:38.700<br>
然後，apply ReLU 的 activation function<br>
<br>
0:04:38.700,0:04:42.000<br>
output 4 維，Softmax，然後用 Adam 這樣<br>
<br>
0:04:42.700,0:04:43.820<br>
這個拿掉<br>
<br>
0:04:44.300,0:04:48.460<br>
那等一下就直接 print 正確率，跑一下<br>
<br>
0:04:54.380,0:04:55.380<br>
跑一下<br>
<br>
0:05:02.445,0:05:04.675<br>
很快阿<br>
<br>
0:05:05.755,0:05:09.055<br>
沒多少才 900 筆 training data，秒 train、秒 train<br>
<br>
0:05:09.220,0:05:12.160<br>
哇，這個正確率是 76% 阿<br>
<br>
0:05:12.160,0:05:14.160<br>
做出來也不是 100<br>
<br>
0:05:14.160,0:05:17.460<br>
但是，我不會就這樣放棄，因為你想想看<br>
<br>
0:05:17.460,0:05:18.605<br>
如果<br>
<br>
0:05:18.605,0:05:20.900<br>
你想想看你看看你的 training data<br>
<br>
0:05:20.900,0:05:23.620<br>
你的 training data 正確率才 80% 而已<br>
<br>
0:05:23.620,0:05:26.500<br>
代表說，你還沒有讓你的 network<br>
<br>
0:05:26.500,0:05:28.660<br>
真的在 training data 上面學起來<br>
<br>
0:05:28.660,0:05:30.980<br>
它還沒有真的去 fit 那個 training data<br>
<br>
0:05:30.980,0:05:33.420<br>
我們知道說，做 deep learning 的起手式是<br>
<br>
0:05:33.440,0:05:35.600<br>
先想辦法 fit 你的 training data<br>
<br>
0:05:35.600,0:05:37.495<br>
怎麼 fit 你的 training data 呢？<br>
<br>
0:05:37.495,0:05:40.255<br>
開一個比較大的 network，舉例來說<br>
<br>
0:05:40.565,0:05:43.645<br>
我們把 hidden layer 的 size 改成 1000<br>
<br>
0:05:52.280,0:05:55.520<br>
那再 train 一下，也是很快，秒 train<br>
<br>
0:05:55.660,0:05:57.580<br>
你看現在上升起來了<br>
<br>
0:05:57.945,0:06:00.285<br>
上升起來，可以跑到 100 嗎？跑到 100 了<br>
<br>
0:06:01.460,0:06:04.640<br>
那你看正確率就是 100%<br>
<br>
0:06:04.640,0:06:06.720<br>
這樣我就拿到那個 job 了<br>
<br>
0:06:06.720,0:06:09.340<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
