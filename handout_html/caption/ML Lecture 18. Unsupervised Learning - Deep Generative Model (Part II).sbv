0:00:00.000,0:00:09.220
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:13.080,0:00:20.080
那我們上次講了 generation 這件事情

0:00:20.080,0:00:24.380
然後，可以用 PixelRNN 來做

0:00:24.380,0:00:29.980
這邊 create 了一個 generative 的 task

0:00:29.980,0:00:33.100
上次還講了 VAE

0:00:33.100,0:00:36.000
我們沒有講太多它的原理

0:00:36.000,0:00:38.580
我們來複習一下 VAE 做的事情

0:00:38.580,0:00:41.280
Auto-encoder 我想大家都很熟悉了

0:00:41.280,0:00:43.480
那 VAE 做的事情是甚麼呢？

0:00:43.480,0:00:46.100
VAE 做的事情是說

0:00:46.100,0:00:49.980
你一樣有一個 encode，有一個 decoder

0:00:49.980,0:00:55.180
你現在 decoder 會 output 兩組 vector

0:00:55.180,0:00:57.600
這邊一組是 m1 到 m3

0:00:57.600,0:01:00.200
另外一組是 σ1 到 σ3

0:01:00.200,0:01:04.860
接下來，你會 generate 一個 normal distribution

0:01:04.860,0:01:07.840
你會 generate 一個 vector

0:01:07.920,0:01:11.560
這個 vector 是從 normal distribution sample 出來的

0:01:11.560,0:01:15.620
接下來，你把 σ 這個 vector 取 exponential

0:01:15.620,0:01:19.220
然後，再乘上 random sample 出來的這個 vector

0:01:19.220,0:01:21.300
再加上原來 m 這個 vector

0:01:21.300,0:01:24.920
得到你的 code, c，然後把那個 code 丟進

0:01:24.920,0:01:28.200
decoder 裡面，產生 image，那你希望說

0:01:28.200,0:01:30.160
input 跟 output 越接近越好

0:01:30.160,0:01:33.080
另外 Auto-encoder 還有另外一項 constraint

0:01:33.080,0:01:35.140
那我回去的時候，我發現我上次的投影片呢

0:01:35.140,0:01:38.080
寫的是相反的，所以

0:01:38.080,0:01:39.420
差了一個負號

0:01:39.420,0:01:42.740
所以如果按照原來的寫法，這邊應該是 maximize

0:01:42.740,0:01:46.040
那如果是用 minimize 的話，下面就應該再加一個負號

0:01:46.980,0:01:49.460
再來的問題就是

0:01:49.460,0:01:52.020
為甚麼要用 VAE 這個方法

0:01:52.120,0:01:55.360
原來的 Auto-encoder 會有甚麼樣的問題呢？

0:01:55.360,0:01:57.720
如果你看文獻上的話

0:01:57.720,0:02:00.240
VAE，如果你看它原來的 paper 的話

0:02:00.240,0:02:03.320
它有很多很多的式子，你就會看的一頭霧水的

0:02:03.320,0:02:05.280
那在講那些式子之前呢

0:02:05.280,0:02:08.820
我們先來看 intuitive 的理由

0:02:08.820,0:02:10.280
為甚麼要用 VAE？

0:02:10.280,0:02:12.980
如果是原來的 Auto-encoder 的話

0:02:12.980,0:02:16.340
原來的 Auto-encoder 它做的事情是

0:02:16.340,0:02:18.800
我們把每一張 image

0:02:18.800,0:02:23.000
變成一個 code，假設我們現在的 code 是一維

0:02:23.000,0:02:25.680
就是圖上的這條紅色的線

0:02:25.680,0:02:27.900
那你把滿月的這個圖

0:02:27.900,0:02:29.680
變成 code 上的一個 value

0:02:29.680,0:02:32.540
然後，再從這個 value 做 decode

0:02:32.540,0:02:35.680
把它變成原來的圖，那如果是弦月的圖

0:02:35.680,0:02:38.340
也是一樣變成 code 上的一個 value

0:02:38.340,0:02:42.920
接下來，你再把它從 code 上的一個 value 變回原來的圖

0:02:43.260,0:02:47.500
假設我們今天是在滿月和弦月的 code 中間

0:02:47.500,0:02:49.580
sample 一個點，你覺得它原來

0:02:49.580,0:02:51.600
然後，再把這個點做 decode

0:02:51.600,0:02:54.380
變回一張 image，它會變成甚麼樣子呢？

0:02:54.380,0:02:56.320
你心裡或許期待著說

0:02:56.320,0:03:02.640
它會變成滿月和弦月中間的樣子

0:03:02.640,0:03:04.700
但是，這只是你的想像而已

0:03:04.700,0:03:08.380
其實，因為我們今天用的 encoder 和 decoder

0:03:08.380,0:03:11.680
我們今天用的 encoder 和 decoder，它都是 non-learner

0:03:11.680,0:03:13.440
它都是一個 neural network

0:03:13.440,0:03:15.560
所以，你其實很難預測說

0:03:15.560,0:03:20.180
在這個滿月和弦月中間到底會發生甚麼事情

0:03:20.180,0:03:22.880
你可能想像是，滿月和弦月中間的月象

0:03:22.880,0:03:25.760
但未必，它可能根本就是另外一個東西

0:03:25.760,0:03:28.480
那如果用 VAE 有甚麼好處呢？

0:03:28.480,0:03:30.360
如果用 VAE 的好處是

0:03:30.360,0:03:34.680
實際上，VAE 在做的事情

0:03:34.680,0:03:36.460
就等於我下面說的這件事情

0:03:36.460,0:03:40.740
當你把這個滿月的圖變成一個 code 的時候

0:03:40.740,0:03:44.920
它會在這 code 上面再加上 noise

0:03:44.920,0:03:48.140
它會希望，在加上 noise 以後

0:03:48.140,0:03:51.640
這個 code reconstruct 以後，還是一張滿月

0:03:51.640,0:03:55.320
也就是說，原來的 Auto encoder 只有這個點

0:03:55.320,0:03:58.540
需要被 reconstruct 回滿月的圖

0:03:58.540,0:04:01.440
但是對 VAE 來說，你會將上 noise

0:04:01.440,0:04:05.120
在這個範圍之內的圖 reconstruct 回來以後

0:04:05.120,0:04:07.260
都應該仍然要是滿月的圖

0:04:07.260,0:04:09.520
這個弦月的圖也是一樣

0:04:09.520,0:04:12.120
弦月的 code 再加一個 noise

0:04:12.120,0:04:15.080
reconstruct 回來以後

0:04:15.080,0:04:17.480
這個 range 的 code 都要變成弦月的圖

0:04:17.480,0:04:19.680
你會發現說，在這個位置

0:04:19.680,0:04:22.820
在這個地方，這個 code 的點

0:04:22.820,0:04:27.900
它同時希望被 reconstruct 回弦月的圖

0:04:27.900,0:04:31.600
同時也希望被 reconstruct 回滿月的圖

0:04:31.600,0:04:34.640
那可是你只能夠 reconstruct 回一張圖而已

0:04:34.640,0:04:36.180
怎麼辦？

0:04:36.180,0:04:38.060
那 VAE training 的時候

0:04:38.060,0:04:41.520
你要 minimize  這個 mean square error

0:04:41.520,0:04:44.120
所以，最後產生這個位置

0:04:44.120,0:04:45.380
所產生的圖

0:04:45.380,0:04:49.380
會是一張介於滿月和弦月中間的圖

0:04:49.380,0:04:51.700
同時讓它最像滿月，也最像弦月

0:04:51.700,0:04:53.540
那你產生的圖會是甚麼樣子呢？

0:04:53.540,0:04:57.860
或許就是，界於滿月和弦月之間的月象圖

0:04:57.860,0:05:00.360
所以，如果你用 VAE 的話

0:05:00.360,0:05:03.000
你從你 code 的 space上面

0:05:03.000,0:05:05.680
去 sample 一個 code

0:05:05.680,0:05:07.120
在產生 image 的時候

0:05:07.120,0:05:09.800
你可能會得到比較好的 image

0:05:09.800,0:05:11.980
如果是原來的 Auto-encoder 的話

0:05:11.980,0:05:13.700
你 random sample 一個 point

0:05:13.700,0:05:18.700
你得到的，可能看起來都不像是一個真實的 image

0:05:18.700,0:05:21.520
所以，VAE 就是這樣

0:05:21.520,0:05:27.200
這個 m，這個 encoder 的 output, m 代表是原來的 code

0:05:27.200,0:05:33.620
那這個 c，代表是加上 noise 以後的 code

0:05:33.620,0:05:37.600
decoder 要根據加上 noise 以後的 code

0:05:37.600,0:05:40.400
把它 reconstruct 回原來的 image

0:05:40.400,0:05:43.280
那這個 σ 跟 e 是甚麼意思呢？

0:05:43.280,0:05:47.600
這個 σ，它就代表了

0:05:47.600,0:05:50.160
現在這個 noise的 variance

0:05:50.160,0:05:53.640
它代表了你的 noise 應該要有多大

0:05:53.640,0:05:55.600
因為 variance 是正的

0:05:55.600,0:05:58.640
所以，這一邊會取一個 exponential

0:05:58.640,0:06:00.600
因為 neural network 的 output

0:06:00.600,0:06:03.320
假設你沒有用 activation function 去控制它的話

0:06:03.320,0:06:05.520
它的 output 可正可負

0:06:05.520,0:06:08.220
假設你這一邊是 linear的 output 的話

0:06:08.220,0:06:09.700
那 output 可正可負

0:06:09.700,0:06:12.780
所以取一個 exponential 確保它一定是正的

0:06:12.780,0:06:15.880
可以被當做是 variance 來看待

0:06:15.880,0:06:19.500
那現在當你把這個 σ

0:06:19.500,0:06:21.540
乘上這個 e

0:06:21.540,0:06:25.920
這個 e 是從一個 normal distribution sample 出來的值

0:06:25.920,0:06:30.160
當你把這個 σ 乘上 e 再加到 m 的時候

0:06:30.160,0:06:34.460
就等於是你把這個 m 加上了 noise

0:06:34.460,0:06:37.380
就等於是你把原來 code 加上 noise

0:06:37.380,0:06:38.700
那這個 e 呢

0:06:38.700,0:06:41.320
是從 normal distribution sample 出來的

0:06:41.320,0:06:43.380
所以，它的 variance 是固定的

0:06:43.380,0:06:46.400
但是，乘上不同 σ 以後呢

0:06:46.400,0:06:49.180
它的 variance 的大小就有所改變

0:06:49.180,0:06:52.500
所以這一個 variance 決定了 noise的大小

0:06:52.500,0:06:54.240
而這一個 variance大小

0:06:54.240,0:06:56.820
這個 variance 是從 encoder 產生的

0:06:56.820,0:06:58.820
也就是說 machine 在 training 的時候

0:06:58.820,0:07:03.600
它會自動去 learn 說，這個 variance 應該要有多大

0:07:03.600,0:07:07.460
但是，如果就只是這樣子是不夠的

0:07:07.460,0:07:10.500
假如你現在的 training 就只考慮說

0:07:10.500,0:07:12.280
我現在 input 一張 image

0:07:12.280,0:07:14.740
然後，我中間有這個加 noise 的機制

0:07:14.740,0:07:16.960
noise 的 variance 是自己 learn 的

0:07:16.960,0:07:18.680
然後，decoder 要 reconstruct 回原來的 image

0:07:18.680,0:07:22.020
那你要 minimize 這個 reconstruction error

0:07:22.020,0:07:24.340
如果你只有做這一件事情的話，是不夠的

0:07:24.340,0:07:25.760
你 train 出來的結果呢

0:07:25.760,0:07:28.620
是不會，並不會如同你預期的樣子

0:07:28.620,0:07:29.520
為甚麼呢？

0:07:29.520,0:07:32.400
因為這個 variance 現在自己學的

0:07:32.400,0:07:35.480
假設你讓 machine 自己決定說 variance 是多少

0:07:35.480,0:07:37.580
那它一定會決定說

0:07:37.580,0:07:39.940
variance 是 0 就好了 ，對不對

0:07:39.940,0:07:41.960
就讓大家自己決定分數是多少

0:07:41.960,0:07:43.440
那每一個人都會是 100 分

0:07:43.440,0:07:46.300
所以，這邊這個 variance

0:07:46.300,0:07:48.360
如果你只讓 machine 自己決定的話

0:07:48.360,0:07:50.740
它就會覺得說，variance 是 0 就好了

0:07:50.740,0:07:53.040
那你就等於是原來的 Auto-encoder

0:07:53.040,0:07:55.240
因為 variance 是 0 的話，就不會有這個

0:07:55.240,0:07:57.160
不同的 image overlap的情形

0:07:57.160,0:07:59.520
這樣你 reconstruction error 是最小的

0:07:59.520,0:08:02.240
所以，你要在這個 variance 上面呢

0:08:02.240,0:08:03.720
去做一些限制

0:08:03.720,0:08:06.400
你要強迫它的 variance 不可以太小

0:08:06.400,0:08:10.200
怎麼做呢？所以，我們另外再加了這一項

0:08:10.200,0:08:14.620
其實就是對 variance 做了一些限制

0:08:14.620,0:08:17.680
怎麼說呢，這一項是這樣子

0:08:17.680,0:08:24.380
你看它這一邊有 exp(σi) - (1 + σi)

0:08:24.380,0:08:28.440
那 exp(σi) 畫在圖上的話

0:08:28.440,0:08:31.020
它是藍色的這一條線

0:08:31.020,0:08:37.300
(1+σi) 畫在圖上，它是紅色的這一條線

0:08:37.300,0:08:41.100
當你把藍色這一條線減紅色這一條線的時候

0:08:41.100,0:08:43.100
你得到是綠色的這一條線

0:08:43.100,0:08:45.700
綠色這一條線的最低點呢

0:08:45.700,0:08:48.000
是落在 σ = 0 的地方

0:08:48.000,0:08:52.700
注意一下，σ 之後會再乘 exponential，所以σ = 0

0:08:52.700,0:08:55.220
意味這說，它的 variance 是 1

0:08:55.220,0:08:56.620
exp = 0，是 1

0:08:56.620,0:09:00.180
所以 σ = 0 的時候，loss 最低

0:09:00.180,0:09:02.180
意味著說， 你的 variance 等於 1 的時候

0:09:02.180,0:09:04.220
loss 最低

0:09:04.220,0:09:07.300
所以，machine 就不會說，讓 variance 等於 0，然後

0:09:07.340,0:09:09.940
minimizes reconstruction error，它還要考慮說

0:09:09.940,0:09:12.960
variance 是不能夠太小

0:09:12.960,0:09:16.420
那最後這一項 ( mi )^2

0:09:16.420,0:09:18.860
對這個 code 做這個

0:09:18.860,0:09:21.200
要 minimize code 的這個 L2-norm

0:09:21.200,0:09:24.100
怎麼解釋呢，其實很容易解釋

0:09:24.100,0:09:27.080
你就想成是我們現在加了 L2 的 regularization

0:09:27.080,0:09:29.680
我們本來常常在 train Auto-encoder 的時候

0:09:29.680,0:09:31.440
你就會在你的 code 上面呢

0:09:31.440,0:09:33.300
加一些 regularization

0:09:33.300,0:09:35.120
讓它結果比較 sparse

0:09:35.120,0:09:36.820
比較不會 overfitting

0:09:36.820,0:09:40.380
比較不會 learn 出太多 trivial 的 solution

0:09:40.380,0:09:44.000
那這個是直觀的理由

0:09:44.000,0:09:46.360
如果比較正式解釋的話

0:09:46.360,0:09:48.100
要怎麼解釋它呢

0:09:48.100,0:09:53.300
以下就是 paper 上，比較常見的說法

0:09:53.300,0:09:56.980
假設我們回歸到我們到底要做的事情是什麼

0:09:56.980,0:09:59.800
假設你現在要叫 machine 做的事情

0:09:59.800,0:10:01.940
是 generate 寶可夢的圖

0:10:01.940,0:10:04.480
每一張寶可夢的圖

0:10:04.480,0:10:09.340
你都可以想成是高維空間中的一個點

0:10:09.340,0:10:11.660
一張 image，假設它是 20*20 的 image

0:10:11.660,0:10:13.260
它在高維的空間中

0:10:13.260,0:10:16.280
就是一個 20*20，也就是一個 400 維的點

0:10:16.280,0:10:17.960
我們這邊寫做 x

0:10:17.960,0:10:20.580
雖然在圖上，我們只用一維來描述它

0:10:20.580,0:10:23.680
但它其實是一個高維的空間

0:10:23.680,0:10:26.080
那我們現在要做的事情

0:10:26.080,0:10:32.400
其實就是 estimate 高維空間上面的機率分佈，P(x)

0:10:32.400,0:10:35.280
我們要做的事情就是 estimate 這個 P(x)

0:10:35.280,0:10:38.920
只要我們能夠 estimate 出這個 P(x) 的樣子

0:10:38.920,0:10:42.380
注意，這個 x 其實是一個 vector

0:10:42.380,0:10:45.500
假設我們可以 estimate 出 P(x) 的樣子

0:10:45.500,0:10:47.340
我們就可以根據這個 P(x)

0:10:47.340,0:10:48.940
去 sample 出一張圖

0:10:48.940,0:10:50.200
那找出來的圖

0:10:50.200,0:10:52.100
就會像是寶可夢的樣子

0:10:52.100,0:10:55.740
因為你取 P(x) 的時候，你會從機率高的地方

0:10:55.740,0:10:57.760
比較容易被 sample 出來

0:10:57.760,0:11:00.820
所以，這個 P(x) 理論上應該是在

0:11:00.820,0:11:03.120
有寶可夢的圖的地方

0:11:03.120,0:11:05.500
這有寶可夢的圖，如果你今天

0:11:05.500,0:11:09.220
這個圖長得像一隻寶可夢的話，它的機率是大的

0:11:09.220,0:11:10.300
它的機率是大的

0:11:10.300,0:11:13.140
這個是噴火龍家族，他們的機率是大的

0:11:13.140,0:11:14.980
這個是水箭龜家族，他們機率是大的

0:11:14.980,0:11:17.020
如果是一張怪怪的圖的話

0:11:17.020,0:11:19.900
比如說，這一個看起來像是皮卡丘，又有一點不像

0:11:19.900,0:11:22.940
這個看起來像是一隻綿羊，又像是一個雲

0:11:22.940,0:11:24.540
這一邊呢，機率是低的

0:11:24.540,0:11:25.980
這一邊機率是低的

0:11:25.980,0:11:28.180
如果我們今天能夠 estimate 出

0:11:28.180,0:11:30.040
這一個 probability distribution

0:11:30.040,0:11:31.520
那就結束了

0:11:31.520,0:11:34.960
那怎麼 estimate 一個 probability 的 distribution 呢？

0:11:34.960,0:11:39.180
我們可以用 Gaussian mixture model

0:11:39.180,0:11:42.560
我不知道在座有多少人知道 Gaussian mixture model

0:11:42.560,0:11:43.980
我好奇問一下

0:11:43.980,0:11:46.500
知道 Gaussian mixture model 的同學舉手一下

0:11:46.500,0:11:48.340
好手放下，謝謝

0:11:48.340,0:11:50.620
很多人都知道 Gaussian mixture model

0:11:50.620,0:11:52.200
那太好了

0:11:52.200,0:11:56.920
有學過語音課的話，就聽過 Gaussian mixture model

0:11:56.920,0:11:58.100
Gaussian mixture model 在做甚麼呢？

0:11:58.100,0:12:00.100
我們現在有一個 distribution，它長這個樣子

0:12:00.100,0:12:03.260
黑色的、很複雜

0:12:03.260,0:12:06.620
我們說這個很複雜的黑色 distribution，它其實是

0:12:06.620,0:12:08.780
很多的 Gaussian

0:12:08.780,0:12:10.920
我這一邊藍色的代表 Gaussian

0:12:10.920,0:12:12.420
有很多的 Gaussian

0:12:12.420,0:12:16.000
用不同的 weight 疊合起來的結果

0:12:16.000,0:12:18.520
假設你今天 Gaussian 的數目夠多

0:12:18.520,0:12:22.360
你就可以產生很複雜的 distribution

0:12:22.360,0:12:23.780
所以，雖然黑色很複雜

0:12:23.780,0:12:26.780
但它背後其實是有很多 Gaussian 疊合起來的結果

0:12:26.980,0:12:29.440
那這個式子怎麼寫它呢？

0:12:29.440,0:12:34.440
你會把它寫成這樣

0:12:34.440,0:12:39.440
首先，如果你要從 P(x) sample 一個東西的時候

0:12:39.440,0:12:40.860
你怎麼做？

0:12:40.860,0:12:44.780
你先決定你要從哪一個 Gaussian sample 東西

0:12:44.780,0:12:47.600
假設現在有一把 Gaussian

0:12:47.600,0:12:49.040
有一把 Gaussian

0:12:49.040,0:12:51.960
每一個 Gaussian 背後都有一個 weight

0:12:51.960,0:12:53.820
每一個 Gaussian 都有自己的 weight

0:12:53.820,0:12:57.060
這一邊有一把 Gaussian，每一個都有它自己的 weight

0:12:57.060,0:12:58.260
那接下來呢

0:12:58.260,0:13:02.680
你再根據每一個 Gaussian 的 weight

0:13:02.680,0:13:06.100
去決定你要從哪一個 Gaussian sample data

0:13:06.100,0:13:09.600
然後，再從你選擇的那個 Gaussian 裡面 sample data

0:13:09.600,0:13:12.400
如果你選擇 1 這個 Gaussian的話

0:13:12.400,0:13:14.580
那你就從這個地方sample data

0:13:14.580,0:13:16.700
如果選擇 2 的話，就從這個地方

0:13:16.700,0:13:19.480
3 就從這個地方、4 就從這個地方、5 就從這個地方

0:13:19.480,0:13:20.600
以此類推

0:13:20.600,0:13:25.240
所以，怎麼從 Gaussian mixture model 
sample 一個 data 呢？

0:13:25.240,0:13:26.280
你就這樣做

0:13:26.280,0:13:30.500
首先，你有一個 multinomial  的 distribution

0:13:30.500,0:13:32.160
你從這一個 multinomial distribution 裡面

0:13:32.160,0:13:35.460
決定你要去 sample 哪一個 Gaussian

0:13:35.460,0:13:37.640
今天 m 代表第幾個 Gaussian

0:13:37.640,0:13:39.180
它是一個 integer

0:13:39.180,0:13:42.540
決定好你要從哪一個 m sample Gaussian 以後

0:13:42.540,0:13:45.220
你要從哪一個 Gaussian sample data 以後

0:13:45.220,0:13:46.520
決定哪一個 Gaussian 以後

0:13:46.520,0:13:49.480
每一個 Gaussian 有自己的 mean

0:13:49.480,0:13:55.100
μ(上標 m)，有一個自己的 variance,  Σ(上標 m)

0:13:55.100,0:13:57.300
所以，你有了這個 m 以後

0:13:57.300,0:13:59.560
你就可以從，你就可以找到這個

0:13:59.560,0:14:00.740
mean 跟 variance

0:14:00.740,0:14:02.160
根據這個 mean 跟 variance

0:14:02.160,0:14:04.720
你就可以 sample 一個 x 出來

0:14:04.720,0:14:07.140
所以，今天整個 P(x) 寫成

0:14:07.140,0:14:09.960
summation over 所有的 Gaussian

0:14:09.960,0:14:14.880
那一個 Gaussian 的 weight, P(m) 再乘上

0:14:14.880,0:14:16.860
有了那一個 Gaussian以後

0:14:16.860,0:14:21.720
從那一個 Gaussian sample 出 x 的機率，P(x|m)

0:14:21.720,0:14:25.980
那在 Gaussian mixture model 裡面

0:14:25.980,0:14:29.720
有種種的問題，比如說你需要決定 mixture 的數目

0:14:29.720,0:14:33.240
但是，如果你知道 mixture的數目的話

0:14:33.240,0:14:37.080
接下來給你一些 data，x

0:14:37.080,0:14:39.380
你要 estimate 這一把 Gaussian

0:14:39.380,0:14:41.160
跟它的每一個 Gaussian 的 weight

0:14:41.160,0:14:43.280
跟它的 mean 跟 variance

0:14:43.280,0:14:44.780
其實是很容易的

0:14:44.780,0:14:46.680
你只要用 EM Algorithm 就好了

0:14:46.680,0:14:48.460
你不知道這個是甚麼沒有關係

0:14:48.460,0:14:50.720
反正這是很容易的事情

0:14:54.180,0:14:56.280
現在每一個 x

0:14:56.280,0:14:59.760
它都是從某一個 mixture 被 sample 出來的

0:14:59.760,0:15:02.820
這件事情其實就很像是

0:15:02.820,0:15:05.360
在做 classification 一樣

0:15:05.360,0:15:07.240
每一個我們所看到的 x

0:15:07.240,0:15:10.380
它都是來自於某一個分類

0:15:10.380,0:15:12.780
它都是來自於某一個 class

0:15:12.780,0:15:14.760
但是，我們之前有講過說

0:15:14.760,0:15:17.140
把 data 做 classification

0:15:17.140,0:15:19.640
做 clustering 其實是不夠的

0:15:19.640,0:15:22.460
更好的表示方式是用

0:15:22.460,0:15:25.480
Distributed 的 representation

0:15:25.480,0:15:27.880
也就是說，每一個 x

0:15:27.880,0:15:29.740
它並不是屬於某一個 class

0:15:29.740,0:15:32.800
某一個 cluster，而是它有一個 vector

0:15:32.800,0:15:36.180
來描述它的各個不同面相的 attribute

0:15:36.180,0:15:38.440
描述它各個不同面向的特質

0:15:38.440,0:15:42.740
所以 ，VAE 其實就是 Gaussian mixture model 的

0:15:42.740,0:15:45.440
distributive representation 的版本

0:15:45.440,0:15:47.860
怎麼說？首先呢

0:15:47.860,0:15:51.800
我們要 sample 一個 z

0:15:51.800,0:15:56.320
這個 z 是從一個 normal distribution sample 出來的

0:15:56.320,0:15:57.420
z 是一個 vector

0:15:57.420,0:16:00.680
它從一個 normal distribution 被 sample 出來

0:16:00.680,0:16:02.660
那這個 z 是一個 vector

0:16:02.660,0:16:05.240
這個 vector 的每一個 dimension

0:16:05.240,0:16:08.020
就代表了某種 attribute

0:16:08.020,0:16:12.060
代表你現在要 sample 的那個東西的某種特質

0:16:12.060,0:16:13.700
z 的每一個 dimension

0:16:13.700,0:16:16.220
就代表了它要 sample 的某種東西的特質

0:16:16.220,0:16:18.400
假設 z 它長這樣

0:16:18.400,0:16:19.420
假設 z 長這樣

0:16:19.420,0:16:21.140
它是一個 Gaussian distribution

0:16:21.140,0:16:22.440
現在我們在這個圖上呢

0:16:22.440,0:16:24.520
就假設它是一維的

0:16:24.520,0:16:27.740
但在實際上 z 可能是一個 10 維的、100 維的 vector

0:16:27.740,0:16:29.800
到底有幾維，是由你自己決定

0:16:29.800,0:16:33.440
假設現在 z 呢，就是一維的 Gaussian

0:16:33.440,0:16:38.120
接下來，你sample 出這個 z 以後

0:16:38.120,0:16:42.340
根據 z ，你可以決定 μ 跟 σ

0:16:42.340,0:16:45.260
你可以決定 Gaussian 的 mean 跟 variance

0:16:45.260,0:16:48.000
剛才在 Gaussian mixture model 裡面

0:16:48.000,0:16:50.180
你有 10 個 mixture，你就是 10 個 mean

0:16:50.180,0:16:52.120
跟 10 個 variance

0:16:52.120,0:16:53.740
但是，今天在這個地方

0:16:53.740,0:16:56.740
你的 z 有無窮多個可能

0:16:56.740,0:16:58.800
它是 continuous，不是 discrete

0:16:58.800,0:17:01.100
所以你的 z，有無窮多的可能

0:17:01.100,0:17:05.060
所以，你的 mean 跟 variance

0:17:05.060,0:17:06.580
也有無窮多的可能

0:17:06.580,0:17:09.160
那怎麼找到這個 mean 跟 variance 呢？

0:17:09.160,0:17:11.680
怎麼給一個 z，找 mean 跟 variance 呢？

0:17:11.680,0:17:13.280
你這一邊的做法就是

0:17:13.280,0:17:19.780
假設 mean 跟 variance 都來自一個 function

0:17:19.780,0:17:21.020
都來自一個 function

0:17:21.020,0:17:23.760
你把 z 代進產生 mean 的這個 function

0:17:23.760,0:17:26.340
它就給你 μ(z)

0:17:26.340,0:17:30.180
μ(z)代表說，現在，如果你的

0:17:30.180,0:17:34.780
你的 hidden 的東西

0:17:34.780,0:17:36.820
你的 attribute 是 z 的時候

0:17:36.820,0:17:40.840
你在這個 x 這個 space 上的 mean是多少

0:17:40.840,0:17:46.620
同理，σ(z) 代表說你的 variance 是多少

0:17:46.620,0:17:48.900
代表說，你現在如果從

0:17:48.900,0:17:51.740
這個 latent 的 space 裡面得到 z 的時候

0:17:51.740,0:17:53.280
你的 variance 是多少

0:17:53.280,0:17:55.520
所以，實際上

0:17:57.020,0:18:00.740
所以，實際上這個 P(x) 是怎麼產生的呢？

0:18:00.740,0:18:05.360
在 z 這個  space上面

0:18:05.360,0:18:08.500
每一個點都有可能被 sample到

0:18:08.500,0:18:10.200
只是在中間這邊呢

0:18:10.200,0:18:12.200
這個點被 sample 到的機率比較大

0:18:12.200,0:18:15.660
在 tail 的地方，點被 sample 到的機率比較小

0:18:15.660,0:18:18.380
當你 sample 出一個點以後

0:18:18.380,0:18:21.020
你在 z 的 space 上面 sample 出一個 point 以後

0:18:21.020,0:18:24.880
那一個 point 會對應到 一個 Gaussian

0:18:24.880,0:18:28.160
這一個點對應到這一個 Gaussian

0:18:28.160,0:18:31.780
這個點對應到這一個 Gaussian

0:18:31.780,0:18:33.640
這一個 點對應到這一個Gaussian，等等

0:18:33.640,0:18:35.320
每一個點都對應到一個 Gaussian

0:18:35.320,0:18:38.520
至於某一個點對應到什麼樣的 Gaussian

0:18:38.520,0:18:40.140
它的 mean 跟 variance是多少

0:18:40.140,0:18:43.440
是由某一個 function 所決定的

0:18:43.440,0:18:45.780
所以，當你用這一個概念

0:18:45.780,0:18:48.840
當你今天你的這個 Gaussian

0:18:48.840,0:18:52.380
是從一個 normal distribution 所產生的時候

0:18:52.380,0:18:55.540
現在你等於就是有無窮多的 Gaussian

0:18:55.540,0:18:56.980
原來 Gaussian mixture model 裡面

0:18:56.980,0:18:59.140
最多甚麼 512 的，那個太少

0:18:59.140,0:19:01.040
我們現在有無窮多個 Gaussian

0:19:01.040,0:19:03.000
那另外一個問題就是

0:19:03.000,0:19:05.700
我們怎麼知道每一個 z

0:19:05.700,0:19:07.840
應該對應到怎樣的 mean 跟 variance

0:19:07.840,0:19:09.580
這個 function 怎麼找呢？

0:19:09.580,0:19:13.260
我們知道說 neural network 就是一個 function

0:19:13.260,0:19:15.520
所以，你可以說我就是 traing 一個 neural network

0:19:15.520,0:19:17.620
這個 neural network input z

0:19:17.620,0:19:21.000
然後，它的 output  就是兩個 vector

0:19:21.000,0:19:24.660
第一個 vector 代表了 input 是 z 的時候

0:19:24.660,0:19:26.080
你 Gaussian 的 mean

0:19:26.080,0:19:28.380
這個 σ 代表了 variance

0:19:28.380,0:19:31.780
那 variance 時常說，它是一個 matrix

0:19:31.780,0:19:34.680
你可以說，你可以把 matrix 拉直當作它的 output

0:19:34.680,0:19:37.540
或者是你可以只 output diagonal 的地方

0:19:37.540,0:19:40.680
然後，假設 non-diagonal 的地方都是 0

0:19:40.680,0:19:42.880
這樣都是可以的

0:19:42.880,0:19:45.880
反正，我們有一個 neural network

0:19:45.880,0:19:47.540
它可以告訴我們說

0:19:47.540,0:19:50.980
在 z 這一個 space 上每一個點

0:19:50.980,0:19:52.960
它對應到 x space 的時候呢

0:19:52.960,0:19:55.980
你的這一個 distribution，mean 跟 variance 分別是多少

0:19:55.980,0:20:00.680
那現在這個 P(x) 的 distribution 會長什麼樣子呢？

0:20:00.680,0:20:02.460
這個 P(x) 的 distribution 呢

0:20:02.460,0:20:05.440
就會變成是 P(z)的機率

0:20:05.440,0:20:11.420
跟我們知道 z 的時候，x 的機率

0:20:11.420,0:20:15.560
再對所有可能的 z 做積分

0:20:15.560,0:20:17.900
這邊不能夠是相加

0:20:17.900,0:20:20.340
不能夠是 summation，必須要是積分

0:20:20.340,0:20:22.360
因為這個 z 是 continuous 的

0:20:22.360,0:20:25.300
那有人可能會有一個困惑就是

0:20:25.300,0:20:29.660
為甚麼這邊一定是這個 Gaussian 呢

0:20:29.660,0:20:31.600
為什麼這邊一定是 Gaussian 呢？

0:20:31.600,0:20:33.740
你可以不是 Gaussian 這個樣子

0:20:33.740,0:20:35.440
它可以是一種花的樣子

0:20:35.440,0:20:39.160
在文獻上確實有人會把它弄成一朵花的樣子

0:20:39.160,0:20:42.900
它可以是任何的東西

0:20:42.900,0:20:45.060
這個是你自己決定的

0:20:45.060,0:20:48.380
當然這個 Gaussian，說起來是合理的

0:20:48.380,0:20:52.860
你就假設說，每一個 attribute 它的分佈就是 Gaussian

0:20:52.860,0:20:54.680
比較極端的 case 總是比較少的嘛

0:20:54.680,0:20:57.020
比較沒有特色的東西總是比較多的嘛

0:20:57.020,0:21:01.060
然後，attribute 跟 attribute之間是 independent 的

0:21:01.060,0:21:03.480
這樣子的假設其實也是合理的

0:21:03.480,0:21:05.940
不過，這個形狀是你自己假設的

0:21:05.940,0:21:07.620
你可以假設是任何的形狀

0:21:07.620,0:21:08.880
你可以假設任何形狀

0:21:08.880,0:21:10.920
那現在這個

0:21:10.920,0:21:12.760
但是，你不用擔心說

0:21:12.760,0:21:17.500
你如果假設 Gaussian 會不會對 P(x) 帶來很大的限制

0:21:17.500,0:21:21.260
會不會說，如果假設 z 是 Gaussian distribution 的話

0:21:21.260,0:21:23.720
有一些 P(x)，你就沒有辦法描述

0:21:23.720,0:21:25.540
其實，你不用太擔心這個問題

0:21:25.540,0:21:27.620
你不要忘了這個 NN 是非常 powerful 的

0:21:27.620,0:21:30.460
NN 可以 represent 任何 function

0:21:30.460,0:21:33.020
只要 neuron 夠多，NN 可以 represent 任何 function

0:21:33.020,0:21:36.620
所以，今天從 z 到 x 中間的 mapping 可以是很複雜

0:21:36.620,0:21:39.640
所以，就算你的 z 是一個 normal distribution

0:21:39.640,0:21:41.080
最後這個 P(x) 呢

0:21:41.080,0:21:44.280
它也可以是一個非常複雜的 distribution

0:21:44.280,0:21:48.280
再來呢，所以我們現在的式子是這樣子的

0:21:48.280,0:21:53.740
我們知道 P(x) 可以寫成對 z 的積分

0:21:53.740,0:21:56.860
然後乘上P(z)，還有乘上 P(x|z)

0:21:56.860,0:21:59.200
P(z) 是一個 normal distribution

0:21:59.200,0:22:01.840
這個 x given z 呢

0:22:01.840,0:22:05.620
我們先知道 z 是什麼，然後我們就可以決定 x

0:22:05.620,0:22:09.800
它是從什麼樣子的 mean 跟 variance 的 Gaussian裡面

0:22:09.800,0:22:10.980
被 sample 出來的

0:22:10.980,0:22:15.960
但是這一個 function 有 z

0:22:15.960,0:22:18.280
它有什麼樣的 mean 跟 variance

0:22:18.280,0:22:20.340
它們中間的關係是不知道的

0:22:20.340,0:22:23.420
是等著要被找出來的

0:22:23.420,0:22:25.440
但是，問題是要怎麼找呢？

0:22:25.440,0:22:29.980
它的 criterion 就是要 maximize 我們的  likelihood

0:22:29.980,0:22:33.640
我們現在手上已經有一筆 data x

0:22:33.640,0:22:35.900
那你希望找到一組

0:22:35.900,0:22:38.620
找到一個 μ 的 function

0:22:38.620,0:22:40.420
找到一個 σ 的 function

0:22:40.420,0:22:42.820
它可以讓這個

0:22:42.820,0:22:45.280
你現在已經觀察到的 data

0:22:45.280,0:22:47.400
你現在手上已經有的 image

0:22:47.400,0:22:50.340
它的每一個 x 代表了一個 image

0:22:50.340,0:22:51.960
你現在手上已經有的 image

0:22:51.960,0:22:54.360
它的 P(x) 取 log 以後

0:22:54.360,0:22:58.060
它的值相加以後是被 maximize 的

0:22:58.060,0:23:03.100
這個就是 maximize 我們已經看到 image 的  likelihood

0:23:03.100,0:23:07.840
這邊只是複習一下這個 z 怎麼產生這個 μ 跟 σ 呢

0:23:07.840,0:23:09.140
它是透過了一個 NN

0:23:09.140,0:23:12.800
所以，我們要做的事情就是，調這個 NN 裡面的參數

0:23:12.800,0:23:15.500
調這個 NN 裡面每一個 neuron 的 weight 跟 bias

0:23:15.500,0:23:19.840
使得這個 likelihood 可以被 maximize

0:23:19.840,0:23:25.440
但是在這邊，等一下會引入另外一個 distribution

0:23:25.440,0:23:28.580
它叫做 q(z|x)

0:23:28.580,0:23:30.900
它跟這個 NN 是相反的

0:23:30.900,0:23:35.020
它是 given z，決定這個 x 的 mean 跟 variance

0:23:35.020,0:23:39.280
這邊是 given x，決定在 z 這個 space 上面的

0:23:39.280,0:23:41.380
mean  跟 variance

0:23:41.380,0:23:43.460
也就是說，我們有另外一個 NN

0:23:43.460,0:23:46.620
這邊寫成 NN'，你 input x 以後

0:23:46.620,0:23:49.780
它會告訴你說，對應的 z 的 mean

0:23:49.780,0:23:52.880
跟對應的 z  的 variance

0:23:52.880,0:23:54.460
你給它 x 以後

0:23:54.460,0:23:57.960
它會決定這個 z 要從什樣的 mean 跟甚麼樣的 variance

0:23:57.960,0:23:59.600
被 sample 出來

0:23:59.600,0:24:04.040
上面這個 NN，其實就是 VAE 裡面的 decoder

0:24:04.040,0:24:08.800
下面這個 NN，其實就是 VAE 裡面的 encoder

0:24:08.800,0:24:13.100
那我們現在先不要管 NN 這一件事情

0:24:13.100,0:24:16.080
我們現在就先只看 這個式子就好

0:24:16.080,0:24:20.360
P(x|z) 我們就先不要在意它是不是從 NN 產生的

0:24:20.360,0:24:24.360
反正這個就是一個機率，我們要去把它找出來

0:24:24.360,0:24:26.160
怎麼找呢？

0:24:26.160,0:24:28.060
這個 log P(x)

0:24:28.060,0:24:31.300
它可以寫成積分

0:24:31.300,0:24:40.020
over z 的積分，然後 q(z|x) * logP(x) dz這樣

0:24:40.020,0:24:42.520
我們想說為什麼是這樣呢？

0:24:42.520,0:24:47.380
因為 q(z|x)，它是一個 distribution

0:24:47.380,0:24:51.120
這個式子對任何 distribution 都成立

0:24:51.120,0:24:55.200
我們假設 q(z|x) 是一個從路邊撿來的 distribution

0:24:55.200,0:24:57.160
它可以是任何一個 distribution

0:24:57.160,0:25:01.360
那任何一個 distribution，你都可以寫成

0:25:01.360,0:25:03.860
寫成這個樣子

0:25:03.860,0:25:08.080
對不對？因為這個積分跟 P(x) 是無關的

0:25:08.080,0:25:10.000
所以，可以把 P(x)這一項提出 來

0:25:10.000,0:25:12.960
然後，積分的部分就會變成 1

0:25:12.960,0:25:15.140
所以，左式就等於右式

0:25:15.140,0:25:17.240
這個沒有什麼好講的，這式子什麼都沒有做

0:25:17.240,0:25:20.800
再來呢，也是一個其實什麼都沒有做的式子

0:25:20.800,0:25:26.640
這個 P(x) 可以寫成 P(z,x) / P(z|x)

0:25:26.640,0:25:30.320
那你把 P(z|x) 展開 一下就會發現說

0:25:30.320,0:25:33.180
這一項等於這一項，這也沒什麼好講的

0:25:33.180,0:25:37.080
那接下來又是一個甚麼都沒有做的式子

0:25:37.080,0:25:45.000
就是，本來我們把 P(z, x) / q(z|x)

0:25:45.000,0:25:50.940
然後，再把 q(z|x) / P(z|x)

0:25:51.100,0:25:52.920
左式也等於右式

0:25:52.940,0:25:55.760
因為這個 q 其實是可以消掉的

0:25:55.760,0:25:59.000
這個小學生應該就知道

0:25:59.000,0:26:01.600
這個式子也等於是什麼事都沒有做這樣子

0:26:01.600,0:26:06.040
接下來，這個東西被放在 log 裡面

0:26:06.040,0:26:11.380
我們知道 log 相乘等於拆開後相加

0:26:11.380,0:26:15.540
所以，log 這一項乘這一項

0:26:15.540,0:26:19.200
等於 log 這一項加 log 這一項

0:26:19.200,0:26:24.480
那接下來呢，觀察一下這兩項到底代表什麼事情

0:26:24.480,0:26:29.260
右邊這一項，它代表了一個 KL divergence

0:26:29.260,0:26:36.100
這個 P(z|x) 是一個 distribution

0:26:36.100,0:26:41.320
q(z|x) 是另外一個 distribution

0:26:41.320,0:26:45.380
現在 x 是給定的，所以你有兩個 distribution

0:26:45.380,0:26:48.140
當有兩個 distribution 的時候

0:26:48.140,0:26:50.900
你可以算一個東西，叫做 KL divergence

0:26:50.900,0:26:56.720
KL divergence 代表的是這兩個 distribution 相近的程度

0:26:56.720,0:26:58.520
它們兩個相近的程度

0:26:58.520,0:27:00.960
如果 KL divergence 它越大

0:27:00.960,0:27:03.140
代表這兩個 distribution 越不像

0:27:03.140,0:27:07.000
這兩個 distribution 一模一樣的時候，
KL divergence 會是 0

0:27:07.000,0:27:10.140
所以，KL divergence 它是一個距離的概念

0:27:10.140,0:27:13.000
它衡量了兩個 distribution 之間的距離

0:27:13.000,0:27:17.540
這一項就是 KL divergence 的式子

0:27:17.540,0:27:19.220
這一項是一個距離

0:27:19.220,0:27:21.200
所以，它一定是大於等於 0 的

0:27:21.200,0:27:23.420
最小也是 0 而已

0:27:23.420,0:27:26.280
至於這個為什麼 KL divergence的這個

0:27:26.280,0:27:28.060
反正你就記起來就是了

0:27:28.060,0:27:33.260
那因為這一項一定是大於等於 0 的

0:27:33.260,0:27:37.680
所以，這一項會是

0:27:37.680,0:27:40.180
L 的 lower bound

0:27:40.180,0:27:42.780
L 一定會大於等於這一項

0:27:42.780,0:27:47.960
這一項你可以再拆一下，P(z, x) = P(x|z) * P(z)

0:27:47.960,0:27:51.020
所以，L 一定會大於這一項

0:27:51.020,0:27:53.700
那這一項就是一個 lower bound

0:27:53.700,0:27:55.540
我們叫它 L(下標 b)

0:27:55.540,0:27:58.880
那現在我們知道事情是這樣

0:27:58.880,0:28:05.000
這個 log Probability，就是我們要 maximize  的對象

0:28:05.000,0:28:08.620
它是由這兩項加起來的結果

0:28:08.620,0:28:11.900
Lb 它長成這個樣子

0:28:11.900,0:28:13.500
它長成這個樣子

0:28:13.500,0:28:14.940
在這個式子裡面

0:28:14.940,0:28:18.660
P(z)是 normal distribution，是已知的

0:28:18.660,0:28:24.380
我們不知道的是 P(x|z)  跟 q(z|x)

0:28:24.380,0:28:26.420
那我們本來要做的事情

0:28:26.420,0:28:34.360
我們本來要做的事情是要找 P(x|z)

0:28:34.360,0:28:40.460
讓這個 P，讓這個 likelihood 越大越好

0:28:40.460,0:28:42.420
現在我們要做的事情

0:28:42.420,0:28:48.560
變成找 P(x|z) 和 q(z|x)

0:28:48.560,0:28:51.820
讓 Lb 越大越好

0:28:51.820,0:28:54.780
我們本來只要找這一項，本來只要找這一項

0:28:54.780,0:28:56.680
現在順便也要找這一項

0:28:56.680,0:28:58.660
把這兩項合起來

0:28:58.660,0:29:00.480
我們希望同時找這兩項

0:29:00.480,0:29:04.460
然後去 maximize 這個 Lb

0:29:04.460,0:29:08.960
突然多找一項是要做什麼呢？

0:29:08.960,0:29:11.080
如果我們現在只找這一項的話

0:29:11.080,0:29:14.200
如果假設我們現在只找這一項

0:29:14.200,0:29:16.100
然後去 maximize Lb 的話

0:29:16.100,0:29:19.660
你如果 maximize 這一項

0:29:19.660,0:29:21.480
你如果調整這一項

0:29:21.480,0:29:23.540
你如果找這一項 P(x|z)

0:29:23.540,0:29:26.440
讓 Lb 被 maximize 的話

0:29:26.440,0:29:28.800
那因為你要找的這一個 log

0:29:28.800,0:29:31.460
你要找的這個 likelihood，它是 Lb 的 upper bound

0:29:31.460,0:29:34.940
所以，你增加 Lb 的時候

0:29:34.940,0:29:38.480
你有可能會增加你的 likelihood

0:29:38.480,0:29:41.100
但是，你不知道你的這個 likelihood

0:29:41.100,0:29:43.760
跟你的 lower bound 之間到底有什麼樣的距離

0:29:43.760,0:29:46.200
你想像你希望能做到的事情是

0:29:46.200,0:29:49.540
當你的 lower bound 上升的時候

0:29:49.540,0:29:52.340
當你的 lower bound 上升的時候

0:29:52.340,0:29:56.240
你的 likelihood 是會比 lower  bound 高，
然後你的 likelihood 也跟著上升

0:29:56.240,0:29:58.740
但是，你有可能會遇到一個比較糟糕的狀況是

0:29:58.740,0:30:01.080
你的 lower bound 上升的時候

0:30:01.080,0:30:02.520
likelihood 反而下降

0:30:02.520,0:30:04.920
雖然，它還是 lower bound，它還是比 lower bound 大

0:30:04.920,0:30:07.620
但是，它有可能下降
因為根本不知道它們之間的差距是多少

0:30:07.620,0:30:10.400
所以，引入 q 這一項呢

0:30:10.400,0:30:13.800
其實可以解決剛才說的那一個問題

0:30:13.800,0:30:17.620
為什麼呢？因為你看這個是 likelihood

0:30:17.620,0:30:21.420
likelihood = Lb + KL divergence

0:30:21.420,0:30:29.240
如果你今天去這個調這個 q(z|x)，調 q 這一項

0:30:29.240,0:30:32.540
去 maximize Lb 的話，會發生什麼事呢？

0:30:32.540,0:30:35.040
你會發現說，首先 q 這一項

0:30:35.040,0:30:39.160
跟 log P(x) 是一點關係都沒有的

0:30:39.160,0:30:43.580
對不對？log P(x) 只跟 P(x|z) 有關

0:30:43.580,0:30:47.640
這個 q 代什麼東西，這個值都是不變的

0:30:47.640,0:30:50.200
所以，這個值都是不變的

0:30:50.200,0:30:52.700
藍色這一條長度都是一樣的

0:30:52.700,0:30:59.200
但是，我們現在卻去 maximize Lb

0:30:59.200,0:31:04.400
maximize Lb 代表說你 minimize 了這個 KL divergence

0:31:04.400,0:31:06.380
也就是說你會讓

0:31:06.380,0:31:10.120
你的 lower bound 跟你的這個 likelihood

0:31:10.120,0:31:11.900
越來越接近

0:31:11.900,0:31:16.840
如果你 maximize q 這一項的話

0:31:16.840,0:31:21.800
所以，今天假如你固定住這個

0:31:21.800,0:31:28.360
假如你固定住這個 P，假如你固定住 P(x|z) 這一項

0:31:28.360,0:31:31.740
然後一直去調 q(z|x) 這一項的話

0:31:31.740,0:31:34.400
你會讓這個 Lb 一直上升，一直上升，一直上升

0:31:34.400,0:31:37.140
最後這一個 KL divergence 會完全不見

0:31:37.140,0:31:39.700
假如你最後可以找到一個 q

0:31:39.700,0:31:44.140
它跟這個 p(z|x) 正好完全 distribution 一模一樣的話

0:31:44.140,0:31:46.600
你就會發現說你的 likelihood 就會跟

0:31:46.600,0:31:49.980
lower bound 完全停在一起

0:31:49.980,0:31:53.060
它們就完全是一樣大

0:31:53.060,0:31:56.380
這個時候呢，如果你再把 lower bound 上升的話

0:31:56.380,0:32:00.140
因為你的 likelihood 一定要比 lower bond 大

0:32:00.140,0:32:01.920
所以這個時候你的 likelihood 呢

0:32:01.920,0:32:03.580
你就可以確定它一定會上升

0:32:03.580,0:32:07.700
所以，這個就是引入 q 這一項它有趣的地方

0:32:07.700,0:32:09.620
今天我會得到一個副產物

0:32:09.620,0:32:12.760
當你在 maximize q 這一項的時候

0:32:12.760,0:32:15.420
你會讓這個 KL divergence 越來越小

0:32:15.420,0:32:20.720
意謂這說，你就是讓這個 q 跟 P(z|x)

0:32:20.720,0:32:23.120
注意一下，這兩項是不一樣的

0:32:23.120,0:32:24.500
這個方向是不一樣的

0:32:24.500,0:32:31.620
你會讓這個 q(z|x) 跟 P(z|x) 越來越接近

0:32:31.620,0:32:33.860
所以我們接下要做的事情呢

0:32:33.860,0:32:37.620
就是找這一個跟這一個

0:32:37.620,0:32:39.960
然後可以讓 Lb 越大越好

0:32:39.960,0:32:41.380
讓 Lb 越大越好

0:32:41.380,0:32:44.660
就等同於我們可以讓 likelihood 越來越大

0:32:44.660,0:32:47.060
而且你順便會找到

0:32:47.060,0:32:51.640
這個 q 可以去 approximation of p(z|x)

0:32:51.640,0:32:54.200
那這一項 Lb 它長什麼樣子呢

0:32:54.200,0:32:58.240
這一項 Lb 我們剛才講過它就是長這個樣子

0:32:58.240,0:33:02.000
然後 log 裡面相乘，可以把它拆開

0:33:02.000,0:33:04.360
可以把它拆開

0:33:04.360,0:33:09.180
我們把 P (z) 跟 q(z|x) 放在一邊

0:33:09.180,0:33:12.860
把這一項放在另外一邊

0:33:12.860,0:33:14.680
那如果你觀察一下的話

0:33:14.680,0:33:16.700
會發現 P(z) 是一個 distribution

0:33:16.700,0:33:18.820
q(z|x) 也是一個 distribution

0:33:18.820,0:33:22.880
所以，這一項是一個 KL divergence

0:33:22.880,0:33:29.820
這一項是 P(z) 跟 q(z|x) 的 KL divergence

0:33:29.820,0:33:33.700
那如果複習一下，這個 q 是什麼呢

0:33:33.700,0:33:34.980
q 是一個 neural network

0:33:34.980,0:33:38.600
q是一個 neural network

0:33:38.600,0:33:41.560
當你給 x 的時候，它會告訴你說

0:33:41.560,0:33:44.480
q(z|x) 它是從什麼樣的

0:33:44.480,0:33:47.960
mean 跟 variance 的 Gaussian 裡面 sample 出來的

0:33:47.960,0:33:51.960
所以，我們現在如果你要

0:33:51.960,0:33:58.960
minimize 這個 P(z) 跟 q(z|x) 的 KL divergence 的話呢

0:33:58.960,0:34:03.560
你就是去調這個 output、這個 output

0:34:03.560,0:34:08.440
你去調你的這個 q 對應的那一個 neural network

0:34:08.440,0:34:10.700
你去調你的那個 q 對應的那一個 neural network

0:34:10.700,0:34:14.340
讓它產生的 distribution 可以跟這個

0:34:14.340,0:34:16.960
一個 normal distribution 越接近越好

0:34:16.960,0:34:19.980
這一件事情的這個推導呢

0:34:19.980,0:34:24.740
我們就把他放在，你就參照 VAE 原始的 paper

0:34:24.740,0:34:26.920
那 minimize 這一項

0:34:26.920,0:34:30.700
其實就是我們剛才說的這一項

0:34:30.700,0:34:33.440
剛才說的在 reconstruction error 外

0:34:33.440,0:34:36.900
另外再加的那一個，看起來像是 regularization 的式子

0:34:36.900,0:34:40.220
它要做的事情就是 minimize 這個 KL divergence

0:34:40.220,0:34:41.980
它要做的事情就是希望說

0:34:41.980,0:34:44.580
這一個 q(z|x) 的 output

0:34:44.580,0:34:47.320
跟 normal distribution 是接近的

0:34:47.320,0:34:49.580
那我們還有另外一項

0:34:49.580,0:34:51.040
另外一項是這樣子

0:34:51.040,0:34:55.160
另外一項是要這個積分

0:34:55.160,0:35:04.240
over q(z|x) * log[P(x|z)] 對 z 做積分

0:35:04.240,0:35:07.860
這一項的意思就是

0:35:07.860,0:35:11.180
你可以想像，我們有一個 log P(x|z)

0:35:11.180,0:35:19.740
然後，它用 q(z|x) 來做這個 weighted sum

0:35:19.740,0:35:22.840
所以，你可以把它寫成

0:35:22.840,0:35:29.920
[log P(x|z)] 根據 q(z|x) 的這個期望值

0:35:29.920,0:35:31.460
根據它的期望值

0:35:31.460,0:35:34.260
所以這一邊這個式子的意思呢

0:35:34.280,0:35:36.480
這一邊這個式子的意思就好像是說

0:35:36.480,0:35:40.980
我們從 q(z|x)  去 sample data

0:35:40.980,0:35:42.620
給我們一個 x 的時候

0:35:42.620,0:35:47.680
我們去計算，我們去根據這個 q(z|x)，這個機率分佈

0:35:47.680,0:35:49.180
去 sample 一個 data

0:35:49.180,0:35:53.960
然後，要讓 log P(x|z) 的機率越大越好

0:35:53.960,0:35:57.960
那這一件事情其實就 Auto-encoder 在做的事情

0:35:57.960,0:35:59.320
什麼意思呢？

0:35:59.320,0:36:03.520
怎麼從 q(z|x) 去 sample 一個 data 呢？

0:36:03.520,0:36:07.220
你就把 x 丟到 neural network 裡面去

0:36:07.220,0:36:11.420
它產生一個 mean 跟一個 variance

0:36:11.420,0:36:13.860
根據這個 mean 跟 variance

0:36:13.860,0:36:16.740
你就可以 sample 出一個 z

0:36:16.740,0:36:19.460
接下來，我們要做的事情

0:36:19.460,0:36:20.660
你已經做這一項了

0:36:20.660,0:36:22.420
這一邊就是這一項

0:36:22.420,0:36:25.000
你已經根據現在的 x sample 出 一個 z

0:36:25.000,0:36:29.060
接下來，你要 maximize 這一個 z

0:36:29.060,0:36:32.080
產生這個 x 的機率

0:36:32.080,0:36:35.100
那這個 z 產生這個 x 的 機率是甚麼呢

0:36:35.100,0:36:36.760
這個 z 產生這個 x 的機率

0:36:36.760,0:36:39.560
是把這個 z 丟到另外一個 neural network 裡面去

0:36:39.560,0:36:43.460
它產生一個 mean 跟 variance

0:36:43.460,0:36:48.580
要怎麼讓這個機率越大越好呢？

0:36:48.580,0:36:50.940
要怎麼讓這個 NN output

0:36:50.940,0:36:54.480
所代表 distribution 產生 x 的 機率越大越好呢？

0:36:54.480,0:36:58.200
假設我們無視 variance 這一件事情的話

0:36:58.200,0:37:00.080
後來在一般實作裡面

0:37:00.080,0:37:02.660
你可能不會把 variance  這一件事情考慮進去

0:37:02.660,0:37:05.040
你只考慮 mean 這一項的話

0:37:05.040,0:37:06.600
那你要做的事情就是

0:37:06.600,0:37:08.100
讓這個 mean 呢

0:37:08.100,0:37:11.960
讓你的這個 mean 跟你的 x 越接近越好

0:37:11.960,0:37:13.220
你現在是一個 normal

0:37:13.220,0:37:15.800
你現在是一個 Gaussian distribution

0:37:15.800,0:37:18.940
那 Gaussian distribution 在 mean 的地方機率是最高的

0:37:18.940,0:37:20.980
所以，如果你讓這個 NN

0:37:20.980,0:37:25.020
output 的這個 mean 正好等於你現在這個 data x 的話

0:37:25.020,0:37:29.420
這一項 log P(x|z) 它的值是最大的

0:37:29.420,0:37:31.980
所以，現在這整個 case 就變成說

0:37:31.980,0:37:36.020
input 一個 x，然後，產生兩個 vector

0:37:36.020,0:37:39.100
然後 sample 一下產生一個 z，再根據這個 z

0:37:39.100,0:37:40.760
你要產生另外一個 vector

0:37:40.760,0:37:42.840
這個 vector 要跟原來的 x 越接近越好

0:37:42.840,0:37:44.980
這件事情其實就是

0:37:44.980,0:37:47.920
就是 Auto-encoder 在做的事情

0:37:47.920,0:37:51.020
你要讓你的 input 跟 output 越接近越好

0:37:51.020,0:37:53.140
它就是 Auto-encoder 在做的事情

0:37:53.140,0:37:55.180
所以這兩項合起來

0:37:55.180,0:38:00.080
就是剛才我們前面看到的 VAE 的 loss function

0:38:00.080,0:38:04.220
如果你聽不懂的話也沒有關係

0:38:04.220,0:38:08.900
前面有提供了比較  intuitive 的想法

0:38:08.900,0:38:14.040
那其實 VAE 有另外一個是叫做 conditional 的 VAE

0:38:14.060,0:38:17.120
conditional VAE 這邊我們就簡單講一下概念就好

0:38:17.120,0:38:19.360
conditional VAE 它可以做的事情是說

0:38:19.360,0:38:23.960
比如說，如果你現在讓 VAE 可以產生手寫的數字

0:38:23.960,0:38:26.080
讓 VAE 可以產生手寫的數字

0:38:26.080,0:38:29.220
它就是看一個，給它一個 digit

0:38:29.220,0:38:33.500
然後，它把這個 digit 的特性抽出來

0:38:33.500,0:38:35.380
它抽出它的特性

0:38:35.380,0:38:38.560
比如說，它的筆劃的粗細等等

0:38:38.560,0:38:40.080
然後，接下來呢

0:38:40.080,0:38:41.860
你在丟進 encoder 的時候

0:38:41.860,0:38:43.294
你一方面給它

0:38:43.294,0:38:46.880
有關這一個數字的特性的 distribution

0:38:46.880,0:38:49.400
另外一方面告訴 decoder 說

0:38:49.400,0:38:50.680
它是什麼數字

0:38:50.680,0:38:53.720
那你就可以 generate 一大排

0:38:53.720,0:38:55.760
你就可以根據這一個 digit

0:38:55.760,0:38:59.400
generate 跟它 style 很相近的 digit

0:38:59.400,0:39:02.320
這個應該是在 MNIST 上面的結果

0:39:02.320,0:39:04.160
我的 reference 在下面，這是在 MNIST 上面的結果

0:39:04.160,0:39:06.760
這是在另外一個數字的 corpus 上面的結果

0:39:06.760,0:39:08.180
你會發現說

0:39:08.180,0:39:11.420
conditional VAE 確實可以根據某一個 digit

0:39:11.420,0:39:16.000
畫出其他的這個 style 相近的數字

0:39:16.000,0:39:20.140
這一邊是一些 reference 給大家參考

0:39:20.140,0:39:23.600
那 VAE 其實有一個很嚴重的問題

0:39:23.600,0:39:24.820
就是因為它有這問題，所以

0:39:24.820,0:39:28.420
之後又 propose 了 GAN

0:39:28.420,0:39:30.820
那 VAE 有什麼樣的問題呢？

0:39:30.820,0:39:35.740
VAE 其實它從來沒有去真的學怎麼產生一張

0:39:35.740,0:39:39.460
看起來像真的 image，對不對？

0:39:39.460,0:39:41.340
因為它所學到的事情是

0:39:41.340,0:39:45.360
它想要產生一張 image

0:39:45.360,0:39:50.040
跟我們在 data base 裡面某張 image 越接近越好

0:39:50.040,0:39:52.280
但是，它不知道的事情是

0:39:52.280,0:39:55.080
我們在 evaluate 它產生的 image

0:39:55.080,0:39:57.560
跟 data base 裡面的 image 的相似度的時候

0:39:57.560,0:40:00.880
我們是用，比如說，mean square error 等等

0:40:00.880,0:40:03.060
來 evaluate 兩張 image 中間的相似度

0:40:03.060,0:40:06.400
今天呢，假設我們這個

0:40:06.400,0:40:09.960
這個 decoder 的 output 跟真的 image 之間

0:40:09.960,0:40:11.580
有一個 pixel 的差距

0:40:11.580,0:40:13.520
它們有某一個 pixel 是不一樣的

0:40:13.520,0:40:16.600
但是，這個不一樣的 pixel，它落在不一樣的位置

0:40:16.600,0:40:19.120
其實是會得到非常不一樣的結果

0:40:19.120,0:40:21.720
假設這個不一樣的 pixel

0:40:21.720,0:40:24.500
它是落在這個地方

0:40:24.500,0:40:25.660
它落在這個地方

0:40:25.660,0:40:27.640
它只是讓 7 的筆劃比較長一點

0:40:27.640,0:40:30.960
跟它落在另外一個地方

0:40:30.960,0:40:33.160
它落在這個地方

0:40:33.160,0:40:36.260
對人來說 ，你一眼就可以看出說

0:40:36.260,0:40:39.700
這個是 machine generate，是怪怪的 digit

0:40:39.700,0:40:41.480
這個搞不好是真的

0:40:41.480,0:40:44.200
因為你根本看不出來跟原來的 7 有什麼差異

0:40:44.200,0:40:46.560
它只是稍微長一點，看起來還是很正常

0:40:46.560,0:40:50.480
但是，對 VAE 來說，都是一個 pixel 的差異

0:40:50.480,0:40:56.340
對它來說，這兩張 image 是一樣的好或一樣的不好

0:40:56.340,0:40:58.160
所以，VAE 它學的事情

0:40:58.160,0:41:04.520
只是怎麼產生一張 image 
跟 data base 裡面的 image 一模一樣

0:41:04.520,0:41:06.940
它從來沒有想過說要

0:41:06.940,0:41:10.220
真的產生一張可以假亂真的 image

0:41:10.220,0:41:14.220
所以，如果你用 VAE 來做 training 的時候

0:41:14.220,0:41:16.840
其實你產生出來的 image

0:41:16.840,0:41:18.640
VAE 所產生出來的 image

0:41:18.640,0:41:21.040
往往都是 data base 裡面的 image

0:41:21.040,0:41:22.740
的 linear combination 而已

0:41:22.740,0:41:26.160
因為它從來沒有學過要產生新的 image

0:41:26.160,0:41:28.480
它唯一做的事情只有模仿而已

0:41:28.480,0:41:31.640
它唯一做的事情只有

0:41:31.640,0:41:35.560
希望它產生的 image 跟 data base 的某張 image 越像越好

0:41:35.560,0:41:36.700
它只是模仿而已

0:41:36.700,0:41:40.200
或者最多就是把原來 data base 裡面的image 做 linear combination

0:41:40.200,0:41:44.500
它做一些 combination，它沒辦法產生一些新的 image

0:41:44.500,0:41:48.560
所以，這樣感覺沒有非常的 intelligent

0:41:48.560,0:41:50.040
所以，接下來就有人 propose

0:41:50.040,0:41:55.880
有另外一個方法叫做 Generative Adversarial Network

0:41:55.880,0:41:58.680
Adversarial 是對抗的意思

0:41:58.680,0:42:00.120
它縮寫是 GAN

0:42:00.120,0:42:02.380
你會發現它是很新的 paper

0:42:02.380,0:42:06.400
它最早出現的時候 是 2014 年的 1 2月

0:42:06.400,0:42:08.900
所以，大概是兩年前的 paper

0:42:08.900,0:42:13.880
以下呢，我們引用了 Yann LeCun 對 GAN 的 comment

0:42:13.880,0:42:16.100
就是有人在 Quora 上面

0:42:16.100,0:42:17.600
問了說這個

0:42:17.600,0:42:22.620
Unsupervised learning 的 approach 
哪一個是最有 potential 的

0:42:22.620,0:42:26.900
然後，Yann LeCun 他親自來回答，他說呢

0:42:26.900,0:42:31.500
Adversarial Training is the coolest thing since sliced bread.

0:42:31.500,0:42:33.400
since sliced bread，大家知道是什麼意思嗎？

0:42:33.400,0:42:34.660
我 google了一下，這是個片語

0:42:34.660,0:42:37.800
如果翻譯成中文的話，是有史以來的意思

0:42:37.800,0:42:40.000
since sliced bread 是什麼意思呢？

0:42:40.000,0:42:42.420
sliced bread 是切片麵包的意思

0:42:42.420,0:42:45.960
那這個片語的典故，好像是說

0:42:45.960,0:42:49.980
在過去麵包店是不幫你切麵麵包的

0:42:49.980,0:42:52.280
吐司麵包烤完之後，他是不幫你切的

0:42:52.280,0:42:53.620
所以你買回去之後，要自己切很麻煩

0:42:53.620,0:42:57.100
後來就人發明說，應該先切了以後再賣

0:42:57.100,0:42:58.460
然後，大家都很高興這個樣子

0:42:58.460,0:43:00.420
所以，since sliced bread

0:43:00.420,0:43:03.560
它的英文片語就是有史以來的意思

0:43:03.560,0:43:07.560
它說這是有史以來最強的、最酷的方法

0:43:07.560,0:43:10.460
他這邊還講了一些別的，他說

0:43:10.460,0:43:14.600
What's missing at the moment is a good understanding of it.

0:43:14.600,0:43:18.080
so we can make it work reliably.

0:43:18.080,0:43:20.820
It's very finicky.

0:43:20.820,0:43:24.140
Sort of like CovNet were in the 1990s,

0:43:24.140,0:43:30.520
when I had the reputation of being the only person who could make them work(which wasn't true).

0:43:30.520,0:43:32.860
這其實是 GAN 非常難 train

0:43:32.860,0:43:36.220
感覺好像只有 Ian 跟 Goodfellow

0:43:36.220,0:43:37.960
才 propose 他們可以做得起來

0:43:37.960,0:43:41.800
其他人做起來，你可以 google 一下那個 GAN 的 code

0:43:41.800,0:43:43.000
很多都在 MNIST 上面

0:43:43.000,0:43:45.920
他們產生的 digit，都不是很好看

0:43:45.920,0:43:48.260
我們用 VAE 隨便做都可以打爆這些東西

0:43:48.260,0:43:50.480
所以產生的 image 很怪

0:43:50.480,0:43:52.520
但是，你如果看 paper 的話

0:43:52.520,0:43:54.760
它的 performance 是滿好的

0:43:54.760,0:43:59.160
所以，它裡面還有很多不為人知的技巧

0:43:59.160,0:44:02.980
像過去大家相信說只有 Yann LeCun 可以 train 起來 CNN

0:44:02.980,0:44:04.740
不過其實不是這樣子

0:44:04.740,0:44:09.480
那其實我很無聊，我又收集了，找到另外一則這樣

0:44:09.480,0:44:11.160
就是有人問說

0:44:11.160,0:44:16.240
有沒有什麼最近的 breakthroughs 在 deep learning 裡面

0:44:16.240,0:44:19.760
然後，Yann LeCun 又來回答了，他說

0:44:19.760,0:44:23.540
The most important one, in my opinion, 
is adversarial training.

0:44:23.540,0:44:26.420
also called GAN.

0:44:26.420,0:44:29.520
This is an idea proposed by Ian Goodfellow.

0:44:29.560,0:44:33.100
他說這個是 the most interest idea

0:44:33.100,0:44:36.160
in the last ten years in ML

0:44:36.160,0:44:42.480
所以，你就來看這個十年來最有趣的想法到底是怎麼樣

0:44:42.480,0:44:46.000
這個 GAN 的概念

0:44:46.000,0:44:49.200
有點像似擬態的演化

0:44:49.200,0:44:51.840
比如說，這是一個枯葉蝶

0:44:51.840,0:44:53.400
這個是一個枯葉蝶

0:44:53.400,0:44:55.900
他長得就跟枯葉一模一樣

0:44:55.900,0:45:00.240
枯葉蝶是怎麼變的跟枯葉一模一樣呢？

0:45:00.240,0:45:01.820
怎麼變成這麼像的呢？

0:45:01.820,0:45:04.600
也許一開始他長的是這個樣子

0:45:04.600,0:45:07.760
然後呢，但是他有天敵

0:45:07.760,0:45:09.580
類似麻雀的天敵

0:45:09.580,0:45:11.940
比如像波波這樣子的天敵

0:45:11.940,0:45:16.460
天敵會吃這個蝴蝶

0:45:16.460,0:45:21.080
天敵辨識是不是蝴蝶的方式，就是他知道蝴蝶不是棕色

0:45:21.080,0:45:23.100
他就吃不是棕色的東西

0:45:23.100,0:45:27.220
所以蝴蝶就演化，他就變成是棕色的

0:45:27.220,0:45:31.040
但是，他的天敵也會跟著演化

0:45:31.040,0:45:33.360
波波就會變成比比鳥這樣

0:45:33.360,0:45:37.480
然後這個比比鳥知道說，蝴蝶是沒有葉脈的

0:45:37.480,0:45:40.240
所以，他會吃沒有葉脈的東西

0:45:40.240,0:45:42.020
他會 ignore 有葉脈的東西

0:45:42.020,0:45:44.300
所以，蝴蝶又再演化

0:45:44.300,0:45:46.620
就會變成枯葉蝶，他就產生葉脈

0:45:46.620,0:45:49.060
但是，他的天敵也會再演化

0:45:49.060,0:45:50.520
這個好像是神獸

0:45:50.520,0:45:53.020
這個好像不是波波演化來的，不過沒有關係

0:45:53.020,0:45:55.860
然後，他的天敵也還會再演化

0:45:55.860,0:45:57.640
所以，這兩個東西呢

0:45:57.640,0:46:03.320
天敵和枯葉蝶，他們就會共同的演化

0:46:03.320,0:46:05.840
所以，枯葉蝶就會長得越來越樣枯葉

0:46:05.840,0:46:08.080
直到最後沒有辦法分辨為止

0:46:08.080,0:46:12.200
所以這跟 GAN 的概念，是非常類似的

0:46:12.200,0:46:13.760
GAN 的概念是這個樣子

0:46:13.760,0:46:16.120
首先，有一個第一代的 Generator

0:46:16.120,0:46:19.700
第一代的 Generator 它很廢，它可能根本就是 random 的

0:46:19.700,0:46:22.140
它會 generate 一大堆奇怪的東西

0:46:22.140,0:46:24.220
看起來不像是真正地 image 的東西

0:46:24.220,0:46:26.440
假如我們現在叫它 Generate 4 個 digit

0:46:26.440,0:46:30.820
那接下來有一個的第一代 Discriminator

0:46:30.820,0:46:32.200
他就是那個天敵

0:46:32.200,0:46:33.780
Discriminator 做的事情是

0:46:33.780,0:46:37.280
他會根據 real 的 image

0:46:37.280,0:46:40.500
跟 Generator 所產生的 image

0:46:40.500,0:46:43.080
去調整它裡面的參數

0:46:43.080,0:46:46.840
去評斷說，一張 image 是真正的 image

0:46:46.840,0:46:49.500
還是 Generator 所產生的 image

0:46:49.500,0:46:53.660
接下來呢，這個 Generator 根據這個 Discriminator

0:46:53.660,0:46:55.440
等一下會講說 Generator

0:46:55.440,0:46:57.940
怎麼根據 Discriminator去演化

0:46:57.940,0:47:00.100
Generator 根據 Discriminator

0:47:00.100,0:47:02.780
他又去調整了他的參數

0:47:02.780,0:47:04.500
所以，第二代的 Generator

0:47:04.500,0:47:09.220
他產生的參數，他產生的 digit 就可能就更像真的

0:47:09.220,0:47:13.420
接下來，Discriminator 會再根據第二代的Generator

0:47:13.420,0:47:16.200
產生的 digit 跟真正的 digit

0:47:16.200,0:47:20.160
去 update 他的參數

0:47:20.160,0:47:22.440
接下來，有了第二代的 Discriminator

0:47:22.440,0:47:24.940
就會再產生第三代的 Generator

0:47:24.940,0:47:31.200
第三代 Generator 產生的數字又更像真正的這個數字

0:47:31.200,0:47:33.980
就是第三代 Generator 他產生的這些數字

0:47:33.980,0:47:35.660
可以騙過第二代的 Discriminator

0:47:35.660,0:47:38.180
第二代產生的這些數字，可以騙過第一代的 Generator

0:47:38.180,0:47:40.220
但是，這個第一代的 Discriminator

0:47:40.220,0:47:42.740
他產生的數字，可以騙過第二代的 Discriminator

0:47:42.740,0:47:45.800
但是，Discriminator 會再演化

0:47:45.800,0:47:50.120
可能又可以再分辨第三代 Generator 產生的數字

0:47:50.120,0:47:51.840
跟真正的數字之間的差距

0:47:51.840,0:47:55.520
你要注意一個地方就是，這個 Generator 啊

0:47:55.520,0:47:58.980
他從來沒有看過真正的 image長什麼樣子

0:47:58.980,0:48:02.420
Discriminator 有看過真正的 image 長什麼樣子

0:48:02.500,0:48:05.800
它會比較真正的 image 跟 Generator 的 output 的不同

0:48:05.800,0:48:09.220
但是，Generator 從來沒有看過真正的 image

0:48:09.220,0:48:12.040
他做的事情，只是想去騙過 Discriminator

0:48:12.040,0:48:15.860
所以，因為 generator 從來沒有看過真正 image

0:48:15.860,0:48:20.040
所以，Generator 他可以產生出來的那一些 image

0:48:20.040,0:48:22.920
是 data base 裡面從來都沒有見過的

0:48:22.920,0:48:26.220
所以，這比較像是，我們想要 machine 做的事情

0:48:26.220,0:48:28.860
我們現在看 Discriminator 是怎麼 train 的

0:48:28.860,0:48:30.700
這一邊是比較直覺的

0:48:30.700,0:48:33.260
這個 Discriminator 他就是一個 neural network

0:48:33.260,0:48:35.600
他的 input 就是一張 image

0:48:35.600,0:48:38.740
他的 output  就是一個 number

0:48:38.740,0:48:41.860
它的 output 就是一個 scalar

0:48:41.860,0:48:46.580
你可能通過 sigmoid function，讓他的值介於 0 到 1 之間

0:48:46.580,0:48:50.380
1 就代表說 input 這張 image 是真正的 image

0:48:50.380,0:48:52.500
假如你是要做手寫數字辨識的話

0:48:52.500,0:48:55.160
那 input image 就是真正的人手寫的數字

0:48:55.160,0:48:59.300
0 代表是假的，是 Generator 所產生的

0:48:59.300,0:49:01.600
那 Generator 是什麼呢？

0:49:01.600,0:49:03.940
Generator 在這一邊，其實

0:49:03.940,0:49:07.820
他的那個架構就跟 VAE 的 decoder 是一模一樣的

0:49:07.820,0:49:09.700
他也是一個 neural network

0:49:09.700,0:49:12.580
他的 input 就是從一個 distribution

0:49:12.580,0:49:15.240
他可以是某某 distribution 或是任何其他的 distribution

0:49:15.240,0:49:17.700
從某一個 distribution sample 出來的一個 vector

0:49:17.700,0:49:20.080
你把這個 sample 出來的 vector

0:49:20.080,0:49:22.120
丟到 Generator 裡面

0:49:22.120,0:49:24.560
他就會產生一個數字

0:49:24.560,0:49:25.600
產生一個 image

0:49:25.600,0:49:26.960
你給他不同的 vector

0:49:26.960,0:49:29.920
他就產生不同樣子的 image

0:49:29.920,0:49:35.440
那先用 Generator 產生一堆假的 image

0:49:35.440,0:49:41.600
然後，我們有真正的 image

0:49:41.600,0:49:45.100
Discriminator 就是把這一些 Generator

0:49:45.100,0:49:47.280
所產生的 image

0:49:47.280,0:49:49.920
都 label 為 0，也都 label 為 fake

0:49:49.920,0:49:52.700
然後，把這個真正的 image

0:49:52.700,0:49:54.120
都 label 為 1

0:49:54.120,0:49:55.700
也就是都 label 為 true

0:49:55.700,0:49:58.700
接下來，就只是一個 binary classification 的 problem

0:49:58.700,0:50:00.300
大家都很熟

0:50:00.300,0:50:03.500
你就可以 learn 一個 Discriminator

0:50:03.500,0:50:06.960
接下來，怎麼 learn 這個 Generator 呢？

0:50:06.960,0:50:08.760
Generator 的 learn 法是這個樣子

0:50:08.760,0:50:12.780
現在已經有了第一代的 Discriminator

0:50:12.780,0:50:15.360
怎麼根據第一代的 Discriminator

0:50:15.360,0:50:19.920
把第一代的 Generator 再 update 呢

0:50:19.920,0:50:23.720
首先，如果我們隨便給，輸入一個 vector

0:50:23.720,0:50:27.380
他會產生一張隨便的 image

0:50:27.380,0:50:32.640
那這一個 image 可能沒有辦法騙過這個 Discriminator

0:50:32.640,0:50:36.020
你把 Generator 產生的 image 丟到 Discriminator 裡面

0:50:36.020,0:50:38.760
他可能說，這有 87% 像這樣子

0:50:38.760,0:50:42.400
然後，接下來要做的事情是甚麼呢？

0:50:42.400,0:50:46.080
接下來，我們要做的事情是調這個 Generator 的參數

0:50:46.080,0:50:47.960
調這個 Generator 的參數

0:50:47.960,0:50:52.380
讓 Discriminator 會認為說 Generator

0:50:52.380,0:50:54.980
generate 出來的 image 是真的，也就是說

0:50:54.980,0:50:57.500
要讓 Generator generate 出來的 image

0:50:57.500,0:51:01.720
丟到 Discriminator 以後，Discriminator 的 output

0:51:01.720,0:51:04.240
必須要越接近越好

0:51:04.240,0:51:08.860
所以，你希望 Generator generate 是長這樣子的 image

0:51:08.860,0:51:11.180
他可以騙過 Discriminator

0:51:11.180,0:51:12.860
Discriminator output 是 1.0

0:51:12.860,0:51:14.460
覺得他是一個真正的 image

0:51:14.460,0:51:16.320
這件事情怎麼做呢

0:51:16.320,0:51:21.120
其實，因為你知道這個 Generator 是一個 neural network

0:51:21.120,0:51:24.540
那 Discriminator 也是一個 neural network

0:51:24.540,0:51:26.720
你把這個 Generator 的 output

0:51:26.720,0:51:30.500
丟到這個當作 Discriminator 的 input

0:51:30.500,0:51:32.720
然後，再讓他產生一個 scalar

0:51:32.720,0:51:36.040
這一件事情，其實就好像是

0:51:36.040,0:51:41.600
你有一個很大很大的 neural network

0:51:41.600,0:51:43.420
他這邊有很多層

0:51:43.420,0:51:44.580
他這一邊也有很多層

0:51:44.580,0:51:46.620
然後，你丟一個 random 的 vector

0:51:46.620,0:51:49.080
他 output 就是一個 scalar

0:51:49.080,0:51:51.680
所以，一個 Generator 加一個 Discriminator，他合起來

0:51:51.680,0:51:53.620
就是一個很大的 network

0:51:53.620,0:51:55.260
他既然乘起來是一個很大的 network

0:51:55.260,0:51:57.520
那你要讓這個 network

0:51:57.520,0:52:00.500
再丟進一個 random vector

0:52:00.500,0:52:04.320
他 output 1 是很容易的，你就做 Gradient Descent 就好

0:52:04.320,0:52:05.840
你就用 Gradient Descent 調參數

0:52:05.840,0:52:08.200
希望丟進這一個 vector 的時候

0:52:08.200,0:52:11.420
他的 output 是要接近 1 的

0:52:11.420,0:52:13.380
但是，你這邊要注意的事情是

0:52:13.380,0:52:17.080
你在調這個參數的時候

0:52:17.080,0:52:19.460
你在調這個 network 參數的時候

0:52:19.460,0:52:20.640
你在做 Backpropagation 的時候

0:52:20.640,0:52:24.160
你只能夠調整這個 Generator 的參數

0:52:24.160,0:52:28.760
你只能算 generator 的參數對 output 的 gradient

0:52:28.760,0:52:30.840
然後去 update Generator 的參數

0:52:30.840,0:52:32.540
你必須要 fix 住 Discriminator 的參數

0:52:32.540,0:52:36.640
如果你今天不 fix 住 Discriminator 的參數，
會發生什麼事情呢？

0:52:36.640,0:52:38.620
你會發生，對 Discriminator 來說

0:52:38.620,0:52:41.400
要讓他 output 1 很簡單阿

0:52:41.400,0:52:44.380
就他最後output 的時候，bias 設 1

0:52:44.380,0:52:47.780
然後其他都設 2，weight 都設 0，它 output 就 1 了

0:52:47.780,0:52:51.620
所以，Discriminator，你要讓這整個 network

0:52:51.620,0:52:54.400
input 一個 random 的 vector，output 是1 的時候

0:52:54.400,0:52:57.060
你要把 Discriminator 這個參數鎖住

0:52:57.060,0:53:00.560
Discriminator 參數必須要是 fix 住的

0:53:00.560,0:53:05.300
然後，input 一個 Generator，只調 generator 的參數

0:53:05.300,0:53:07.300
這樣 generator 產生出來的 image

0:53:07.300,0:53:11.980
才會像是，才是一個可以騙過 Discriminator 的 image

0:53:11.980,0:53:16.240
這邊有一個來自 GAN 原始 paper 的 Toy example

0:53:16.240,0:53:20.000
我們來說明一下，這個 Toy example 是什麼意思

0:53:20.000,0:53:22.020
這個 Toy example 是這樣子

0:53:22.020,0:53:26.920
他說，現在我們的這個 z space

0:53:26.920,0:53:31.640
也就是這個 decoder 的 input

0:53:31.640,0:53:34.080
我們知道 decoder 的 input 就是一個 z

0:53:34.080,0:53:36.040
就是一個 hidden 的 vector

0:53:36.040,0:53:38.280
hidden 的這個 vector

0:53:38.280,0:53:41.680
這個 z 他是一個 one dimensional 的東西

0:53:41.680,0:53:44.020
那他丟到 Generator 裡面

0:53:44.020,0:53:48.060
他會產生另外一個 one dimension 的東西

0:53:48.060,0:53:51.420
這個 z 可以從任何的 distribution 裡面 sample 出來

0:53:51.420,0:53:52.860
這邊在這個例子裡面

0:53:52.860,0:53:55.320
他顯然是從一個 uniform 的 distribution 裡面

0:53:55.320,0:53:57.080
sample 出來的

0:53:57.080,0:54:00.820
然後，你把這一個 z 通過 neural network 以後

0:54:00.820,0:54:05.220
每一個不同的 z，他會給你不同的 x

0:54:05.320,0:54:09.080
這個 x 的分布，就是綠色的這個分布

0:54:09.080,0:54:12.340
綠色這個分布，現在要做的事情是

0:54:12.340,0:54:17.360
希望這個 Generator 的 output 可以越像 real data 越好

0:54:17.360,0:54:20.420
他這一邊的 real data 就是黑色的這個點

0:54:20.420,0:54:23.660
假設有一組 real data 就是黑色的這個點

0:54:23.660,0:54:26.180
你要找的這個 distribution 是黑色的這個點

0:54:26.180,0:54:27.480
那你希望你的 Generator 的 output

0:54:27.480,0:54:29.200
也就是這個綠色 distribution

0:54:29.200,0:54:32.500
可以跟黑色的這個點，越接近越好

0:54:32.500,0:54:34.560
如果按照 GAN 的概念的話

0:54:34.560,0:54:37.540
你就是把這個 Generator 的 output x

0:54:37.540,0:54:39.440
跟這個 real 的 data

0:54:39.440,0:54:42.720
這些黑色的點，丟到 Discriminator 裡面

0:54:42.720,0:54:45.380
然後，讓 Discriminator 去判斷說

0:54:45.380,0:54:49.060
現在這個 value，其實現在這個 x

0:54:49.060,0:54:51.700
real data 都只是一個 scalar 而已

0:54:51.700,0:54:57.020
現在這個 scalar，他是來自真正的 data 的機率

0:54:57.020,0:55:00.240
跟來自於 Generator 的 output 的機率

0:55:00.240,0:55:02.820
如果他是真正的 data 的話就是 1

0:55:02.820,0:55:04.100
反之就是 0

0:55:04.100,0:55:08.120
Discriminator 的 output，就是綠色的 curve

0:55:08.120,0:55:11.860
那假設現在，Generator 他還很弱

0:55:11.860,0:55:13.540
所以，他產生出來的 distribution

0:55:13.540,0:55:15.360
是這個綠色的 distribution

0:55:15.360,0:55:19.900
那這個 Discriminator 他根據 real data

0:55:19.900,0:55:25.260
跟這個 Generator distribution 他的樣子呢

0:55:25.260,0:55:27.520
你給他這個 x 的值，他的 output

0:55:27.520,0:55:30.480
可能就會像是這一條藍色的線

0:55:30.480,0:55:32.680
這一條藍色的線告訴我們說

0:55:32.680,0:55:36.760
Discriminator 認為說，如果是在這一帶的點

0:55:36.760,0:55:40.300
他比較有可能是假的

0:55:40.300,0:55:42.280
他的這個值是比較低的

0:55:42.280,0:55:43.700
如果是落在這一帶的點

0:55:43.700,0:55:45.940
他比較有可能是從 Generator 產生的

0:55:45.940,0:55:47.800
落在這一帶的點

0:55:47.800,0:55:50.760
他比較有可能是 real data

0:55:50.760,0:55:53.900
接下來，Generator 就根據

0:55:53.900,0:55:56.900
Discriminator 的結果去調整他的參數

0:55:56.900,0:56:00.560
Generator 要做的事情是騙過 Discriminator

0:56:00.560,0:56:02.520
既然 Discriminator 認為

0:56:02.520,0:56:06.120
這個地方比較有可能是 real data

0:56:06.120,0:56:09.340
Generator 就把他的 output 往左邊移

0:56:09.340,0:56:11.780
他就把他的 output 往左邊移

0:56:11.780,0:56:14.080
那你說有沒有可能會移太多

0:56:14.080,0:56:17.300
比如說，通通偏到左邊去，是有可能的

0:56:17.300,0:56:18.800
所以 GAN 很難 train 這樣

0:56:18.800,0:56:22.100
這個要小心的調參數

0:56:22.100,0:56:24.680
小心的調參數，讓他不要移太多

0:56:24.680,0:56:28.460
這綠色的 distribution 就可以稍微偏一點

0:56:28.460,0:56:32.600
比較接近真正 real 的黑色的點的 distribution

0:56:32.600,0:56:34.640
所以，Generator 會騙過他

0:56:34.640,0:56:36.200
他就產生新的 distribution

0:56:36.200,0:56:38.180
然後，接下來 Discriminator

0:56:38.180,0:56:40.760
會再 update 綠色的這一條線

0:56:40.760,0:56:44.920
這一個 process 就不斷反覆地去進行

0:56:44.920,0:56:46.280
直到最後呢

0:56:46.280,0:56:49.320
Generator 產生的 output 跟 real data 一模一樣

0:56:49.320,0:56:52.400
那 Discriminator 會沒有任何辦法

0:56:52.400,0:56:54.620
分辨真正的 data

0:56:54.620,0:56:56.020
你有問題嗎？

0:57:04.620,0:57:07.660
其實這個就是現在 train GAN 的時候

0:57:07.660,0:57:09.060
所遇到最大的問題

0:57:09.060,0:57:12.440
你不知道 Discriminator 是不是對的

0:57:12.440,0:57:18.280
因為你說 Discriminator 現在得到一個很好的結果

0:57:18.280,0:57:20.760
那可能是 Generator 太廢

0:57:20.760,0:57:24.120
有時候 Discriminator 得到一個很差的結果

0:57:24.120,0:57:26.460
比如說，他認為說每一個地方

0:57:26.460,0:57:29.300
每一個地方他都無法分辨說

0:57:29.300,0:57:31.520
是 real value 還是 fake value

0:57:31.520,0:57:34.640
這個時候並不代表說 Generator generate 的很像

0:57:34.640,0:57:36.780
有可能只是 Discriminator 太弱了

0:57:36.780,0:57:40.500
所以，這是一個現在還沒有好的 solution 的難題

0:57:40.500,0:57:42.680
所以，真正在 train GAN 的時候，你會怎麼做呢？

0:57:42.680,0:57:47.160
你會一直坐在電腦旁邊，看他產生 image 這樣，你懂嗎 ?

0:57:47.160,0:57:51.720
因為你從 Discriminator 跟 Generator 的 loss

0:57:51.720,0:57:54.160
你看不出來他 generate 的 image 有沒有比較好

0:57:54.160,0:57:58.120
所以，變成說你 Generator 每 update 一次參數

0:57:58.120,0:57:59.940
Discriminator 每 update 一次參數

0:57:59.940,0:58:01.180
你就去看看他

0:58:01.180,0:58:03.940
你就拿 generated 的 image 看看有沒有比較好

0:58:03.940,0:58:05.680
如果變差以後

0:58:05.680,0:58:08.180
方向走錯了，再重新調一下參數這樣子

0:58:08.180,0:58:11.380
所以，這個非常非常的困難

0:58:11.380,0:58:12.480
非常非常的困難

0:58:12.480,0:58:15.640
我們這一邊其實有人在線上放了一個 demo

0:58:15.640,0:58:17.420
我們來看一下這個 demo

0:58:17.420,0:58:20.740
非常 realistic 的 image

0:58:20.740,0:58:23.680
這個是 OpenAI 產生的 image

0:58:23.680,0:58:28.240
如果我們問你說

0:58:28.240,0:58:30.780
你覺得左邊是 real image

0:58:30.780,0:58:31.960
還是右邊是 real image

0:58:31.960,0:58:35.980
你覺得左邊是電腦產生的 image 的同學舉手一下

0:58:35.980,0:58:39.340
有人，請放下

0:58:39.340,0:58:42.240
覺得右邊是電腦產生的 image 的同學舉手一下

0:58:42.240,0:58:43.560
好，手放下

0:58:43.560,0:58:45.540
其實他還是沒有辦法騙過人

0:58:45.540,0:58:47.540
你看這邊還有很多怪怪的的東西就是了

0:58:47.540,0:58:49.300
很多東西很像

0:58:49.300,0:58:51.560
這個馬還蠻像

0:58:51.560,0:58:54.620
這個有飛魚，有大嘴巴的貓

0:58:54.620,0:58:55.900
有很多怪怪的東西

0:58:55.900,0:58:57.480
所以，他其實沒有辦法騙過人

0:58:57.480,0:59:00.740
我覺得如果放單一一張，光看這個馬

0:59:00.740,0:59:02.760
他可能可以騙過人

0:59:02.760,0:59:05.800
OpenAI 他們有做那個實驗

0:59:05.800,0:59:09.840
好像有 21% 的 image

0:59:09.840,0:59:13.540
就有 21% machine generate 的 image 會被誤判成 real

0:59:13.540,0:59:15.900
所以，他其實是可以騙過部分的人

0:59:15.900,0:59:19.740
另外，這一邊又有一個很驚人的結果

0:59:19.740,0:59:21.600
在文獻上非常驚人的結果

0:59:21.600,0:59:24.800
就是說先拿很多房間的照片

0:59:24.800,0:59:26.460
讓 machine 去 train GAN

0:59:26.460,0:59:28.060
他可以 generate 房間的照片

0:59:28.060,0:59:31.140
那我們說，那個 Generator 就是你 input 一個 vector 給他

0:59:31.140,0:59:32.640
他就會 output 一張 image 給你

0:59:32.640,0:59:35.080
那你現在可以在那個

0:59:35.080,0:59:40.580
input 的 space 上調你的 vector，去產生不同的 output

0:59:40.580,0:59:43.780
所以，他說他先 random 找幾個 vector

0:59:43.780,0:59:45.460
random 找 5 個 vector

0:59:45.460,0:59:48.580
產生 5 張房間的圖

0:59:48.580,0:59:56.460
接著，再從這一個點移動你的這個 vector 到這個點這樣

0:59:56.460,0:59:59.020
所以，就發現說你的 image 逐漸地變化

0:59:59.020,1:00:01.020
逐漸的變化，然後跑到這個點

1:00:01.020,1:00:03.900
然後再逐漸的變化，再跑到這個點

1:00:03.900,1:00:06.280
你會發現一些有趣的地方，比如說，這邊有一個窗戶

1:00:06.280,1:00:09.200
它慢慢的就變成一個類似電視的東西

1:00:09.200,1:00:10.620
或是這邊有一個電視

1:00:10.620,1:00:13.700
它慢慢的就變成了窗戶這樣子

1:00:13.700,1:00:15.940
我覺得最驚人結果

1:00:15.940,1:00:19.160
是有人，有日本人他用 GAN

1:00:19.160,1:00:22.200
很神奇的，很神奇的東西

1:00:22.200,1:00:23.700
就傳說中，你只要能夠

1:00:23.700,1:00:25.620
一旦你能夠成功使用他

1:00:25.620,1:00:27.360
他就可以召喚不可思議的力量

1:00:27.360,1:00:31.200
但是，大部分的時候，你都沒有辦法成功的召喚它

1:00:31.200,1:00:33.920
它有點像是神之卡的感覺這樣

1:00:33.920,1:00:37.640
你只要能夠操控那個神，就可以獲得不可思議的力量

1:00:37.640,1:00:39.840
他大部分的時候你都無法操控他

1:00:39.840,1:00:41.600
昨天晚上我想說，我可不可以自己

1:00:41.600,1:00:43.480
generate 一些寶可夢

1:00:43.480,1:00:46.780
弄到 5 點我搞不起來，所以後來我想我還是去睡好了

1:00:46.780,1:00:51.160
就很麻煩

1:00:51.160,1:00:56.600
因為，它最大的問題就是你沒有一個很明確的 signal

1:00:56.600,1:00:59.600
它可以告訴你說，現在的 Generator

1:00:59.600,1:01:01.460
到底做的怎麼樣

1:01:01.460,1:01:05.480
沒有一個很明確的 signal 可以告訴你這件事

1:01:05.480,1:01:07.940
在一個 stander NN 的 training 裡面

1:01:07.940,1:01:10.180
你就看那一個 loss，loss 越來越小

1:01:10.180,1:01:12.280
代表說現在 training 越來越好

1:01:12.280,1:01:16.620
但是，在 GAN 裡面，你其實要做的事情是

1:01:16.620,1:01:21.340
keep 你的 Generator 跟 Discriminator，
他們是 well match 的

1:01:21.340,1:01:24.220
他們必需要不斷屬於一種競爭的狀態

1:01:24.220,1:01:26.740
他們必須要不斷處於可以

1:01:26.740,1:01:28.800
他們要像塔史亮跟進藤光一樣

1:01:28.800,1:01:31.740
不斷處於這種勢均力敵的狀態

1:01:31.740,1:01:33.880
他們必須要成為對手

1:01:33.880,1:01:36.460
那個第三堂課的時候

1:01:36.460,1:01:40.720
會請作業一、二、三做得特別好的同學來分享一下

1:01:40.720,1:01:42.520
他是怎麼做的

1:01:45.720,1:01:48.520
作業三就有人用 GAN

1:01:48.520,1:01:50.280
所以，代表是有人有做起來

1:01:56.800,1:02:01.120
那這很麻煩，因為在 GAN 裡面

1:02:01.120,1:02:04.600
你要讓 Discriminator 跟 Generator

1:02:04.600,1:02:07.660
他們一直維持一種勢均力敵的狀態

1:02:07.660,1:02:09.900
所以，你必須要用不可思議的平衡感

1:02:09.900,1:02:15.220
來調整這兩個 Discriminator 跟 Generator 的參數

1:02:15.220,1:02:17.860
讓他們一直處於勢均力敵的狀態

1:02:17.860,1:02:22.940
今天這個其實很像是在做 Alpha Go 一樣

1:02:22.940,1:02:24.240
你有兩個 agent

1:02:24.240,1:02:26.760
然後，你要讓他們一直是處於一樣強的狀態

1:02:26.760,1:02:29.880
當今天你的 Discriminator fail 的時候

1:02:29.880,1:02:33.820
因為我們最後 training 的終極的目標

1:02:33.820,1:02:36.080
是希望 Generator 產生出來的東西

1:02:36.080,1:02:38.180
是 Discriminator 完全無法分別的

1:02:38.180,1:02:42.380
就是 Discriminator 在鑑別真的或假的 image 上面

1:02:42.380,1:02:44.280
它的正確率是 0

1:02:44.280,1:02:48.420
但是，往往當你發現你的 Discriminator 整個 fail 掉的時候

1:02:48.420,1:02:52.360
並不代表說 Generator 真的 generate 很好的 image

1:02:52.360,1:02:56.040
往 你遇到的狀況是你的 Generator 太弱

1:02:56.040,1:02:59.520
那很多時候，我在 train 的時候還會遇到的狀況

1:02:59.520,1:03:04.400
就是 Generator 它不管 input 什麼樣的 vector

1:03:04.400,1:03:07.480
它 output 都給你一張非常像的東西

1:03:07.480,1:03:09.700
那一張非常像的東西

1:03:09.700,1:03:11.840
不知道怎麼回事就騙過了 Discriminator

1:03:11.840,1:03:13.960
那個就是 Discriminator 的罩門

1:03:13.960,1:03:15.880
它無法分辨那一張 image

1:03:15.880,1:03:18.740
那它整個就 fail 掉了，但並不代表說你的 machine

1:03:18.740,1:03:21.400
真的得到好的結果

1:03:21.400,1:03:24.680
我要說的大概就是這樣
後面是一些 reference 給大家參考

1:03:24.680,1:03:30.580
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
