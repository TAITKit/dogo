<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
0:00:00.000,0:00:03.950<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中<br>
http://aintu.tw<br>
<br>
0:00:04.020,0:00:06.180<br>
方法之間呢<br>
如果你很熟悉 machine learning 原則的話<br>
<br>
0:00:06.180,0:00:08.980<br>
不同方法間他們其實是非常像的<br>
<br>
0:00:09.020,0:00:13.720<br>
其實就算你沒有辦法把所有方法都學過<br>
其實也是一法通，萬法通<br>
<br>
0:00:13.820,0:00:16.040<br>
machine learning 的方法是非常多的<br>
<br>
0:00:16.040,0:00:22.980<br>
我覺得你並不需要很汲汲營營把所有方法都覺得你應該要學過一遍<br>
<br>
0:00:22.980,0:00:26.700<br>
掌握幾個大原則其實你就可以了解大部分的方法<br>
<br>
0:00:27.040,0:00:29.040<br>
Support Vector Machine 是甚麼呢<br>
<br>
0:00:29.160,0:00:32.500<br>
Support Vector Machine 有兩個特色<br>
<br>
0:00:32.500,0:00:34.660<br>
第一個是他用了 Hinge Loss<br>
<br>
0:00:34.660,0:00:36.560<br>
等一下我們會講 Hinge Loss 是甚麼<br>
<br>
0:00:36.560,0:00:40.680<br>
另外一個是我覺得最厲害的地方，他有個 Kernel Trick<br>
<br>
0:00:40.680,0:00:45.480<br>
把 Hinge Loss 加上 Kernel Trick 就是 Support Vector Machine，就是SVM<br>
<br>
0:00:45.600,0:00:47.920<br>
先來講一下 Hinge Loss 是甚麼<br>
<br>
0:00:48.000,0:00:53.440<br>
回到我們在開學的時候講過的 Binary Classification 的問題<br>
<br>
0:00:54.000,0:00:56.620<br>
我們講過在理想上<br>
<br>
0:00:56.620,0:00:58.860<br>
Binary Classification 應該要怎麼做呢<br>
<br>
0:00:59.300,0:01:05.920<br>
理想上我們知道一個 Machine Learning 的 solution 往往就有三個 step<br>
<br>
0:01:06.060,0:01:08.240<br>
在 Binary Classification 裡面<br>
<br>
0:01:08.240,0:01:12.200<br>
第一個 step 是<br>
<br>
0:01:12.200,0:01:14.580<br>
我們定一個 function g(x)<br>
<br>
0:01:14.580,0:01:18.600<br>
這個 g(x) 裡面有另外一個 function f(x)<br>
<br>
0:01:18.600,0:01:21.480<br>
當 f(x) > 0 的時候<br>
<br>
0:01:21.480,0:01:25.040<br>
他的 output 就是 +1 代表某個 class<br>
<br>
0:01:25.040,0:01:28.400<br>
當 f(x) < 0 的時候，他的 output 就是 -1<br>
<br>
0:01:28.400,0:01:30.520<br>
代表另外一個 class<br>
<br>
0:01:30.900,0:01:33.020<br>
我們現在的 training data<br>
<br>
0:01:33.020,0:01:35.180<br>
我們是 supervised problem<br>
<br>
0:01:35.180,0:01:38.440<br>
所以每一筆 data 都有 label : y hat<br>
<br>
0:01:38.440,0:01:41.700<br>
我們現在 y hat 就用 +1 跟 -1 來表示<br>
<br>
0:01:41.700,0:01:43.620<br>
分別代表兩個不同的 class<br>
<br>
0:01:43.620,0:01:46.200<br>
之前在講 Logistic Regression 的時候<br>
<br>
0:01:46.200,0:01:48.260<br>
我知道我們在講 y hat 的時候<br>
<br>
0:01:48.260,0:01:50.140<br>
我們是用 1 跟 0 表示<br>
<br>
0:01:50.140,0:01:53.940<br>
但這邊要用 +1 和 -1 來表示比較順<br>
<br>
0:01:53.940,0:01:59.460<br>
你知道說用 +1 和 -1 來表示跟用 0 跟 1 來表示是講同一件事情<br>
<br>
0:01:59.620,0:02:02.240<br>
就只是朝三暮四的差別而已<br>
<br>
0:02:02.300,0:02:05.020<br>
但是這邊如果用 +1 和 -1 的話<br>
<br>
0:02:05.020,0:02:08.980<br>
等一下寫式子是會比較順，所以這邊用 +1 和 -1<br>
<br>
0:02:08.980,0:02:11.040<br>
只要記得他的差別就好<br>
<br>
0:02:11.800,0:02:13.540<br>
Loss Function 呢，理想上<br>
<br>
0:02:13.540,0:02:18.840<br>
一個最理想的 Loss Function 就是寫成像下面這個樣子<br>
<br>
0:02:19.640,0:02:25.460<br>
當今天這個 g(x) 的 output 跟 y hat 不一樣的時候<br>
<br>
0:02:25.500,0:02:27.880<br>
machine 就得到一個 loss<br>
<br>
0:02:27.880,0:02:31.600<br>
如果一樣的時候，就沒有 loss<br>
<br>
0:02:32.280,0:02:34.800<br>
所以一個很理想的 Loss Function 是<br>
<br>
0:02:34.800,0:02:38.600<br>
summation 我們所有的 training data<br>
<br>
0:02:38.600,0:02:41.660<br>
然後對每筆 training data 都帶進 g(x) 裡面<br>
<br>
0:02:41.660,0:02:45.420<br>
看他的 output 多少，+1 也可能 -1<br>
<br>
0:02:45.560,0:02:49.720<br>
接下來看他的 output 跟 y hat 是一樣的還是不一樣的<br>
<br>
0:02:49.880,0:02:53.440<br>
如果是不一樣的你就得到一個 loss 1 反之就是 0<br>
<br>
0:02:53.440,0:02:56.160<br>
這邊用 delta 加一個括號的意思是<br>
<br>
0:02:56.160,0:03:00.980<br>
裡面這件事情如果為真的話，delta function 的 output 就是 1<br>
<br>
0:03:00.980,0:03:04.740<br>
如果 g 不等於 y，delta 的 output 就是 1<br>
<br>
0:03:04.740,0:03:10.240<br>
所以我們現在的 loss 就變成 g 在 training set  上總共犯了幾次錯誤<br>
<br>
0:03:10.380,0:03:14.200<br>
我們當然希望他犯的錯誤是越小越好<br>
<br>
0:03:14.200,0:03:16.300<br>
但是在今天這個 task 裡面<br>
<br>
0:03:16.300,0:03:19.200<br>
你要做 optimization<br>
<br>
0:03:19.200,0:03:22.460<br>
你要用第三步找一個好的 function 這件事情變得有點困難<br>
<br>
0:03:22.460,0:03:25.080<br>
因為你的 loss 是不可微分的<br>
<br>
0:03:25.140,0:03:28.160<br>
這個東西是沒有辦法微分的<br>
<br>
0:03:28.160,0:03:31.460<br>
所以根本沒有辦法用 Gradient Descent 來解他<br>
<br>
0:03:31.460,0:03:32.700<br>
怎麼辦呢<br>
<br>
0:03:32.700,0:03:38.480<br>
我們把這個 delta 用另外一個<br>
approximate 的 loss 來表示<br>
<br>
0:03:38.480,0:03:41.160<br>
直接 minimize 這個 delta 做不到<br>
<br>
0:03:41.160,0:03:44.660<br>
所以我們不直接 minimize 這個 delta<br>
<br>
0:03:44.660,0:03:46.840<br>
我們 minimize 另外一個 loss function<br>
<br>
0:03:46.840,0:03:50.000<br>
這邊寫成小寫的 L<br>
<br>
0:03:50.000,0:03:52.860<br>
至於這個 loss function 可以長甚麼樣子<br>
<br>
0:03:52.900,0:03:56.200<br>
你就可以自己隨便定了<br>
<br>
0:03:56.200,0:04:00.100<br>
這是我們的 g，裡面有個 f(x)<br>
<br>
0:04:00.160,0:04:06.600<br>
這個圖上的橫軸是 y hat 乘上 f(x)<br>
<br>
0:04:11.640,0:04:16.880<br>
因為這個 y n hat 可以是 +1，也可能是 -1<br>
<br>
0:04:16.880,0:04:19.620<br>
我們希望 y n hat 是 +1 的時候<br>
<br>
0:04:19.760,0:04:24.060<br>
f(x) 要越 positive 越好<br>
<br>
0:04:24.060,0:04:27.040<br>
y n hat 是 +1 的話<br>
f(x) 要越大越好<br>
<br>
0:04:27.040,0:04:31.760<br>
y n hat 如果是 -1 的時候<br>
f(x) 應該要是越負越好<br>
<br>
0:04:31.760,0:04:35.960<br>
整體說來你會希望 y n hat 乘上 f(x)<br>
<br>
0:04:35.960,0:04:39.620<br>
兩者相乘以後他的值越大越好<br>
<br>
0:04:39.620,0:04:43.920<br>
他們要是同號的，他們兩個要乘起來越大越好<br>
<br>
0:04:43.920,0:04:45.660<br>
所以今天原則就是<br>
<br>
0:04:45.660,0:04:50.240<br>
如果我們把縱軸當作是 loss 的話<br>
<br>
0:04:50.240,0:04:53.780<br>
原則就是越往右，y n hat 乘以 f(x) 越大<br>
<br>
0:04:53.780,0:04:56.740<br>
loss 就應該要越小<br>
<br>
0:04:57.160,0:05:01.440<br>
我們剛才講的理想狀況是<br>
<br>
0:05:01.740,0:05:07.660<br>
假設 y n hat 和 f(x) 他們是反向的<br>
<br>
0:05:07.660,0:05:12.540<br>
他們相乘以後，得到的值是負數的話<br>
<br>
0:05:12.540,0:05:15.280<br>
你得到的 loss 就是 1<br>
<br>
0:05:15.540,0:05:18.840<br>
反之如果他們是同向的<br>
你的 loss 就是 0<br>
<br>
0:05:18.840,0:05:22.700<br>
這是理想的狀況<br>
但這件事情是沒辦法微分<br>
<br>
0:05:22.700,0:05:30.480<br>
那我們現在做一個 approximation<br>
我們把 delta 用 L 來取代<br>
<br>
0:05:30.480,0:05:34.060<br>
縱軸就是 L 的值<br>
<br>
0:05:34.060,0:05:41.840<br>
你可以選各種、各式各樣不同的 function 來當作 L 這個 function<br>
<br>
0:05:42.300,0:05:46.220<br>
舉例來說，我現在 loss 定法是<br>
<br>
0:05:46.220,0:05:49.000<br>
我希望當 y n hat = 1 的時候<br>
<br>
0:05:49.000,0:05:51.940<br>
f(x) 跟 1 越接近越好<br>
<br>
0:05:51.940,0:05:53.440<br>
y n hat = -1 的時候<br>
<br>
0:05:53.440,0:05:55.600<br>
f(x) 跟 -1 越接近越好<br>
<br>
0:05:55.600,0:05:58.220<br>
這個是 square loss<br>
<br>
0:05:58.520,0:06:01.480<br>
square loss 其實可以寫成這個樣子<br>
<br>
0:06:01.520,0:06:08.500<br>
square loss 這個 loss 可以寫成<br>
( y n hat * f(x^n) -1 )^2<br>
<br>
0:06:08.500,0:06:13.440<br>
為甚麼呢<br>
如果 y n hat 帶 1 的話<br>
<br>
0:06:13.440,0:06:16.320<br>
這個 function 就變成 (f(x^n) - 1 )^2<br>
<br>
0:06:16.320,0:06:18.140<br>
如果 y n hat 帶 -1 的話<br>
<br>
0:06:18.140,0:06:22.480<br>
這個 function 就變成 ( -f (x^n) - 1 )^2<br>
<br>
0:06:22.760,0:06:25.640<br>
( -f (x^n) - 1 )^2 可以寫成<br>
<br>
0:06:25.640,0:06:27.880<br>
因為他在平方裡面所以可以把負號拿掉<br>
<br>
0:06:27.880,0:06:32.880<br>
(f (x^n) + 1) ^ 2<br>
也就是 f (x^n) 減 -1 的平方<br>
<br>
0:06:32.880,0:06:35.400<br>
當你寫這個式子的時候<br>
<br>
0:06:35.400,0:06:37.940<br>
就意味著如果 y n hat = 1 的時候<br>
<br>
0:06:37.940,0:06:39.720<br>
f(x^n) 要跟 1 越接近越好<br>
<br>
0:06:39.720,0:06:43.620<br>
y n hat = -1 的時候<br>
f(x^n) 要跟 -1 越接近越好<br>
<br>
0:06:43.620,0:06:46.940<br>
square loss 的 function<br>
你把 function 畫出來<br>
<br>
0:06:46.940,0:06:52.000<br>
他對 y n hat * f(x) 的變化是寫成這樣子<br>
<br>
0:06:52.500,0:06:55.260<br>
但是這個東西是不合理的<br>
<br>
0:06:55.260,0:06:57.520<br>
我們一開始在講 Binary Classification 的時候<br>
<br>
0:06:57.520,0:07:01.660<br>
我們就講過你用 square loss 是不合理的<br>
<br>
0:07:01.740,0:07:04.860<br>
從這個圖上你又可以更明確地看出他的不合理性<br>
<br>
0:07:04.860,0:07:09.140<br>
我們不希望說 y n hat 跟 f(x) 乘起來很大的時候<br>
<br>
0:07:09.140,0:07:11.500<br>
居然有一個很大的 loss<br>
<br>
0:07:11.780,0:07:16.300<br>
那另外一個是 sigmoid + square loss<br>
<br>
0:07:16.720,0:07:20.740<br>
這邊我們 sigmoid function 就用一個 sigma 來表示<br>
<br>
0:07:20.880,0:07:23.180<br>
我們希望 y n hat = 1 的時候<br>
<br>
0:07:23.180,0:07:25.420<br>
sigma(f(x^n) 趨近於 1<br>
<br>
0:07:25.420,0:07:26.740<br>
等於 -1 的時候<br>
<br>
0:07:26.740,0:07:28.800<br>
sigma(f(x^n) 趨近於 0<br>
<br>
0:07:28.800,0:07:32.940<br>
這個式子可以寫成這個樣子<br>
<br>
0:07:32.940,0:07:37.220<br>
為甚麼呢<br>
如果你把 y n hat 帶 1 的話<br>
<br>
0:07:37.400,0:07:42.640<br>
那沒有問題<br>
很直覺的就是希望這個 output 跟 1 越接近越好<br>
<br>
0:07:42.640,0:07:44.900<br>
帶 -1 的話呢<br>
<br>
0:07:44.900,0:07:51.180<br>
變成 sigma ( f ( sigma (-f(x)) )) 減 1 的平方<br>
<br>
0:07:51.180,0:07:55.400<br>
sigma ( -f (x)) 是甚麼呢<br>
<br>
0:07:55.400,0:07:58.620<br>
如果你了解 sigmoid function 特性的話<br>
<br>
0:07:59.040,0:08:01.900<br>
你把裡面的值取一個負號<br>
<br>
0:08:01.900,0:08:05.060<br>
其實就是 1 減 sigma ( f(x))<br>
<br>
0:08:05.060,0:08:09.640<br>
我想這個你們想一下的話應該可以體會<br>
減 1 的平方<br>
<br>
0:08:09.640,0:08:12.660<br>
這個 1 可以消掉<br>
<br>
0:08:12.660,0:08:15.100<br>
所以就變成 sigma (f(x))^2 的平方<br>
<br>
0:08:15.240,0:08:17.720<br>
總之你寫這個式子的意思<br>
<br>
0:08:17.720,0:08:21.280<br>
就是希望 f(x) 通過 sigma function 以後<br>
<br>
0:08:21.280,0:08:23.940<br>
如果他的答案是 1 他就要接近 1<br>
<br>
0:08:23.940,0:08:27.100<br>
如果答案是 -1 他就要接近 0<br>
<br>
0:08:27.100,0:08:33.120<br>
這個式子寫出來是藍色這條線<br>
<br>
0:08:33.120,0:08:35.780<br>
那我們之前有講過說<br>
<br>
0:08:35.780,0:08:37.740<br>
其實你在做 Logistic Regression 的時候<br>
<br>
0:08:37.740,0:08:40.440<br>
你不會用 square loss 當作你的 loss<br>
<br>
0:08:40.440,0:08:44.960<br>
因為這樣 performance 不好<br>
之前也有實際操作過<br>
<br>
0:08:44.960,0:08:49.420<br>
你不會用 square loss 其實你會用 cross entropy<br>
<br>
0:08:49.740,0:08:52.120<br>
那如果用 cross entropy 的話<br>
<br>
0:08:52.120,0:08:55.460<br>
在 Logistic Regression 其實我們真正是用 cross entropy<br>
<br>
0:08:55.460,0:08:57.980<br>
然後 entropy 的意思我們之前講過<br>
<br>
0:08:57.980,0:09:00.420<br>
就是你的 sigma(f(x))<br>
<br>
0:09:00.420,0:09:08.640<br>
代表了一個 distribution<br>
<br>
0:09:08.640,0:09:10.960<br>
你的 ground truth 是另外一個 distribution<br>
<br>
0:09:10.960,0:09:14.120<br>
這樣的 distribution 之間的 cross entropy<br>
<br>
0:09:14.120,0:09:17.420<br>
就是你要去 minimize 的 loss<br>
<br>
0:09:17.440,0:09:21.680<br>
如果你回頭過去把 square loss 的 function 寫出來的話<br>
<br>
0:09:21.680,0:09:25.180<br>
你會發現其實這個 function 可以寫成<br>
<br>
0:09:25.340,0:09:31.360<br>
log ( 1 + exp( - y n hat * f(x)))<br>
<br>
0:09:31.640,0:09:34.780<br>
這邊我們就不做多餘的推倒<br>
這個比較麻煩<br>
<br>
0:09:34.780,0:09:36.680<br>
你可能自己回去推一下就知道<br>
<br>
0:09:37.920,0:09:40.800<br>
但這個式子是不是合理的呢<br>
<br>
0:09:40.800,0:09:42.160<br>
他其實是合理的<br>
<br>
0:09:42.380,0:09:48.520<br>
你想想看當你今天的 y n hat 乘上 f(x^n) 趨近無窮大的時候<br>
<br>
0:09:48.520,0:09:52.740<br>
當他趨近無窮大的時候<br>
exponential 負的無窮大是 0<br>
<br>
0:09:52.820,0:09:56.340<br>
那就變成 log (1+0) 得到結果是 0<br>
<br>
0:09:56.340,0:09:59.640<br>
所以當這一項趨近無窮大的時候<br>
<br>
0:09:59.640,0:10:03.460<br>
這一個 sigmoid + cross entropy 這個 loss function<br>
<br>
0:10:03.460,0:10:09.160<br>
他是綠色這一條，他會趨近於 0<br>
<br>
0:10:09.160,0:10:15.180<br>
如果今天 y n hat * f(x) 負很大的時候<br>
<br>
0:10:15.180,0:10:19.900<br>
exponential 裡面會是正的很大再加上 1 他的值會很大<br>
<br>
0:10:19.900,0:10:22.280<br>
再取 log 以後他還是無窮大<br>
<br>
0:10:22.280,0:10:25.640<br>
log 只能跟 exponential 消掉，如果 exponential 這裡面是無窮大的<br>
<br>
0:10:25.640,0:10:29.960<br>
得到結果還是無窮大<br>
所以你得到的值還是無窮大<br>
<br>
0:10:30.100,0:10:35.480<br>
所以這一項是這一條線<br>
<br>
0:10:35.800,0:10:40.340<br>
在這邊我特別偷偷把他除了一個 log 2<br>
<br>
0:10:40.490,0:10:43.280<br>
你對 loss function 除一個 constant<br>
<br>
0:10:43.280,0:10:46.480<br>
不會影響你最後找出來的解，對不對<br>
<br>
0:10:46.480,0:10:49.920<br>
這邊偷偷除了一個 log 2，為甚麼呢<br>
<br>
0:10:49.920,0:10:52.500<br>
因為我們要偷偷除 log 2 以後呢<br>
<br>
0:10:52.500,0:11:00.440<br>
可以讓他變成一個剛才 ideal loss 的 upper bound<br>
<br>
0:11:02.940,0:11:06.060<br>
我們雖然沒有辦法 minimize ideal 的 loss<br>
<br>
0:11:06.060,0:11:07.740<br>
黑色這條線<br>
<br>
0:11:07.920,0:11:12.400<br>
但是我們可以去 minimize 他的 upper bound<br>
<br>
0:11:12.400,0:11:17.260<br>
就可以順便 minimize 這個 ideal  的 loss function<br>
<br>
0:11:18.770,0:11:23.770<br>
我們可以比較一下這兩條曲線<br>
<br>
0:11:23.820,0:11:28.640<br>
你就可以比較了解為甚麼我們之前會選擇用 cross entropy<br>
<br>
0:11:28.640,0:11:33.440<br>
而不是用 square error 來當作我們的 loss function<br>
<br>
0:11:33.520,0:11:35.560<br>
在我們做 Logistic Regression 的時候<br>
<br>
0:11:36.040,0:11:45.520<br>
你想想看，我們今天如果把 y n hat * f(x) 從 -2 移到 -1 的時候<br>
<br>
0:11:45.820,0:11:53.720<br>
如果是 sigmoid + square error 他的變化很小<br>
<br>
0:11:53.740,0:11:58.880<br>
如果是 sigmoid + cross entropy 他的變化就非常大<br>
<br>
0:11:58.880,0:12:03.960<br>
那所以對 sigmoid 這個 function 來說他在這種極端的 case<br>
<br>
0:12:04.100,0:12:09.060<br>
在你的值非常 negative 的時候<br>
<br>
0:12:09.140,0:12:12.380<br>
照理說應該要有很大的 gradient<br>
<br>
0:12:12.380,0:12:13.920<br>
應該趕快調整你的值<br>
<br>
0:12:14.000,0:12:16.060<br>
但是實際上不是如此<br>
<br>
0:12:16.060,0:12:17.900<br>
因為在值很大的時候<br>
<br>
0:12:18.040,0:12:24.180<br>
當你在這一項 y n hat * f(x) 非常 negative 的時候<br>
<br>
0:12:24.280,0:12:27.020<br>
你調整你的值對你最後 total loss 影響不大<br>
<br>
0:12:27.020,0:12:29.060<br>
所以就會變成說<br>
<br>
0:12:29.140,0:12:31.740<br>
對這個 sigmoid + square loss 的 case 來說<br>
<br>
0:12:31.740,0:12:35.320<br>
就算調整了他 negative 的值<br>
<br>
0:12:35.320,0:12:38.100<br>
他也沒有辦法得到太多的回報<br>
<br>
0:12:38.100,0:12:41.140<br>
所以他就會不想調整那些非常 negative 的值<br>
<br>
0:12:41.140,0:12:44.280<br>
那對這個 cross entropy 來說<br>
<br>
0:12:44.280,0:12:46.460<br>
他的努力是可以得到回報的<br>
<br>
0:12:46.460,0:12:50.520<br>
所以他就會很樂意把原來很 negative 的值<br>
<br>
0:12:50.520,0:12:52.240<br>
把他往正的地方推<br>
<br>
0:12:52.240,0:12:55.080<br>
所以我們用 cross entropy 的時候<br>
在實作上<br>
<br>
0:12:55.080,0:12:57.880<br>
你會比 square error 還更容易 training<br>
<br>
0:12:57.880,0:13:01.180<br>
所以我們在做 Logistic Regression 的時候都是用 cross entropy<br>
<br>
0:13:01.420,0:13:06.720<br>
那 hinge loss 他寫成一個很特別的式子<br>
<br>
0:13:06.720,0:13:11.060<br>
他說我們的 loss function 是 maximum<br>
<br>
0:13:11.240,0:13:14.260<br>
其實我們剛才在講 Zero-shot learning 的時候<br>
<br>
0:13:14.260,0:13:16.540<br>
其實有看到類似的 function<br>
<br>
0:13:16.660,0:13:27.680<br>
我們這邊寫成 max(0,1 - y n hat * f(x))<br>
<br>
0:13:27.700,0:13:32.980<br>
如果 y n hat 帶 1<br>
<br>
0:13:32.980,0:13:37.260<br>
loss function 就是 max 0 跟 1 - f(x)<br>
<br>
0:13:37.920,0:13:43.700<br>
什麼樣的狀況下會有 zero 的 loss 呢<br>
<br>
0:13:44.600,0:13:47.640<br>
因為這邊是 max 0 跟 1 - f(x)<br>
<br>
0:13:47.640,0:13:49.740<br>
所以這個 function 最小值是 0<br>
<br>
0:13:49.740,0:13:55.440<br>
甚麼時候會是達到 loss function 是 0 最完美的狀況呢<br>
<br>
0:13:55.440,0:13:58.820<br>
只要 1 - f(x) < 0 就行<br>
<br>
0:13:58.820,0:14:02.780<br>
1 - f(x) < 0 的時候這個 loss 的值就會是 0<br>
<br>
0:14:02.780,0:14:07.620<br>
也就是說只要 f(x) > 1 的時候<br>
<br>
0:14:07.700,0:14:10.300<br>
這個 loss 就會是 0<br>
<br>
0:14:10.300,0:14:12.980<br>
如果 y n hat = -1 的時候<br>
<br>
0:14:12.980,0:14:17.300<br>
這個 loss function 會寫成 0 跟 1 + f(x) 的 max<br>
<br>
0:14:17.300,0:14:18.840<br>
要讓 loss 等於 0<br>
<br>
0:14:18.840,0:14:20.960<br>
我們就是要讓 1 + f(x) < 0<br>
<br>
0:14:20.960,0:14:24.020<br>
也就是要讓 f(x) < 1<br>
<br>
0:14:24.020,0:14:27.720<br>
所以用 hinge loss 做 training 的時候<br>
<br>
0:14:27.720,0:14:31.300<br>
你想要讓 machine 做到的事情<br>
甚麼時候 machine 會覺得他已經<br>
<br>
0:14:31.300,0:14:34.020<br>
他的 loss 是 0<br>
他已經做到完美的 case 了呢<br>
<br>
0:14:34.020,0:14:37.320<br>
他如果對一個 positive 的 example 來說<br>
<br>
0:14:37.440,0:14:40.300<br>
f(x) > 1 的時候就是完美的 case<br>
<br>
0:14:40.300,0:14:42.740<br>
對一個 negative example 來說<br>
<br>
0:14:42.980,0:14:46.520<br>
他的 f(x) < -1 的時候他就是一個完美的 case<br>
<br>
0:14:46.520,0:14:48.540<br>
他的值不用太大<br>
<br>
0:14:48.540,0:14:51.000<br>
你把 f(x) 變成 2<br>
<br>
0:14:51.000,0:14:52.120<br>
loss 也不會比較小<br>
<br>
0:14:52.160,0:14:55.060<br>
你把 f(x) 變成 -2 loss 也不會比較小<br>
<br>
0:14:55.060,0:14:59.320<br>
他只要大過 1 跟小於 -1 就可以了<br>
<br>
0:14:59.320,0:15:04.680<br>
如果你把 hinge loss 畫出來的話呢他長得像這個樣子<br>
<br>
0:15:04.680,0:15:09.580<br>
他是紫色的這條線<br>
<br>
0:15:09.580,0:15:13.060<br>
你會發現在紫色的這條線上<br>
<br>
0:15:13.060,0:15:20.180<br>
在這一段只要 y n hat * f(x) > 1 的時候<br>
<br>
0:15:20.180,0:15:22.920<br>
就已經夠好了，你的 loss 就已經是 0<br>
<br>
0:15:22.920,0:15:24.880<br>
再更大都沒有幫助<br>
<br>
0:15:24.880,0:15:29.660<br>
但如果 y n hat, f(x^n) 是 positive 的 example<br>
<br>
0:15:29.660,0:15:32.840<br>
是 positive 還不夠好<br>
<br>
0:15:32.840,0:15:36.740<br>
如果 y n hat 跟 f(x^n) 是同向<br>
<br>
0:15:36.740,0:15:40.620<br>
machine 在做 classification 的時候他已經可以得到正確的答案<br>
<br>
0:15:40.620,0:15:45.200<br>
根據我們剛才前面定出來的 step 1、定出來的 function<br>
<br>
0:15:45.480,0:15:49.560<br>
但是對 hinge loss 來說這樣還不夠好<br>
<br>
0:15:49.560,0:15:53.760<br>
他會說你只得到正確的答案還不夠<br>
<br>
0:15:53.760,0:15:59.120<br>
你要比正確的答案還要好過一段距離<br>
<br>
0:15:59.120,0:16:01.540<br>
這個距離就是 margin<br>
<br>
0:16:01.540,0:16:08.120<br>
也就是說當 y n hat * f(x) 還沒有大於 1 的時候<br>
<br>
0:16:08.120,0:16:09.840<br>
其實還是會有 penalty<br>
<br>
0:16:09.840,0:16:14.560<br>
促使 machine 讓 y n hat 乘上 f(x) 大於 0<br>
<br>
0:16:14.560,0:16:20.900<br>
你可能會問這邊為甚麼是 1 呢<br>
<br>
0:16:20.900,0:16:23.500<br>
你可以想想看如果這邊是 1 的話<br>
<br>
0:16:23.500,0:16:29.020<br>
hinge loss 才會是 ideal loss 的一個 type 的 upper bound<br>
<br>
0:16:29.020,0:16:32.680<br>
如果你用其他值的話<br>
<br>
0:16:32.740,0:16:38.160<br>
他就不是一個那麼 tight 的 upper bound<br>
<br>
0:16:38.160,0:16:41.820<br>
所以 hinge loss 跟我們剛看到的 cross entropy 一樣<br>
<br>
0:16:41.820,0:16:45.060<br>
他也是 ideal loss function 的 upper bound<br>
<br>
0:16:45.060,0:16:49.700<br>
所以我們會期待說我們可以 minimize hinge loss<br>
<br>
0:16:49.760,0:16:57.200<br>
然後就可能可以得到 minimize ideal loss function 的效果<br>
<br>
0:16:57.940,0:17:02.940<br>
如果我們比較 hinge loss 跟 cross entropy 的話<br>
<br>
0:17:02.940,0:17:10.260<br>
你會發現他們最大的不同來自於他們對待已經做得好的 example 的態度<br>
<br>
0:17:10.560,0:17:18.020<br>
如果我們把 y n hat * f(x) 從1 挪到 2<br>
<br>
0:17:18.580,0:17:23.840<br>
對 cross entropy 來說可以得到 loss 的下降<br>
<br>
0:17:23.840,0:17:27.860<br>
所以 cross entropy 他會想要好，還可以更好<br>
<br>
0:17:27.860,0:17:33.720<br>
如果你已經可以把 y n hat * f(x) 的值做的夠大<br>
<br>
0:17:33.720,0:17:40.180<br>
cross entropy 他有動機讓他的值更大，因為這樣可以減少 loss<br>
<br>
0:17:40.180,0:17:44.360<br>
但是對 hinge loss 來說，如果採用 hinge loss function<br>
<br>
0:17:44.360,0:17:47.440<br>
他是一個及格就好的 loss function<br>
<br>
0:17:47.440,0:17:56.500<br>
所以只要你的值大過 margin 的時候就結束了<br>
<br>
0:17:56.500,0:17:59.260<br>
他就不會想要再做得更好<br>
<br>
0:17:59.280,0:18:05.220<br>
你可能會問說在實作上到底 hinge loss 跟 cross entropy 他們的 performance 有甚麼差別呢<br>
<br>
0:18:05.220,0:18:10.840<br>
在實作上的差別可能沒有你想像的那麼顯著<br>
<br>
0:18:11.000,0:18:15.540<br>
有一些時候我們可以看到 hinge loss 略勝 cross entropy<br>
<br>
0:18:15.540,0:18:18.000<br>
但其實也沒有贏那麼多<br>
<br>
0:18:18.180,0:18:24.040<br>
非常 hinge loss 的時候有一個好處是他比較不害怕 outlier<br>
<br>
0:18:24.040,0:18:27.940<br>
learn 出來的結果會是比較 robust 的<br>
<br>
0:18:27.940,0:18:33.840<br>
這個等下在講 kernel 的時候會比較明顯的看到這個結果<br>
<br>
0:18:34.120,0:18:39.320<br>
如果你看這 hinge loss 跟 cross entropy 的差別的話<br>
<br>
0:18:40.720,0:18:45.620<br>
這個 cross entropy 就好像是說現在期末考有很多科目要讀<br>
<br>
0:18:45.620,0:18:48.840<br>
但你就是想要拚死讀一科而已<br>
<br>
0:18:48.840,0:18:50.840<br>
你想要把某一科考到一百分<br>
<br>
0:18:50.840,0:18:53.580<br>
其他如果你念不起來的你就會放棄<br>
<br>
0:18:53.580,0:18:55.320<br>
最後可能就被二一了<br>
<br>
0:18:55.320,0:18:58.340<br>
hinge loss 會顧全所有的科目<br>
<br>
0:18:58.340,0:19:00.240<br>
所有的科目考到及格就好<br>
<br>
0:19:00.240,0:19:02.760<br>
他其實是會 all pass 的<br>
<br>
0:19:02.760,0:19:05.000<br>
所以如果你有 outlier 的時候<br>
<br>
0:19:05.000,0:19:09.560<br>
hinge loss 會給你比較好的結果<br>
cross entropy 會給你比較差的結果<br>
<br>
0:19:09.680,0:19:13.000<br>
等一下我們在別的角度來看這個問題可能是會更清楚<br>
<br>
0:19:14.580,0:19:17.320<br>
甚麼是 linear SVM<br>
<br>
0:19:17.560,0:19:21.040<br>
linear SVM 是說我們現在的 function<br>
<br>
0:19:21.040,0:19:23.220<br>
就是 linear 的<br>
<br>
0:19:23.220,0:19:28.160<br>
我們現在的 f(x) 就是<br>
<br>
0:19:32.500,0:19:37.520<br>
x 裡面的每一個 feature，x i 乘上他對應的 weight ，w i<br>
<br>
0:19:37.720,0:19:41.340<br>
summation 起來再加上 b<br>
<br>
0:19:41.340,0:19:45.820<br>
我們可以把這件事情看作兩個 vector 的 inner product<br>
<br>
0:19:45.820,0:19:52.900<br>
其中一個 vector 是 model 的 parameter w 跟 b concatenate 的一個 vector<br>
<br>
0:19:52.900,0:20:00.220<br>
另外一個 vector 是 feature 的 vector x 跟 1 concatenate 的結果<br>
<br>
0:20:00.220,0:20:02.800<br>
如果你把這兩個 vector 做 inner product 的話<br>
<br>
0:20:03.000,0:20:05.560<br>
那你就會得到左邊這個式子<br>
<br>
0:20:05.560,0:20:07.900<br>
所以在等一下的說明裡面<br>
<br>
0:20:08.000,0:20:10.740<br>
我們把 w 跟 b 串起來的那個 vector<br>
<br>
0:20:10.740,0:20:12.420<br>
直接就用 w 來表示<br>
<br>
0:20:12.580,0:20:17.060<br>
這個東西是你的 model 的參數<br>
是要透過 training data 把它找出來的<br>
<br>
0:20:17.060,0:20:19.520<br>
下面 x  跟 1 concatenate 起來的結果<br>
<br>
0:20:19.520,0:20:24.360<br>
我們就當作是一個新的 feature<br>
你就想像成每一個 feature 下面都有 concatenate 1<br>
<br>
0:20:24.360,0:20:27.020<br>
這樣你就可以把 bias 這一項省略掉<br>
<br>
0:20:27.020,0:20:32.440<br>
所以一個 function 你就寫成 w 的 transpose 乘上 x<br>
<br>
0:20:32.460,0:20:36.120<br>
或 w 跟 x 的 inner product<br>
<br>
0:20:36.120,0:20:38.140<br>
那在 SVM 裡面<br>
<br>
0:20:38.680,0:20:40.940<br>
你的 f 長這個樣子<br>
<br>
0:20:40.940,0:20:43.420<br>
你就說 f > 0 的時候是屬於某一個 class<br>
<br>
0:20:43.420,0:20:45.300<br>
f < 0 的時候是另外一個 class<br>
<br>
0:20:45.300,0:20:48.580<br>
如果是 loss function 的話<br>
<br>
0:20:48.580,0:20:55.200<br>
在 SVM 裡面它的特色就是他採用了 hinge loss 這個 loss function<br>
<br>
0:20:55.380,0:21:00.340<br>
通常你還會加上 regularization 的 term<br>
<br>
0:21:01.020,0:21:05.520<br>
這個 loss function 他是一個 convex 的 function<br>
<br>
0:21:05.520,0:21:09.220<br>
為甚麼呢?因為我們知道 hinge loss 的 loss function<br>
<br>
0:21:09.220,0:21:13.820<br>
他長得是這個樣子，就像 RELU 那個 activation function 的形狀<br>
<br>
0:21:13.820,0:21:16.580<br>
他是一個 convex 的 function<br>
<br>
0:21:16.580,0:21:21.240<br>
而這一項，L2 的 regularization<br>
<br>
0:21:21.240,0:21:23.820<br>
他也是一個 convex 的 function<br>
<br>
0:21:23.820,0:21:26.620<br>
所以所有的 loss 都是 convex 的 function<br>
<br>
0:21:26.620,0:21:29.400<br>
regularization 也是 convex 的 function<br>
<br>
0:21:29.400,0:21:33.460<br>
當你把這些 convex 的 function 疊加起來以後會發生甚麼事情呢<br>
<br>
0:21:33.460,0:21:38.000<br>
其實你可以很輕易的證明，把 convex 的 function 疊加起來<br>
<br>
0:21:38.160,0:21:40.460<br>
仍然是 convex function<br>
<br>
0:21:40.460,0:21:47.360<br>
所以今天的 loss function 其實就是一個 convex 的 function<br>
<br>
0:21:48.000,0:21:49.540<br>
如果是 convex function 的話<br>
<br>
0:21:49.540,0:21:51.820<br>
做 gradient descent 就很簡單了<br>
<br>
0:21:51.820,0:21:53.740<br>
不管從哪個地方做 initialization<br>
<br>
0:21:53.740,0:21:57.320<br>
最後找出來的結果都會是一樣的<br>
<br>
0:21:57.320,0:22:04.180<br>
你可能會問說這個東西在某些點不可微<br>
<br>
0:22:04.180,0:22:10.100<br>
他有這種菱菱角角的東西<br>
<br>
0:22:10.100,0:22:14.640<br>
Hinge  Loss 他在某些點是不可微的<br>
<br>
0:22:14.640,0:22:19.800<br>
你把很多不可微的 convex function 疊起來他就長這樣<br>
<br>
0:22:19.800,0:22:21.580<br>
他有很多菱菱角角的地方<br>
<br>
0:22:21.580,0:22:25.240<br>
感覺不是每一個位子都是可微<br>
<br>
0:22:25.280,0:22:28.560<br>
但是這個不是甚麼大問題<br>
這個其實可以做的<br>
<br>
0:22:28.560,0:22:33.900<br>
我們之前在講 Deep Learning 的時候，你有 RELU 的 Activation Function<br>
<br>
0:22:33.900,0:22:38.560<br>
你有 Maxout Network<br>
他們表面上看起來也是不可微的<br>
<br>
0:22:38.640,0:22:43.200<br>
但其實你都可以用 Gradient Descent 去做 optimization<br>
<br>
0:22:43.200,0:22:44.700<br>
所以今天這個 case 也一樣<br>
<br>
0:22:44.700,0:22:47.720<br>
我們可以用 Gradient Descent 去做 optimization<br>
<br>
0:22:47.720,0:22:51.500<br>
等一下我們會看看怎麼做這一件事情<br>
<br>
0:22:53.490,0:22:58.470<br>
其實如果我們比較 Logistic Regression<br>
<br>
0:22:58.540,0:23:03.260<br>
跟 Linear SVM 的差別，他唯一的差別就只是<br>
<br>
0:23:03.340,0:23:04.980<br>
我們怎麼定 Loss Function<br>
<br>
0:23:04.980,0:23:08.240<br>
你用 Hinge Loss 就是 Linear SVM<br>
<br>
0:23:08.240,0:23:12.960<br>
Cross Entropy 的 Loss 就是 Logistic Regression<br>
<br>
0:23:13.280,0:23:17.720<br>
而這個 function 他沒有一定要是 linear 的<br>
<br>
0:23:17.720,0:23:24.400<br>
他如果是 linear 的話有很多好的特質<br>
但是如果他不是 linear 的也 ok<br>
<br>
0:23:24.400,0:23:26.260<br>
你也可以用 Gradient Descent 來 train<br>
<br>
0:23:26.260,0:23:31.620<br>
所以 SVM 是可以有 deep 的版本的<br>
<br>
0:23:31.680,0:23:34.880<br>
這邊列一個 reference 給大家參考<br>
<br>
0:23:34.880,0:23:38.100<br>
當你今天在做 Deep Learning 的時候<br>
<br>
0:23:38.100,0:23:43.340<br>
你不是用 Cross Entropy 當作你 minimization  的對象<br>
當作你的 Loss Function<br>
<br>
0:23:43.340,0:23:44.780<br>
而是想要用 Hinge Loss 的話<br>
<br>
0:23:44.780,0:23:47.160<br>
你其實就是有一個 deep 版本的 SVM<br>
<br>
0:23:47.160,0:23:49.740<br>
所以其實沒有必要說<br>
<br>
0:23:49.740,0:23:53.200<br>
哦～我做的是 Deep Learning、我做的是 SVM<br>
<br>
0:23:53.200,0:23:55.700<br>
他們其實可以是一樣的東西<br>
<br>
0:23:55.700,0:23:59.580<br>
這背後的精神其實都是可以相通的<br>
<br>
0:23:59.580,0:24:04.300<br>
再來的問題就是怎麼用 Gradient Descent 來 learn SVM 呢<br>
<br>
0:24:04.300,0:24:07.880<br>
我知道你傳統上聽到的方法都不是用 Gradient Descent 來 learn SVM<br>
<br>
0:24:07.880,0:24:12.300<br>
但 SVM 確實是可以用 Gradient Descent 來做 training 的<br>
<br>
0:24:13.020,0:24:17.860<br>
有一個用 Gradient Descent 來 train SVM 的方法叫做 Picasso<br>
<br>
0:24:17.860,0:24:21.720<br>
我這邊忘了附 reference 我之後再附上去<br>
<br>
0:24:21.720,0:24:24.420<br>
總之 SVM 是可以用 Gradient Descent train<br>
<br>
0:24:24.420,0:24:28.700<br>
怎麼 train 呢<br>
我們現在的 Loss Function 長這個樣子<br>
<br>
0:24:28.700,0:24:31.340<br>
他是一個 Hinge Loss<br>
<br>
0:24:31.340,0:24:35.020<br>
Gradient Descent 很簡單你只要能夠對他做微分就好了<br>
<br>
0:24:35.020,0:24:38.540<br>
我們只要能夠對 model 裡面的某一個 weight w i<br>
<br>
0:24:38.710,0:24:43.530<br>
對這個 summation 裡面的 loss 可以做偏微分<br>
<br>
0:24:43.620,0:24:45.500<br>
我們就可以做 Gradient Descent<br>
<br>
0:24:45.500,0:24:49.480<br>
這個偏微分的值是甚麼呢<br>
<br>
0:24:49.480,0:24:54.840<br>
w i 只跟 f(x) 有關<br>
<br>
0:24:54.840,0:25:04.360<br>
所以我們可以把這一項拆成 partial f 分之 partial log Loss Function<br>
<br>
0:25:04.360,0:25:07.420<br>
我們先用 f 對 Loss Function 做偏微分<br>
<br>
0:25:07.420,0:25:11.980<br>
再乘上用 w 對 f 做偏微分<br>
<br>
0:25:11.980,0:25:17.120<br>
用 w 對 f 做偏微分的結果是很簡單的<br>
<br>
0:25:17.120,0:25:21.300<br>
因為我們知道 f(x^n) 就是一個 linear 的 function<br>
<br>
0:25:21.300,0:25:24.840<br>
他是兩個 vector 的 Inner Product<br>
<br>
0:25:24.840,0:25:28.660<br>
如果你用 w i 對 f 做偏微分的話<br>
<br>
0:25:28.660,0:25:35.320<br>
你得到的其實就是 xn 的第 i 個 dimension 的 value<br>
<br>
0:25:35.320,0:25:40.460<br>
前面這個用 f 對 Loss Function 做偏微分他的解是怎樣呢<br>
<br>
0:25:40.460,0:25:43.920<br>
這個 f 就是這個 Hinge Loss 的 Loss Function<br>
<br>
0:25:43.920,0:25:49.800<br>
這個 Hinge Loss 的 Loss Function 他長得是 RELU 這個樣子<br>
<br>
0:25:49.800,0:25:52.580<br>
他有兩個 operation 的 region<br>
<br>
0:25:52.580,0:25:58.080<br>
它可以 operate 在 0 是 max 的 case<br>
<br>
0:25:58.440,0:26:03.220<br>
它可以 operate 在 1 - y * f(x) 是 max 的 case<br>
<br>
0:26:03.300,0:26:07.000<br>
如果它 operate 在 0 是 max 的 case 就是這個 region<br>
<br>
0:26:07.000,0:26:14.580<br>
它 operate 在 1 - y * f(x) 是 max 的 case 它就是這個 region<br>
<br>
0:26:14.580,0:26:18.120<br>
甚麼時候會 operate 在哪一個 case 呢<br>
<br>
0:26:18.120,0:26:22.820<br>
這個是 depend 你現在的 model w 是多少<br>
<br>
0:26:22.820,0:26:29.580<br>
也就是說假如你現在的 1 - y hat * f(x) > 0<br>
<br>
0:26:29.580,0:26:35.560<br>
這個 f(x) 的值取決於你現在的 model w 的值是多少<br>
<br>
0:26:35.560,0:26:38.420<br>
假設這個東西大於零<br>
<br>
0:26:38.420,0:26:44.440<br>
那就是 y hat * f(x) < 1 這樣<br>
<br>
0:26:45.180,0:26:47.940<br>
當 y hat * f(x) < 1 的時候<br>
<br>
0:26:48.140,0:26:52.420<br>
你的 model 作用在這個 region<br>
<br>
0:26:52.620,0:26:57.260<br>
把它對 f 作微分你得到的值就是 - y n hat<br>
<br>
0:26:57.260,0:27:03.080<br>
在另外一個 case 因為值就是 0 所以做微分以後還是 0<br>
<br>
0:27:03.180,0:27:05.480<br>
今天的微分值就有兩個可能<br>
<br>
0:27:05.480,0:27:10.560<br>
如果作用在這個 region 就得到這個微分值<br>
<br>
0:27:12.720,0:27:16.840<br>
所以微分的值你把這一整項<br>
<br>
0:27:16.840,0:27:19.600<br>
大 L 對 w 做偏微分以後<br>
<br>
0:27:19.600,0:27:22.400<br>
你得到的值就是這個樣子<br>
<br>
0:27:22.420,0:27:24.340<br>
summation over 所有的 Training Data<br>
<br>
0:27:24.340,0:27:29.180<br>
再看每一筆 Training Data 的 y n hat * f(x^n) 是不是小於 1<br>
<br>
0:27:29.180,0:27:32.140<br>
如果是的話這一項 delta 值就是 1<br>
<br>
0:27:32.140,0:27:37.180<br>
這個 1 就會乘上負的 y n hat<br>
<br>
0:27:37.340,0:27:40.260<br>
這前面還會乘上一項<br>
<br>
0:27:40.260,0:27:43.440<br>
我寫錯這邊應該是上標 n<br>
<br>
0:27:43.440,0:27:48.280<br>
這邊這一項就應該是 x 上標 n 下標 i<br>
<br>
0:27:48.280,0:27:51.580<br>
前面這一項 depend on 現在的參數是甚麼<br>
<br>
0:27:51.580,0:27:53.960<br>
所以我們把它寫成 c n(w)<br>
<br>
0:27:53.960,0:27:58.280<br>
所以 update 參數的時候就把 w i 減掉 Learning Rate<br>
<br>
0:27:58.280,0:28:03.640<br>
乘上 c n(w) 乘上第 n 個 feature 裡面的的第 i 維<br>
<br>
0:28:03.640,0:28:06.900<br>
就可以 update 參數 w i<br>
<br>
0:28:08.200,0:28:10.320<br>
就這樣子<br>
<br>
0:28:10.800,0:28:14.920<br>
所以 SVM 是可以用 Gradient Descent 來解<br>
<br>
0:28:17.200,0:28:20.200<br>
如果你只是要寫 linear 的 SVM<br>
<br>
0:28:20.200,0:28:22.740<br>
其實你用 Keras 就可以秒做<br>
<br>
0:28:22.740,0:28:27.140<br>
你可能會說這跟我平常看到的 SVM 不一樣<br>
<br>
0:28:27.140,0:28:29.420<br>
我知道你平常看到的 SVM 是甚麼樣子<br>
<br>
0:28:29.420,0:28:35.380<br>
我把我現在講的這個 Hinge Loss Function 變成你平常看到的 SVM<br>
<br>
0:28:35.380,0:28:38.900<br>
你平常看到的 SVM 是怎麼樣呢<br>
<br>
0:28:40.140,0:28:48.680<br>
我們把 Hinge Loss 換成用一個 notation epsilon n 來取代它<br>
<br>
0:28:48.680,0:28:52.640<br>
epsilon n 就等於<br>
<br>
0:28:52.640,0:28:59.220<br>
max(0,1 - y n hat * f(x n))<br>
<br>
0:28:59.220,0:29:00.840<br>
上標 n<br>
<br>
0:29:00.840,0:29:04.940<br>
max(0,1 - y n hat * f(x n))<br>
<br>
0:29:04.940,0:29:09.780<br>
我接下來甚麼都沒有做只是換了一下 notation 而已<br>
<br>
0:29:09.780,0:29:14.760<br>
我們現在的目標是 minimize Total Loss<br>
<br>
0:29:15.360,0:29:21.780<br>
這一項其實可以有另外一個寫法<br>
<br>
0:29:24.600,0:29:30.480<br>
我們要 0 跟 1 - y n hat * f(x) 裡面取大的那一個當作 epsilon n<br>
<br>
0:29:30.700,0:29:35.720<br>
所以 epsilon n 它會大於 0 也大於 1 - y n hat * f(x)<br>
<br>
0:29:37.300,0:29:40.920<br>
這件事情其實就等於 epsilon n >= 0<br>
<br>
0:29:40.920,0:29:45.500<br>
epsilon n >= 1 - y n hat * f(x)<br>
<br>
0:29:45.760,0:29:55.140<br>
假設我們不考慮 loss，無視上面這一塊<br>
<br>
0:29:55.580,0:29:58.460<br>
這個紅框框，上面紅框框裡面的式子<br>
<br>
0:29:58.460,0:30:01.500<br>
等於下面裡面紅框框的式子嗎<br>
<br>
0:30:01.980,0:30:04.640<br>
給大家三秒鐘的時間想一下<br>
<br>
0:30:06.220,0:30:09.920<br>
覺得相等的同學舉手<br>
<br>
0:30:10.740,0:30:14.780<br>
手放下，覺得不相等的同學舉手<br>
<br>
0:30:14.780,0:30:18.280<br>
手放下，好像差距有點大<br>
<br>
0:30:18.280,0:30:21.280<br>
我們來想想看<br>
<br>
0:30:22.300,0:30:27.080<br>
這個 epsilon n 它是 0 跟 1 - y n hat * f(x) 的 max<br>
<br>
0:30:27.100,0:30:30.980<br>
我今天說 epsilon n 可以大於 0 也可以大於它<br>
<br>
0:30:30.980,0:30:33.180<br>
但是它不見得是正好等於他們的 max<br>
<br>
0:30:33.180,0:30:36.260<br>
它可以是 max + 1、+ 2、+ 一百萬<br>
<br>
0:30:36.260,0:30:39.320<br>
所以如果我這樣講的話<br>
<br>
0:30:39.320,0:30:43.880<br>
我們再問一下<br>
你覺得上面下面這兩個式子是一樣的嗎<br>
<br>
0:30:43.880,0:30:46.960<br>
你覺得它不一樣的同學舉手<br>
<br>
0:30:48.020,0:30:51.440<br>
謝謝手放下，其實人沒有變多<br>
<br>
0:30:52.440,0:30:54.840<br>
其實他們是不一樣的<br>
<br>
0:30:54.840,0:30:57.480<br>
如果我們無視上面這個式子的話<br>
<br>
0:30:57.480,0:30:59.740<br>
這兩個式子是不一樣的<br>
<br>
0:30:59.740,0:31:01.780<br>
這兩個 epsilon n<br>
<br>
0:31:01.780,0:31:06.880<br>
一個符合這上面式子的 epsilon n 跟符合下面這兩個 constrain 的 epsilon n<br>
<br>
0:31:07.400,0:31:09.640<br>
不同的 epsilon n 他們是不同的值<br>
<br>
0:31:09.640,0:31:12.160<br>
上面的式子跟下面的式子好像是不一樣的<br>
<br>
0:31:12.840,0:31:14.820<br>
但是這個只是表象上<br>
<br>
0:31:14.820,0:31:16.820<br>
假設我們不考慮這個 Loss Function 的話<br>
<br>
0:31:16.820,0:31:18.960<br>
上下這兩個疑似是不一樣<br>
<br>
0:31:18.960,0:31:20.420<br>
我們這邊整理一下式子<br>
<br>
0:31:20.420,0:31:23.640<br>
把 y n hat * f(x) 移到左邊<br>
<br>
0:31:23.640,0:31:30.920<br>
epsilon 移到右邊變成 y n hat * f(x) >= 1 - epsilon n<br>
<br>
0:31:32.460,0:31:34.960<br>
所以上面這個式子跟下面這兩個是不一樣的<br>
<br>
0:31:35.280,0:31:49.040<br>
但是今天重點就是今天加了 minimize Loss Function L 這件事情以後<br>
<br>
0:31:49.040,0:31:56.820<br>
上面這個式子跟下面這個式子就會變得一模一樣<br>
<br>
0:31:56.820,0:31:58.700<br>
為甚麼呢<br>
<br>
0:31:58.760,0:32:07.560<br>
因為現在我們要去 minimize L(f)<br>
<br>
0:32:07.560,0:32:11.920<br>
你要選擇一個最小的 epsilon n<br>
<br>
0:32:11.920,0:32:15.540<br>
讓你的 L 能夠最小<br>
<br>
0:32:15.540,0:32:19.020<br>
雖然我們下面只有下 constrain 說<br>
<br>
0:32:19.120,0:32:21.560<br>
epsilon n 要大於等於 0<br>
<br>
0:32:21.560,0:32:24.600<br>
epsilon n 要大於等於 1- y n hat * f(x)<br>
<br>
0:32:24.600,0:32:26.880<br>
理論上它不需要正好等於 0<br>
<br>
0:32:26.880,0:32:29.080<br>
它不需要正好等於 1- y n hat * f(x)<br>
<br>
0:32:29.120,0:32:31.640<br>
它可以是任何值<br>
<br>
0:32:31.640,0:32:36.480<br>
或者說我舉個最簡單的例子，epsilon n 我帶一兆<br>
<br>
0:32:36.480,0:32:39.200<br>
帶一個無窮大，它就符合這個 constrain<br>
<br>
0:32:39.840,0:32:41.780<br>
epsilon n 隨便帶一個很大的值<br>
<br>
0:32:41.780,0:32:44.000<br>
它大於 0，也大於它就符合這個 constrain<br>
<br>
0:32:44.000,0:32:46.740<br>
它不需要等於他們之間兩個比較大的那個<br>
<br>
0:32:46.760,0:32:48.680<br>
理論上這兩個東西是不相等的<br>
<br>
0:32:48.680,0:32:50.460<br>
epsilon n 帶任何一個很大的值<br>
<br>
0:32:50.460,0:32:52.180<br>
就符合下面這個 constrain<br>
<br>
0:32:52.180,0:32:54.320<br>
但問題是我們現在要做的事情<br>
<br>
0:32:54.320,0:33:00.620<br>
是要去 minimize L<br>
<br>
0:33:00.620,0:33:03.460<br>
當我們要做的事情是要 minimize L 的時候<br>
<br>
0:33:04.780,0:33:07.780<br>
我們就要想辦法讓 epsilon n 越小越好<br>
<br>
0:33:07.780,0:33:12.880<br>
當 epsilon n 它的 constrain 要 >= 0<br>
跟大於 1 - y n hat * f(x)<br>
<br>
0:33:12.880,0:33:17.560<br>
要他最小的辦法就是讓 epsilon n 等於他們最大的<br>
<br>
0:33:17.560,0:33:21.040<br>
所以加上我們的目標要 minimize total loss 的時候<br>
<br>
0:33:21.040,0:33:27.580<br>
上面這個紅框框的式子會跟下面這個紅框框的式子<br>
是一樣的<br>
<br>
0:33:27.580,0:33:33.080<br>
當我們把上面紅框框的式子轉成下面紅框框的式子<br>
<br>
0:33:33.520,0:33:37.680<br>
這個就是你所熟悉的 SVM 了<br>
<br>
0:33:37.680,0:33:43.620<br>
你所熟悉的 SVM 就是告訴我們 y n hat * f(x)<br>
<br>
0:33:43.620,0:33:46.360<br>
y n hat 和 f(x) 要是同號的<br>
<br>
0:33:46.360,0:33:49.540<br>
他們相乘以後要 >= 一個 margin 1<br>
<br>
0:33:49.580,0:33:53.060<br>
但是這個 margin 是 soft 的，因為 soft 是軟的<br>
<br>
0:33:53.060,0:33:55.340<br>
有時候你沒辦法滿足這個 margin<br>
<br>
0:33:55.620,0:33:58.780<br>
沒有辦法讓 y n hat * f(x) >= 1<br>
<br>
0:33:58.780,0:34:00.040<br>
那怎麼辦呢<br>
<br>
0:34:00.040,0:34:05.680<br>
你把你的 margin 稍微放寬，把它減掉一個 epsilon n<br>
<br>
0:34:05.840,0:34:08.020<br>
這 epsilon n 會放寬你的 margin<br>
<br>
0:34:08.020,0:34:10.860<br>
這個 epsilon n 叫做 slack variable<br>
<br>
0:34:10.860,0:34:13.080<br>
slack 大概是鬆弛的意思<br>
<br>
0:34:13.080,0:34:16.020<br>
讓你的 margin 變得比較鬆<br>
<br>
0:34:16.200,0:34:21.120<br>
但是這個鬆弛的 epsilon 絕對不能夠是負的<br>
<br>
0:34:21.120,0:34:23.320<br>
因為這負的不符合它的目的<br>
<br>
0:34:23.320,0:34:28.040<br>
如果 epsilon n 是負的話你就是把 margin 變大而不是把 margin 變小<br>
<br>
0:34:28.040,0:34:34.160<br>
所以這個 epsilon n 必須要有一個 constrain，要大於等於 0<br>
<br>
0:34:35.440,0:34:38.840<br>
那把這些事情合起來以後<br>
<br>
0:34:38.840,0:34:42.540<br>
你會有一個你要 minimize 的對象<br>
<br>
0:34:42.540,0:34:45.820<br>
再加上一些 constrain<br>
<br>
0:34:45.820,0:34:51.700<br>
這個 formulation 是一個 Quadratic Programming 的 problem<br>
<br>
0:34:51.700,0:34:56.220<br>
你就可以帶一個 Quadratic Programming 的 solver<br>
<br>
0:34:56.220,0:34:58.020<br>
然後把它解出來<br>
<br>
0:34:58.020,0:35:00.900<br>
你不見得要帶 Quadratic Programming 的 solver 才能解它<br>
<br>
0:35:00.940,0:35:05.260<br>
剛才看到說 SVM 其實可以用 Gradient Descent 來解<br>
<br>
0:35:48.660,0:35:53.160<br>
這個是這樣子的，Kernel Method<br>
<br>
0:35:53.160,0:35:55.680<br>
你隨便 google 就有一大堆東西<br>
<br>
0:35:55.680,0:35:58.220<br>
在這一整套東西裡面<br>
<br>
0:35:58.300,0:36:01.720<br>
我認為最重要的、大家最容易卡住的地方<br>
<br>
0:36:01.720,0:36:04.360<br>
只要先說服你這樣一件事情<br>
<br>
0:36:04.360,0:36:10.100<br>
要說服你說，實際上我們找出來的 weight<br>
<br>
0:36:10.100,0:36:14.140<br>
我們實際找出可以 minimize Loss Function 的 weight<br>
<br>
0:36:14.140,0:36:16.020<br>
我們寫作 w hat<br>
<br>
0:36:16.020,0:36:21.020<br>
它其實是我們 data 的 linear combination<br>
<br>
0:36:22.900,0:36:31.020<br>
w star 其實是 summation over 所有 training data 的 vector point，x^n<br>
<br>
0:36:31.020,0:36:36.420<br>
然後對所有的 x^n 都乘上一個 weight<br>
<br>
0:36:36.420,0:36:39.300<br>
alpha(上標 *, 下標 n)<br>
<br>
0:36:39.300,0:36:46.380<br>
也就是說你找出來的 model 其實就是你的 data point 的 Linear Combination<br>
<br>
0:36:46.380,0:36:51.540<br>
一般說服你這件事情的方法都是做<br>
<br>
0:36:51.560,0:36:57.120<br>
用 Lagrange multiplier 解一下剛才的那個式子然後說服你這件事情<br>
<br>
0:36:57.120,0:36:59.880<br>
我們這邊試著從另外一個角度來說服你<br>
<br>
0:36:59.940,0:37:05.380<br>
我們剛才說我們可以用 Gradient Descent 來 minimize SVM<br>
<br>
0:37:05.380,0:37:11.480<br>
所以我們算出來的 Gradient Descent 的式子就是長這個樣子<br>
<br>
0:37:11.480,0:37:15.740<br>
這個是對 w i update 的時候的式子<br>
<br>
0:37:15.740,0:37:20.600<br>
對 w 1 到 w k，假設現在 w 有 k 維<br>
<br>
0:37:20.600,0:37:22.940<br>
update 的式子都是一樣的<br>
<br>
0:37:22.960,0:37:30.460<br>
唯一不一樣的地方只是你會換最後乘上去的 value<br>
<br>
0:37:30.460,0:37:35.240<br>
在 update 第 1 維的時候，你乘上去的 value 就是 x n 的第 1 維<br>
<br>
0:37:35.240,0:37:37.940<br>
在 update 第 i 維的時候，你乘上去的 value 就是 x n 的第 i 維<br>
<br>
0:37:37.940,0:37:41.000<br>
在 update 第 k 維的時候，你乘上去的 value 就是 x n 的第 k 維<br>
<br>
0:37:41.000,0:37:43.160<br>
把他們全部合起來<br>
<br>
0:37:43.160,0:37:47.560<br>
把這邊 w 1 到 w k 串成一個 vector<br>
<br>
0:37:47.560,0:37:51.720<br>
這邊 x1(上標 n) 到 xk(上標 n) 也串成一個 vector<br>
<br>
0:37:51.720,0:37:55.380<br>
你得到的結果就是每次你 update w 的時候<br>
<br>
0:37:55.380,0:38:00.580<br>
你都是把 w 減掉 eta 乘上 summation over x n<br>
<br>
0:38:00.680,0:38:05.420<br>
乘上一個 weight<br>
<br>
0:38:05.800,0:38:10.980<br>
這意味著假設我們 initialize 的時候 w 是一個 zero vector<br>
<br>
0:38:11.040,0:38:13.360<br>
你每次在 update w 的時候<br>
<br>
0:38:13.360,0:38:16.800<br>
你都是加上 data points 的 Linear Combination<br>
<br>
0:38:16.800,0:38:18.920<br>
最後得到的 solution<br>
<br>
0:38:18.920,0:38:21.800<br>
用 Gradient Descent 解出來的 w<br>
<br>
0:38:21.800,0:38:26.960<br>
得到的結果就是 w 的 Linear Combination<br>
<br>
0:38:27.620,0:38:33.140<br>
這個 c^n(w) 是甚麼呢<br>
<br>
0:38:33.140,0:38:41.080<br>
這個 c^n(w) 是 f 對 Loss Function 的偏微分<br>
<br>
0:38:41.080,0:38:45.100<br>
如果我們用 Hinge Loss<br>
<br>
0:38:45.100,0:38:50.080<br>
Hinge Loss 他像 RELU 它有兩個 operation 的 region<br>
<br>
0:38:50.080,0:38:53.380<br>
如果他作用在 max = 0 的 region 的話<br>
<br>
0:38:53.380,0:38:57.940<br>
那它的這一項就會是 0<br>
<br>
0:38:57.940,0:39:05.940<br>
所以當你在用 Hinge Loss 的時候你的這一項往往都是 0<br>
<br>
0:39:05.940,0:39:12.160<br>
也就是不是所有的 x^n 都會被拿來加到 w 裡面去<br>
<br>
0:39:12.660,0:39:15.340<br>
所以最後解出來的 w*<br>
<br>
0:39:15.340,0:39:19.480<br>
它的 Linear Combination 的 weight 可能會是 sparse<br>
<br>
0:39:19.480,0:39:22.340<br>
Linear Combination 的 weight 是 sparse 的意思是<br>
<br>
0:39:22.360,0:39:30.280<br>
可能有很多的 data point 它對應的 alpha* 值等於 0<br>
<br>
0:39:30.280,0:39:36.400<br>
那些 alpha* 值不等於 0 的 x^n<br>
<br>
0:39:36.400,0:39:38.580<br>
它就是 support vector<br>
<br>
0:39:38.580,0:39:41.520<br>
如果 alpha = 0 就一點作用都沒有<br>
<br>
0:39:41.580,0:39:44.220<br>
對這個 model 完全沒有影響力<br>
<br>
0:39:44.220,0:39:48.200<br>
alpha 是 non-zero 的你才會決定<br>
<br>
0:39:48.280,0:39:53.200<br>
最後的 model、parameter 長甚麼樣子<br>
<br>
0:39:53.200,0:39:57.220<br>
而這些可以決定 parameter 長甚麼樣子的 data point<br>
<br>
0:39:57.220,0:40:01.880<br>
他們就叫做 support vector<br>
所以叫做 Support Vector Machine<br>
<br>
0:40:01.880,0:40:06.940<br>
在 data point 裡面不是所有的點都會被選作 support vector<br>
<br>
0:40:06.940,0:40:11.780<br>
只有少數的點、data point 會被選作 support vector<br>
<br>
0:40:11.780,0:40:16.560<br>
所以 SVM 相較於其他方法可能是比較 robust 的<br>
<br>
0:40:18.900,0:40:23.600<br>
如果 Loss Function 選的是 Cross Entropy<br>
<br>
0:40:23.600,0:40:27.920<br>
在做 Logistic Regression 選的是 Cross Entropy，就沒有 sparse 的特性<br>
<br>
0:40:28.060,0:40:30.460<br>
如果你看 Cross Entropy 的 Loss Function<br>
<br>
0:40:30.460,0:40:33.140<br>
它在每一個地方微分都是不等於 0 的<br>
<br>
0:40:33.140,0:40:35.700<br>
他沒有微分等於 0 的地方，微分都是不等於 0 的<br>
<br>
0:40:35.700,0:40:39.700<br>
所以解出來的 alpha 就不會是 sparse<br>
<br>
0:40:39.700,0:40:43.020<br>
如果用 Hinge Loss，解出來的 alpha 就會是 sparse<br>
<br>
0:40:43.060,0:40:45.280<br>
解出來的 alpha 是 sparse 的意思是<br>
<br>
0:40:45.280,0:40:48.420<br>
那些不是 support vector 的 data point<br>
<br>
0:40:48.520,0:40:53.840<br>
把它從 data base 裡面 remove 掉，對最後的結果一點影響都沒有<br>
<br>
0:40:53.840,0:40:58.460<br>
如果有一個奇怪的 outlier，只要不要把它選做 support vector<br>
<br>
0:40:58.600,0:41:01.900<br>
對你最後 train 出來的 model 就不會有任何影響<br>
<br>
0:41:01.900,0:41:05.020<br>
而不像其他的方法<br>
如果用 Logistic Regression<br>
<br>
0:41:05.020,0:41:07.340<br>
每一個 data 都 count<br>
<br>
0:41:07.480,0:41:11.960<br>
每一筆 data 對你最後的結果都會造成影響<br>
<br>
0:41:15.220,0:41:21.800<br>
今天我們把 w 寫成是 data point 的 Linear Combination<br>
<br>
0:41:21.800,0:41:26.840<br>
最大的好處就是我們可以使用等一下說的 kernel 的 trick<br>
<br>
0:41:26.840,0:41:29.300<br>
這個想法是這樣<br>
<br>
0:41:29.300,0:41:35.980<br>
我們已經知道 w 就是 data point 的 Linear Combination<br>
<br>
0:41:37.960,0:41:39.420<br>
本來我們的 w 可以寫成這樣<br>
<br>
0:41:39.520,0:41:45.020<br>
這個 w 可以把它寫成<br>
我們把所有 data point x^1 到 x^N 排起來<br>
<br>
0:41:45.020,0:41:47.940<br>
排成一個 matrix X<br>
<br>
0:41:48.180,0:41:54.080<br>
然後 alpha 就是一個 vector<br>
裡面的值是 alpha^1、alpha^2 到 alpha^N<br>
<br>
0:41:54.080,0:41:56.880<br>
我們把這個 matrix 乘上這個 vector<br>
<br>
0:41:57.120,0:42:02.640<br>
就會得到 X column 的 Linear Combination<br>
<br>
0:42:02.640,0:42:06.340<br>
也就是 matrix X 乘上 vector alpha<br>
<br>
0:42:06.340,0:42:12.200<br>
w 可以寫成 matrix X 乘上 vector alpha<br>
<br>
0:42:12.420,0:42:16.080<br>
當我們知道 w 可以這麼寫以後<br>
<br>
0:42:16.080,0:42:17.840<br>
可以做甚麼呢<br>
<br>
0:42:17.840,0:42:21.500<br>
我們可以改一下 function 的樣子<br>
<br>
0:42:21.500,0:42:25.040<br>
本來的 function 是寫成 (w^T) * x<br>
<br>
0:42:25.040,0:42:28.340<br>
現在已經知道 w = X * alpha<br>
<br>
0:42:28.340,0:42:34.140<br>
所以 f(x) = (alpha^T) * (X^T) * x<br>
<br>
0:42:36.600,0:42:38.440<br>
x 是一個 vector<br>
<br>
0:42:38.440,0:42:41.200<br>
X 的 transpose 是一堆 row 疊在一起<br>
<br>
0:42:41.200,0:42:46.560<br>
alpha 的 transpose 是一個倒下來的 vector<br>
<br>
0:42:46.720,0:42:54.220<br>
把這個 vector 乘上這個 matrix 再乘上這個 vector 得到的是一個 scalar<br>
<br>
0:42:54.480,0:43:00.540<br>
把這個 vector x 乘上這個 X 的 transpose 以後<br>
<br>
0:43:00.540,0:43:02.980<br>
得到的是一個 vector<br>
得到的結果是甚麼呢<br>
<br>
0:43:02.980,0:43:08.320<br>
得到的結果是第一個 dimension 就是 x^1 跟 x 的 inner product<br>
<br>
0:43:08.320,0:43:10.300<br>
第二個 dimension 就是 x^2 跟 x 的 inner product<br>
<br>
0:43:10.300,0:43:12.840<br>
最後一個 dimension 就是 x^N 跟 x 的 inner product<br>
<br>
0:43:12.840,0:43:16.900<br>
接下來把這個 vector 跟這個 vector 在做 inner product 以後<br>
<br>
0:43:16.900,0:43:23.320<br>
得到的結果就是 f(x) 等於 summation over alpha N<br>
<br>
0:43:23.320,0:43:25.720<br>
乘上 x^N 跟 x 的 inner product<br>
<br>
0:43:25.720,0:43:27.340<br>
f(x) 怎麼算<br>
<br>
0:43:27.340,0:43:32.060<br>
把 x 帶進來，它跟 data base 裡面的每一個 x^N<br>
<br>
0:43:32.160,0:43:34.860<br>
都乘上 inner product 以後<br>
<br>
0:43:35.000,0:43:40.180<br>
再把 inner product 的結果用 alpha N 做 weighted sum 就是你的 f(x)<br>
<br>
0:43:40.260,0:43:47.980<br>
你可能會擔心 database 每一個 x^N 算一遍 inner product 會不會很費事<br>
<br>
0:43:47.980,0:43:52.000<br>
其實還好，因為假如用 Hinge Loss，alpha 是 sparse 的<br>
<br>
0:43:52.000,0:43:57.740<br>
所以只要考慮那些 alpha 不等於 0 的 vector 就好<br>
<br>
0:43:57.740,0:44:04.120<br>
在等一下會把 x^N 跟 x 做 inner product 這件事情<br>
<br>
0:44:04.120,0:44:06.160<br>
寫成一個 function<br>
<br>
0:44:06.200,0:44:09.220<br>
寫一個 function k(x^n, x)<br>
<br>
0:44:09.220,0:44:12.620<br>
k(x^n, x) 就是 x^N 跟 x 的 inner product<br>
<br>
0:44:12.620,0:44:15.820<br>
這個 function 叫做 Kernel Function<br>
<br>
0:44:16.480,0:44:24.000<br>
今天已經知道 step 1 就是把 x^N 跟 x 帶進 Kernel Function<br>
<br>
0:44:24.000,0:44:26.620<br>
再乘上 alpha^N 再 summation 以後的結果<br>
<br>
0:44:26.780,0:44:31.460<br>
在 step 2 跟 step 3 呢<br>
我們要 maximize 的對象變成甚麼呢<br>
<br>
0:44:31.460,0:44:33.520<br>
我們的 model 寫成這個樣子<br>
<br>
0:44:33.520,0:44:36.100<br>
不知道的東西其實變成是 alpha^N<br>
<br>
0:44:36.100,0:44:40.600<br>
inner product 裡面沒有參數你本來就知道的<br>
<br>
0:44:40.600,0:44:43.780<br>
你不知道的東西是 alpha^N<br>
<br>
0:44:43.780,0:44:47.440<br>
在 step 2 跟 step 3 的問題就變成<br>
<br>
0:44:47.540,0:44:52.560<br>
要找一組最好的 alpha^N<br>
<br>
0:44:52.620,0:44:55.700<br>
它可以讓我們的 total loss 最小<br>
<br>
0:44:55.700,0:45:01.300<br>
這個最好的 alpha^N 可以長甚麼樣子呢<br>
<br>
0:45:01.300,0:45:06.780<br>
Loss Function 就寫成 summation over 每一筆 data 的<br>
<br>
0:45:06.780,0:45:08.660<br>
小 L 的 Loss Function<br>
<br>
0:45:08.660,0:45:11.140<br>
小 L 的 Loss Function 它吃兩個 input<br>
<br>
0:45:11.140,0:45:14.140<br>
一個是 f(x^n)，一個是 y hat of n<br>
<br>
0:45:14.140,0:45:17.040<br>
f(x^n) 就是 step 1 的這個 function<br>
<br>
0:45:17.120,0:45:18.880<br>
可以把這個 function 帶進來<br>
<br>
0:45:18.880,0:45:21.440<br>
他寫起來就是這個樣子<br>
<br>
0:45:21.440,0:45:23.980<br>
f(x^n) 就是下面這一項<br>
<br>
0:45:23.980,0:45:26.480<br>
因為 summation over n 前面已經用過了<br>
<br>
0:45:26.480,0:45:29.000<br>
所以這邊 summation over n'<br>
<br>
0:45:29.000,0:45:33.000<br>
但是都是 summation over 所有的 training data<br>
<br>
0:45:33.160,0:45:38.160<br>
觀察投影片上的所有式子你會發現<br>
<br>
0:45:38.220,0:45:44.680<br>
我們不再需要真的知道 x 的 vector 是多少<br>
<br>
0:45:46.060,0:45:56.380<br>
真正需要知道的其實只有 x 跟另外一個 vector z 他們之間的 inner product 值<br>
<br>
0:45:56.760,0:46:03.600<br>
或者是只要知道 Penal Function 就可以做所有的 optimization<br>
<br>
0:46:03.720,0:46:08.720<br>
今天只需要知道 K(x^n', x^n) 的 value 是甚麼<br>
<br>
0:46:08.760,0:46:11.680<br>
只需要知道 K(x^n, x) 的 value 是甚麼<br>
<br>
0:46:11.680,0:46:16.000<br>
並不需要真的去知道 x^n 跟 x 他的 vector 長甚麼樣子<br>
<br>
0:46:16.000,0:46:22.160<br>
我只要能夠算個出這一項、這一項其實就結束了<br>
<br>
0:46:23.060,0:46:25.720<br>
等一下會看到這招可以給我們帶來一些好處<br>
<br>
0:46:25.720,0:46:27.920<br>
這招就叫 Kernel Trick<br>
<br>
0:46:27.920,0:46:32.320<br>
Kernel Trick 不是只能用在 SVM 裡面<br>
<br>
0:46:32.320,0:46:38.480<br>
如果你回過頭去看 w 等於 data 的 Linear Combination 這件事情<br>
<br>
0:46:38.580,0:46:43.420<br>
其實不是只有 SVM 適用，Logistic Regression 也可以用同樣的方法<br>
<br>
0:46:43.420,0:46:47.460<br>
所以你也可以有 Kernel based 的 Logistic Regression<br>
<br>
0:46:47.460,0:46:50.040<br>
Linear Regression 也可以用同樣的方法<br>
<br>
0:46:50.040,0:46:53.900<br>
所以你也可以有 Kernel based 的 Regression<br>
<br>
0:46:53.900,0:46:58.860<br>
這些都是可以的，只是這些都不限在 SVM 上面<br>
<br>
0:46:59.960,0:47:04.160<br>
Kernel Trick 怎麼用呢<br>
Kernel Trick 是這樣<br>
<br>
0:47:08.380,0:47:14.160<br>
我們之前有說過如果是 Linear 的 model<br>
他有很多的限制<br>
<br>
0:47:14.320,0:47:17.420<br>
可能要對 input 的 feature 做一個 Feature Transform<br>
<br>
0:47:17.420,0:47:20.480<br>
他才能用 Linear model 來處理<br>
<br>
0:47:20.480,0:47:25.840<br>
如果在 Neural Network 裡面就好幾個 Hidden Layer 來做 Feature Transform<br>
<br>
0:47:25.840,0:47:29.340<br>
假設有一筆 data<br>
<br>
0:47:29.400,0:47:34.720<br>
他是二維的 x1, x2<br>
想要先對他做 Feature Transform<br>
<br>
0:47:34.720,0:47:38.900<br>
在 Feature Transform 上面再去 apply Linear SVM<br>
<br>
0:47:38.900,0:47:42.760<br>
在 Feature Transform 以後再去 apply Linear Model<br>
<br>
0:47:42.760,0:47:46.580<br>
假設 Feature Transform 的結果是我們把 phi(x)<br>
<br>
0:47:46.580,0:47:50.860<br>
變成 (x1)^2, 更號2 * x1 * x2 跟 (x2)^2<br>
<br>
0:47:50.860,0:47:58.640<br>
想要考慮 feature 和 feature 之間，也就是 x1 和 x2 之間的關係<br>
<br>
0:47:59.220,0:48:02.040<br>
如果要算 K(x, z) 的時候<br>
<br>
0:48:02.040,0:48:05.820<br>
想要算 x 跟 z 的 Kernel Function<br>
<br>
0:48:05.820,0:48:11.920<br>
也就是 x 跟 z 做完 Feature Transform 以後，做 inner product 的值<br>
<br>
0:48:11.920,0:48:16.080<br>
可以怎麼做呢<br>
最簡單的方法就是我把 x 跟 z<br>
<br>
0:48:16.080,0:48:18.860<br>
都帶到這個 Feature Transform 的 function 裡面<br>
<br>
0:48:18.860,0:48:20.860<br>
把他們變成新的 feature<br>
<br>
0:48:21.140,0:48:25.280<br>
變成新的 feature 就可以直接做 Inner Product<br>
<br>
0:48:25.320,0:48:29.940<br>
算出來的結果就是這樣<br>
國中生都會算的東西<br>
<br>
0:48:30.000,0:48:34.300<br>
可以把這一項做一下轉化<br>
<br>
0:48:34.420,0:48:41.260<br>
這一項是 (x1)^2 * (z1)^2 + 2(x1)(x2)(z1)(z2) + (x2)^2 * (z2)^2<br>
<br>
0:48:41.260,0:48:45.180<br>
它可以被簡化成 ((x1)(z1) + (x2)(z2))^2<br>
<br>
0:48:45.180,0:48:48.040<br>
((x1)(z1) + (x2)(z2))^2 是甚麼<br>
<br>
0:48:48.040,0:48:54.980<br>
((x1)(z1) + (x2)(z2))^2 正好就是 [x1, x2], [z1, z2] 這兩個 vector 的 Inner Product<br>
<br>
0:48:54.980,0:49:00.400<br>
所以說我們把 x 跟 z<br>
<br>
0:49:02.060,0:49:04.740<br>
做 Feature Transform 再做 Inner Product<br>
<br>
0:49:04.740,0:49:09.040<br>
等同於他們在原來 Feature Transform 之前的 space 上面<br>
<br>
0:49:09.040,0:49:13.380<br>
先做 Inner Product 以後再平方<br>
<br>
0:49:14.140,0:49:17.660<br>
那這招有時候可以給我們帶來好處<br>
<br>
0:49:17.660,0:49:21.000<br>
因為有時候直接計算這個結果<br>
<br>
0:49:21.000,0:49:25.220<br>
直接計算 x 跟 z 帶進 Kernel Function 以後的 output<br>
<br>
0:49:25.220,0:49:30.220<br>
會比先做 Feature Transform 再做 Inner Product 還要更快速<br>
<br>
0:49:30.240,0:49:34.600<br>
舉例來說，假設現在要做的事情是<br>
<br>
0:49:34.600,0:49:38.080<br>
我的 x 跟 z 都不是 2 維，是高維，是 k 維<br>
<br>
0:49:38.080,0:49:41.960<br>
想要把她投影到一個更高維的平面<br>
<br>
0:49:41.960,0:49:46.960<br>
這更高維裡面，會考慮所有 feature 兩兩之間的關係<br>
<br>
0:49:46.960,0:49:49.540<br>
假設原來有 k 維<br>
<br>
0:49:49.560,0:49:53.500<br>
在更高維的平面就至少是 (C k 取 2) 維<br>
<br>
0:49:53.520,0:49:56.340<br>
要考慮所有 feature 之間兩兩關係<br>
<br>
0:49:58.120,0:50:01.100<br>
所以 phi(x) 就是 (x1)^2 到 (xk)^2<br>
<br>
0:50:01.100,0:50:07.000<br>
√2(x1)(x2), √2(x1)(x3), √2(x2)(x3) 以此類推<br>
<br>
0:50:07.580,0:50:10.780<br>
如果用 Kernel Trick 的話可以輕易的把<br>
<br>
0:50:10.780,0:50:17.300<br>
phi(x) 跟 phi(z) 的 Inner Product 的結果輕易的算出來<br>
<br>
0:50:17.320,0:50:22.220<br>
怎麼算呢？其實 phi(x) 跟 phi(z) 的 Inner Product 就是<br>
<br>
0:50:22.220,0:50:24.640<br>
(x · z)^2<br>
<br>
0:50:24.640,0:50:28.200<br>
直接把 x 跟 z 做 Inner Product 再平方<br>
<br>
0:50:28.200,0:50:33.280<br>
只需要算 k 個 element 的相乘再做一次平方就好<br>
<br>
0:50:33.340,0:50:37.600<br>
但是如果先把他 project 到 high dimension 再做 Inner Product 的話<br>
<br>
0:50:38.900,0:50:42.580<br>
這個 dimension 很大，是 (C k 取 2) 維<br>
<br>
0:50:42.580,0:50:47.300<br>
如果 feature、dimension 越大，k 值的話<br>
這個 feature 就越長<br>
<br>
0:50:47.300,0:50:54.380<br>
先做完 Feature Transform 再做 Inner Product 是會比<br>
先做 Inner Product 再取平方運算量還要大<br>
<br>
0:50:54.380,0:50:56.100<br>
這個是比較快的<br>
<br>
0:50:56.100,0:51:02.200<br>
這個平方可以拆成 ((x1)(z1) + (x2)(z2) + ⋅⋅⋅ + (xk)(zk))^2<br>
<br>
0:51:02.200,0:51:07.980<br>
這個平方展開的話裡面有二次項裡面有 (x1)^2 * (z1)^2, (x2)^2 * (z2)^2 等等<br>
<br>
0:51:07.980,0:51:14.040<br>
會有兩兩相乘的 2(x1)(x2)(z1)(z2) + 2(x1)(x3)(z1)(z3)<br>
<br>
0:51:14.040,0:51:17.800<br>
2(x2)(x3)(z2)(z3), 2(x2)(x4)(z2)(z4) 等等<br>
<br>
0:51:17.800,0:51:22.460<br>
把 x 集中到一邊就可以得到這個 vector<br>
<br>
0:51:22.460,0:51:27.280<br>
把 z 集中到另外一邊就得到 z 做完 Feature Transform 的東西<br>
<br>
0:51:27.280,0:51:30.280<br>
所以這一項呢<br>
<br>
0:51:30.280,0:51:34.660<br>
就會等價於先做 Feature Transform 再做 Inner Product<br>
<br>
0:51:35.660,0:51:37.640<br>
還有一些更驚人的結果<br>
<br>
0:51:38.720,0:51:40.700<br>
做 Kernel Basis 的<br>
<br>
0:51:43.100,0:51:45.380<br>
做 RBF Kernel<br>
<br>
0:51:45.500,0:51:49.720<br>
做 RBF Kernel 的意思是<br>
<br>
0:51:49.720,0:52:00.380<br>
K(x, z) 就等於 x 跟 z 的距離乘上 (-1/2) 再取 exponential<br>
<br>
0:52:01.020,0:52:05.400<br>
這個東西就是在衡量 x 跟 z 之間的相似度<br>
<br>
0:52:05.400,0:52:11.620<br>
如果 x 跟 z 越像 Kernel 的值就越大<br>
<br>
0:52:11.620,0:52:13.540<br>
如果 x = z 的話，值就是 1<br>
<br>
0:52:13.540,0:52:16.240<br>
如果 x 跟 z 完全不一樣的話<br>
<br>
0:52:16.240,0:52:18.800<br>
它們的值就是 0<br>
<br>
0:52:18.980,0:52:27.840<br>
這個式子 exp((-1/2) ||x-z||2) 其實也可以化成<br>
<br>
0:52:27.840,0:52:32.500<br>
兩個 high dimension 的 vector 做 Inner Product 以後的結果<br>
<br>
0:52:32.700,0:52:39.200<br>
這兩個 vector 其實他們的 dimension 是有無窮多維<br>
<br>
0:52:39.200,0:52:45.340<br>
本來如果要把一個 x project 到無窮多維再做 Inner Product，你做不到<br>
<br>
0:52:45.340,0:52:47.380<br>
因為無窮多維是什麼樣的根本就不知道<br>
<br>
0:52:47.380,0:52:50.680<br>
但是如果直接算 x 跟 z 的距離<br>
<br>
0:52:50.740,0:52:58.380<br>
然後再乘 -1/2 再取 exponential<br>
其實等同於在無窮多維的空間裡面去做 Inner Product<br>
<br>
0:52:58.380,0:53:00.900<br>
無窮多維的空間長甚麼樣子<br>
<br>
0:53:00.900,0:53:06.620<br>
可以把 exp((-1/2) ||x-z||2)<br>
<br>
0:53:06.620,0:53:15.000<br>
變成 exp((-1/2) ||x||2 - (1/2) ||z||2 + x ⋅ z)<br>
<br>
0:53:15.000,0:53:21.300<br>
接下來把 x 的 norm、z 的 norm 這兩項提出來<br>
剩下 exp(x ⋅ z)<br>
<br>
0:53:21.580,0:53:27.800<br>
把 x 的 norm 這一項用 Cx 來表示它，它跟 x 有關<br>
<br>
0:53:27.800,0:53:32.260<br>
這一項用 Cz 來表示它，它跟 z 有關<br>
剩下 exp(x ⋅ z)<br>
<br>
0:53:32.260,0:53:38.240<br>
exp(x ⋅ z) 用泰勒展開式、用 Taylor Expansion 就變成<br>
<br>
0:53:38.240,0:53:42.420<br>
它等於 Cx * Cz summation over i = 0 到無窮大<br>
<br>
0:53:42.420,0:53:47.220<br>
((x ⋅ z)^i)/ i!<br>
<br>
0:53:47.220,0:53:53.220<br>
這個有無窮大項<br>
從 0 次方一直 summation 到無窮多次方<br>
<br>
0:53:53.220,0:53:55.800<br>
如果把它展開的話看起來就像這樣<br>
<br>
0:53:55.800,0:54:03.900<br>
CxCz + CxCz ( x ⋅ z ) + CxCz(1/2)( x ⋅ z )^2 ⋅⋅⋅<br>
<br>
0:54:03.900,0:54:08.880<br>
如果把每一項都拆開的話會得到甚麼<br>
<br>
0:54:08.880,0:54:13.060<br>
Cx 跟 Cz 可以看成是兩個 vector<br>
<br>
0:54:13.060,0:54:14.580<br>
他們都只有一個 dimension<br>
<br>
0:54:14.580,0:54:17.320<br>
一個是 Cx，一個是 Cz 的 Inner Product<br>
<br>
0:54:17.620,0:54:22.800<br>
這一項可以看成是把原來 x 的 vector 乘上 Cx<br>
<br>
0:54:22.800,0:54:29.200<br>
把 z 的 vector 乘上 Cz 再做 Inner Product 以後的結果<br>
<br>
0:54:29.200,0:54:32.580<br>
這一項 x 跟 z 的平方<br>
<br>
0:54:32.600,0:54:39.140<br>
x 跟 z 先做 Inner Product 再平方<br>
剛剛在前面例子已經看過<br>
<br>
0:54:39.140,0:54:45.400<br>
其實可以拆成兩個 high dimension 的 vector 再 Inner Product<br>
<br>
0:54:45.400,0:54:51.620<br>
所以 x 跟 z 的平方等於兩個 high dimension 的 vector 做 Inner Product 的結果<br>
<br>
0:54:51.800,0:54:57.100<br>
這 high dimension 的 vector 它會考慮兩個 dimension 之間的關係<br>
<br>
0:54:57.280,0:55:02.240<br>
如果是三次方它就會考慮三個 dimension 之間的關係<br>
<br>
0:55:02.720,0:55:06.400<br>
現在把屬於 x 的 vector 都串起來<br>
<br>
0:55:06.400,0:55:09.820<br>
串成一個很長的 vector<br>
<br>
0:55:09.900,0:55:13.160<br>
屬於 z 這邊的也都串起來<br>
<br>
0:55:13.220,0:55:14.520<br>
因為這邊有無窮多項<br>
<br>
0:55:14.520,0:55:18.580<br>
所以串起來以後 x 跟 z 都有各自無窮長的 vector<br>
<br>
0:55:18.820,0:55:22.200<br>
他們做 Inner Product 最後得到的結果<br>
<br>
0:55:22.200,0:55:27.120<br>
就是這個 Kernel 所給你的結果<br>
<br>
0:55:27.120,0:55:30.640<br>
所以使用 RBF Kernel 的時候<br>
<br>
0:55:30.640,0:55:37.640<br>
就是在無窮多維的平面上去做事情<br>
<br>
0:55:38.920,0:55:41.380<br>
不過如果在無窮多維的平面上做事情<br>
<br>
0:55:41.380,0:55:44.880<br>
可以想像它其實是滿容易 Overfitting<br>
<br>
0:55:44.880,0:55:47.120<br>
所以如果用 RBF Kernel 有時候要小心<br>
<br>
0:55:47.120,0:55:51.120<br>
可能在 training data 上得到很好的 performance<br>
<br>
0:55:51.180,0:55:54.320<br>
但是在 testing data 上得到很糟的 performance<br>
<br>
0:55:54.320,0:55:56.980<br>
你也可以做 Sigmoid Kernel<br>
<br>
0:55:57.040,0:56:04.740<br>
Sigmoid Kernel 是 K(x, z) = tanh(x · z)<br>
<br>
0:56:04.740,0:56:12.280<br>
至於 tanh(x · z) 是哪兩個 high dimension vector 做 Inner Product 的結果<br>
<br>
0:56:12.400,0:56:17.500<br>
自己回去用 Taylor Expansion 展開來看就知道了<br>
<br>
0:56:17.840,0:56:20.000<br>
之前已經說過<br>
<br>
0:56:23.280,0:56:27.560<br>
當要把 x 拿來做 testing 的時候、帶到 f 裡面的時候<br>
<br>
0:56:27.560,0:56:35.780<br>
其實是計算 x 跟所有 training data 裡面的 x^n 的 Kernel 的 function output 然後再乘上 alpha n<br>
<br>
0:56:35.780,0:56:38.360<br>
如果用的是 Sigmoid Kernel 的時候<br>
<br>
0:56:38.360,0:56:42.260<br>
就是把 data 裡面所有的 x n 跟 x 做 Inner Product<br>
<br>
0:56:42.260,0:56:47.300<br>
再去 Hyperbolic Tangent 再乘 alpha n 再全部合起來以後的結果<br>
<br>
0:56:49.520,0:56:52.760<br>
如果用的是 Sigmoid Kernel<br>
<br>
0:56:53.160,0:57:05.500<br>
這個 f(x) 就可以想成它其實是一個只有一個 hidden layer 的 Neural Network<br>
<br>
0:57:05.500,0:57:11.940<br>
把 x 拿進來，它會跟所有的 x n 都做 Inner Product<br>
<br>
0:57:11.940,0:57:14.680<br>
再通過 Hyperbolic Tangent<br>
<br>
0:57:14.720,0:57:17.180<br>
對每一個 x 做 Inner Product 這件事情<br>
<br>
0:57:17.180,0:57:23.400<br>
就好像是有一個 neural 它的 weight 就是某一筆 data<br>
<br>
0:57:23.400,0:57:28.260<br>
把 x1 那一筆 data 拿出來當作這個 neural 的 weight<br>
<br>
0:57:28.260,0:57:31.680<br>
把 x2 那一筆 data 拿出來當作第二個 neural 的 weight<br>
<br>
0:57:31.680,0:57:34.760<br>
一直到把第 n 筆 data 拿出來當作第 n 個 neural 的 weight<br>
<br>
0:57:34.760,0:57:40.500<br>
把他們都通過 Hyperbolic Tangent 得到 output<br>
<br>
0:57:40.500,0:57:44.560<br>
然後再把它全部乘上 alpha<br>
<br>
0:57:44.760,0:57:48.040<br>
然後把他們全部加起來<br>
<br>
0:57:48.040,0:57:51.920<br>
突然發現我犯了一個錯<br>
這個 alpha 應該要下標<br>
<br>
0:57:51.920,0:57:59.460<br>
上標下標都可以<br>
前面是下標，後面也要是下標才合適<br>
<br>
0:58:05.760,0:58:13.940<br>
我們找出這些 alpha 把它 Weighted Sum 起來就得到最後的 f(x)<br>
<br>
0:58:13.940,0:58:18.600<br>
這就是一個 Neural Network 只是它只有一個 Hidden Layer<br>
<br>
0:58:18.700,0:58:22.460<br>
在這個 Neural Network 裡面它裡面的每一個 neural 的 weight<br>
<br>
0:58:22.460,0:58:24.340<br>
就是某一筆 data<br>
<br>
0:58:24.340,0:58:26.840<br>
那 neural 的數目呢<br>
<br>
0:58:26.840,0:58:32.600<br>
neural 的數目就是看有幾個 support vector 就有幾個 neural<br>
<br>
0:58:35.580,0:58:44.120<br>
既然有了 Kernel Trick，其實可以去直接設計 Kernel Function<br>
<br>
0:58:44.120,0:58:49.780<br>
可以根本完全不用理會 x 跟 z 的 feature 長甚麼樣子<br>
<br>
0:58:49.960,0:58:55.260<br>
只要有一個 Kernel Function 可以把 x 跟 z 這兩個東西帶進去<br>
<br>
0:58:55.260,0:58:57.900<br>
它可以給你一個 value<br>
<br>
0:58:57.900,0:59:03.460<br>
這個 value 代表了 x 跟 z 在某一個高維平面上的 vector 的 Inner Product 的話<br>
<br>
0:59:03.460,0:59:08.220<br>
你根本就不需要在意 x 跟 z 他們的 vector 長甚麼樣子<br>
<br>
0:59:08.320,0:59:14.900<br>
甚麼時候這招會有用呢<br>
假設 x 是有 structure 的 data<br>
<br>
0:59:14.900,0:59:17.780<br>
比如說它是一個 sequence<br>
<br>
0:59:17.780,0:59:23.100<br>
如果是一個 sequence 的話，其實不容易把 sequence 表示成 vector<br>
<br>
0:59:23.100,0:59:28.760<br>
假設每一個 sequence 長度都不一樣<br>
<br>
0:59:28.760,0:59:34.500<br>
就不容易把這些不同長度的 sequence 都用一個 vector 來描述他<br>
<br>
0:59:34.500,0:59:40.620<br>
所以你根本不知道 x 長甚麼樣子<br>
你就不知道 phi(x) 應該長甚麼樣子<br>
<br>
0:59:41.820,0:59:45.720<br>
但是可以直接定它的 Kernel Function<br>
<br>
0:59:45.720,0:59:50.760<br>
我們知道 Kernel Function 就是投影到高維以後的 Inner Product<br>
<br>
0:59:50.760,0:59:54.640<br>
所以 Kernel Function 往往就是一個類似 similarity 的東西<br>
<br>
0:59:54.640,1:00:00.080<br>
如果你可以定一個 Function<br>
它是 evaluate x 跟 z 的 similarity<br>
<br>
1:00:00.080,1:00:03.620<br>
就算 x 跟 z 它是有 structure 的 object<br>
<br>
1:00:03.620,1:00:05.320<br>
比如說它是 tree structure<br>
<br>
1:00:05.320,1:00:11.660<br>
它是 sequence 也沒有關係<br>
只要知道怎麼算兩個 sequence 之間的 similarity<br>
<br>
1:00:11.660,1:00:19.940<br>
只要知道怎麼算兩個 tree structure similarity<br>
就有機會把它的 similarity 當作 Kernel 來使用<br>
<br>
1:00:19.940,1:00:24.800<br>
你可能會懷疑<br>
我胡亂定一個 similarity<br>
<br>
1:00:24.800,1:00:28.520<br>
它背後有 feature vector 可以 support 它嗎<br>
<br>
1:00:28.520,1:00:33.520<br>
我們說這個 Kernel 是兩個 vector 做 Inner Product 以後的結果<br>
<br>
1:00:33.520,1:00:38.680<br>
你胡亂定一個 function 它可以拆成兩個 vector Inner Product 以後的結果嗎<br>
<br>
1:00:38.680,1:00:44.060<br>
不是所有的 function 都可以，但是有一個叫 Mercer's theory<br>
<br>
1:00:44.060,1:00:49.180<br>
可以告訴你那些 function 是可以的<br>
<br>
1:00:49.180,1:00:58.320<br>
所以你有辦法 check 定出來的 Kernel Function 它背後有沒有兩個 vector 做 Inner Product 這件事情<br>
<br>
1:00:58.320,1:01:01.400<br>
也是可以 check 的<br>
<br>
1:01:01.400,1:01:08.660<br>
所以在語音上，假設現在要做的分類對象<br>
其實是 Audio Segment<br>
<br>
1:01:08.740,1:01:16.360<br>
Audio Segment，每一個 segment、每一段聲音訊號就是會用 vector sequence 來描述他<br>
<br>
1:01:16.360,1:01:18.840<br>
每一段聲音訊號長度都不一樣<br>
<br>
1:01:18.840,1:01:21.640<br>
所以 vector sequence 長度可能都不一樣<br>
<br>
1:01:21.640,1:01:23.640<br>
現在做的 task 可能是<br>
<br>
1:01:23.640,1:01:31.760<br>
給你一段聲音訊號，這是分類的問題<br>
它要看這段聲音訊號裡面語者的情緒<br>
<br>
1:01:31.780,1:01:37.480<br>
它可能分成高興、生氣等等之類的<br>
<br>
1:01:37.480,1:01:39.400<br>
要把它做分類<br>
<br>
1:01:39.400,1:01:44.460<br>
你想要用SVM，但是一段聲音訊號沒有辦法直接用一個 vector 來描述他<br>
<br>
1:01:44.960,1:01:48.280<br>
怎麼辦<br>
你可以直接定它的 Kernel<br>
<br>
1:01:48.280,1:01:53.600<br>
就不要管一段聲音訊號變成 vector 之後長甚麼樣子<br>
<br>
1:01:53.600,1:01:59.060<br>
直接定它的 Kernel<br>
直接定一個 function K(x, z)<br>
<br>
1:01:59.060,1:02:01.960<br>
然後把 x 是一段聲音訊號帶進去<br>
<br>
1:02:01.960,1:02:05.060<br>
z 是另外一段聲音訊號帶進去的時候<br>
<br>
1:02:05.060,1:02:06.880<br>
這個 function 的 output 應該是甚麼<br>
<br>
1:02:06.880,1:02:11.120<br>
你定好這個你就可以直接用 Kernel Trick 在 SVM<br>
<br>
1:02:11.120,1:02:16.620<br>
就算你不知道一段聲音訊號描述成一個 vector 應該是甚麼樣子<br>
<br>
1:02:18.020,1:02:22.960<br>
怎麼定兩個 sequence 間的 Kernel<br>
<br>
1:02:23.120,1:02:28.800<br>
我就把 reference 留在這邊給大家參考<br>
<br>
1:02:30.360,1:02:35.480<br>
其實在這邊還有很多，比如說 SVM 可以做 Regression<br>
<br>
1:02:35.480,1:02:39.360<br>
SVM 做 regression 就是 Support Vector Regression<br>
<br>
1:02:40.240,1:02:43.880<br>
它的精神是這樣，用幾句話講一下它的精神<br>
<br>
1:02:43.880,1:02:49.160<br>
它的精神是，原來在做 Regression 的時候希望 model 的 output 跟 target 越近越好<br>
<br>
1:02:49.160,1:02:51.740<br>
如果做 Support Vector Regression 的時候<br>
<br>
1:02:51.740,1:02:54.540<br>
近到某一個距離裡面<br>
<br>
1:02:56.540,1:03:00.460<br>
我本來想說某一個劍圍的距離<br>
但我想大家不知道甚麼是劍圍就算了<br>
<br>
1:03:01.100,1:03:05.760<br>
張遼的劍圍<br>
算了我覺得這太沒有關係了<br>
<br>
1:03:06.380,1:03:10.920<br>
進入 target 某一個距離裡面<br>
<br>
1:03:11.080,1:03:14.360<br>
它的 loss 就是 0<br>
這個是 Support Vector Regression<br>
<br>
1:03:14.360,1:03:15.880<br>
Ranking SVM<br>
<br>
1:03:15.880,1:03:22.040<br>
Ranking SVM 常常被用在當你要考慮的東西是一個排序是 list 的時候<br>
<br>
1:03:22.040,1:03:26.460<br>
比如說在 final project 裡面不是有個 recommendation 的題目<br>
<br>
1:03:26.460,1:03:29.660<br>
他要你的 output 是一個 list<br>
<br>
1:03:30.300,1:03:34.700<br>
你可以說我把它當作是一個 regression 的題目<br>
<br>
1:03:34.700,1:03:40.380<br>
我給每一個 element 一個分數<br>
然後按照分數由高到低做搜索<br>
<br>
1:03:40.380,1:03:43.760<br>
但這樣並沒有直接 optimize 你的問題<br>
<br>
1:03:43.760,1:03:47.060<br>
其實可以直接考慮這個 list 的 ranking<br>
<br>
1:03:47.140,1:03:51.980<br>
如果直接考慮 ranking 的 SVM 叫做 Ranking SVM<br>
<br>
1:03:52.100,1:03:56.320<br>
或許在 final project 裡面用的到這個東西<br>
<br>
1:03:56.320,1:03:58.820<br>
還有另外一個東西叫 One-class SVM<br>
<br>
1:03:58.820,1:04:03.860<br>
它是會希望說屬於 positive 的 example 都自成一類<br>
<br>
1:04:03.860,1:04:07.220<br>
negative 的 example 就散佈在其他地方<br>
<br>
1:04:07.380,1:04:10.220<br>
這個都留一些 reference 給大家參考就好<br>
<br>
1:04:10.220,1:04:13.580<br>
我們可以比較一下 Deep Learning 跟 SVM 的差別<br>
<br>
1:04:13.580,1:04:19.460<br>
Deep Learning 的前幾個 layer 可以看作是 Feature Transformation<br>
<br>
1:04:19.460,1:04:23.400<br>
最後一個 layer 可以看作是 Linear Classifier<br>
<br>
1:04:23.400,1:04:27.360<br>
SVM 做的也是很類似的事情<br>
<br>
1:04:27.580,1:04:31.740<br>
它前面先 apply 一個 Kernel Function<br>
<br>
1:04:31.740,1:04:34.720<br>
把 feature 轉到 high dimension 上面<br>
<br>
1:04:34.720,1:04:38.940<br>
在 high dimension space 上面就可以 apply Linear Classifier<br>
<br>
1:04:38.940,1:04:44.240<br>
在 SVM 裡面一般 Linear Classifier 都會用 Hinge Loss<br>
<br>
1:04:44.240,1:04:49.740<br>
事實上 SVM 的 kernel 是 learnable<br>
<br>
1:04:49.740,1:04:53.500<br>
我列了一個 reference 給大家參考，事實上它是 learnable<br>
<br>
1:04:53.500,1:04:59.580<br>
但是它沒有辦法 learn 的像 Deep Learning 那麼多<br>
<br>
1:04:59.580,1:05:03.220<br>
你可以做的是你有好幾個不同的 kernel<br>
<br>
1:05:03.220,1:05:05.480<br>
然後把不同 kernel combine 起來<br>
<br>
1:05:05.480,1:05:07.840<br>
他們中間的 weight 是可以 learn 的<br>
<br>
1:05:07.860,1:05:09.860<br>
當你只有一個 kernel 的時候<br>
<br>
1:05:09.860,1:05:14.980<br>
SVM 就好像是只有一個 Hidden Layer 的 Neural Network<br>
<br>
1:05:14.980,1:05:17.720<br>
當你把 kernel 在做 Linear Combination 的時候<br>
<br>
1:05:17.780,1:05:22.660<br>
他就像一個有兩個 layer 的 Neural Network<br>
<br>
1:05:23.000,1:05:26.600<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
