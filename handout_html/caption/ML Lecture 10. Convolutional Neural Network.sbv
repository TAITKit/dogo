0:00:00.000,0:00:02.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.000,0:00:02.700
supervised learning 的話呢，有哪些 suggestion

0:00:02.700,0:00:04.480
所以，就算我們上課還沒有講到

0:00:04.480,0:00:07.140
也沒有關係，我相信對大家來說

0:00:07.140,0:00:08.460
應該不是一個問題

0:00:08.460,0:00:10.320
我們今天唯一需要做的就是

0:00:10.320,0:00:13.280
講完 CNN 的部分

0:00:15.400,0:00:19.860
我們都知道說，CNN 它常常被用在

0:00:19.860,0:00:21.840
影像處理上

0:00:21.840,0:00:25.380
如果你今天用 CNN 來做影像處理

0:00:25.380,0:00:26.940
比如說，你用 CNN 來

0:00:26.940,0:00:29.000
我當然可以用

0:00:29.000,0:00:30.500
一般的 neural network

0:00:30.500,0:00:33.580
來做影像處理，不一定要用 CNN

0:00:33.580,0:00:36.060
比如說，你想要做影像的分類

0:00:36.060,0:00:37.660
那你就是 train 一個 neural network

0:00:37.660,0:00:40.120
input 是一張圖片

0:00:40.120,0:00:44.100
那這張圖片你就把它表示成裡面的 pixel

0:00:44.100,0:00:46.280
也就是一個很長很長的 vector

0:00:46.280,0:00:49.020
那 output 就是看你，假設你有 1000 個類別

0:00:49.020,0:00:51.060
output 就是 1000 個 dimension

0:00:51.060,0:00:54.200
那我相信，根據剛才那堂課的內容，大家

0:00:54.200,0:00:57.480
應該都可以給你 training data，大概就可以秒做出來

0:00:58.000,0:00:59.320
但是

0:00:59.320,0:01:02.760
我們現在會遇到的問題是這樣子

0:01:04.520,0:01:08.680
實際上，如果我們 train neural network 的時候

0:01:08.680,0:01:13.520
我們會期待說，在這個 network 的 structure 裡面

0:01:13.520,0:01:18.060
每一個 neuron，其實就代表了一個最基本的 classifier

0:01:18.060,0:01:19.820
事實上，在文獻上呢

0:01:19.820,0:01:22.800
根據訓練的結果，也有很多人得到這樣的結論

0:01:22.800,0:01:25.500
舉例來說，第一層的 classifier

0:01:25.500,0:01:26.920
第一層的 neuron

0:01:26.920,0:01:28.580
它是最簡單的 classifier

0:01:28.580,0:01:31.100
它做的事情就是 detect 有沒有綠色出現

0:01:31.100,0:01:34.100
有沒有黃色出現，有沒有斜的條紋

0:01:34.100,0:01:36.720
那第二個 layer，它做的事情是

0:01:36.720,0:01:38.100
detect 更複雜的東西

0:01:38.100,0:01:39.760
根據第一個 layer 的 output

0:01:39.760,0:01:41.740
它如果看到直線橫線

0:01:41.740,0:01:43.420
就是窗框的一部分

0:01:43.420,0:01:45.900
如果看到棕色的直條紋就是木紋

0:01:45.900,0:01:49.020
看到斜條紋加灰色的

0:01:49.020,0:01:52.620
這個有可能是很多東西，比如說，輪胎的一部分等等

0:01:53.100,0:01:56.600
再根據第二個 hidden layer 的 output

0:01:56.600,0:01:58.860
第三個 hidden layer 會做更複雜的事情

0:01:58.860,0:02:00.800
比如說，它可以知道說

0:02:00.800,0:02:03.260
某一個 neuron 看到蜂巢，它就被 activate

0:02:03.260,0:02:05.140
某一個 neuron 看到車子，就被 activate

0:02:05.140,0:02:07.500
某一個 neuron 看到人的上半身

0:02:07.500,0:02:08.680
就被 activate

0:02:08.680,0:02:10.920
那現在的問題是這樣子

0:02:10.920,0:02:13.180
當我們直接用一般的

0:02:13.180,0:02:15.860
fully connected 的 feedforward network 來做影像處理

0:02:15.860,0:02:18.620
的時候，往往我們會需要太多的參數

0:02:18.620,0:02:19.740
舉例來說

0:02:19.740,0:02:23.280
假設這是一張 100*100 的彩色圖片

0:02:23.280,0:02:26.460
它解析度才 100*100，那這已經是很小張的 image 了

0:02:26.460,0:02:28.520
那你把它拉成一個 vector

0:02:28.520,0:02:30.440
它有多少的 pixel

0:02:30.440,0:02:32.280
它有 100*100*3

0:02:32.280,0:02:34.120
對不對，每個 pixel 其實需要

0:02:34.120,0:02:37.180
3 個 value 來描述它，如果是彩色的圖的話

0:02:37.180,0:02:42.780
是三萬維，那 input vector 如果是三萬維

0:02:42.780,0:02:47.040
這個 hidden layer 假設 1000 個 neuron 就好了

0:02:47.040,0:02:49.700
你的這個第一層 hidden layer 的參數

0:02:49.700,0:02:51.600
就已經有 30000*1000 了

0:02:51.600,0:02:53.980
這樣太多了

0:02:53.980,0:02:57.640
所以，CNN 它做的事情其實是

0:02:57.640,0:03:01.740
我們來簡化這個 neural network 的架構

0:03:01.740,0:03:05.360
我們把這裡面一些，根據人的知識

0:03:05.360,0:03:08.020
我們根據我們對影像處理的理解，就知道說

0:03:08.020,0:03:10.800
某些 weight 其實是用不上的，我們一開始

0:03:10.800,0:03:12.380
就把它濾掉

0:03:12.380,0:03:15.980
我們一開始就想一些辦法，
不要用 fully connected network

0:03:15.980,0:03:17.580
而是用比較少的參數

0:03:17.580,0:03:19.940
來做影像處理這件事情

0:03:19.940,0:03:22.960
所以，CNN 其實是比一般的 DNN

0:03:22.960,0:03:25.200
還要更簡單的

0:03:25.200,0:03:27.780
等一下，我們講完以後你就會發現說

0:03:27.780,0:03:29.480
你可能覺得說，CNN 看起來

0:03:29.480,0:03:31.100
它的運作很複雜

0:03:31.100,0:03:32.900
它應該是個比較複雜的東西

0:03:32.900,0:03:35.060
但事實上，它的模型呢

0:03:35.060,0:03:38.320
是比 DNN 還要更簡單的

0:03:38.320,0:03:40.220
我們就是用 prior knowledge

0:03:40.220,0:03:43.540
去把原來 fully connected 的 layer 裡面的一些參數拿掉

0:03:43.540,0:03:45.100
就變成 CNN

0:03:46.120,0:03:48.980
我們先講一下，為甚麼

0:03:48.980,0:03:52.220
我們有可能把一些參數拿掉

0:03:52.220,0:03:54.840
為甚麼我們有可能只用比較少的參數

0:03:54.840,0:03:56.860
就來做影像處理這件事情

0:03:56.860,0:04:00.020
那這邊有幾個觀察，第一個是

0:04:00.460,0:04:02.000
在影像處理裡面

0:04:02.000,0:04:04.900
如果我們說，第一層的 hidden layer

0:04:04.900,0:04:08.460
那些 neuron 要做的事情就是偵測某一種 pattern

0:04:08.460,0:04:11.040
有沒有某一種東西，有沒有某一種 pattern 出現

0:04:11.040,0:04:15.140
那大部分的 pattern 其實是比整張 image 還要小

0:04:15.140,0:04:16.820
所以，對一個 neuron 來說

0:04:16.820,0:04:19.460
假設它要知道說，一張 image 裡面

0:04:19.460,0:04:21.320
有沒有某一個 pattern 出現

0:04:21.320,0:04:23.700
它其實不需要看整張 image

0:04:23.700,0:04:25.420
它只要看 image 的一小部分

0:04:25.420,0:04:27.640
它就可以決定這件事情了

0:04:27.640,0:04:28.960
舉例來說

0:04:28.960,0:04:32.060
假設我們現在，有一張圖片

0:04:32.060,0:04:34.760
那第一個 hidden layer 的某一個 neuron

0:04:34.760,0:04:35.960
它的工作是

0:04:35.960,0:04:38.920
要偵測有沒有鳥嘴的存在

0:04:38.920,0:04:40.500
那你可能

0:04:40.500,0:04:42.260
有一些 neuron 偵測有沒有鳥嘴的存在

0:04:42.260,0:04:43.960
有一些 neuron 偵測有沒有爪子的存在

0:04:43.960,0:04:46.360
有一些 neuron 偵測有沒有翅膀的存在

0:04:46.360,0:04:48.060
有沒有尾巴的存在，之後合起來

0:04:48.060,0:04:50.240
就可以偵測，圖片中有沒有一隻鳥

0:04:50.240,0:04:52.620
假設有某一個 neuron 它的工作是

0:04:52.620,0:04:55.180
要偵測有沒有鳥嘴的存在

0:04:55.180,0:04:57.620
那它其實並不需要看整張圖

0:04:57.620,0:04:59.640
因為，其實我們只要給

0:04:59.640,0:05:02.080
neuron 看這樣一個小的部分

0:05:02.080,0:05:04.280
這個小的紅色框框裡面的區域

0:05:04.280,0:05:06.300
它其實就可以知道說

0:05:06.300,0:05:08.180
它是不是一個鳥嘴

0:05:08.180,0:05:10.800
對人來說也是一樣，只要看這個小的區域你就會知道說

0:05:10.800,0:05:14.920
這是鳥嘴，不需要看整張圖才知道這件事情

0:05:15.340,0:05:18.260
所以，每一個 neuron 其實只要連接到

0:05:18.260,0:05:21.320
一個小塊的區域就好，它不需要連接到

0:05:21.320,0:05:22.840
整張完整的圖

0:05:24.280,0:05:26.220
那第二個觀察是這樣子

0:05:26.740,0:05:29.560
同樣的這個 pattern

0:05:29.560,0:05:32.380
在 image 裡面，它可能會出現在

0:05:32.380,0:05:35.080
image 的不同的部分

0:05:35.080,0:05:38.120
它會出現在 image 的不同的部分

0:05:38.120,0:05:41.860
但是，它們代表的是同樣的含意，它們也有同樣的形狀

0:05:41.860,0:05:44.940
它們可以用同樣的 neuron

0:05:44.940,0:05:47.560
同樣的參數，然後，detector 就可以偵測出來

0:05:47.560,0:05:51.060
比如說，在這張圖裡面，有一個在左上角的鳥嘴

0:05:51.060,0:05:53.660
那這張圖裡面，有一個在

0:05:53.660,0:05:55.220
中央的鳥嘴

0:05:55.220,0:05:59.340
但是，你並不需要說，我去訓練兩個不同的 detector

0:05:59.340,0:06:01.500
一個是專門偵測左上角有沒有鳥嘴

0:06:01.560,0:06:03.940
一個是偵測中央有沒有鳥嘴

0:06:03.940,0:06:05.540
如果這樣做的話呢，它就

0:06:05.540,0:06:09.280
太冗了，所以，我們要 cost down

0:06:09.280,0:06:13.480
所以，它不需要太多的冗員，因為這個

0:06:13.480,0:06:16.080
這個 neuron，偵測左上角鳥嘴的 neuron

0:06:16.080,0:06:18.600
跟中央的偵測有沒有鳥嘴的 neuron，他們

0:06:18.600,0:06:21.420
做的事情，其實可能就是一樣的

0:06:21.420,0:06:23.760
所以，我們並不需要有兩個 neuron

0:06:23.760,0:06:26.380
兩組參數，來做

0:06:26.380,0:06:29.700
duplicate 的事情，所以，我們可以

0:06:29.700,0:06:34.060
要求這兩個 neuron，他們就用同一組參數

0:06:34.060,0:06:37.260
它們就 share 它們的參數，它們就共用同一組參數

0:06:37.260,0:06:41.040
這樣你就可以減少，你需要的參數的量

0:06:41.640,0:06:44.700
那第三個，是我們知道一個 image

0:06:44.700,0:06:47.200
你可以對它做 subsampling

0:06:47.200,0:06:50.020
你把一張 image 的奇數行

0:06:50.020,0:06:51.600
偶數列的 pixel 拿掉

0:06:51.600,0:06:54.500
變成原來的 1/10 的大小

0:06:54.500,0:06:57.720
它其實部會影響人對這張 image 的理解

0:06:57.720,0:07:00.600
對你來說，這張 image 跟這張 image

0:07:00.600,0:07:02.340
看起來可能沒有太大差別

0:07:02.340,0:07:04.480
你都可以辨識裡面有甚麼物件

0:07:04.480,0:07:06.900
所以，做 subsampling 這件事情

0:07:06.900,0:07:11.220
對影像辨識來說，能是沒有太大的影響的

0:07:11.220,0:07:16.600
所以，我們可以用這個概念把 image 變小

0:07:16.600,0:07:19.480
這樣，你就可以減少你需要用的參數

0:07:19.480,0:07:22.180
所以，整個 CNN 的架構是這樣

0:07:22.180,0:07:24.560
等一下，我們會一個一個解釋

0:07:24.560,0:07:26.420
在 CNN 的整個架構裡面

0:07:26.420,0:07:29.460
每一個 block 它做的事情是什麼

0:07:32.080,0:07:34.660
首先，input 一張 image 以後呢

0:07:34.660,0:07:36.360
首先，這個 image 會通過

0:07:36.360,0:07:38.320
convolution 的 layer

0:07:38.320,0:07:41.820
然後，接下來，做 Max pooling 這件事

0:07:41.820,0:07:43.820
那這每一件事，我們等一下都會解釋

0:07:43.820,0:07:45.820
然後，再做 convolution 這件事

0:07:45.820,0:07:48.240
然後，再做 Max Pooling 這件事

0:07:48.240,0:07:51.340
那這個 process 可以反覆數次

0:07:51.340,0:07:53.800
那反覆的次數，你覺得夠多以後呢

0:07:53.800,0:07:56.200
但反覆幾次這個你要事先決定

0:07:56.200,0:07:59.060
它就是 network 的架構，就好像 network 有幾層一樣

0:07:59.060,0:08:01.660
你要做幾次 convolution，做幾次 Max Pooling

0:08:01.660,0:08:04.460
你在定你那個 network的架構時，就要事先決定好

0:08:05.240,0:08:06.920
那你做完

0:08:06.920,0:08:09.240
你做完你決定要做的 convolution 和

0:08:09.240,0:08:11.480
Max Pooling 的次數以後，那你會做另外一件事情

0:08:11.480,0:08:13.140
你要做一件事情叫 Flatten

0:08:13.140,0:08:15.560
那做完 Flatten 以後，你把 Flatten output

0:08:15.560,0:08:18.720
丟到一般的 fully connected 的 network 裡面去

0:08:18.720,0:08:22.180
然後，得到最後影像辨識的結果

0:08:22.720,0:08:24.180
那我們剛才講說

0:08:24.180,0:08:26.660
這個我們基於

0:08:26.660,0:08:29.200
3 個對影像處理的觀察

0:08:29.200,0:08:31.360
所以，設計了 CNN 這樣的架構

0:08:31.360,0:08:33.060
那第一個觀察是

0:08:33.060,0:08:34.060
同樣的 pattern

0:08:34.060,0:08:36.700
要偵測一個 pattern，你不需要看整張 image

0:08:36.700,0:08:38.700
你只要看 image 的一個小部分

0:08:38.700,0:08:40.580
第二個是，同一個 pattern 它

0:08:40.580,0:08:43.480
不出現在一張圖片的不同的區域

0:08:43.480,0:08:45.600
第三個是，我們可以做 subset

0:08:45.600,0:08:47.600
前面這兩個 property 呢

0:08:47.600,0:08:49.720
它就是用 convolution 的 layer

0:08:49.720,0:08:50.760
來處理掉

0:08:50.760,0:08:52.320
最後這個 property

0:08:52.320,0:08:55.480
是用 Max Pooling

0:08:55.480,0:08:58.640
來處理，那等一下，我們就要介紹一下

0:08:58.640,0:09:01.820
每一個 layer 在做的事情

0:09:01.820,0:09:05.700
那我們就先從 convolution 開始看起

0:09:07.440,0:09:09.320
假設現在

0:09:09.320,0:09:14.520
我們的 network 的 input 是一張 6*6 的 image

0:09:14.520,0:09:16.900
假設這是黑白的

0:09:16.900,0:09:20.400
如果是黑白的，每一個 pixel 就只需要用一個 value

0:09:20.400,0:09:21.540
來描述它

0:09:21.540,0:09:23.960
比如說，1 就代表有塗墨水

0:09:23.960,0:09:27.200
0 就代表，沒有塗到墨水

0:09:27.200,0:09:30.240
那在 convolution layer 裡面

0:09:30.240,0:09:33.340
它有一組 Filter

0:09:33.340,0:09:34.980
它有一堆 filter

0:09:34.980,0:09:37.540
那這邊的每一個 filter

0:09:37.540,0:09:40.360
我等一下會講說，它其實就等同於是

0:09:40.360,0:09:42.760
fully connected layer 裡面的一個 neuron

0:09:42.760,0:09:45.660
我們等一下會講，現在先想說，我們有

0:09:45.660,0:09:47.500
一組 filter

0:09:47.500,0:09:51.080
那每一個 filter，其實就是一個 matrix

0:09:51.080,0:09:52.680
比如說，這邊的每一個 filter

0:09:52.680,0:09:55.180
都是 3*3 的 matrix

0:09:55.580,0:09:58.780
那這邊的每一個 filter 裡面的參數

0:09:58.780,0:10:01.540
這個 matrix 裡面的每一個 element 的值

0:10:01.540,0:10:03.360
就是 network 的 parameter

0:10:03.360,0:10:07.480
它就跟那些 neuron 的 weight 跟 bias 一樣，它們是

0:10:07.480,0:10:09.660
network 的 parameter

0:10:09.660,0:10:11.820
它們是必須學出來的

0:10:11.820,0:10:14.080
根據 training data 學出來，並不是人去設計的

0:10:14.180,0:10:15.600
所以，每一個 filter

0:10:15.600,0:10:17.040
它裡面的值是做甚麼

0:10:17.040,0:10:18.840
它裡面的值是什麼，要做甚麼事情

0:10:18.840,0:10:20.360
也是自動被學出來的

0:10:21.380,0:10:23.400
這邊呢，我們

0:10:23.400,0:10:26.360
每一個 filter，如果你是 3*3 的 size

0:10:26.360,0:10:29.140
意味著它就是在偵測

0:10:29.140,0:10:32.440
一個 3*3 的 pattern

0:10:32.440,0:10:34.760
它在偵測一個 pattern 的時候，它不看整張 image

0:10:34.760,0:10:37.280
它只看一個 3*3 的範圍內

0:10:37.280,0:10:40.200
就可以決定有沒有某一個 pattern 的出現

0:10:40.200,0:10:43.220
這個就是我們考慮的第一個 property

0:10:43.220,0:10:46.360
整個 pattern 其實比整張 image 還要小

0:10:47.400,0:10:50.700
那這個 filter 怎麼跟這個 image

0:10:50.700,0:10:52.360
做 operation 呢

0:10:52.360,0:10:55.280
這個 operation 的 process 是這樣

0:10:55.280,0:10:57.300
首先，你有第一個 filter

0:10:57.300,0:11:00.480
它是一個 3*3 的 matrix

0:11:00.480,0:11:05.580
那你把這個 filter 放在 image 的左上角

0:11:05.580,0:11:07.820
那我猜你可能會很困惑說

0:11:07.820,0:11:11.160
突然出現甚麼 filter 阿，放在左上角阿

0:11:11.160,0:11:13.080
跟我們之前講的 neural network 都不一樣

0:11:13.080,0:11:15.880
等一下其實會告訴你說，這個就是一個 neural network

0:11:15.880,0:11:18.680
那 training 就跟之前那個 Backpropagation 是一模一樣的

0:11:18.680,0:11:22.400
那我們現在只是從 convolution 的角度來看

0:11:22.400,0:11:23.620
它是怎麼運作的

0:11:23.620,0:11:25.160
它運作的方式是這樣

0:11:25.160,0:11:29.540
我們把一個 filter 放在一個 image 的左上角

0:11:29.540,0:11:32.020
然後，你把這 9 個值

0:11:32.020,0:11:33.760
跟這 9 個值

0:11:33.760,0:11:35.400
做內積

0:11:35.400,0:11:38.180
所以，內積的結果，這邊是 1, 1, 1

0:11:38.180,0:11:41.340
這邊是1, 1, 1，內積的結果你就得到 3

0:11:42.200,0:11:45.540
接下來，你挪動一下，你的 filter 的位置

0:11:45.540,0:11:48.600
那至於要挪多少，這個你要事先決定好

0:11:48.600,0:11:51.380
那這個挪動的距離

0:11:51.380,0:11:54.200
欸，大家有問題嗎？

0:11:54.200,0:11:56.960
講道這邊，如果大家沒有問題的話，我們就繼續

0:11:56.960,0:11:58.440
這個挪動的距離

0:11:58.440,0:12:00.860
叫做 stride

0:12:00.860,0:12:03.840
這個 stride 等於多少，你要自己設啦

0:12:03.840,0:12:05.240
所以，你設 stride = 1

0:12:05.240,0:12:07.640
那 filter 就放在這邊，所以，1, 1, 1

0:12:07.640,0:12:09.280
乘上 -1, -1, 1

0:12:09.280,0:12:11.160
心算一下，得到是 -1

0:12:11.420,0:12:13.040
你可以設 stride = 2

0:12:13.040,0:12:15.320
stride = 2 的話，你就得到 -3

0:12:15.320,0:12:17.260
那 filter 移到這邊，1, 1, 1

0:12:17.260,0:12:19.240
-1, -1, -1 得到 -3

0:12:20.400,0:12:22.960
那我們等一下就設 stride = 1

0:12:22.960,0:12:24.320
所以，設 stride = 1

0:12:24.320,0:12:27.400
你把 filter 往右移一格，得到 -1

0:12:27.400,0:12:30.860
往右移一格，得到 -1；再往右移一格，得到 -3

0:12:30.860,0:12:32.960
再往右移一格，得到

0:12:32.960,0:12:36.700
這邊是 1, 1, 1，這邊是  -1, 1, -1，得到  -1

0:12:36.700,0:12:40.340
然後，你接下來往下移一格

0:12:40.920,0:12:43.160
得到 -3，就這樣，以此類推

0:12:43.160,0:12:45.640
把這件事情，就每一次都

0:12:45.640,0:12:48.280
stride = 1，就每次都移動一格

0:12:48.280,0:12:50.700
那最後，你就會得到一個

0:12:50.700,0:12:53.680
最後，直到你把 filter 移到右下角的時候

0:12:53.680,0:12:55.960
這邊是 1, 1, 1，這邊是 -1, 1, -1

0:12:56.400,0:12:58.700
你就得到 -1

0:12:58.700,0:13:01.000
所以，做這件事情以後

0:13:01.000,0:13:03.340
本來是一個 6*6 的 matrix

0:13:03.340,0:13:06.420
經過這個 convolution process，就得到一個

0:13:06.420,0:13:08.120
4*4 的 matrix

0:13:08.460,0:13:10.720
如果你看這個 filter 的值

0:13:10.720,0:13:13.400
它的斜對角的地方是 1, 1, 1

0:13:13.400,0:13:16.680
所以，它的工作就是 detect 有沒有 1, 1, 1

0:13:16.680,0:13:19.740
有沒有這個連續的，左上到右下的 1, 1, 1

0:13:19.740,0:13:21.860
出現在這個 image 裡面

0:13:21.860,0:13:24.900
比如說，它出現在這個地方

0:13:24.900,0:13:28.000
比如說，它出現在這個地方

0:13:28.000,0:13:30.260
所以，這個 filter 就會告訴你說

0:13:30.260,0:13:33.560
你看現在左上跟左下

0:13:33.560,0:13:35.020
出現最大的值

0:13:35.020,0:13:37.200
就代表說，這個 filter 要偵測的 pattern

0:13:37.200,0:13:39.920
它出現在這個 image 的左上角

0:13:40.380,0:13:43.500
所以，左上角最大值跟左下角

0:13:43.500,0:13:45.580
有最大的值，這件事情呢

0:13:45.580,0:13:47.260
就考慮了 property 2

0:13:47.260,0:13:49.000
因為，同一個 pattern

0:13:49.000,0:13:52.360
它出現在左上角的位置，跟出現在左下角的位置

0:13:52.360,0:13:54.900
我們都用 filter 1 就可以

0:13:54.900,0:13:58.080
偵測出來，我們不需要用不同的 filter 來做這件事情

0:13:58.080,0:14:00.840
我們用同一個 filter 就可以把它偵測出來

0:14:01.740,0:14:04.960
在一個 convolution 的 layer 裡面，它會有

0:14:04.960,0:14:07.360
一打 filter，它會有一大堆的 filter

0:14:07.360,0:14:09.500
那剛才只是一個 filter 的結果

0:14:09.500,0:14:12.800
會有另外一個 filter，它裡面會有不一樣的參數

0:14:12.800,0:14:14.380
比如說，這裡會有一個 filter 2

0:14:14.380,0:14:17.460
會有不一樣的參數，那它也做跟 filter 1

0:14:17.460,0:14:19.060
一模一樣的事情

0:14:19.060,0:14:21.240
你就先把 filter 2 放在左上角

0:14:21.240,0:14:23.340
再做 inner product 得到 -1

0:14:23.340,0:14:28.880
再挪動一格，得到  -1，就這樣，以此類推

0:14:29.420,0:14:33.060
所以，你把 filter 2 跟 input 的 image

0:14:33.060,0:14:35.020
做完 convolution 以後

0:14:35.020,0:14:39.260
你就得到另外一個 4*4 的 matrix

0:14:39.260,0:14:41.480
你就得到另外一個 4*4 的 matrix

0:14:41.480,0:14:43.620
那這個紅色的 4*4 的 matrix 跟

0:14:43.620,0:14:45.600
藍色的 4*4 的 matrix 合起來

0:14:45.600,0:14:48.560
就叫做，他們叫做 Feature Map

0:14:48.560,0:14:50.040
這個東西叫做 Feature Map

0:14:50.040,0:14:55.980
那看你有幾個 filter，你就會得到多少的 image

0:14:55.980,0:15:00.200
比如說，你有 100 個 filter

0:15:00.200,0:15:04.440
那這邊你會得到 100 個 image

0:15:04.440,0:15:07.380
講到這邊大家有沒有甚麼問題想要問呢？

0:15:17.180,0:15:20.820
沒錯，因為現在每一個 filter size 都是一樣的

0:15:20.820,0:15:23.340
意味著說，如果你今天有同一個 pattern

0:15:23.340,0:15:25.800
它有不同的 size，有大的鳥嘴，也有小的鳥嘴

0:15:25.800,0:15:27.480
它必須要用

0:15:27.480,0:15:31.700
如果你可以事先知道的話，你當然可以用 normalize

0:15:31.700,0:15:34.100
但是，input 一張 image，你沒有辦法事先知道

0:15:34.100,0:15:35.720
知道它是大的鳥嘴還是小的鳥嘴，所以

0:15:35.720,0:15:37.160
你也沒辦法 normalize

0:15:37.160,0:15:41.460
所以，CNN 並不能夠自動處理這個問題

0:15:41.460,0:15:43.300
對它來說，這種不同 scale 的東西，它其實

0:15:43.300,0:15:44.680
不見得能夠處理

0:15:44.680,0:15:48.760
那你其實可以看看這個

0:15:50.340,0:15:52.480
DeepMind 最近有解一篇 paper

0:15:52.480,0:15:55.480
是當你 input 一張 image 的時候

0:15:55.480,0:15:58.220
它在 CNN 前面，再接另外一個 network

0:15:58.220,0:15:59.940
這個 network 做的事情是甚麼呢？

0:15:59.940,0:16:01.460
這個 network 做的事情是

0:16:01.460,0:16:02.720
它 output 一些 scalar

0:16:02.720,0:16:05.200
告訴你說，它要把這個 image 的

0:16:05.200,0:16:06.620
裡面的哪些位置

0:16:06.620,0:16:08.560
做旋轉、縮放

0:16:08.560,0:16:10.100
然後，再丟到 CNN 裡面

0:16:10.100,0:16:12.360
這樣你其實會得到比較好的結果

0:16:14.000,0:16:15.920
大家還有甚麼問題嗎？

0:16:18.320,0:16:20.400
那剛才舉的例子是

0:16:20.400,0:16:24.020
黑白的 image，所以你 input 的是一個 matrix

0:16:24.280,0:16:26.940
如果今天是彩色的 image 怎麼樣呢？

0:16:26.940,0:16:28.900
我們知道彩色的 image 就是

0:16:28.900,0:16:30.480
RGB 組成的

0:16:30.480,0:16:32.560
對不對，RGB 組成的

0:16:34.840,0:16:38.940
所以一個彩色的 image，它就是好幾個 matrix 疊在一起

0:16:38.940,0:16:41.580
它就是一個立方體

0:16:41.580,0:16:44.260
如果我今天要處理彩色的 image

0:16:44.260,0:16:46.080
我們要怎麼做呢？

0:16:46.080,0:16:49.060
這個時候你的 filter，就不是一個 matrix

0:16:49.060,0:16:51.560
你的 filter 也是一個立方體

0:16:51.560,0:16:56.880
如果你今天是 RGB 這三個顏色來表示

0:16:56.880,0:16:58.520
一個 pixel 的話

0:16:58.520,0:17:01.620
那你的 input 就是 3*6*6

0:17:01.620,0:17:04.380
那你的 filter 就是 3*3*3

0:17:04.380,0:17:06.480
你的 filter 的高就是 3

0:17:06.480,0:17:08.600
你的 filter 高就是 3

0:17:08.600,0:17:10.960
你在做這個 convolution 的時候

0:17:10.960,0:17:13.900
你就是把這個 filter 的 9 個值

0:17:13.900,0:17:17.260
跟這個 image 裡面的 9 個值

0:17:17.260,0:17:18.780
做內積

0:17:18.780,0:17:23.040
了解嗎？我們並不是把每一個

0:17:23.040,0:17:26.120
把每一個 RGB 顏色都分開算

0:17:26.120,0:17:28.280
那其實這個在影像上，我們叫做 channel

0:17:28.280,0:17:30.920
並不是把每一個 channel 分開來算

0:17:30.920,0:17:33.080
而是，合在一起算

0:17:33.080,0:17:35.000
每一個 filter 同時就考慮了

0:17:35.380,0:17:38.520
不同顏色所代表的 channel

0:17:38.520,0:17:40.920
講道這邊大家聽得懂嗎？

0:17:41.700,0:17:45.380
那我們現在要講的東西就是

0:17:45.380,0:17:48.620
這個 convolution 跟 fully connected

0:17:48.620,0:17:50.020
有什麼關係？

0:17:50.020,0:17:52.460
你可能覺得說，它是一個很特別的 operation

0:17:52.460,0:17:54.520
感覺跟 neural network 沒半毛錢的關係

0:17:54.520,0:17:57.160
其實，它就是一個 neural network

0:17:57.460,0:17:59.620
怎麼說呢？我現在要講的就是

0:17:59.620,0:18:02.220
這個 convolution，這件事情

0:18:02.220,0:18:04.440
其實，它就是一個

0:18:04.440,0:18:07.780
fully connected 的 layer 把一些 weight 拿掉而已

0:18:07.780,0:18:08.960
怎麼說呢？

0:18:08.960,0:18:11.560
我們假設這邊這個 output

0:18:11.560,0:18:13.080
這個 feature map 的 output

0:18:13.080,0:18:15.100
其實就是

0:18:15.100,0:18:18.280
一個 hidden layer 的 neuron 的 output

0:18:18.280,0:18:21.440
如果你把這兩個東西 link 在一起的話

0:18:21.440,0:18:25.240
這個 convolution 其實就是一個

0:18:25.240,0:18:28.280
fully connected 的 layer 拿掉一些 weight 的結果

0:18:28.280,0:18:29.900
怎麼說呢？

0:18:29.900,0:18:32.500
我們在做 convolution 的時候

0:18:32.500,0:18:35.600
我們把 filter，假設我們今天考慮 filter 1

0:18:35.600,0:18:37.800
我們把 filter 1 放在左上角

0:18:37.800,0:18:40.920
然後，你做 inner product，得到一個值 3

0:18:40.920,0:18:42.960
這件事情等同於

0:18:42.960,0:18:45.680
我們現在把這個 6*6 的 image 拉直

0:18:45.680,0:18:47.260
拉直變成這樣子

0:18:49.480,0:18:54.240
然後，你有一個 neuron 的 output 是 3

0:18:54.240,0:18:56.640
那這個 neuron 的 output 怎麼來的呢？

0:18:56.640,0:18:59.280
這個 neuron 的 output 它是考慮了

0:18:59.280,0:19:00.700
你看這個 filter

0:19:00.700,0:19:02.520
它把它放在左上角

0:19:02.520,0:19:04.240
它在左上角

0:19:04.240,0:19:07.500
所以，這個 filter，它考慮的 pixel

0:19:07.500,0:19:11.060
是 1, 2, 3、1, 2, 3

0:19:11.060,0:19:12.660
這邊是 4, 5, 6

0:19:12.660,0:19:13.860
欸，不是 4, 5, 6

0:19:13.860,0:19:16.560
這邊 1, 2, 3, 4, 5, 6，所以這邊是 7, 8, 9

0:19:16.560,0:19:17.380
這邊是 7, 8, 9

0:19:17.480,0:19:21.060
這邊是 13, 14, 15

0:19:21.060,0:19:25.420
假設你把這一個 6*6 的 image 的 36 個 pixel 拉成

0:19:25.420,0:19:26.620
直的

0:19:26.620,0:19:29.380
那這 9 個 pixel 分別就是

0:19:29.380,0:19:32.100
編號 1, 2, 3 的 pixel 跟 7, 8, 9 的 pixel

0:19:32.100,0:19:33.640
跟 13, 14, 15 的 pixel

0:19:33.640,0:19:37.220
那如果我們說這個 filter 做 inner product 以後的 output 3

0:19:37.220,0:19:39.440
就是某一個 neuron 的 output 3 的話

0:19:39.440,0:19:41.460
就代表說有 1 個 neuron

0:19:41.460,0:19:42.480
這個 neuron 的 weight

0:19:42.480,0:19:44.820
只連接到 1, 2, 3

0:19:44.820,0:19:47.300
7, 8, 9 跟 13, 14, 15

0:19:47.300,0:19:49.340
這 3 個 pixel 而已

0:19:49.340,0:19:52.480
而這個 neuron 它的這 9 個 weight

0:19:52.480,0:19:55.880
就是 filter 這個 matrix 裡面的這 9 個 weight

0:19:55.880,0:19:59.040
花很多時間故意把它用同樣的顏色

0:19:59.040,0:20:01.500
這樣看的出來嗎？這個 1 是

0:20:01.500,0:20:03.960
框棕色，所以這邊才是棕色的

0:20:03.960,0:20:06.780
這個框紅色，這真的搞很久，所以這是紅色

0:20:06.780,0:20:08.980
這個橙色的

0:20:08.980,0:20:12.040
這邊有 9 個不同的顏色，所以

0:20:12.040,0:20:15.540
這 9 個不同顏色裡面框起來的 weight

0:20:15.540,0:20:19.020
分別就是這 9 個不同的 weight

0:20:19.020,0:20:21.060
大家看的懂嗎？

0:20:21.740,0:20:23.480
所以，現在是這樣子的

0:20:23.480,0:20:25.580
我們應該在 fully connected 的 layer 裡面

0:20:25.580,0:20:28.940
一個 neuron，照理說是連接到所有的 input

0:20:28.940,0:20:32.060
你有 36 個 pixel 當作 input

0:20:32.060,0:20:36.460
那這個 neuron 本來應該連接到 36 個 input

0:20:36.460,0:20:39.520
但是，我們現在只連接 9 個 input

0:20:39.520,0:20:41.900
只連接 9 個 input，因為我們知道說

0:20:41.900,0:20:44.200
要 detect 一個 pattern，我們不需要看整張 image

0:20:44.200,0:20:46.300
我就看 9 個 input 就好

0:20:46.300,0:20:49.920
所以，當我們這麼做的時候，你就用了

0:20:49.920,0:20:53.380
比較少的參數，你就用了比較少的參數

0:20:54.080,0:20:59.040
當我們移動一格，把 filter 做 stride = 1 的移動的時候

0:20:59.040,0:21:00.120
發生甚麼事呢？

0:21:00.120,0:21:01.920
我們得到另外一個值 -1

0:21:01.920,0:21:05.680
我們假設這個 -1 是另外一個

0:21:05.680,0:21:07.800
neuron 的 output

0:21:07.800,0:21:10.940
那這個 neuron 連接到哪些 input 的 weight 呢

0:21:10.940,0:21:13.560
這一個框起來的地方啊

0:21:13.560,0:21:18.420
這個框起來的地方，它正好就對應到 pixel 2, 3, 4

0:21:18.420,0:21:22.180
2, 3, 4，這個是 8, 9, 10

0:21:22.180,0:21:25.200
8, 9, 10 跟 14, 15, 16

0:21:25.200,0:21:27.540
跟 14, 15, 16，你會發現說這邊呢

0:21:27.540,0:21:31.120
同樣的 weight 代表同一個顏色，所以

0:21:31.120,0:21:33.340
這 9 個 matrix

0:21:33.340,0:21:36.460
這 9 個在這個 filter 所對應的matrix 裡面的 weight

0:21:36.460,0:21:39.840
就是這個圖上 9 個不同的顏色

0:21:39.840,0:21:43.040
所以，當我們做這件事情的時候，意味著說

0:21:43.040,0:21:44.380
這裡個 neuron

0:21:44.380,0:21:46.500
本來在 fully connected layer 裡面

0:21:46.500,0:21:49.100
每一個 neuron 都有獨立、自己的 weight

0:21:49.100,0:21:51.800
但是，當我們做這個

0:21:51.800,0:21:54.180
convolution 的時候

0:21:54.180,0:21:58.420
首先，我們把每一個 neuron 它前面連接的 weight 減少

0:21:58.420,0:22:00.320
再來，我們強迫說

0:22:00.320,0:22:02.300
某些 neuron、這兩個 neuron

0:22:02.300,0:22:04.840
他們一定要共用同一組 weight

0:22:04.840,0:22:06.940
他們的 weight 永遠都必須是一樣的

0:22:06.940,0:22:09.900
它連接到 pixel 1 的 weight

0:22:09.900,0:22:13.860
要等於它連接到 pixel 2 的 weight

0:22:13.860,0:22:15.600
它連接到 pixel 2 的 weight

0:22:15.600,0:22:17.840
要等於它連接到 pixel 3 的 weight

0:22:17.840,0:22:20.400
它連接到 pixel 3 的 weight，
要等於它連接到 pixel 4 的 weight

0:22:20.400,0:22:23.980
我故意用同樣的顏色來表示，希望你可以看的出來

0:22:24.820,0:22:27.860
這件事情叫做 weight 的 sharing

0:22:27.860,0:22:30.540
當我們做這件事情的時候，我們用

0:22:30.540,0:22:32.140
用的這個參數呢

0:22:32.140,0:22:34.680
又比原來又更少

0:22:34.680,0:22:36.300
我們強迫一些 weight

0:22:36.300,0:22:37.860
一定要 share 在一起

0:22:37.860,0:22:40.020
我們用的 weight 就更少

0:22:41.020,0:22:43.200
這邊有一些事情是投影片上沒有講的

0:22:43.200,0:22:44.300
可能會問說

0:22:44.300,0:22:46.260
這怎麼 train

0:22:46.260,0:22:49.200
首先，第一件事情就是這都是用 toolkit 做的

0:22:49.200,0:22:50.320
所以，你大概不會自己寫

0:22:50.320,0:22:51.820
如果你要自己寫的話

0:22:51.820,0:22:54.620
它的做法就是，你就跟原來的 Backpropagation

0:22:54.620,0:22:56.100
用一模一樣的作法

0:22:56.100,0:22:57.760
只是有一些 weight

0:22:57.760,0:23:00.020
就永遠是 0，你就不 train 它，它就永遠是 0

0:23:00.460,0:23:01.460
然後，再來呢

0:23:01.460,0:23:04.900
你說怎麼讓這個 weight 的值永遠都是一樣的

0:23:04.900,0:23:07.560
你就用一般的 Backpropagation 的方法

0:23:07.560,0:23:10.080
然後，這個 weight 你算出一個 gradient

0:23:10.080,0:23:11.580
這個 weight 你算出一個 gradient

0:23:11.580,0:23:13.440
再把本來要 tight 在一起

0:23:13.440,0:23:16.440
要 share weight 的那些 weight 的  gradient 平均

0:23:16.440,0:23:19.440
然後，他們 update 同樣的值，就結束了

0:23:20.560,0:23:21.900
你聽不懂沒有關係，因為

0:23:21.900,0:23:24.100
你大概沒有機會自己實作這個東西

0:23:26.780,0:23:29.500
如果你真的有空的話，等一下下課可以跟我討論

0:23:32.320,0:23:33.380
這件事情呢

0:23:33.380,0:23:36.280
再來，接下來要做 Max pooling

0:23:36.280,0:23:37.540
Max pooling 是甚麼呢？

0:23:37.540,0:23:39.980
相較於 convolution，max pooling 是比較簡單的

0:23:39.980,0:23:41.600
它就是做 subsampling

0:23:41.600,0:23:43.000
根據 filter 1

0:23:43.000,0:23:46.500
我們得到一個 4*4 的 matrix

0:23:46.500,0:23:49.460
根據 filter 2，你得到另外一個 4*4 的 matrix

0:23:49.460,0:23:51.200
接下來，我們做甚麼事呢？

0:23:51.200,0:23:54.180
我們說，把這個

0:23:54.180,0:23:56.940
output，4 個一組，4 個一組

0:23:56.940,0:23:58.400
4 個一組，4 個一組

0:23:58.400,0:24:01.580
每一組裡面，你可以選他們的平均

0:24:01.580,0:24:03.760
或者是你可以選最大，其實都可以

0:24:03.760,0:24:08.080
你就是把原來 4 個 value 合成 1 個 value

0:24:08.080,0:24:10.160
那你可以用自己想要的方法來做這件事情

0:24:10.160,0:24:12.420
這件事情就可以讓你的 image 縮小

0:24:12.420,0:24:15.520
比如說，假設我們現在就是選 4 個裡面

0:24:15.520,0:24:17.300
最大的那個保留下來

0:24:17.300,0:24:20.140
所以，3, -1, 3, 1，我就選 3

0:24:21.080,0:24:24.560
就選最大的保留下來，那你這邊可能會有一個問題就是

0:24:24.560,0:24:27.940
把這個東西放到 network 裡面，你不就沒辦法微分了嗎

0:24:28.100,0:24:30.480
max 這個東西，感覺不能微分阿

0:24:30.480,0:24:32.220
它其實可以的

0:24:32.220,0:24:34.380
我們之後講 Maxout network 的時候，再來跟大家講

0:24:34.380,0:24:38.820
這個，其實有辦法用微分的方式來處理它

0:24:40.420,0:24:42.880
所以，結論是這樣

0:24:42.880,0:24:44.040
結論是這樣

0:24:44.040,0:24:48.060
我們現在，這邊有 4 個值，這邊有 4 個值

0:24:48.060,0:24:50.600
所以，做完一次 convolution

0:24:50.600,0:24:52.120
加一次 Max pooling

0:24:52.120,0:24:54.760
我們就把原來 6*6 的 image

0:24:54.760,0:24:57.340
變成一個 2*2 的 image

0:24:57.340,0:25:01.080
至於這一個 2*2 的 image，它每一個 pixel 的深度

0:25:01.080,0:25:02.620
它的這個深度

0:25:02.620,0:25:05.180
每一個 pixel 用幾個 value 來表示

0:25:05.180,0:25:07.920
depend on 你有幾個 filter，如果你有 50 個 filter

0:25:07.920,0:25:09.420
這邊就是 50 維

0:25:09.420,0:25:12.080
兩個 filter，這邊就 2 維

0:25:12.080,0:25:13.680
所以，這個就是一個

0:25:13.680,0:25:16.700
新的，但是比較小的 image

0:25:16.700,0:25:19.400
那每一個 filter 就代表了一個 channel

0:25:19.400,0:25:22.900
這一件是可以 repeat 很多次

0:25:22.900,0:25:25.100
你得到一個、你通過一個 convolution

0:25:25.100,0:25:27.240
再加一個 Max pooling

0:25:27.240,0:25:31.600
你就得到一個新的 image

0:25:31.600,0:25:33.280
你就得到一個新的 image

0:25:33.280,0:25:36.640
那它是一個比較小的 image

0:25:36.640,0:25:40.240
所以，你可以把這個比較小的 image，再做一樣的事情

0:25:40.240,0:25:41.700
再通過 convolution

0:25:41.700,0:25:46.280
再通過 Max pooling，再得到一個更小的 image

0:25:46.280,0:25:47.660
那我這邊舉的例子

0:25:47.660,0:25:49.220
input image 已經夠小了

0:25:49.220,0:25:51.100
所以，再做一次就不見了

0:25:51.100,0:25:52.540
我就沒有再做一次

0:25:52.700,0:25:56.340
講到這邊，大家可以了解我意思嗎？

0:25:56.340,0:25:57.940
有甚麼問題要問的嗎？

0:25:58.640,0:26:00.800
其實我講到這邊的時候，我常常會

0:26:00.800,0:26:03.220
被問到、遇到一個問題

0:26:03.220,0:26:05.300
大概是我講 deep learning 的 talk 已經太多了

0:26:05.300,0:26:08.780
我都可以 predict 聽眾的問題

0:26:08.780,0:26:12.240
這邊每次都會有人問我一個問題，就是

0:26:14.160,0:26:16.400
如果我這邊有 25 個 filter

0:26:16.400,0:26:18.820
我得到 25 個

0:26:18.820,0:26:20.100
feature map

0:26:20.100,0:26:22.440
假設我第一個 convolution 有 25 個 filter

0:26:22.440,0:26:24.100
我得到 25 個 feature map

0:26:24.100,0:26:26.340
第二個 convolution 也有 25 個 filter

0:26:26.340,0:26:28.000
那這樣做完，我是不是得到

0:26:28.000,0:26:30.220
25 平方個 feature map

0:26:31.180,0:26:32.620
是這樣嗎？

0:26:33.840,0:26:35.860
其實不是這樣，就是說

0:26:35.860,0:26:39.000
你這邊做完一次 convolution，你得到 25 個 feature map

0:26:39.000,0:26:42.240
你這邊做完，還是得到 25 個 feature map

0:26:42.240,0:26:45.280
這樣大家了解我意思嗎？因為你在這邊

0:26:45.280,0:26:46.780
假設你這邊有

0:26:46.780,0:26:48.920
第一層 filter 有兩個好了

0:26:49.780,0:26:52.320
那第二層的 filter

0:26:52.320,0:26:55.400
它在考慮這個 input 的時候

0:26:55.400,0:26:57.060
它是會考慮深度的

0:26:57.060,0:26:59.660
它並不是每一個 channel 分開考慮

0:26:59.660,0:27:01.820
它是一次考慮所有的 channel

0:27:01.820,0:27:04.440
所以，你 convolution 這邊有多少個 filter

0:27:04.440,0:27:05.460
你 output 就有多少個 filter

0:27:05.460,0:27:06.720
你這的25 個 filter

0:27:06.720,0:27:07.780
output 就 25 個 filter

0:27:07.780,0:27:10.880
你這邊有 25 個 filter，output 還是 25 個 filter

0:27:10.880,0:27:12.840
只是這邊的每一個 25 個 filter

0:27:12.840,0:27:15.100
它都是一個 cubic

0:27:15.100,0:27:16.520
它都是一個立方體

0:27:16.520,0:27:21.040
它的高有 25 個 value 那麼高

0:27:21.040,0:27:23.780
這樣大家有問題嗎？

0:27:25.160,0:27:27.460
如果大家沒有問題的話呢

0:27:27.460,0:27:29.620
我們就繼續

0:27:29.620,0:27:32.300
然後，有一個，再來就是

0:27:32.300,0:27:34.700
最後這個 flatten 跟 fully connected 的部分

0:27:34.700,0:27:36.840
這個就很簡單，這個怎麼做呢？

0:27:36.840,0:27:39.100
flatten 的意思就是，我把

0:27:39.100,0:27:41.340
這個 feature map 拉直

0:27:41.340,0:27:43.760
這邊完全沒有學問在，我就把它拉直

0:27:43.760,0:27:47.300
拉直以後，你就把它丟到一個 fully connected 的

0:27:47.300,0:27:50.100
feedforward network，然後

0:27:50.100,0:27:52.360
就沒有然後，就結束了

0:27:53.040,0:27:56.420
接下來，剩下一點時間，我就要

0:27:56.420,0:27:59.140
秒講一下，怎麼用 Keras

0:27:59.140,0:28:01.600
來 implement CNN

0:28:01.600,0:28:05.680
那在 training, compile 跟 fitting 的部分

0:28:05.680,0:28:06.700
其實是一模一樣

0:28:06.700,0:28:08.820
你唯一需要改的只有說

0:28:08.820,0:28:11.540
你要改一下你的 network structure

0:28:11.540,0:28:12.780
還有 input 的 format

0:28:12.780,0:28:15.720
本來在原來的 DNN 裡面，input 是一個 vector

0:28:15.720,0:28:17.600
現在如果是 CNN 的話

0:28:17.600,0:28:20.940
它是會考慮 input 的 image 的幾何空間的

0:28:20.940,0:28:22.480
所以，不能 input 給它一個 vector

0:28:22.480,0:28:28.160
你要 input 給它一個 tensor 這樣

0:28:28.160,0:28:30.200
tensor 就是高維的 vector 啦

0:28:30.200,0:28:31.500
你要給它一個三維的 vector

0:28:31.500,0:28:33.040
為什麼三維的 vector 呢

0:28:33.040,0:28:35.580
因為一個 image 的長寬各是一維

0:28:35.580,0:28:37.880
如果它是彩色的話，RGB 就是第三維

0:28:37.880,0:28:39.780
所以，你要給它三維的 vector

0:28:39.780,0:28:42.020
你 assign 一個三維的 matrix

0:28:42.020,0:28:45.260
這個高維的 matrix 就叫做 tensor

0:28:45.820,0:28:49.160
那怎麼 implement 一個 convolution 的 layer 呢

0:28:49.160,0:28:50.820
你就這麼做

0:28:50.820,0:28:52.300
model.add

0:28:52.300,0:28:56.400
剛才是 dense，現在就改成 convolution2D

0:28:56.400,0:28:58.900
它其實也有 1D 的，現在就改成 2D

0:29:00.220,0:29:03.260
然後，這邊 25, 3, 3 是甚麼意思呢？

0:29:03.260,0:29:05.160
這邊 25, 3, 3 的意思就是說

0:29:05.160,0:29:06.420
你有 25 個

0:29:06.420,0:29:08.240
25 代表 25 個 filter

0:29:08.240,0:29:12.220
後面的 3, 3 就代表，你的 filter 是一個 3*3 的 matrix

0:29:12.220,0:29:15.700
你的 filter 都是 3*3 的 size

0:29:16.460,0:29:17.560
那 input 呢？

0:29:17.560,0:29:19.360
你要告訴它 input shape 要是甚麼樣子

0:29:19.360,0:29:21.420
假設我現在是要做手寫數字辨識

0:29:21.420,0:29:23.340
input 是 28*28 的 image

0:29:23.340,0:29:25.600
它的每一個 pixel 都是

0:29:26.120,0:29:28.420
都是只有單一顏色

0:29:28.420,0:29:32.500
所以，你 input 的 shape 就是 1*28*28

0:29:32.500,0:29:35.560
前面這個 1 阿，如果你是黑白的話，它就是 1

0:29:35.560,0:29:39.500
如果你是彩色的圖，每個 pixel 用 3 個值來表示

0:29:39.500,0:29:43.820
這邊你就要放 3，代表 RGB 3 個顏色

0:29:43.820,0:29:48.140
那 28*28 就代表這個 image 總共 28*28 個 pixel

0:29:48.140,0:29:49.960
那 Max pooling 也很簡單

0:29:49.960,0:29:52.480
就 add 一個 Max pooling 的 layer

0:29:52.480,0:29:54.600
那這邊的 (2, 2) 的意思是說呢

0:29:54.600,0:29:59.260
我們把 2*2 的這個

0:29:59.260,0:30:01.660
feature map 裡面的 pixel 拿出來

0:30:01.660,0:30:05.060
然後，每次就選最大的那一個

0:30:06.400,0:30:09.780
所以，假設我們 input 是一個

0:30:09.780,0:30:11.920
1*28*28 的 image

0:30:11.920,0:30:15.040
那你就這樣寫 add(Convolution2D(25, 3, 3,

0:30:15.040,0:30:17.100
input_shape = (1, 28, 28)))

0:30:17.100,0:30:20.380
那通過 convolution 以後

0:30:20.380,0:30:23.460
你得到的東西是甚麼呢？

0:30:23.460,0:30:25.600
你得到的 output 是甚麼呢？

0:30:25.600,0:30:28.020
你得到的 output，首先，你這個

0:30:28.020,0:30:29.500
channel 的數目會是 25

0:30:29.500,0:30:30.900
也就是 25 個 filter

0:30:30.900,0:30:33.920
你得到的 channel 的數目就是 25

0:30:33.920,0:30:36.860
然後，因為你現在的 filter 的 size 是 3

0:30:36.860,0:30:39.040
所以，本來 28*28 的 image

0:30:39.040,0:30:40.640
通過 3, 3 的 filter

0:30:40.640,0:30:43.540
就假設你那個邊邊的地方沒有考慮的話

0:30:43.540,0:30:46.120
就會變成 26*26

0:30:46.120,0:30:47.580
這樣大家了解我意思嗎？

0:30:47.580,0:30:48.860
當然你也可以考慮邊邊啦

0:30:48.860,0:30:50.220
如果你想要考慮邊邊的話

0:30:50.220,0:30:52.400
你就在原來的 image 旁邊

0:30:52.400,0:30:54.340
補 0，把它變成比較大的 image

0:30:56.620,0:31:00.040
然後，接下來你做 Max pooling

0:31:00.040,0:31:03.180
你做 Max pooling，把 2*2 的

0:31:03.180,0:31:05.500
這個 pixel 一組

0:31:05.500,0:31:06.860
然後，裡面選一個最大的

0:31:06.860,0:31:09.980
做完以後，你就變成 25*13*13

0:31:09.980,0:31:12.400
變 25*13*13

0:31:13.640,0:31:15.780
然後，接下來

0:31:15.780,0:31:19.000
你再做一次 convolution，假設我這次選

0:31:19.000,0:31:20.280
50 個 filter

0:31:20.280,0:31:22.820
然後，每個 filter size 是 3*3 的話

0:31:22.820,0:31:26.640
那現在變成 output 的 channel 就有 50 個

0:31:26.640,0:31:28.400
那 13*13 的 image

0:31:28.400,0:31:30.040
通過 3*3 的 filter

0:31:30.040,0:31:32.660
就會變成 11*11

0:31:32.660,0:31:35.820
我們剛才舉的例子，裡面是 6*6 的 image

0:31:35.820,0:31:39.080
通過 3*3 的 filter，變成 4*4

0:31:39.080,0:31:42.380
所以，你通過 3*3 的 filter 都會是

0:31:42.380,0:31:44.420
長跟寬都會減 2

0:31:44.420,0:31:47.460
所以，本來是 13*13 的 image

0:31:47.460,0:31:50.620
通過 3*3 的 filter，就變成 11*11

0:31:50.620,0:31:54.500
然後，你接下來再做 Max pooling

0:31:54.500,0:31:56.980
2*2 的 Max pooling 變成 50*5*5

0:31:58.920,0:32:02.800
那這邊呢，就問一下大家

0:32:02.800,0:32:06.400
在第一個 convolution

0:32:06.400,0:32:08.280
的 layer 裡面

0:32:08.280,0:32:11.560
每一個 filter，它有多少個參數呢？

0:32:11.560,0:32:12.940
它有多少個參數

0:32:12.940,0:32:15.080
是不是 9 個參數

0:32:15.080,0:32:18.440
因為，它就是一個 3*3 的 matrix 嘛，就是 9 個參數

0:32:18.440,0:32:21.500
但是，在第二個 convolution layer 裡面

0:32:21.500,0:32:23.400
雖然，你每一個 neuron

0:32:23.400,0:32:26.780
每一個 filter 都是 3*3，但它其實不是

0:32:26.780,0:32:28.660
3*3 個參數

0:32:28.660,0:32:31.080
因為，它 input 的 channel 有 25 個

0:32:31.080,0:32:33.880
所以，它的參數是 3*3*25

0:32:33.880,0:32:37.960
它的高是 25，所以這邊是 225

0:32:37.960,0:32:40.000
這樣大家有問題嗎？

0:32:43.100,0:32:45.900
如果大家沒有問題的話

0:32:45.900,0:32:47.160
我們就繼續看下去

0:32:47.160,0:32:51.420
所以，通過兩次 convolution、兩次 Max pooling

0:32:51.420,0:32:53.800
原來 input 是 28*28

0:32:53.800,0:32:56.280
做完這些事以後，變成 50*5*5

0:32:56.280,0:32:57.600
50*5*5

0:32:57.600,0:32:59.040
那做 flatten 就是

0:32:59.040,0:33:01.340
把這一個東西拉直

0:33:01.340,0:33:03.780
那你只要 call 一個 add flatten 就好

0:33:03.780,0:33:04.820
把它拉直

0:33:04.820,0:33:08.260
拉直以後就變成一個 1250 維的 vector

0:33:08.260,0:33:10.280
你再把 1250 維的 vector

0:33:10.280,0:33:12.840
丟到一個 fully connected 的 network 裡面

0:33:12.840,0:33:17.160
fully connected 的 network 大家應該都很熟悉，就跟

0:33:17.160,0:33:19.540
我們剛才 demo 的是一樣的

0:33:19.540,0:33:22.180
然後就得到 output，我們就現場實作

0:33:22.180,0:33:24.480
秒做一下 CNN 這樣

0:33:24.480,0:33:27.080
那這 code 你只要再胡亂改一下，就可以交作業三

0:33:30.440,0:33:32.560
我們的 CNN 的架構

0:33:32.560,0:33:37.200
我們很快、非常非常快地來複習一下

0:33:37.200,0:33:40.580
比如說，我們現在要做手寫數字辨識

0:33:40.580,0:33:44.620
那 input 呢，哦，忘了開聲音

0:33:51.420,0:33:54.660
假設我們要做手寫數字辨識，input 是一張

0:33:54.660,0:33:57.600
28*28 的影像

0:33:57.600,0:34:00.540
然後，通過 convolution layer 以後呢

0:34:00.540,0:34:06.200
假設我們現在有 25 個 filter

0:34:06.200,0:34:09.760
然後，每個 filter 的 size 是 3*3

0:34:09.760,0:34:13.260
那 input 28*28 的 image，它的 output 呢

0:34:13.260,0:34:15.720
通過 3*3 的 filter 以後

0:34:15.720,0:34:19.040
你得到 image size 就是 26*26

0:34:19.040,0:34:22.500
那這個 26*26 的 image，它的每一個 pixel

0:34:22.500,0:34:25.920
都是由一個 25 維的 vector 來表示

0:34:25.920,0:34:28.100
如果你有 25 個 filter 的話呢

0:34:28.100,0:34:30.160
這是 25 維的 vector

0:35:12.540,0:35:14.500
然後做 Max poling

0:35:14.500,0:35:18.320
就把原來 26*26 的 image 變成 13*13

0:35:18.320,0:35:21.880
然後，你可以再加另外一層 convolution

0:35:21.880,0:35:25.340
另外一層 convolution，假設它是 3*3 的話

0:35:25.340,0:35:27.700
做完 convolution 就是 11*11

0:35:27.700,0:35:29.980
因為有 50 個 filter，所以

0:35:29.980,0:35:33.440
你的每一個 pixel 現在是用 50 個 value 來描述

0:35:33.440,0:35:35.240
然後，你可以再做 pooling 等等

0:35:35.240,0:35:39.960
最後，把這個結果接進一個 fully connected layer 得到

0:35:39.960,0:35:42.060
最後的 output，上次也示範說

0:35:42.060,0:35:44.440
如果你要用一個 Keras 來實作一個 CNN 的話

0:35:44.440,0:35:47.100
你要怎麼秒做

0:35:47.100,0:35:49.220
接下來，我想要講的是

0:35:49.220,0:35:51.080
大家、很多人常會說

0:35:51.080,0:35:54.400
deep learning 就是一個黑盒子

0:35:54.400,0:36:00.340
然後，你 learn 完以後，你不知道它得到了甚麼

0:36:00.340,0:36:02.400
所以，有很多人不喜歡這種方法

0:36:02.400,0:36:05.640
其實，我是覺得，今天有一個方法

0:36:05.640,0:36:07.740
它可以讓你輕易地理解

0:36:07.740,0:36:10.260
它為甚麼會下這樣子的判斷

0:36:10.260,0:36:12.660
為甚麼個方法會下這樣子的決策的話

0:36:12.660,0:36:15.900
那其實這個方法，你就會覺得它不夠 intelligent

0:36:15.900,0:36:17.640
因為你可以輕易地理解它

0:36:17.640,0:36:20.340
那今天如果它非常 intelligent 的話

0:36:20.340,0:36:24.700
它就必須要是你無法理解的東西

0:36:24.700,0:36:28.520
這樣，它才夠 intelligent，至少你會感覺它夠 intelligent

0:36:28.520,0:36:31.760
所以，大家常說 deep learning

0:36:31.760,0:36:35.120
就是一個黑盒子，你 learn 出來以後，你根本不知道

0:36:35.120,0:36:36.620
為甚麼是這樣子

0:36:36.620,0:36:39.080
但是，其實還是有很多方法可以分析的

0:36:39.080,0:36:41.200
比如說，今天這邊我們來示範一下

0:36:41.200,0:36:43.840
怎麼分析 CNN

0:36:43.840,0:36:47.480
它到底學到了甚麼

0:36:47.480,0:36:51.300
那我今天的例子，不一樣就是做在 MNIST 上面

0:36:51.300,0:36:53.120
那你其實可以在你的作業三

0:36:53.160,0:36:56.940
在 CIFAR-10 上面呢，複製看看
(一個語料庫的名字)

0:36:56.940,0:36:58.820
你能不能看到類似的結果

0:36:58.820,0:37:02.480
那要分析第一個 input layer 的 filter 是比較不容易的

0:37:02.480,0:37:04.400
因為第一個 layer 裡面，每一個 filter

0:37:04.400,0:37:06.940
它就是一個 3*3 的 matrix

0:37:06.940,0:37:10.680
它對應到 3*3 的範圍內的 9 個 pixel

0:37:10.680,0:37:14.060
所以，你只要看這個 filter 的值，就可以知道說

0:37:14.060,0:37:16.220
它在 detect 甚麼東西

0:37:16.220,0:37:18.900
所以，第一層的 filter 是很容易理解的

0:37:18.900,0:37:22.460
但是，你比較沒有辦法想像它在做甚麼事情的

0:37:22.460,0:37:23.900
是第二層的 filter

0:37:23.900,0:37:27.180
在第二層，我們也是 3*3 的 filter，有 50 個

0:37:27.180,0:37:31.440
但是，這些 filter 的 input 並不是 pixel

0:37:31.440,0:37:33.440
這些 filter 的 9 個 input

0:37:33.440,0:37:36.260
它的 3*3 的 input 並不是 pixel

0:37:36.260,0:37:40.000
而是做完 convolution，再做 Max pooling 的結果

0:37:40.000,0:37:44.560
所以，這個 3*3 的 filter，你就算把它的 weight 拿出來

0:37:44.560,0:37:47.620
你也不知道它在做甚麼

0:37:47.620,0:37:51.540
另外這個 3*3 的 filter，它考慮的範圍並不是

0:37:51.540,0:37:54.740
3*3 個 pixel，並不是 9 個 pixel

0:37:54.740,0:37:57.180
而是比 9 個 pixel 更大的範圍

0:37:57.180,0:37:58.380
不要忘了它的 input

0:37:58.380,0:38:00.380
這 3*3 個 element 的 input

0:38:00.380,0:38:04.540
是做完 convolution 再加 Max pooling 的結果

0:38:04.540,0:38:07.760
所以，它實際上在 image 上看到的範圍

0:38:07.760,0:38:09.720
是比 3*3 還要更大的

0:38:09.720,0:38:16.060
那我們怎麼來分析一個 filter 它做的事情是甚麼呢？

0:38:16.060,0:38:18.040
以下，是一個方法

0:38:18.040,0:38:19.940
那你可以這樣做

0:38:20.440,0:38:22.180
我們知道說

0:38:22.180,0:38:26.480
現在在這 convolution layer 裡面的

0:38:26.480,0:38:27.760
50 個 filter

0:38:27.760,0:38:30.300
每一個 filter 的 output

0:38:30.300,0:38:33.180
就是一個 matrix，對不對？

0:38:33.180,0:38:38.080
每一個 filter 的 output 就是 11*11 的 matrix

0:38:38.080,0:38:43.800
假設，我們現在把第 k 個 filter 拿出來

0:38:43.800,0:38:47.880
我們把 input 一張 image 的第 k 個 output 拿出來

0:38:47.880,0:38:52.060
它可能長這樣子，是一個 11*11 的 matrix

0:38:52.060,0:38:53.620
那裡面每一個 element

0:38:53.620,0:38:58.880
我們就叫它 a上標 k，下標 i, j

0:38:58.880,0:39:01.760
上標 k 的意思是說，這是第 k 個 filter

0:39:01.760,0:39:04.480
i, j 代表說它在這個 matrix 裡面的

0:39:04.480,0:39:06.780
第 i 個 row，第 j個 column

0:39:06.780,0:39:09.560
接下來，我們訂一個東西叫做

0:39:09.560,0:39:13.340
Degree of activation of the k-th filter

0:39:13.340,0:39:15.140
我們訂一個值代表說

0:39:15.140,0:39:17.880
現在的第 k 個 filter

0:39:17.880,0:39:22.060
它有多被 activate、它有多被啟動

0:39:22.060,0:39:26.260
現在 input 的東西

0:39:26.260,0:39:29.360
跟第 k 個 activate 有多相近

0:39:29.360,0:39:31.780
有多 match

0:39:31.780,0:39:36.060
那這個第 k 個 filter 呢

0:39:36.060,0:39:39.340
第 k 個 filter，它被啟動的 degree 呢

0:39:39.340,0:39:40.860
我們這邊就定義成

0:39:40.860,0:39:44.780
它的 11*11 matrix 裡面的

0:39:44.780,0:39:48.480
這些全部 element 的 summation

0:39:48.480,0:39:50.120
這個 filter 呢

0:39:50.120,0:39:52.100
就我們 input 一張 image

0:39:52.100,0:39:55.580
然後，看這個 filter 它 output 的 11*11 個值

0:39:55.580,0:39:56.800
把它全部加起來

0:39:56.800,0:40:01.120
當作說，現在這個 filter 被 activate 的程度

0:40:01.700,0:40:04.140
接下來，我們要做的事情是這樣子

0:40:04.140,0:40:05.720
我們想要知道

0:40:05.720,0:40:08.260
這個第 k 個 filter 它的作用是甚麼

0:40:08.260,0:40:10.920
所以我們想要找一張 image

0:40:10.920,0:40:14.580
這一張 image 它可以讓第 k 個filter

0:40:14.580,0:40:17.580
被 activate 的程度最大

0:40:17.580,0:40:19.280
怎麼做到這件事情呢？

0:40:19.280,0:40:23.260
假設 input 的 image 我們稱之為 x

0:40:23.260,0:40:26.240
那我們現在要解的問題就是

0:40:26.240,0:40:29.040
找一個 x

0:40:29.040,0:40:32.220
它可以讓，現在我們定義的這個

0:40:32.220,0:40:35.200
activation 的 degree a, 上標 k

0:40:35.200,0:40:38.960
它可以讓我們定義的這個 activation 的 degree 最大

0:40:38.960,0:40:40.940
那這件事情要怎麼做到呢？

0:40:40.940,0:40:43.040
你其實就是用 gradient descent

0:40:43.040,0:40:44.960
因為我們現在要 maximize

0:40:44.960,0:40:46.600
如果是 minimize，你可以用 gradient descent

0:40:46.600,0:40:48.580
那 maximize，你用 gradient ascent

0:40:48.580,0:40:51.240
你就可以做到這件事了

0:40:51.240,0:40:52.500
就結束了

0:40:52.500,0:40:54.560
這樣大家聽得懂嗎？

0:40:54.560,0:40:56.800
我覺得還頗神妙

0:40:56.800,0:41:01.960
因為，我們現在是把 x 當作我們要找的參數

0:41:01.960,0:41:05.880
對它去用 gradient descent 或 gradient ascent

0:41:05.880,0:41:07.560
做 update

0:41:07.560,0:41:09.440
原來在 train 這個 CNN

0:41:09.440,0:41:10.980
在 train neural network 的時候

0:41:10.980,0:41:12.500
input 是固定的

0:41:12.500,0:41:15.260
那你的這個 model 的參數

0:41:15.260,0:41:17.800
是要用 gradient descent 去找出來的

0:41:17.800,0:41:20.640
用 gradient descent 找一組參數

0:41:20.640,0:41:22.740
可以讓你的 loss 可以被 minimize

0:41:22.740,0:41:25.520
但是，現在這個立場是被反過來的

0:41:25.520,0:41:27.320
現在，在這個 task 裡面呢

0:41:27.320,0:41:29.360
model 的參數是固定的

0:41:29.360,0:41:33.000
那我們要用這個 gradient descent 去 update 這個 x

0:41:33.000,0:41:35.260
或是，我們用 gradient ascent 去 update 這個 x

0:41:35.260,0:41:39.420
我們 update 這個 x，讓它可以讓這個

0:41:39.420,0:41:42.260
activation function 的這個 degree of activation

0:41:42.260,0:41:43.540
是最大的

0:41:43.540,0:41:45.940
這邊大家有問題嗎？

0:41:45.940,0:41:49.120
沒有哦，這個還滿神妙的

0:41:49.120,0:41:52.240
這個是得到的結果，如果我們

0:41:52.240,0:41:56.280
隨便取 12 個 filter 出來的話

0:41:56.280,0:41:59.800
那每一個 filter 呢，我們說我們對每一個 filter

0:41:59.800,0:42:01.920
都去找一張 image

0:42:01.920,0:42:04.440
這個 image 可以讓那個 filter

0:42:04.440,0:42:05.920
它的 activation 最大

0:42:05.920,0:42:07.720
那現在有 50 個 filter

0:42:07.720,0:42:10.540
所以，理論上可以找 50 張 image

0:42:10.540,0:42:13.680
它可以讓這些 filter 的 activation 最大

0:42:13.680,0:42:17.800
那我就隨便取了前 12 個 filter

0:42:17.800,0:42:20.520
可以讓它最 activate 的 image 出來

0:42:20.520,0:42:23.180
可以看它的結果長這樣子

0:42:23.180,0:42:26.280
那這些 image 它有一個共同的特徵就是

0:42:26.280,0:42:29.340
它是某種 texture

0:42:29.340,0:42:31.340
它是某種紋路

0:42:31.340,0:42:33.920
在圖上不斷地反覆

0:42:33.920,0:42:36.520
為甚麼呢？為甚麼會這樣呢

0:42:36.520,0:42:39.600
比如說，我們看看

0:42:39.600,0:42:41.680
這一張 image 好了

0:42:41.680,0:42:44.520
這一張 image 上面，第三張圖上面呢

0:42:44.520,0:42:47.360
它都是小小的斜條紋

0:42:47.360,0:42:50.000
這意味著甚麼事？這意味著

0:42:50.000,0:42:52.900
第三個 filter 它的工作就是 detect 圖上

0:42:52.900,0:42:55.180
有沒有斜的條紋

0:42:55.180,0:42:57.660
那不要忘了說，現在每一個 filter

0:42:57.660,0:43:01.700
其實它考慮的範圍，都是每一個圖上的

0:43:01.700,0:43:03.160
小小的範圍而已

0:43:03.160,0:43:05.820
所以，今天一個圖上如果出現一個

0:43:05.820,0:43:08.680
小小的，隨便一個斜的條紋的話

0:43:08.680,0:43:10.960
這一個 filter

0:43:10.960,0:43:13.780
就會被 activate，這個 filter output 的值

0:43:13.780,0:43:16.520
就會比較大、就會很大

0:43:16.520,0:43:19.160
如果，今天讓圖上

0:43:19.160,0:43:22.740
所有的範圍通通出現這個小小的斜條紋的話

0:43:22.740,0:43:25.260
那這個時候，它的 activation

0:43:25.260,0:43:27.640
它的 degree of activation 會是最大

0:43:27.640,0:43:32.220
因為它的工作就是，偵測有沒有一個斜的條紋

0:43:32.220,0:43:35.160
所以你給它一個完整的數字的時候，它不會最興奮

0:43:35.160,0:43:38.620
你給它通通都是斜的條紋的時候，這個時候它最興奮

0:43:38.620,0:43:40.240
雖然，它完全不是一個數字

0:43:40.240,0:43:43.140
所以，你就會發現說，每一個 filter 它的工作就是

0:43:43.140,0:43:45.220
detect 某一種 pattern

0:43:45.220,0:43:47.400
detect 某一種線條

0:43:47.400,0:43:50.820
比如說，第 3 個圖它 detect 斜的線條

0:43:50.820,0:43:53.600
第四個圖 detect 短的、直的線條

0:43:53.600,0:43:57.300
這個 detect 這個方向斜的線條

0:43:57.300,0:43:59.520
這個 detect 這個方向斜的線條

0:43:59.520,0:44:01.320
這個也是 detect 這個方向斜的線條

0:44:01.320,0:44:03.440
不過跟這個 degree 可能有點不一樣

0:44:03.440,0:44:04.960
它這個 degree 比較正，這個也是

0:44:04.960,0:44:06.720
detect 斜的線條，但是它比較平

0:44:06.720,0:44:09.420
這個 detect 是直的線條，等等

0:44:09.420,0:44:11.760
你會發現每一個 filter 做的事情

0:44:11.760,0:44:15.100
就是 detect 不同角度的線條

0:44:15.100,0:44:17.320
如果今天 input 有不同線條的話

0:44:17.320,0:44:19.800
你就會讓某一個 activation function

0:44:19.800,0:44:23.380
某一個 filter 它 output 的值最大

0:44:27.480,0:44:30.080
我們接下來可以分析這個

0:44:30.080,0:44:31.700
fully connected 的 layer

0:44:31.700,0:44:35.820
我們做完 convolution 和 Max pooling 以後呢

0:44:35.820,0:44:38.440
接下來，我們會做一件事情叫做 flatten

0:44:38.440,0:44:40.200
然後，把 flatten 以後的結果

0:44:40.200,0:44:41.740
丟到 neural network 裡面去

0:44:41.740,0:44:45.060
那我們也會想要知道說，在這個 neural network 裡面

0:44:45.060,0:44:48.280
每一個 neuron 它的工作是什麼

0:44:48.280,0:44:50.740
所以，我們就如法炮製剛才的作法

0:44:50.740,0:44:52.520
我們要做的事情是這樣子

0:44:52.520,0:44:55.960
我們定義第 j 個 neuron

0:44:55.960,0:44:58.660
它的 output 叫做 a_j

0:44:59.020,0:45:00.800
接下來，我們要做的事情就是

0:45:00.800,0:45:02.260
找一張 image x

0:45:02.260,0:45:04.980
用 gradient descent 的方法去找一張 image x

0:45:04.980,0:45:08.620
這個 image x，你把它丟到這個 neural network 裡面去

0:45:08.620,0:45:12.140
它可以讓 a_j 的值被 maximize

0:45:12.140,0:45:13.880
那找到甚麼樣的結果呢

0:45:13.880,0:45:15.660
我們找到的結果就是這樣

0:45:15.660,0:45:19.620
就是隨便取前 9 個 neuron 出來

0:45:19.620,0:45:21.520
隨便取 9 個 neuron 出來

0:45:21.520,0:45:26.400
那甚麼樣的圖丟到 CNN 裡面，可以讓這 9 個 neuron

0:45:26.400,0:45:29.300
最被 activate，output 的值最大呢？

0:45:29.300,0:45:32.280
就是這 9 張圖

0:45:32.280,0:45:35.740
你會發現說，跟剛才的

0:45:35.740,0:45:38.460
filter 所觀察到的情形，是很不一樣的

0:45:38.460,0:45:40.820
在剛才的 filter 裡面，我們觀察到的是

0:45:40.820,0:45:43.160
類似紋路的東西

0:45:43.160,0:45:45.360
在整張圖上，反覆同樣的紋路

0:45:45.360,0:45:48.260
那是因為每一個 filter，它考慮的只是一個

0:45:48.260,0:45:51.360
小小的 vision，圖上的一部份的 vision

0:45:51.360,0:45:53.340
所以，它 detect 是某一種 texture

0:45:53.340,0:45:55.340
但是，現在每一個 neuron

0:45:55.340,0:45:58.020
在你做 flatten 以後，每一個 neuron 它的工作

0:45:58.020,0:46:00.040
就是去看整張圖

0:46:00.040,0:46:02.560
它不再是只看整張圖的一小部分

0:46:02.560,0:46:05.420
它現在的工作是看整張圖

0:46:05.420,0:46:09.720
所以，每一個 neuron 你可以讓它最 activate 的圖

0:46:09.720,0:46:13.840
並不再是 texture 那個樣子，而是一個完整的圖形

0:46:13.840,0:46:16.660
而是一個完整的圖形，雖然它看起來完全

0:46:16.660,0:46:19.520
不像數字，比如說，這個看起來像是

0:46:19.520,0:46:21.140
千年眼這個樣子

0:46:21.140,0:46:24.480
算了，沒有人知道我在說甚麼

0:46:25.480,0:46:29.980
就是，某一個 neuron，像這個 neuron

0:46:29.980,0:46:31.960
這個會偵測千年眼的 neuron 呢

0:46:31.960,0:46:33.460
它的工作其實就是

0:46:33.460,0:46:35.580
看到這樣子的線條

0:46:35.580,0:46:37.160
或到這樣子的線條

0:46:37.160,0:46:38.900
或看到小小的圓圈

0:46:38.900,0:46:40.480
就可以讓它被 activate

0:46:40.480,0:46:44.140
所以，它偵測的也是一個完整的數字

0:46:44.140,0:46:47.840
是一個比較大的 pattern

0:46:49.840,0:46:52.520
接下來，我就會想要知道說

0:46:52.520,0:46:54.600
如果我們考慮的是 output 呢

0:46:54.600,0:46:56.780
如果我們今天考慮的是 output 呢

0:46:56.780,0:46:59.920
如果我們今天把，output 就是 10 維嘛

0:46:59.920,0:47:01.820
把每一維都對應到一個 digit

0:47:01.820,0:47:05.760
那我們把某一維拿出來

0:47:05.760,0:47:07.820
然後，我們說找一張 image

0:47:07.820,0:47:10.800
讓那一個維度的 output 最大

0:47:10.800,0:47:13.000
那我們會得到甚麼樣的 image 呢

0:47:13.460,0:47:15.620
你可以想像說，現在既然

0:47:15.620,0:47:17.440
每一個 output 的每一個 dimension

0:47:17.440,0:47:19.920
就對應到某一個數字

0:47:19.920,0:47:23.740
那我們現在如果把對應到數字 1 的那張圖

0:47:23.740,0:47:26.300
我們如果先找一張 image

0:47:26.300,0:47:29.700
它可以讓對應到數字 1 的那個 output layer 的 neuron

0:47:29.700,0:47:31.820
它的 output 最大

0:47:31.820,0:47:35.300
那一張 image 顯然看起來像是個數字 1

0:47:35.300,0:47:37.420
對不對，這樣大家了解我意思嗎？

0:47:37.420,0:47:40.660
你可以期待說，搞不好我們用這個方法就可以

0:47:40.660,0:47:43.960
讓 machine 自動畫出數字

0:47:43.960,0:47:47.200
但是，實際上我們得到的結果是這樣子

0:47:47.200,0:47:49.200
得出來的結果是這樣子

0:47:49.200,0:47:51.320
那這邊呢，每一張圖

0:47:51.320,0:47:55.440
分別代表數字 0, 1, 2 到 8

0:47:55.440,0:47:58.520
我就只畫了 9 個，實際上 output 有 10 個

0:47:58.520,0:48:01.920
我就只畫了 9 個，實際上 output 有 10 個

0:48:01.920,0:48:05.700
也就是說，我們找出來可以讓

0:48:05.700,0:48:07.760
對應到 0 的那個

0:48:07.760,0:48:10.560
output layer 裡面對應到 0 的那個 neuron

0:48:10.560,0:48:14.400
它的 output 最大的 image 其實是長這個樣子

0:48:14.400,0:48:17.680
可以讓 1 的 neuron output 最大的 image 其實是長這樣

0:48:17.680,0:48:20.580
2 的其實是長這樣，以此類推，每張看起來都差不多

0:48:20.580,0:48:22.740
都像是電視壞掉的樣子

0:48:23.380,0:48:27.500
你可就會有個疑惑，為甚麼會這樣

0:48:27.500,0:48:29.660
是不是程式有 bug

0:48:29.660,0:48:32.760
為了確定程式沒有 bug，所以我就再做了一個實驗是

0:48:32.760,0:48:34.780
我把這每一張 image

0:48:34.780,0:48:37.640
都真的丟到 CNN 裡面

0:48:37.640,0:48:41.140
然後，看說它 classify 的結果是什麼

0:48:41.140,0:48:43.220
那 CNN 確實就是說

0:48:43.220,0:48:46.100
它 classify 說這個是 0、這個是 1、這個是 2

0:48:46.100,0:48:48.240
一直到這個是 8，CNN 就覺得說

0:48:48.240,0:48:49.880
你如果拿這張 image

0:48:49.880,0:48:53.640
我們 train  出來的正確率有 98.2 給 CNN 看

0:48:53.640,0:48:57.120
它就會說，這個東西就叫做 8

0:48:57.120,0:48:59.980
所以，很神奇吧

0:48:59.980,0:49:06.960
所以，這個結果，其實已經在很多地方都被觀察到了

0:49:06.960,0:49:10.760
就是今天這個 neural network，它所學到的東西

0:49:10.760,0:49:14.300
跟我們人類是不太一樣的

0:49:14.300,0:49:15.960
那它所學到的東西

0:49:15.960,0:49:19.620
跟我們人類一般所想像認知是不一樣的

0:49:19.620,0:49:24.120
你可以看一下這段 video，那它也有對應的 paper

0:49:24.120,0:49:27.140
那裡面呢，machine 會把各種你看起來

0:49:27.140,0:49:30.200
不像企鵝的東西也是企鵝，
看起來不像公車的東西也是公車

0:49:30.520,0:49:33.060
不然像我們這邊看起來，這完全不像一個數字

0:49:33.060,0:49:36.660
但對 machine 來說，它就是這個數字 8

0:49:37.940,0:49:41.260
所以，這個 machine 它學到的東西

0:49:41.260,0:49:44.460
和我們人類，是非常不一樣的

0:49:45.780,0:49:49.420
我們就想說，那這樣我們沒有辦法畫一個數字

0:49:49.420,0:49:52.700
所以我們要把這個數字，我們有沒有辦法

0:49:52.700,0:49:57.480
讓這個圖看起來更像數字呢？

0:49:57.480,0:50:00.380
所以，這邊的想法是這個樣子

0:50:00.380,0:50:03.800
因為，我們知道說

0:50:03.800,0:50:06.360
一張圖是不是一個數字，它有一些

0:50:06.360,0:50:08.300
基本的假設

0:50:08.300,0:50:12.360
比如說，這些東西就算是

0:50:12.360,0:50:16.900
你不知道它是甚麼數字，
你也會知道說它顯然就不是一個 digit

0:50:16.900,0:50:20.600
人類手寫出來的東西，就不是長這個樣子

0:50:20.600,0:50:25.940
所以，我們應該對這個 y 做一些 Regularization

0:50:25.940,0:50:28.640
做一些 constraint，我們應該告訴 machine 說

0:50:28.640,0:50:34.280
說錯了，不是 y，我們應該對 x 做一些 constraint

0:50:34.280,0:50:37.500
我們應該對找出來的 x 做一些 constraint

0:50:37.500,0:50:39.220
我們應該告訴 machine 說

0:50:39.220,0:50:43.200
有一些 x，它雖然可以讓你的 y 很大

0:50:43.200,0:50:45.900
但是，它不是數字

0:50:45.900,0:50:49.580
我們天生、根據我們人的 required knowledge 就知道說

0:50:49.580,0:50:52.580
這些 x，它不可能是一個數字

0:50:52.580,0:50:55.120
那我們可以加上甚麼樣的 constraint 呢

0:50:55.120,0:50:57.060
比如說，最簡單的想法是說

0:50:57.060,0:51:00.020
今天這個圖上阿

0:51:00.020,0:51:01.460
白色的亮點

0:51:01.460,0:51:05.820
畫這個圖的時候，白色代表是有墨水的

0:51:05.820,0:51:09.180
有墨水就是有 ink、有筆畫的地方

0:51:09.180,0:51:12.320
是白色的，希望大家可以想像

0:51:12.320,0:51:14.480
對一個 digit 來說呢

0:51:14.480,0:51:18.320
塗白的部分，其實是有限的

0:51:18.320,0:51:20.420
塗白的部分不會太多

0:51:20.420,0:51:23.980
你整張圖都是白白的，它一定不是一個數字

0:51:23.980,0:51:25.280
那對數字來說

0:51:25.280,0:51:28.480
只有一整張圖的某一個小部分

0:51:28.480,0:51:30.080
會有筆畫

0:51:30.080,0:51:34.000
所以，我們應該要對這一個 x 做一些限制

0:51:34.000,0:51:37.100
對 x 做一些限制

0:51:37.100,0:51:40.420
比如說，這邊的限制是

0:51:40.420,0:51:43.100
我發現呢，我寫錯了

0:51:43.100,0:51:45.020
因為這邊是 max

0:51:45.020,0:51:48.580
所以，這邊這個加號應該是減號

0:51:48.580,0:51:52.200
不好意思，這邊加號應該是減號

0:51:52.200,0:51:55.160
因為我們這邊要做的事情是這樣子

0:51:55.160,0:51:57.980
假設這個 image 裡面的每一個 pixel

0:51:57.980,0:52:00.160
我們用 x_ij 來表示

0:52:00.160,0:52:03.520
所以，每一個 image 有 28*28 個 pixel

0:52:03.520,0:52:06.300
i 就是 1~28，j 就是 1~28

0:52:06.300,0:52:11.120
我們把所有的 pixel 的值

0:52:11.120,0:52:16.060
我們把所有今天 image 上面的 x_ij 的值呢

0:52:16.060,0:52:20.840
取絕對值加起來，如果你熟悉 machine learning 的話呢

0:52:20.840,0:52:24.580
這一項就是 L1 的 Regularization

0:52:24.580,0:52:27.260
那我們把這個 pixel 的值

0:52:27.260,0:52:30.640
全部加起來

0:52:30.640,0:52:34.300
然後，我們希望說，我們再找一個 x

0:52:34.300,0:52:36.860
它可以讓 y_i 最大的同時

0:52:36.860,0:52:39.620
這邊其實應該要是減號

0:52:39.620,0:52:43.800
它同時應該要讓 x_ij 的 summation

0:52:43.940,0:52:45.440
越小越好

0:52:45.440,0:52:47.400
也就是說，我們希望找出來的 image

0:52:47.400,0:52:50.420
大部分的地方，是沒有塗顏色的

0:52:50.420,0:52:52.140
沒有塗白色的、沒有筆畫的

0:52:52.140,0:52:53.980
只有非常少的地方

0:52:53.980,0:52:55.660
是有塗筆畫的

0:52:55.660,0:52:58.220
那如果我們加上這個 constraint 以後呢

0:52:58.220,0:53:01.880
我們得到的結果看起來就像是這樣

0:53:01.880,0:53:04.420
其實，跟左邊的圖比起來呢

0:53:04.420,0:53:08.240
你已經有些隱約可以看出來

0:53:08.240,0:53:10.760
它是一個數字，比如說

0:53:10.760,0:53:14.040
這個就有一點點像 6

0:53:14.040,0:53:17.820
雖然說，它這邊還多了一條，但它有一點點像 6

0:53:17.820,0:53:20.400
這個，有一點點像 8

0:53:20.400,0:53:25.540
這個，它好像找到了 5 的第一個筆劃

0:53:25.540,0:53:28.060
這個好像找到了 7 的第一個筆劃

0:53:28.060,0:53:31.160
等等，所以你加上了這些 constraint 以後呢

0:53:31.160,0:53:34.700
你看起來得到的結果

0:53:34.700,0:53:36.280
就比較像數字了

0:53:36.280,0:53:38.480
講到這邊，你可能會問一個問題

0:53:38.480,0:53:42.440
首先，我們還沒有講過 L1 的 Regularization

0:53:42.440,0:53:44.000
但是，我們等一下會講

0:53:44.000,0:53:47.040
再來你可能會有另外一個問題，這個有絕對值阿

0:53:47.040,0:53:49.520
怎麼微分，其實這個是可以微分的啦

0:53:49.520,0:53:52.440
這個我們下一堂課會講到

0:53:52.440,0:53:54.980
講到這邊，大家有沒有甚麼問題呢？

0:53:58.120,0:54:02.260
其實，我覺得說如果你再加上一些額外的 constraint

0:54:02.260,0:54:06.300
比如說，你希望相鄰的 pixel 是同樣的顏色等等

0:54:06.300,0:54:08.860
你應該可以得到更好的結果

0:54:08.860,0:54:11.840
不過，其實有更多很好的方法可以

0:54:11.840,0:54:13.600
讓我們自動去 generate

0:54:13.600,0:54:15.780
數字，或是 generate 它看到的東西

0:54:15.780,0:54:18.560
所以，這邊我就沒有興致再把它做得更好

0:54:18.800,0:54:22.240
其實，這個東西、這個想法

0:54:22.240,0:54:25.400
就是 Deep Dream 的精神

0:54:25.400,0:54:27.540
不知道大家知不知道 Deep Dream 是甚麼呢？

0:54:27.540,0:54:31.060
Deep Dream 是說，如果你給 machine 一張 image

0:54:31.060,0:54:35.480
它會在這個 image 裡面，加上它看到的東西

0:54:36.500,0:54:38.520
怎麼做這件事情呢

0:54:38.520,0:54:40.360
擬就找一張 image

0:54:40.360,0:54:44.880
這個是我，後面這個不是 photoshop 上去的

0:54:45.600,0:54:48.120
然後，這邊我擺了一個很做作的姿勢

0:54:48.120,0:54:52.420
然後，你把這張相片丟到 CNN 裡面去

0:54:53.160,0:54:56.420
然後，你把它的某一個 hidden layer

0:54:56.420,0:54:59.480
你把某一個 layer 裡面的

0:54:59.480,0:55:02.440
filter，或是 fully connected layer 裡面

0:55:02.440,0:55:04.540
的某一個 hidden layer 拿出來

0:55:04.540,0:55:06.520
那它其實是一個 vector

0:55:06.520,0:55:08.820
你把某一個 hidden layer 拿出來

0:55:08.820,0:55:11.700
它其實是一個 vector，假設它這邊是

0:55:11.700,0:55:14.200
比如說，3.9、-1.5、2.3

0:55:14.200,0:55:16.800
接下來呢，你把

0:55:16.800,0:55:19.680
這個 filter 的值調大

0:55:19.680,0:55:24.500
你把本來是 positive 的 dimension 值調大

0:55:24.500,0:55:26.320
把 positive 的 dimension 值調大

0:55:26.320,0:55:28.940
negative 的 dimension 值調小

0:55:28.940,0:55:32.680
然後，你本來就是正的讓它更正

0:55:32.680,0:55:33.880
負的讓它更負

0:55:33.880,0:55:37.140
你把這個東西，當作是

0:55:37.140,0:55:40.260
新的 image 的目標

0:55:40.260,0:55:42.080
了解我意思嗎？就是

0:55:42.080,0:55:45.700
你把這個 3.9 的值變大

0:55:45.700,0:55:47.420
你把 -1.5 的值變得更負

0:55:47.420,0:55:50.220
你把 2.3 的值變得更大

0:55:50.220,0:55:53.500
然後，你找一張 image

0:55:53.500,0:55:54.860
用 gradient descent 的方法

0:55:54.860,0:55:56.900
讓它在這個 hidden layer 的 output

0:55:56.900,0:56:00.120
是你現在設下的 target

0:56:00.680,0:56:02.780
這樣大家聽得懂嗎？

0:56:02.780,0:56:04.700
你如果這麼做的話

0:56:04.700,0:56:07.480
你如果這麼做的話，意思就是說

0:56:07.480,0:56:11.860
讓 CNN 誇大化它看到的東西

0:56:11.860,0:56:14.040
它本來已經有看到

0:56:14.040,0:56:15.660
某一個東西了

0:56:15.660,0:56:20.100
你讓它看起來更像原來看到的東西

0:56:20.100,0:56:22.640
你讓它本來看起來是有一點像

0:56:22.640,0:56:25.660
某一個東西，它讓某一個 filter 呢

0:56:25.660,0:56:29.220
有被 activate，但你讓它被 activate 的更劇烈

0:56:29.220,0:56:32.860
就是你讓 CNN 誇大化它現在看到的東西

0:56:32.860,0:56:35.340
那如果你把這一張 image

0:56:35.340,0:56:37.260
拿去做 Deep Dream 的話

0:56:37.260,0:56:40.100
你看到的結果，就會是這樣子

0:56:40.100,0:56:43.420
背後出現很多念獸，要凝才看的到這樣

0:56:45.080,0:56:47.520
就是有很多念獸這樣

0:56:47.520,0:56:49.540
比如說，像這邊

0:56:49.540,0:56:51.800
這邊有一隻熊

0:56:51.800,0:56:54.580
你看這個熊，它原來是一個石頭

0:56:54.580,0:56:57.620
它原來是一個石頭，那你不覺得它這個石頭

0:56:57.620,0:57:00.180
哇！看起來其實也是有點像熊的嗎？

0:57:00.180,0:57:03.700
這個是耳朵，這個是眼睛、這個是鼻子

0:57:03.700,0:57:06.540
那對機器來說，它在看這張圖的時候

0:57:06.540,0:57:09.100
它本來就覺得，這個有點像熊

0:57:09.100,0:57:11.500
那你就更強化這件事

0:57:11.500,0:57:15.420
所，它看起來就真的變成了一隻熊

0:57:15.980,0:57:18.160
這個是 Deep Dream

0:57:18.160,0:57:20.380
那 Deep Dream 還有一個進階的版本

0:57:20.380,0:57:24.020
這個叫做 Deep Style

0:57:24.020,0:57:26.980
Deep Style 是讓，今天你 input 一張 image

0:57:26.980,0:57:31.480
你 input 一張相片，然後讓 machine 去修改這張圖

0:57:31.480,0:57:35.500
讓它有另外一張圖的風格

0:57:35.500,0:57:38.440
比如說，讓這個相片，看起來像是吶喊

0:57:38.440,0:57:40.340
那得到的結果就是這樣子

0:57:40.340,0:57:43.060
這畫的還滿好的

0:57:43.060,0:57:45.260
這驚人的好

0:57:45.260,0:57:47.780
那這個東西，是怎麼做的呢

0:57:47.780,0:57:51.220
這邊就不細講，就列一個 reference 給大家參考

0:57:51.220,0:57:55.080
那它這個做的精神是這樣子

0:57:55.080,0:57:58.660
你給它、把原來的 image

0:57:58.660,0:58:03.020
丟給 CNN，然後得到 CNN 的 filter 的 output

0:58:03.320,0:58:05.480
CNN 的 filter 的 output

0:58:05.480,0:58:09.500
代表這一張 image 裡面有甚麼樣的 content

0:58:10.320,0:58:13.440
然後，接下來你把吶喊這張圖

0:58:13.440,0:58:15.120
也丟到 CNN 裡面

0:58:15.120,0:58:18.100
也得到 filter 的 output

0:58:18.100,0:58:20.540
但是，這個時候我們考慮的不是

0:58:20.540,0:58:24.060
filter output 的 absolute 的 value

0:58:24.060,0:58:26.940
我們並不在意一個 filter output 的 value 是什麼

0:58:26.940,0:58:31.180
而是在意 filter 和 filter 之間 output 的 correlation

0:58:31.180,0:58:36.320
那這個 correlation 代表了一張 image 的 style

0:58:36.320,0:58:39.860
接下來，你就用同個 CNN

0:58:39.860,0:58:42.540
找一張 image，這張 image 呢

0:58:42.540,0:58:45.660
它的 content 像左邊這張相片

0:58:45.660,0:58:48.500
比如說，這張 image 它的 filter output 的 value

0:58:48.500,0:58:50.340
像左邊這張相片

0:58:50.340,0:58:53.640
同時呢，這張 image 的 style

0:58:53.640,0:58:55.140
像右邊這張相片

0:58:55.140,0:58:57.380
所謂的 style 像右邊這張相片是

0:58:57.380,0:59:00.940
你這個 image output 的 filter 間的 correlation

0:59:00.940,0:59:02.780
像右邊這張相片

0:59:02.780,0:59:05.720
那你找一張 image，同時可以

0:59:05.720,0:59:08.680
maximize 左邊這個東西，
也可以 maximize 右邊這個東西

0:59:08.680,0:59:11.860
那你得到的結果，就是像這樣子的一個圖

0:59:11.860,0:59:15.860
那你用的就是，我們剛才講的 gradient descent 的方法

0:59:15.860,0:59:17.360
找一張 image

0:59:17.360,0:59:20.120
maximize 這兩個 criteria 的結果，就是左邊這個圖

0:59:21.660,0:59:26.640
那我們現在知道說，CNN 可以被應用在很多不同的

0:59:26.640,0:59:30.100
應用上，不是只有影像處理

0:59:30.100,0:59:32.760
比如說，CNN 現在有一個非常

0:59:32.760,0:59:36.800
知名的應用，就是用在下圍棋上面

0:59:37.580,0:59:41.500
為甚麼 CNN 可以用在下圍棋上面呢

0:59:42.000,0:59:45.200
我們知道說，你要讓

0:59:45.200,0:59:48.840
machine 來下圍棋，你不見得要用 CNN

0:59:48.840,0:59:50.980
其實，一般 typical 的 neural network

0:59:50.980,0:59:53.100
也可以幫我們做到這件事情

0:59:53.100,0:59:56.360
你只要 learn 一個 network，也就是找一個 function

0:59:56.360,1:00:02.420
它的 input 是棋盤，output 是棋盤上的位置

1:00:02.420,1:00:06.280
也就是說，你下一步根據這個棋盤的盤勢

1:00:06.280,1:00:08.160
如果你下一步要落子的話

1:00:08.160,1:00:09.600
你應該落子的位置

1:00:09.600,1:00:12.560
你其實就可以讓 machine 學會下圍棋了

1:00:13.120,1:00:16.800
所以，你其實用 fully connected 的 feedforward network

1:00:16.800,1:00:20.380
也可以幫我們做到讓 machine 下圍棋這件事情

1:00:20.380,1:00:22.820
也就是說，你只要認知告訴它說 input

1:00:22.820,1:00:24.740
是一個 19*19 的 vector

1:00:24.740,1:00:28.100
vector 的每一個 dimension 對應到

1:00:28.100,1:00:30.420
棋盤上的某一個位置

1:00:30.420,1:00:33.160
如果那一個位置有一個黑子的話，就是 1

1:00:33.160,1:00:35.800
如果有一個白子的話，就是 -1

1:00:35.800,1:00:37.760
反之呢，就是 0

1:00:37.760,1:00:41.360
如果你把棋盤描述成一個 19*19 的 vector

1:00:41.360,1:00:43.960
丟到一個 fully connected 的 feedforward network

1:00:43.960,1:00:46.140
output 也是 19*19 個 dimension

1:00:46.140,1:00:49.720
每一個 dimension 對應到棋盤上的一個位置

1:00:49.720,1:00:53.180
那 machine 就可以學會下圍棋了

1:00:55.160,1:00:58.580
但是，實際上如果我們今天在這邊採用

1:00:58.580,1:01:00.040
CNN 的話

1:01:00.040,1:01:01.740
我們會得到更好的 performance

1:01:01.740,1:01:04.420
採用 CNN 是甚麼意思呢？

1:01:04.420,1:01:09.140
我們之前舉的例子都是把 CNN 用在影像上面

1:01:09.140,1:01:12.220
也就是 input 是一個 matrix

1:01:12.220,1:01:15.540
所以，現在你其實只要把 19*19 的 vector

1:01:15.540,1:01:19.300
表示成一個 19*19 的 matrix

1:01:19.300,1:01:22.780
一個棋盤可以很自然地表示成一個 19*19 的 matrix

1:01:22.780,1:01:26.680
對 CNN 來說，就是把它當成一個 image 來看

1:01:26.680,1:01:30.240
然後，再讓它 output 下一步要落子的位置

1:01:30.240,1:01:33.380
就結束了

1:01:33.380,1:01:35.520
那它 training 的 process 呢

1:01:35.520,1:01:36.660
是這個樣子的

1:01:37.480,1:01:41.780
這個我們講過，你就蒐集很多棋譜

1:01:41.780,1:01:44.300
比如說這個是進藤光和社青春的棋譜

1:01:44.300,1:01:48.740
初手下在 5 之五，然後再下在天元，再下在 5 之五

1:01:48.740,1:01:51.620
接下來，你就告訴 machine 說，看到落子在 5 之五

1:01:51.620,1:01:55.620
CNN 的 output 就是天元的地方是 1，其他的 output 是 0

1:01:55.620,1:01:58.920
看到 5 之五和天元都有子

1:01:58.920,1:02:01.860
那你的 output 就是 5 之五的地方是 1

1:02:01.860,1:02:04.060
其他都是 0，這個是

1:02:04.060,1:02:06.640
supervised 的部分，那其實呢

1:02:06.640,1:02:10.280
AlphaGo 還有 reinforcement learning 的部分

1:02:10.280,1:02:12.640
那這個我們就之後再講

1:02:12.640,1:02:15.280
我們知道說，大家都是

1:02:15.280,1:02:18.080
大家都說得一口 AlphaGo 啦

1:02:18.080,1:02:19.560
大家都是懂懂懂這樣子

1:02:21.620,1:02:24.480
自從 AlphaGo 用了 CNN 以後

1:02:24.480,1:02:27.480
大家都覺得說，好像 CNN 應該很厲害

1:02:27.480,1:02:30.500
所以有時候，如果你沒有用 CNN 來處理你的問題

1:02:30.500,1:02:32.600
人家就會問你說，比如你去面試的時候

1:02:32.600,1:02:35.100
你的碩士論文裡面沒有用 CNN 來處理問題

1:02:35.100,1:02:36.940
口試的人可能不知道 CNN 是甚麼

1:02:36.940,1:02:38.640
但是，他就會問你說為什麼不用 CNN 呢

1:02:38.640,1:02:40.160
CN N 不是比較強嗎？

1:02:40.160,1:02:42.240
這個時候你就可以嗆爆他這樣子

1:02:42.700,1:02:44.960
甚麼時候我們可以用 CNN 呢

1:02:44.960,1:02:49.020
你要有 image 該有的那些特性

1:02:49.020,1:02:51.780
我們之前在講 CNN 開頭的時候，我們有說，根據

1:02:51.780,1:02:57.040
3 個觀察，所以我們設計出了 CNN 這樣的 network 架構

1:02:57.040,1:02:59.440
它在處理 image 的時候

1:02:59.440,1:03:01.320
是特別有效的

1:03:01.320,1:03:04.700
那為甚麼這樣子的架構，也同時可以用在

1:03:04.700,1:03:05.880
圍棋上面呢

1:03:05.880,1:03:10.220
那是因為圍棋有一些特性，和影像處理是很相似的

1:03:10.540,1:03:12.080
第一個是

1:03:12.080,1:03:16.960
我們說，在 image 上面

1:03:16.960,1:03:21.400
有一些 pattern 是比整張 image 還要小的多

1:03:21.400,1:03:25.460
比如說，鳥的喙是比整張 image 還要小的多

1:03:25.460,1:03:28.240
但是，你只要看到一小個部分，你就會知道說

1:03:28.240,1:03:29.460
它是不是鳥的喙

1:03:29.460,1:03:32.840
在圍棋上，可能也有同樣的現象

1:03:32.840,1:03:34.420
我對圍棋知道得很少

1:03:34.420,1:03:38.220
知道的知識都沒有超過棋靈王所教我的範圍

1:03:38.220,1:03:42.320
但是我也知道說，比如說，這個東西、一個白子

1:03:42.320,1:03:45.540
被 3 個黑子圍住，這個叫做叫吃

1:03:45.540,1:03:49.080
如果黑子現在落在這邊，就可以把白子提走

1:03:49.080,1:03:51.860
那白子要接在這邊，才不會被提走

1:03:51.860,1:03:54.960
總之，這個是一個 pattern，你看到這個 pattern 應該會

1:03:54.960,1:03:57.860
做一些應收，做一些相應的事情

1:03:57.860,1:04:00.760
比如說，你會落子在這個地方

1:04:02.200,1:04:06.180
那現在你只需要看這個小小的範圍

1:04:06.180,1:04:09.760
你就可以偵測說，這個白子是不是屬於被叫吃的狀態

1:04:09.760,1:04:13.620
你不需要看整個棋盤，才知道這件事情

1:04:13.620,1:04:17.580
所以，這件事情跟 image 是有同樣的性質

1:04:17.580,1:04:21.180
那在 AlphaGo 裡面，它第一個 layer 的 filter

1:04:21.180,1:04:24.880
其實就是用 5*5 的 filter

1:04:24.880,1:04:30.060
顯然做這個設計的人，覺得圍棋上最基本的 pattern

1:04:30.060,1:04:32.440
可能都是在 5*5 的範圍內

1:04:32.440,1:04:34.060
就可以被偵測出來

1:04:34.060,1:04:36.100
並不需要看整個棋盤

1:04:36.100,1:04:37.640
才能夠知道這件事情

1:04:39.320,1:04:43.380
接下來，我們說 image 還有一個特性是，同樣的 pattern

1:04:43.380,1:04:48.420
會出現在不同的 region，而它們代表的是同樣的意義

1:04:48.420,1:04:51.340
在圍棋上，也可能有同樣的現象

1:04:51.340,1:04:56.080
像這個叫吃的 pattern，它可以出現在棋盤的左上角

1:04:56.080,1:04:58.320
也可以出現在右下角

1:04:58.320,1:05:01.320
它們都是叫吃，它們都代表了同樣的意義

1:05:01.320,1:05:03.740
這些同樣的 pattern，出現在不同的位置

1:05:03.740,1:05:05.580
它們也都代表同樣的意義

1:05:05.580,1:05:07.600
所以，你可以用同一個 detector

1:05:07.600,1:05:11.340
來處理這些在不同位置的同樣的 pattern

1:05:11.340,1:05:13.060
所以，對圍棋來說呢

1:05:13.060,1:05:16.120
它在第一個 observation 和第二個 observation

1:05:16.120,1:05:18.400
是有這個 image 的特性的

1:05:18.400,1:05:22.120
但是，讓我沒有辦法相通的地方，就是第三點

1:05:22.580,1:05:25.780
我們說，我們可以對一個 image

1:05:25.780,1:05:29.040
做 subsampling，你拿掉奇數行、偶數列的 pixel

1:05:29.040,1:05:31.900
把 image 變成原來的 1/4 的大小

1:05:31.900,1:05:36.640
但是，也不會影響你看這張圖的樣子

1:05:36.640,1:05:40.660
但是，因為基於這個觀察，所以

1:05:40.660,1:05:42.280
有 Max pooling 這個 layer

1:05:43.380,1:05:45.540
但是，對圍棋來說

1:05:45.540,1:05:47.700
你可以做這件事情嗎？

1:05:47.700,1:05:51.460
比如說，你可以對一個棋盤丟掉奇數行和偶數列

1:05:51.460,1:05:53.320
它還是同一個函式嗎？

1:05:53.320,1:05:54.480
顯然是不是的

1:05:54.480,1:05:57.060
就算我對圍棋甚麼都不懂，我也覺得說

1:05:57.060,1:06:01.380
這顯然不 work，那這件事就讓我相當地困擾

1:06:01.380,1:06:03.860
那我看網路上一些文章，有一些人覺得說

1:06:03.860,1:06:07.860
因為 AlphaGo 裡面可能有用了 Max pooling 這樣的架構

1:06:07.860,1:06:10.420
它是用 CNN ，所以裡面可能有 Max pooling 這樣的架構

1:06:10.420,1:06:12.140
所以，或許這是一個

1:06:12.140,1:06:14.740
它的弱點

1:06:14.740,1:06:17.960
你要針對這個弱點取攻擊它，就可以擊敗它

1:06:17.960,1:06:20.800
但是，AlphaGo 很強，它可能比李世乭還強

1:06:20.800,1:06:23.340
它可能比高永夏還強了，所以

1:06:23.340,1:06:26.240
它顯然沒有這個顯而易見的弱點了

1:06:26.240,1:06:29.760
它顯然沒有這個顯而易見的弱點了，所以

1:06:29.760,1:06:32.960
到底是怎麼回事呢，我就覺得相當地困惑

1:06:33.340,1:06:36.000
有一天，我突然領悟到

1:06:36.000,1:06:40.060
會不會在這個 AlphaGo 的 CNN 裡面

1:06:40.060,1:06:41.780
有甚麼特別的地方呢

1:06:41.780,1:06:44.480
我相信說，大家都讀過 AlphaGo 的 paper

1:06:44.480,1:06:49.140
這個 paper 也沒幾頁，好像就 6 頁吧，一下就看完了

1:06:49.140,1:06:51.220
但是，它後面有一個很長的附錄

1:06:51.220,1:06:52.860
那我們一般都是不看附錄的

1:06:56.780,1:06:59.760
它的 paper 裡面，從來沒有

1:06:59.760,1:07:02.320
它只說了一句說，它用 CNN

1:07:02.320,1:07:06.560
它沒有在正文裡面，仔細地描述它 CNN 的架構

1:07:06.560,1:07:09.520
會不會實際上它 CNN 的架構裡面有甚麼

1:07:09.520,1:07:11.080
特別的玄機呢

1:07:11.080,1:07:12.860
所以，就讀了一下它的附錄

1:07:12.860,1:07:16.940
附錄裡面，它描述了它 neural network 的 structure

1:07:16.940,1:07:18.260
它是這樣說的

1:07:19.200,1:07:23.540
它的 input 是一個 19*19*48 的 image

1:07:23.540,1:07:26.640
19*19 我們可以理解，因為一個棋盤就是 19*19

1:07:26.640,1:07:28.740
48 是怎麼來的呢

1:07:28.740,1:07:31.140
對 AlphaGo 來說呢

1:07:31.140,1:07:37.280
它把每一個位置都用 48 個 value 來描述它

1:07:39.620,1:07:42.740
它把每一個位置

1:07:42.740,1:07:45.880
都用 48 個 value 來描述它

1:07:45.880,1:07:47.580
那這裡面的 value 呢

1:07:47.580,1:07:51.700
就我們本來說，我們只要在一個位置描述說

1:07:51.700,1:07:54.760
它是不是白子，有沒有黑子就可以了

1:07:54.760,1:07:57.440
那其實 AlphaGo 它有加上了 domain knowledge

1:07:57.440,1:08:00.380
它不只是說有一個位置，它有沒有白子或黑子

1:08:00.380,1:08:05.460
它還會看說，這個位置是不是處於

1:08:05.460,1:08:07.860
叫吃的狀態呢，等等

1:08:07.860,1:08:12.000
所以，我們如果讀完這段的話，你會發現說

1:08:12.000,1:08:15.560
第一個 layer，它有做 zero padding

1:08:15.560,1:08:18.800
也就是說，它把原來 19*19 的 image

1:08:18.800,1:08:23.960
外圍補上更多 0，讓它變成一張

1:08:23.960,1:08:25.980
23*23 的 image

1:08:29.960,1:08:34.220
然後，它的第一個 layer 用的是

1:08:34.220,1:08:36.700
5*5 的 filter

1:08:36.700,1:08:40.040
總共有 k 個 filter，k 的值呢

1:08:40.040,1:08:42.420
它在 paper 裡面用的是 192

1:08:42.420,1:08:45.180
因為它有試 128 跟 256

1:08:45.180,1:08:48.160
接下來，它的 stride 是設 1

1:08:48.160,1:08:51.640
然後，它有用 ReLU 的 activation function

1:08:51.640,1:08:56.440
接下來，它有 2~12 層

1:09:00.060,1:09:05.380
apply 5*5 的 filter 以後，它變成 21*21 的 image

1:09:05.380,1:09:09.200
那接下來的 filter 都是 3*3 的 filter，它的 stride

1:09:09.200,1:09:11.500
都是 1，然後

1:09:11.500,1:09:15.780
你就會發現說，其實 AlphaGo 是沒有用 Max pooling 的

1:09:15.780,1:09:17.120
有沒有很神奇呢

1:09:17.120,1:09:18.920
它是沒有用 Max Pooling 的

1:09:18.920,1:09:21.060
所以，這個 neural network 架構的設計

1:09:21.060,1:09:24.760
是這個應用之道，存乎一心

1:09:24.760,1:09:27.860
雖然說，在這個 image 裡面

1:09:27.860,1:09:29.400
我們都會用 Max pooling 這個架構

1:09:29.400,1:09:32.840
但是，針對圍棋的特性來
設計 neural network 的架構的時候

1:09:33.520,1:09:36.140
我們是不需要 Max pooling 這個架構的

1:09:36.140,1:09:39.040
所以在 AlphaGo 裡面，它沒有用 Max pooling 這個架構

1:09:39.040,1:09:42.540
並不是疏失了甚麼之類的

1:09:42.540,1:09:46.060
而是，根據圍棋的特性，我們本來就不需要再

1:09:46.060,1:09:49.220
圍棋的 CNN 裡面，用 Max pooling 這樣的架構

1:09:50.820,1:09:52.340
那其實 CNN

1:09:52.340,1:09:55.740
也可以被用在其他的、很多的 task 裡面

1:09:55.740,1:09:59.720
比如說，CNN 也被用在影像處理上

1:09:59.720,1:10:01.800
那 CNN 也被用在影像處理上

1:10:01.800,1:10:05.080
舉例來說，這個是一段聲音

1:10:05.080,1:10:09.820
你可以把一段聲音表示成所謂的 spectrogram

1:10:09.820,1:10:11.740
所謂 spectrogram 的意思是說

1:10:11.740,1:10:14.660
這個橫軸是時間

1:10:14.660,1:10:19.260
這個縱軸是那一段時間裡面

1:10:19.260,1:10:21.220
聲音的頻率

1:10:21.220,1:10:24.600
比如說，假設我們看這一段時間

1:10:24.600,1:10:28.960
這個偏紅色就代表說，在那一段時間裡面

1:10:28.960,1:10:32.760
那一個頻率的 energy 是比較大的

1:10:32.760,1:10:37.800
那在這一段時間裡面，這一個頻率跟這一個頻率

1:10:37.800,1:10:42.240
跟這一個頻率的能量是比較高的

1:10:43.380,1:10:46.120
那這一張 image，其實是我說

1:10:46.120,1:10:49.020
你好，然後看到的 spectrogram

1:10:49.020,1:10:53.440
所以，這個東西是你，這個東西是好

1:10:53.440,1:10:57.380
如果有透過訓練的人呢，它其實可以看這張 image

1:10:57.380,1:11:00.920
就知道說這句話的內容是甚麼

1:11:00.920,1:11:03.200
我之前在 MIT 做 postdoc 的時候

1:11:03.200,1:11:06.200
MIT 的語音處理實驗室據說是

1:11:06.200,1:11:09.580
最早發明 spectrogram reading 

1:11:09.580,1:11:12.260
的團隊，所謂 spectrogram reading 的意思就是

1:11:12.260,1:11:15.140
練習看這個頻譜，就知道說

1:11:15.140,1:11:17.360
它裡面內容是甚麼

1:11:17.360,1:11:20.660
然後，他們經年累月的在練習這件事情

1:11:20.660,1:11:22.140
就練習的很強

1:11:22.140,1:11:29.700
他們每周的 group meeting 都要練習做 spectrogram

1:11:29.700,1:11:32.400
就是老師會拿出一張 spectrogram

1:11:32.400,1:11:35.760
然後，大家就要判讀這張 spectrogram 的內容

1:11:37.240,1:11:40.060
老師就會指著一段說

1:11:40.060,1:11:42.440
這個 spectrogram，這一段聲音

1:11:42.440,1:11:43.880
你覺得它是哪一個 phone 呢？

1:11:43.880,1:11:46.340
大家把一段聲音

1:11:46.340,1:11:48.820
phone 就是類似音標的東西

1:11:48.820,1:11:52.080
大家把一段聲音的音標解譯出來以後

1:11:52.080,1:11:55.280
再套 language model 上去，把它解譯成文字

1:11:55.280,1:11:57.120
是這樣，通常都會答對

1:11:57.120,1:11:58.360
那你可能會覺得說

1:11:58.360,1:12:01.940
他們練到最後可能可以看一張 image

1:12:01.940,1:12:03.720
秒反映說它的結果是甚麼

1:12:03.720,1:12:05.240
那其實沒有辦法

1:12:05.240,1:12:08.160
通常這個解譯的過程大概要花一個小時

1:12:10.360,1:12:14.080
這個東西就像念球遊戲之類的，是個很厲害的技術

1:12:14.940,1:12:19.840
既然人可以看這個 image，就知道說

1:12:19.840,1:12:23.180
它是甚麼樣的聲音訊號

1:12:23.180,1:12:24.520
甚麼樣的 phoneme

1:12:24.520,1:12:30.240
我們也可以讓機器把這個 spectrogram

1:12:30.240,1:12:32.060
就當作一張 image

1:12:32.060,1:12:35.180
然後，用 CNN 來判斷說，input 這張 image

1:12:35.180,1:12:37.700
它是對應甚麼樣的聲音訊號

1:12:37.700,1:12:40.000
那這邊通常判斷的單位，比如說是

1:12:40.000,1:12:43.980
phoneme，類似音標這樣子的單位

1:12:43.980,1:12:46.380
也有可能是 phoneme之類的

1:12:46.380,1:12:48.760
但是，這邊神奇的地方就是

1:12:48.760,1:12:51.020
當我們把一段 spectrogram

1:12:51.020,1:12:53.680
當作 image，丟到 CNN 裡面的時候

1:12:53.680,1:12:57.540
在語音上，我們通常只考慮

1:12:57.540,1:13:00.900
在 frequency 方向上移動的 filter

1:13:00.900,1:13:03.980
也就是說，我們的 filter 是這樣，是長方形的

1:13:03.980,1:13:05.600
是長方形的

1:13:05.600,1:13:08.980
它的寬，就跟我們 image 的寬是一樣的

1:13:08.980,1:13:12.200
那我們在移動 filter 的時候呢

1:13:12.200,1:13:14.100
我們是移這個方向

1:13:14.100,1:13:17.420
我們只移這個方向

1:13:17.420,1:13:19.300
為甚麼是這樣子呢？

1:13:19.300,1:13:24.160
當然也有人試過說，把 filter 移時間的方向會怎樣呢

1:13:24.160,1:13:26.180
結果是沒有太大的幫助

1:13:26.180,1:13:29.500
會這樣的原因，我覺得是因為在語音裡面

1:13:29.500,1:13:33.280
這個 CNN 的 output 後面都還會再接別的東西

1:13:33.280,1:13:36.140
比如說，再接 LSTM 阿，等等

1:13:36.140,1:13:38.440
他們都已經有考慮 typical 的 information 

1:13:38.440,1:13:41.780
所以你在 CNN 裡面，你再考慮一次時間的 information

1:13:41.780,1:13:43.520
其實，沒有甚麼特別的幫助

1:13:43.520,1:13:47.320
但是，為甚麼在頻率上的 filter 有幫助呢

1:13:47.320,1:13:50.360
想想看，我們說我們用 filter 的目的

1:13:50.360,1:13:53.320
是為了要 detect 說同樣的 pattern

1:13:53.320,1:13:54.620
出現在不同的 region

1:13:54.620,1:13:57.480
我們都可以用同一個 filter 把它 detect 出來

1:13:57.480,1:13:59.000
但在聲音訊號上面

1:13:59.000,1:14:02.480
雖然，比如說，男生跟女生發同樣的聲音

1:14:02.480,1:14:05.260
男生跟女生同樣說你好

1:14:05.260,1:14:08.420
看起來這個 spectrogram 是非常不一樣的

1:14:08.420,1:14:12.860
但實際上他們的不同，只是一個頻率的 shift 而已

1:14:12.860,1:14:15.300
男生說的你好跟女生說的你好

1:14:15.300,1:14:17.600
他們的 pattern 其實是一樣的

1:14:17.600,1:14:21.620
比如說，你看這個 spectrogram 變化的情形

1:14:21.620,1:14:23.180
其實，可能是一樣的

1:14:23.180,1:14:26.860
那男生跟女生的差別，可能只是頻率上的 shift 而已

1:14:26.860,1:14:29.000
你就只是把這個 pattern

1:14:29.000,1:14:31.960
放在 image 上的不同位置而已

1:14:31.960,1:14:34.660
所以，今天我們把 filter

1:14:34.660,1:14:38.100
在 frequency 的 direction 上移動是有效的

1:14:38.100,1:14:40.620
但是，在 time domain 上的移動

1:14:40.620,1:14:42.820
是沒有必要，是沒有太大幫助的

1:14:43.940,1:14:46.560
所以，這又是另外一個例子，就是

1:14:46.560,1:14:50.100
當你把 CNN 用在另一個 application 的時候呢

1:14:50.100,1:14:51.960
你永遠要想一想說

1:14:51.960,1:14:54.000
這個 application 的特性是什麼

1:14:54.000,1:14:56.500
而根據那個 application 的特性

1:14:56.500,1:14:58.100
來 design 你的 network 的 structure

1:14:59.000,1:15:00.940
那我們也知道說，CNN 會

1:15:00.940,1:15:03.720
被用在文字處理上面

1:15:03.720,1:15:06.000
這個是從 paper 上面截下來的圖

1:15:06.000,1:15:11.240
在文字處理上面，假設你 input 一張

1:15:11.240,1:15:14.620
一個 word sequence，假設你要做的事情是

1:15:14.620,1:15:17.220
讓 machine 偵測說，這個 word sequence

1:15:17.220,1:15:21.400
它代表的是 positive 的意思還是 negative 的意思

1:15:21.400,1:15:24.640
首先，input 一個 word sequence

1:15:24.640,1:15:27.180
你把這個 word sequence 裡面的每一個 word

1:15:27.180,1:15:30.000
都用一個 vector 來表示

1:15:30.000,1:15:31.860
那這邊的每一個 vector

1:15:31.860,1:15:36.300
它代表的這個 word，它本身的 semantic

1:15:36.300,1:15:38.520
那如果兩個 word 本身涵義越接近的話呢

1:15:38.520,1:15:43.060
他們的 vector 在高維的空間上

1:15:43.060,1:15:46.860
就越接近，這個東西叫做 word embedding

1:15:46.860,1:15:49.100
那如果你不知道 word embedding 是什麼的話

1:15:49.100,1:15:53.860
也沒有關係，你就記得說現在每一個

1:15:53.860,1:15:56.860
word 都會用一個 vector 表示

1:15:56.860,1:15:58.740
都會用一個 vector 表示

1:15:58.740,1:16:02.440
當你把每一個 word 都用一個 vector 來表示的時候

1:16:02.440,1:16:06.740
你把一個 sentence 裡面所有的 word 排再一起

1:16:06.740,1:16:09.260
它就變成一個 image

1:16:09.260,1:16:11.420
它就變成一張 image

1:16:11.420,1:16:13.820
那你可以把 CNN

1:16:13.820,1:16:16.900
套用在這個 image 上面

1:16:16.900,1:16:19.640
怎麼做呢？

1:16:19.640,1:16:23.900
當我們要把 CNN 用在文字處理上的時候

1:16:23.900,1:16:26.460
你的 filter 其實是長這樣子

1:16:26.460,1:16:28.680
你的 filter 是長這個樣子

1:16:28.680,1:16:32.100
它的高跟 image 的高

1:16:32.100,1:16:33.780
是一樣的

1:16:33.780,1:16:35.900
然後，你把你的 filter

1:16:35.900,1:16:39.280
沿著 word 的順序

1:16:39.280,1:16:42.460
沿著句子裡面詞彙的順序來移動

1:16:42.460,1:16:44.540
然後，你就會得到一個 vector

1:16:44.540,1:16:47.200
不同的 filter 就會得到不同的 vector

1:16:47.200,1:16:49.100
然後，接下來做 Max pooling

1:16:49.100,1:16:52.500
把 Max pooling 的結果丟到 fully connected layer 裡面

1:16:52.500,1:16:54.780
你就會得到最後的 output

1:16:54.780,1:16:57.220
那在文字處理上呢

1:16:57.220,1:17:02.480
這個 filter 只在時間的序列上移動

1:17:02.480,1:17:07.400
而不在這個 embedding 的 dimension 上移動

1:17:07.400,1:17:11.520
你不會設計 filter，它移動的方向是

1:17:11.520,1:17:13.820
這個方向的

1:17:13.820,1:17:16.080
為甚麼呢？如果你有

1:17:16.080,1:17:18.540
做過類似的 task

1:17:18.540,1:17:20.540
如果你有做過文字處理的 task

1:17:20.540,1:17:23.920
知道這個 embedding 的 dimension 指的是甚麼的話

1:17:23.920,1:17:25.760
知道 word vector 指的是甚麼的話

1:17:25.760,1:17:26.860
你就會知道說

1:17:26.860,1:17:31.000
其實在這個方向上移動，是不 make sense 的

1:17:31.000,1:17:34.400
因為在 word embedding 裡面呢

1:17:34.400,1:17:38.700
每一個 dimension 的含意其實是獨立的

1:17:38.700,1:17:41.620
所以，如果當我們今天使用 CNN 的時候

1:17:41.620,1:17:45.080
你會假設說，第二個 dimension

1:17:45.080,1:17:48.080
跟第一個 dimension 有某種特別的關係

1:17:48.080,1:17:50.880
那第四個 dimension 跟第五個 dimension

1:17:50.880,1:17:52.440
有某種特別的關係

1:17:52.440,1:17:56.080
而這個關係，我們是可以被

1:17:56.080,1:17:59.600
如果這個關係是重複的

1:17:59.600,1:18:02.100
這個同樣的 pattern 出現在不同的位置

1:18:02.100,1:18:03.580
它們代表的是同樣的意思

1:18:03.580,1:18:06.400
但是，在 word embedding 裡面呢

1:18:06.400,1:18:10.440
不同 dimension，它們是 independent ，它們是獨立的

1:18:10.440,1:18:12.920
所以，在這個方向上面

1:18:12.920,1:18:14.840
移動 filter，是沒有意義的

1:18:14.840,1:18:17.020
所以，如果你在做文字處理的時候

1:18:17.020,1:18:21.080
你只會在 sentence 的順序上面移動 filter

1:18:21.080,1:18:24.400
而不會在 word embedding 的 dimension 去移動 filter

1:18:24.400,1:18:28.700
所以，這個又是另外一個例子

1:18:28.700,1:18:31.320
雖然，大家覺得 CNN 很 powerful

1:18:31.320,1:18:34.480
你可以用在各個不同的地方

1:18:34.480,1:18:36.720
但是，你用在一個新的 task 的時候

1:18:36.720,1:18:38.920
你要想一想你的新的 task

1:18:38.920,1:18:42.340
在設計 CNN 的架構的時候，你要怎麼做

1:18:43.920,1:18:47.420
如果你想要知道更多有關 Regularization 的事情的話

1:18:47.420,1:18:51.060
以下是一些 reference

1:18:51.060,1:18:54.360
我們剛才看到說，如果你想要用

1:18:54.360,1:18:56.920
Deep Dream 的方法

1:18:56.920,1:19:01.600
來讓 machine 自動產生一個 digit 這件事情

1:19:01.600,1:19:02.760
是不太成功的

1:19:02.760,1:19:05.680
但是，有很多其他的方法

1:19:05.680,1:19:08.300
可以讓 machine 畫出非常清晰的圖

1:19:08.300,1:19:11.740
而其他方法可以讓 machine 看過 MNIST 裡面的那些

1:19:11.740,1:19:16.240
數字以後，就學會畫出以假亂真的數字

1:19:16.240,1:19:20.160
這邊列了幾個方法，比如說有 PixelRNN

1:19:20.160,1:19:24.120
或者是 VAE, GAN，給大家參考

1:19:24.120,1:19:28.600
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
