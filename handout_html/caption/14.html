<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:02.760<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:03.060,0:00:05.180<br>
現場打一下，有同學可能會問說<br>
<br>
0:00:05.180,0:00:08.180<br>
怎麼不在家事先寫好這樣子<br>
<br>
0:00:08.180,0:00:11.620<br>
就是這個要現場寫才潮這樣子<br>
<br>
0:00:12.780,0:00:15.360<br>
在這裡可以讓你知道說<br>
<br>
0:00:15.360,0:00:18.260<br>
這個 task 有多麼的簡單<br>
<br>
0:00:18.260,0:00:22.240<br>
這個字會不會太大，我們把它縮小一點<br>
<br>
0:00:26.100,0:00:29.240<br>
有人就覺得說，這個 deep learning<br>
<br>
0:00:29.240,0:00:30.760<br>
聽起來這麼潮的東西<br>
<br>
0:00:30.760,0:00:32.940<br>
它應該非常非常的複雜<br>
<br>
0:00:32.940,0:00:34.800<br>
它應該非常難實作，但其實不會<br>
<br>
0:00:34.800,0:00:37.120<br>
因為，現在 toolkit 的 framework<br>
<br>
0:00:37.120,0:00:38.800<br>
都非常的方便<br>
<br>
0:00:38.800,0:00:40.600<br>
我覺得它就像是那個<br>
<br>
0:00:40.600,0:00:44.120<br>
二向箔一樣，隨手拿出來就可以給別人大為打擊<br>
<br>
0:00:44.125,0:00:46.375<br>
沒有人知道二向箔是甚麼就算了<br>
<br>
0:00:48.460,0:00:50.800<br>
這個就是隨手就可以拿出來的東西<br>
<br>
0:00:50.800,0:00:53.515<br>
那你要 import 一些東西<br>
<br>
0:00:53.520,0:00:55.340<br>
你就自己回去 import  一下<br>
<br>
0:00:55.340,0:00:58.520<br>
那你能這邊先寫了一個 load data 的 function<br>
<br>
0:00:58.660,0:01:01.655<br>
這個內部的細節，你就不用管了<br>
<br>
0:01:01.655,0:01:04.755<br>
那你就先 call 一下 load data 的 function<br>
<br>
0:01:04.755,0:01:05.920<br>
把 data 先 load 一下<br>
<br>
0:01:05.920,0:01:08.225<br>
我們現在要做的是手寫數字的辨識<br>
<br>
0:01:11.315,0:01:12.715<br>
然後，load data<br>
<br>
0:01:15.080,0:01:18.880<br>
等一下，我們先看一下 x_train 跟 y_train<br>
<br>
0:01:18.880,0:01:21.200<br>
這邊是 call TensorFlow，那你可以看到它這個<br>
<br>
0:01:21.200,0:01:23.580<br>
有說 using TensorFlow backend<br>
<br>
0:01:24.440,0:01:26.940<br>
我們來看一下 x_train 呢<br>
<br>
0:01:26.940,0:01:30.060<br>
長甚麼樣子，我們打 x_train.shape<br>
<br>
0:01:32.800,0:01:36.820<br>
那這告訴我們說，x_train 是一個二維的向量<br>
<br>
0:01:36.820,0:01:39.360<br>
那它的第一個維度是一萬<br>
<br>
0:01:39.360,0:01:42.580<br>
那第二個維度的 dimension 是 784<br>
<br>
0:01:42.580,0:01:44.580<br>
那這甚麼意思呢？這告訴我們說<br>
<br>
0:01:44.580,0:01:46.940<br>
現在 training data 總共一萬筆，每一筆呢<br>
<br>
0:01:46.940,0:01:51.120<br>
它是由一個 784 維的 vector 所表示的<br>
<br>
0:01:51.120,0:01:54.000<br>
那我們來把第一個 vector 拿出來看<br>
<br>
0:01:54.000,0:01:55.995<br>
它長的就是這個樣子<br>
<br>
0:01:56.000,0:02:00.960<br>
它是一個很長的 vector，有 784 維的 vector<br>
<br>
0:02:01.060,0:02:03.740<br>
那在這個 vector 裡面，多數的值都是 0<br>
<br>
0:02:03.740,0:02:06.100<br>
那有少部分的值，你會發現說它<br>
<br>
0:02:06.100,0:02:08.120<br>
是介於 0~1 之間<br>
<br>
0:02:08.160,0:02:11.220<br>
那這些介於 0~1 之間的值就代表說<br>
<br>
0:02:11.220,0:02:12.760<br>
現在這個 pixel<br>
<br>
0:02:13.260,0:02:15.300<br>
它有沒有被塗黑<br>
<br>
0:02:15.300,0:02:17.520<br>
塗得最黑就是 1<br>
<br>
0:02:17.520,0:02:23.760<br>
所以，這邊這個數值代表說這個 pixel 的顏色有多深<br>
<br>
0:02:25.725,0:02:27.620<br>
其實，你很難從這個<br>
<br>
0:02:27.620,0:02:30.520<br>
vector 裡面就看出說，它是哪一個數字<br>
<br>
0:02:30.520,0:02:33.320<br>
這樣你很難看出說它是哪一個數字<br>
<br>
0:02:33.360,0:02:35.780<br>
所以，我們來看一下它的 label<br>
<br>
0:02:35.780,0:02:37.760<br>
才會知道它是哪一個數字<br>
<br>
0:02:37.760,0:02:39.140<br>
來看一下 label<br>
<br>
0:02:43.100,0:02:46.060<br>
label 一樣第一個 dimension 是一萬維<br>
<br>
0:02:46.060,0:02:47.680<br>
第二個 dimension 是 10 維<br>
<br>
0:02:47.680,0:02:49.280<br>
那我們把第一筆 data<br>
<br>
0:02:50.000,0:02:52.440<br>
它的 label 拿出來看看<br>
<br>
0:02:52.440,0:02:53.575<br>
那你看第一筆 data<br>
<br>
0:02:53.575,0:02:55.585<br>
它的 label 是<br>
<br>
0:02:56.240,0:02:59.060<br>
它總共有 10 維<br>
<br>
0:02:59.060,0:03:01.160<br>
那你會發現說，多數的數字是 0<br>
<br>
0:03:01.160,0:03:04.160<br>
只有某一個維度的數字是 1<br>
<br>
0:03:06.520,0:03:09.460<br>
只有某一個維度的數字是 1<br>
<br>
0:03:09.460,0:03:12.260<br>
那你今天算一下，它是從 0 開始算的<br>
<br>
0:03:12.260,0:03:14.680<br>
這個是對第一個 dimension<br>
<br>
0:03:14.680,0:03:18.420<br>
這個就對應到 0，0, 1, 2, 3, 4, 5<br>
<br>
0:03:18.420,0:03:20.240<br>
對應到 5 的那個維度是 1<br>
<br>
0:03:20.240,0:03:22.880<br>
意味著說我們剛才看到的那一串數字<br>
<br>
0:03:22.880,0:03:24.280<br>
其實代表一個數字 5<br>
<br>
0:03:24.280,0:03:27.320<br>
雖然，你很難看出來它是甚麼<br>
<br>
0:03:27.320,0:03:29.280<br>
不過，我們用這種方式告訴 machine 說<br>
<br>
0:03:29.280,0:03:33.460<br>
剛才那個 vector，它對應的就是數字 5<br>
<br>
0:03:33.555,0:03:36.615<br>
接下來，就是實際的寫一下了<br>
<br>
0:03:36.615,0:03:37.705<br>
那其實呢<br>
<br>
0:03:38.035,0:03:40.895<br>
非常非常快，現場秒寫<br>
<br>
0:03:50.860,0:03:53.760<br>
其實，現在是有錄影的啦<br>
<br>
0:03:53.760,0:03:56.120<br>
Bandicam 免費版只能錄 10 分鐘<br>
<br>
0:03:56.120,0:03:58.195<br>
剛才已經過 5 分鐘，所以還有 5 分鐘<br>
<br>
0:03:58.200,0:04:00.500<br>
要在 5 分鐘以內把它寫完<br>
<br>
0:04:02.665,0:04:04.625<br>
好，網路有點卡<br>
<br>
0:04:11.880,0:04:15.920<br>
input dimension 是 28*28，然後<br>
<br>
0:04:15.920,0:04:18.040<br>
unit 等於<br>
<br>
0:04:18.500,0:04:21.120<br>
就胡亂設個值，就 33<br>
<br>
0:04:23.740,0:04:26.640<br>
你就隨便挑一個你喜歡的 activation function<br>
<br>
0:04:26.640,0:04:28.220<br>
比如說，sigmoid<br>
<br>
0:04:28.225,0:04:30.905<br>
好，行，然後再加一層<br>
<br>
0:04:32.780,0:04:35.780<br>
然後，第二層你就不需要再給它 input dimension<br>
<br>
0:04:35.780,0:04:38.455<br>
你就看 unit 要設什麼，比如說，633<br>
<br>
0:04:38.460,0:04:41.240<br>
如果我有拼錯的話，記得告訴我<br>
<br>
0:04:41.240,0:04:42.820<br>
(activation function = sigmoid)<br>
<br>
0:04:43.560,0:04:46.940<br>
然後，再加一層<br>
<br>
0:04:49.115,0:04:52.335<br>
也是 633 這樣<br>
<br>
0:04:52.745,0:04:54.935<br>
然後，最後 output 的 layer 呢<br>
<br>
0:04:55.040,0:04:58.080<br>
我們就一定要是 10 維<br>
<br>
0:04:58.080,0:04:59.680<br>
弄別的它不會給你過<br>
<br>
0:05:00.125,0:05:01.985<br>
一定只能有 10 個 unit<br>
<br>
0:05:01.985,0:05:03.880<br>
activation function 其實你選別的也行<br>
<br>
0:05:03.880,0:05:05.145<br>
不過，我們就<br>
<br>
0:05:05.145,0:05:08.880<br>
用傳統的方法選 softmax<br>
<br>
0:05:10.455,0:05:13.475<br>
這樣 network 就建完了，再來就是下 config<br>
<br>
0:05:13.475,0:05:16.305<br>
model.compile<br>
<br>
0:05:16.315,0:05:18.975<br>
然後，我們選一個 loss function<br>
<br>
0:05:18.980,0:05:20.560<br>
選甚麼好呢？<br>
<br>
0:05:20.560,0:05:22.400<br>
我們就選 mean square error<br>
<br>
0:05:22.400,0:05:24.135<br>
你可能會問說為甚麼選 mean square error 呢<br>
<br>
0:05:24.135,0:05:27.275<br>
因為它數字最短，打起來最快<br>
<br>
0:05:28.900,0:05:32.560<br>
optimizer 就 SGD，我們都學過 SGD 了<br>
<br>
0:05:32.560,0:05:36.100<br>
然後，要設一下，SGD 這邊指的就是 Gradient Descent<br>
<br>
0:05:36.480,0:05:39.580<br>
learning rate 設 0.1<br>
<br>
0:05:40.060,0:05:42.240<br>
learning rate 你是要手調一下的<br>
<br>
0:05:42.240,0:05:44.200<br>
然後，接下來<br>
<br>
0:05:44.535,0:05:45.835<br>
matrix<br>
<br>
0:05:51.140,0:05:54.060<br>
compile 也完了，再來就是 train 它<br>
<br>
0:05:54.080,0:05:56.580<br>
fit 這樣，非常快<br>
<br>
0:05:58.820,0:06:01.325<br>
然後，有兩個 flag 要下<br>
<br>
0:06:01.325,0:06:04.065<br>
這個在 video 裡面有說明<br>
<br>
0:06:04.975,0:06:07.955<br>
我們有說明甚麼是 batch_size<br>
<br>
0:06:07.955,0:06:10.485<br>
甚麼是 epochs<br>
<br>
0:06:11.225,0:06:13.175<br>
下 20 好了<br>
<br>
0:06:14.060,0:06:17.220<br>
最後，然後下完這行之後<br>
<br>
0:06:17.220,0:06:19.420<br>
你就可以假設你的 model 已經 train 完了<br>
<br>
0:06:19.420,0:06:22.000<br>
到目前為止，才過了一分半而已這樣子<br>
<br>
0:06:22.640,0:06:24.780<br>
再來，我們就是要把<br>
<br>
0:06:24.780,0:06:27.940<br>
拿 model 來做 evaluate，看它作的怎麼樣<br>
<br>
0:06:36.895,0:06:39.965<br>
我們就是把 testing data 拿出來<br>
<br>
0:06:39.965,0:06:42.180<br>
其實 fit function 裡面你也可以 call<br>
<br>
0:06:42.180,0:06:43.800<br>
你也可以放 validation set<br>
<br>
0:06:43.800,0:06:46.780<br>
它可以自動幫你做 validation<br>
<br>
0:06:46.780,0:06:49.760<br>
你只要再查一下怎麼用就可以了<br>
<br>
0:06:53.000,0:06:55.300<br>
把輸出導出來，然後<br>
<br>
0:06:55.300,0:06:57.060<br>
印一下輸出<br>
<br>
0:07:12.725,0:07:15.795<br>
這樣我就寫完了<br>
<br>
0:07:15.795,0:07:17.475<br>
花不到兩分鐘<br>
<br>
0:07:17.800,0:07:20.620<br>
接下來呢，我們就是<br>
<br>
0:07:20.620,0:07:23.600<br>
實際執行一下<br>
<br>
0:07:27.145,0:07:28.680<br>
那你可能會問說<br>
<br>
0:07:28.680,0:07:30.760<br>
聽說做 deep learning 要 GPU<br>
<br>
0:07:30.760,0:07:32.080<br>
GPU 在哪裡呢？<br>
<br>
0:07:32.080,0:07:33.315<br>
不要管那麼多<br>
<br>
0:07:33.315,0:07:35.145<br>
TensorFlow 會幫你處理這個問題的<br>
<br>
0:07:40.120,0:07:42.020<br>
你就 train 很快這樣子<br>
<br>
0:07:45.640,0:07:48.700<br>
我知道為甚麼了，這邊怎麼會打成 score 呢<br>
<br>
0:07:49.780,0:07:53.420<br>
太羞愧了，好，result<br>
<br>
0:08:11.400,0:08:13.660<br>
要不要看一下正確率是多少在 testing 上<br>
<br>
0:08:13.660,0:08:16.820<br>
正確率是 11% 這樣<br>
<br>
0:08:21.995,0:08:24.945<br>
因為是手寫數字辨識阿，所以<br>
<br>
0:08:24.945,0:08:27.300<br>
random 猜也要 10% 這樣<br>
<br>
0:08:27.300,0:08:31.100<br>
怎麼辦呢？怎麼辦，這時候就開始焦躁了<br>
<br>
0:08:32.445,0:08:33.445<br>
調一下參數<br>
<br>
0:08:34.015,0:08:37.105<br>
我知道，一定是 633 不吉利這樣<br>
<br>
0:08:37.105,0:08:40.020<br>
改成 689 這樣<br>
<br>
0:08:40.020,0:08:42.460<br>
胡亂改、亂改<br>
<br>
0:08:46.545,0:08:47.660<br>
好，再 train 一次<br>
<br>
0:08:47.660,0:08:50.100<br>
其實 train 這個，還滿快的啦<br>
<br>
0:08:50.100,0:08:52.220<br>
不過這是因為這是手寫數字辨識<br>
<br>
0:08:52.220,0:08:53.580<br>
用別的 corpus 的話，你 train 一次<br>
<br>
0:08:53.580,0:08:55.980<br>
一天就過去了，三天就過去了<br>
<br>
0:08:55.980,0:08:57.520<br>
你就焦躁到不行這樣<br>
<br>
0:09:04.800,0:09:07.100<br>
欸，好一點是嗎？<br>
<br>
0:09:07.100,0:09:08.540<br>
好一點是嗎<br>
<br>
0:09:08.540,0:09:11.920<br>
那接下來，我再覺得說<br>
<br>
0:09:11.980,0:09:14.580<br>
你知道 deep learning 就是要很 deep 這樣子<br>
<br>
0:09:14.580,0:09:16.980<br>
剛才怎麼才三層，不夠 deep<br>
<br>
0:09:16.980,0:09:18.680<br>
for 迴圈加十層<br>
<br>
0:09:18.680,0:09:20.600<br>
Keras 是可以 call for 迴圈的<br>
<br>
0:09:37.880,0:09:42.240<br>
好，十層，你說時間快到了是嗎？<br>
<br>
0:09:47.820,0:09:50.480<br>
啊！結果還是 10% 這樣子<br>
<br>
0:09:50.480,0:09:53.260<br>
然後，你就開始焦躁不安<br>
<br>
0:09:53.260,0:09:55.145<br>
你可能會這樣，就是<br>
<br>
0:09:55.145,0:09:57.780<br>
你的老師告訴你說 deep learning 很潮<br>
<br>
0:09:57.780,0:09:59.820<br>
回去還不趕快給我做個 application 出來<br>
<br>
0:09:59.820,0:10:01.260<br>
而且 Keras 又這麼簡單<br>
<br>
0:10:01.260,0:10:03.975<br>
你回去還不給我寫個 10 個、8 個，然後<br>
<br>
0:10:03.980,0:10:07.000<br>
你 train 一次三天就過去了，再 train 一次三天又過去了<br>
<br>
0:10:07.000,0:10:09.280<br>
然後，把它改成很 deep 三天又過去了<br>
<br>
0:10:09.280,0:10:12.700<br>
然後，過了一個禮拜，你就發現說你什麼都沒有做出來<br>
<br>
0:10:14.280,0:10:17.580<br>
然後，你參數就調來調去<br>
<br>
0:10:17.580,0:10:21.020<br>
你就發現說，欸，怎麼做都做不起來<br>
<br>
0:10:21.020,0:10:24.240<br>
然後，最後你就只好從入門到放棄<br>
<br>
0:10:25.480,0:10:27.700<br>
所以<br>
<br>
0:10:27.700,0:10:29.235<br>
很多人會擔心說<br>
<br>
0:10:29.240,0:10:32.140<br>
人工智慧會不會統治世界啊？<br>
<br>
0:10:32.140,0:10:34.780<br>
取代人類啊？我覺得在問這個問題之前<br>
<br>
0:10:34.780,0:10:37.480<br>
你要不要先把這個很簡單的 task 做好<br>
<br>
0:10:37.480,0:10:38.860<br>
而且你都做過作業一了<br>
<br>
0:10:38.860,0:10:40.040<br>
我相信做過作業以後<br>
<br>
0:10:40.040,0:10:42.980<br>
你一定不會有人工智慧統治這個世界這個問題<br>
<br>
0:10:42.980,0:10:47.260<br>
你會說怎麼連 Linear Regression 都做不起來呢？<br>
<br>
0:10:50.445,0:10:53.595<br>
就是這個樣子，所以我們今天沒有做起來<br>
<br>
0:10:53.880,0:10:56.180<br>
然後，到底要怎麼把它做起來呢？<br>
<br>
0:10:56.180,0:11:00.200<br>
就是請看介紹的錄影，會告訴你說<br>
<br>
0:11:00.200,0:11:02.320<br>
deep learning 有甚麼樣的 tip<br>
<br>
0:11:02.355,0:11:04.120<br>
當你做不起來的時候，在放棄之前<br>
<br>
0:11:04.120,0:11:06.300<br>
有什麼事情是你可以做的<br>
<br>
0:11:06.300,0:11:08.340<br>
今天就講到這邊，謝謝大家，謝謝！<br>
<br>
0:11:08.340,0:11:12.900<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
