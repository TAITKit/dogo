0:00:00.000,0:00:02.060
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.060,0:00:03.500
那我猜剛才 Policy Gradient 的這個部分

0:00:03.500,0:00:05.500
你其實沒有聽得太懂

0:00:05.500,0:00:08.400
就算你聽得懂的式子

0:00:08.400,0:00:11.060
你也不知道要如何實作

0:00:11.620,0:00:16.260
好，所以我們來講一下，如果是實際實作的時候

0:00:16.260,0:00:19.520
你到底是怎麼做的

0:00:19.520,0:00:21.140
怎麼做呢？

0:00:21.140,0:00:24.720
首先，這整個大的 picture 是這樣

0:00:24.720,0:00:29.280
我們先有一個 actor，它的參數是 θ

0:00:29.280,0:00:33.360
一開始，你就先 random initialize 就好了

0:00:33.360,0:00:39.060
接下來，你有了這個初始的 actor, θ 以後

0:00:39.060,0:00:43.300
你就拿這個初始的 actor, θ 去玩 N 次遊戲

0:00:43.300,0:00:47.620
那玩 N 次遊戲，你就收集到 N 個 trajectory

0:00:47.620,0:00:50.280
那假設你收集到一個 τ^1 (trajectory 1)

0:00:50.280,0:00:53.520
那這個 τ^1 裡面有 state 1

0:00:53.520,0:00:56.420
這個 state 1 採取了 action  a1

0:00:56.420,0:00:57.860
我把我的雷射筆叫出來

0:00:57.860,0:01:01.500
state 2 採取了 action a2

0:01:01.500,0:01:02.760
以此類推

0:01:02.760,0:01:05.000
然後，玩完這個遊戲以後呢

0:01:05.000,0:01:08.220
你可以算出一個  total reward, R(τ)

0:01:08.220,0:01:11.380
那有一個 τ^1，在 τ^2 裡面

0:01:11.380,0:01:13.640
你也有 τ^2 的 state 1

0:01:13.640,0:01:16.480
還有在 state 1  採取 action a1

0:01:16.480,0:01:20.340
你有 τ^2 跟 state 2，你有在 state 2裡面採取 action a2

0:01:20.340,0:01:24.720
你要可以算出 τ^2 的 reward

0:01:24.720,0:01:27.000
好，那這個東西都沒有什麼問題

0:01:27.000,0:01:32.160
你有一個 policy，random 初始的，去

0:01:32.160,0:01:33.680
這個實作上沒有什麼問題

0:01:33.680,0:01:35.400
你把 state action 都記錄下來

0:01:35.400,0:01:36.520
沒有什麼問題

0:01:36.520,0:01:39.540
好，那有了這一些 data 以後

0:01:39.540,0:01:44.320
你就拿這一些 data 去 update 你的參數，θ

0:01:44.320,0:01:46.920
怎麼拿這些 data 去 update 你的 參數 θ 呢

0:01:46.920,0:01:49.240
就用剛才我們在前一頁投影片裡面

0:01:49.240,0:01:51.320
推出來的這個式子

0:01:51.320,0:01:55.300
那等一下，我們會再詳細解釋一下這個式子

0:01:55.300,0:01:59.500
到底實作上，你要怎麼 implement

0:01:59.500,0:02:01.500
現在看起來有點複雜

0:02:01.500,0:02:04.260
或是，式子你懂，實際上要怎麼做

0:02:04.260,0:02:06.480
你也許不太清楚

0:02:06.480,0:02:10.220
那沒關係，反正就是，我們收集，我們等一下再解釋

0:02:10.220,0:02:12.340
反正我們就是收集到一堆 data 以後

0:02:12.340,0:02:16.980
我們可以拿這一些 data 去 update 我們的參數

0:02:16.980,0:02:18.980
update 完參數以後

0:02:18.980,0:02:21.920
你有了一個新的 actor

0:02:21.920,0:02:23.700
你有了一個新的 actor 以後

0:02:23.700,0:02:25.360
你再去玩 N 次遊戲

0:02:25.360,0:02:27.340
因為 actor 是新的

0:02:27.340,0:02:28.940
它跟之前的 actor 不一樣了

0:02:28.940,0:02:31.080
所以在玩 N 次遊戲的時候

0:02:31.080,0:02:34.120
你可能會得到不太一樣的分布

0:02:34.120,0:02:36.040
你會得到一個不太一樣的結果

0:02:36.040,0:02:38.700
你再把這個結果收集起來

0:02:38.700,0:02:41.000
再去調你的 model

0:02:41.000,0:02:45.820
然後，有了新的 model 以後再去跟環境互動 N 次

0:02:45.820,0:02:48.480
再收集資料，再調你的 model

0:02:48.480,0:02:50.360
再收集資料，再調你的 model

0:02:50.360,0:02:53.300
就這樣，陷入一個循環這樣

0:02:53.300,0:02:58.160
所以我想，這個應該是很清楚的，對吧

0:02:58.160,0:03:01.280
這個大家有問題嗎 ？

0:03:01.700,0:03:04.000
如果大家沒有問題的話

0:03:04.000,0:03:05.560
接下來的問題就是

0:03:05.560,0:03:11.080
這一項， 到底是什麼意思

0:03:11.080,0:03:15.480
這個 summation over 所有的 trajectory 你知道

0:03:15.480,0:03:19.480
summation over 某一個 trajectory 所有的 time step

0:03:19.480,0:03:21.600
這個意思，你也知道

0:03:21.600,0:03:23.820
R(τ) 你也知道

0:03:23.820,0:03:27.240
就這個常數項你知道

0:03:27.240,0:03:33.840
但是，∇ log p( a(上標 n, 下標 t) | s(上標n, 下標t) )

0:03:33.840,0:03:37.380
到底是什麼，也許你有點困惑

0:03:37.380,0:03:40.260
那我們來考慮另外一個 case

0:03:40.260,0:03:45.140
我們假設，我們現在要做的是一個分類的問題

0:03:45.140,0:03:48.840
我們現在有一個 network，我們現在有一個 actor

0:03:48.840,0:03:52.400
我們把這個 actor 當作是一個 classify

0:03:52.400,0:03:54.700
這個 classify 做的事情是

0:03:54.700,0:03:57.740
given 一個畫面 S

0:03:57.740,0:04:02.020
它分類說，我們現在應該要採取哪一個 action

0:04:02.020,0:04:04.260
現在有 3 個可以採取的 action

0:04:04.260,0:04:07.640
就是說，現在是一個有 3 個類別的

0:04:10.180,0:04:12.960
3 個類別的，分類的問題

0:04:12.960,0:04:16.260
那我們說，我們在做分類的時候

0:04:16.260,0:04:17.640
你要 train 一個 classifier

0:04:17.640,0:04:22.020
你要有 labeled data， 你要給你的 network 一個 target

0:04:22.020,0:04:24.080
你要給你的 network  一個目標

0:04:24.080,0:04:28.040
那我們就說，現在的目標是 1、0、0

0:04:28.040,0:04:31.700
也就是 1，left 是正確的類別

0:04:31.700,0:04:35.560
right 跟 fire 是錯誤的類別

0:04:35.560,0:04:39.480
那我們把 network 的 output 叫做 yi

0:04:39.480,0:04:43.920
把 target 叫做 yi\head

0:04:43.920,0:04:45.420
那你記不記得

0:04:45.420,0:04:48.820
我們說，在做 classification problem 的時候

0:04:48.840,0:04:50.780
我們 minimize 的是什麼？

0:04:50.780,0:04:54.600
我們 minimize 的是 cross entropy

0:04:54.600,0:04:56.100
對不對

0:04:56.100,0:04:58.280
你記不記得，我們說 

0:04:58.280,0:04:59.500
我們今天要 train 一個

0:04:59.500,0:05:01.920
可以拿來做 classification network 的時候

0:05:01.920,0:05:04.020
如果，我們在做 Regression 的時候

0:05:04.020,0:05:07.780
我們都做 minimize 的就是 cross entropy

0:05:07.780,0:05:09.560
那 cross entropy 就是

0:05:09.560,0:05:13.640
summation over 每一個 dimension、
summation over 所有的 class

0:05:13.640,0:05:20.020
然後，把 yi\head * log(yi)，前面取負號

0:05:20.360,0:05:23.400
但是因為多數的 yi\head 都是零

0:05:23.400,0:05:26.840
這邊 y2\head 跟 y3\head 都是 0

0:05:26.840,0:05:29.220
只有 y1\head 是 1

0:05:29.220,0:05:32.800
所以，實際上我們在做的事情

0:05:32.800,0:05:35.280
這邊本來是一個負號加 minimize

0:05:35.280,0:05:36.680
負號加 minimize

0:05:36.680,0:05:39.380
可以看作是 maximize

0:05:39.380,0:05:42.380
所以，我們實際上在做的事情就是

0:05:42.380,0:05:44.700
maximize log(yi)

0:05:44.700,0:05:49.660
實際上做的事情就是 maximize log(yi)

0:05:49.660,0:05:51.000
那我們知道說

0:05:51.000,0:05:57.480
所謂的 yi ，其實就是 P("left"|s)

0:05:57.480,0:05:59.520
所謂的 y1

0:05:59.520,0:06:03.100
這邊我該把它寫 y1 比較對

0:06:03.100,0:06:07.340
因為今天在這個 specific 的例子裡面，y1\head 是 1

0:06:07.340,0:06:09.800
所以，只有 log(y1) 會被留下

0:06:09.800,0:06:12.000
所以，這項應該是 log(y1)

0:06:12.000,0:06:16.640
log(y1) 其實就是 log[P("left"|s)]

0:06:16.640,0:06:19.120
就是這個 network output

0:06:19.120,0:06:24.260
這個 network 覺得要採取 action left 的機率

0:06:25.260,0:06:30.540
如果你今天要做的事情是

0:06:30.540,0:06:34.260
minimize 這個 cross entropy

0:06:34.260,0:06:35.740
或者是

0:06:35.740,0:06:38.620
同等的 maximize下面這一項

0:06:38.620,0:06:40.460
你會怎麼做呢？

0:06:40.460,0:06:42.060
你是不是就說

0:06:42.060,0:06:44.360
這是我們的 objective function

0:06:44.360,0:06:46.280
我要去 maximize 它

0:06:46.280,0:06:49.560
我當然就是對它算一個 gradient

0:06:49.560,0:06:53.980
所以，我們就對 log[P("left"|s)]

0:06:53.980,0:06:56.520
求它的 gradient

0:06:56.520,0:06:58.040
再乘上 learning rate

0:06:58.040,0:06:59.640
再加給你的參數

0:06:59.640,0:07:04.200
再用這項去 update 你的參數

0:07:04.200,0:07:07.880
所以當你看到你 update 的式子裡面

0:07:07.880,0:07:09.740
有這個項的時候

0:07:09.740,0:07:11.880
當你看到你 update 的式子裡面

0:07:11.880,0:07:16.160
有這個  log[P("left"|s)] 的時候

0:07:16.160,0:07:19.480
它的意思，其實是說

0:07:19.480,0:07:24.100
我們希望這一個 objective function 越大越好

0:07:24.100,0:07:26.040
或這個 objective function 越小越好

0:07:26.040,0:07:30.320
或者是說，我們其實是在解一個分類的問題

0:07:30.320,0:07:32.400
然後，我們希望我們 network 的 output

0:07:32.400,0:07:35.760
跟我們訂下來的 target 越接近越好

0:07:35.760,0:07:37.640
而在我們的 target 裡面

0:07:37.640,0:07:41.020
就是left，就是正確的類別

0:07:41.020,0:07:44.160
而其他的是錯誤的類別

0:07:44.600,0:07:48.180
這樣大家了解我的意思嗎？

0:07:48.180,0:07:50.560
假如你可以了解這個意思的話

0:07:50.560,0:07:56.020
那接下來，我們怎麼解釋這個式子呢？

0:07:56.020,0:07:57.800
怎麼解釋這個式子呢？

0:07:57.800,0:08:01.180
我們先把 R 拿掉

0:08:01.180,0:08:04.580
我們先把 R(τ^n) 拿掉，當作那一項等於 1

0:08:04.580,0:08:08.180
你就當作所有的 trajectory reward 都是 1

0:08:08.180,0:08:09.740
就不要管它了

0:08:09.740,0:08:16.480
那這個 ∇ log[P("left"|s)] 是什麼意思

0:08:16.480,0:08:18.440
它的意思就是說

0:08:18.440,0:08:22.860
我們現在 training data裡面有一個 s1、a1

0:08:22.860,0:08:24.780
有一個 s1，a1

0:08:24.780,0:08:29.640
那我們假設說 a1就是 left

0:08:29.640,0:08:32.760
那我們要做的事情就是

0:08:32.760,0:08:34.480
我們把 s1 丟到 network 裡面

0:08:34.480,0:08:37.620
它給我們 left、right 跟 fire 的機率

0:08:37.620,0:08:41.680
那我們希望這個機率跟 1、0、0 越接近越好

0:08:41.680,0:08:44.820
因為現在的 a1 是 left

0:08:44.820,0:08:46.640
a1 是 left

0:08:46.640,0:08:47.780
所以我們希望

0:08:47.780,0:08:50.380
所以就告訴我們說 left 是正確的

0:08:50.380,0:08:53.120
我們希望 left 的分數越大越好

0:08:53.120,0:08:56.760
然後，right 跟 fire 的分數，越接近 0 越好

0:08:58.080,0:09:01.640
那今天如果我們有一個 state 2

0:09:01.640,0:09:03.500
那如果今天

0:09:03.500,0:09:06.420
我們再另外一個 trajectory 裡面

0:09:06.420,0:09:09.160
我們有一個 s (上標 2, 下標 1)

0:09:09.160,0:09:11.040
那把 s (上標 2, 下標 1)

0:09:11.040,0:09:12.920
也丟到那個 network 裡面

0:09:13.700,0:09:15.100
這樣我們假設在

0:09:15.100,0:09:19.660
這個 s1 在 trajectory 的第一個 state 的時候

0:09:19.660,0:09:22.240
我們採取的 action 是 fire 的話

0:09:22.240,0:09:23.420
那意思就是說

0:09:23.420,0:09:28.440
我們希望 fire 的分數越大

0:09:28.440,0:09:31.080
我們希望這個 fire 的分數是 1

0:09:31.080,0:09:32.800
其他的分數是 0

0:09:32.800,0:09:36.540
所以，這就變成了一個分類的問題

0:09:36.540,0:09:40.640
你有發現嗎？這其實就是一個分類的問題

0:09:40.640,0:09:44.540
你就實際上，我們在 update 這個 式子的時候

0:09:45.760,0:09:48.120
我們真正在做的事情是

0:09:48.120,0:09:49.420
我們希望說

0:09:49.420,0:09:51.420
等於是 machine 告訴我們說

0:09:51.420,0:09:53.420
現在有一筆 training data

0:09:53.420,0:09:55.760
它的 input 就是這個樣子

0:09:55.760,0:09:57.460
它的 target 就是這個樣子

0:09:57.460,0:09:58.900
它的 input 就是這個樣子

0:09:58.900,0:10:01.200
它的 target 是這個樣子，input 就是這樣子

0:10:01.200,0:10:02.200
它的 target 就是這個樣子

0:10:02.200,0:10:05.660
然後，你把這個分類問題做對

0:10:06.160,0:10:09.340
之前，machine 看到 s1 的時候

0:10:09.340,0:10:13.020
它就採取，它採取 a1 這個 action

0:10:13.020,0:10:14.820
今天 machine learning 的目標

0:10:14.820,0:10:17.260
就是一樣， 今天 machine learning 的目標就是

0:10:17.260,0:10:20.020
希望在 input s1 的時候

0:10:20.020,0:10:24.220
它的目標就是要採取 action  a1

0:10:24.220,0:10:26.360
那你可能會想說

0:10:26.360,0:10:30.080
你就跟 machine 原來做的事情是一樣的嗎 ?

0:10:30.080,0:10:34.300
machine 本來就 input s1，就會採取 a1

0:10:34.300,0:10:36.940
那你說你是要 learn 一個 network，它的目標就是

0:10:36.940,0:10:39.960
input s1，output 就是 target a1

0:10:39.960,0:10:41.680
那不就跟原來的 network

0:10:41.680,0:10:44.280
原來的 actor 做的事情是一樣的嗎

0:10:44.280,0:10:46.180
但是有一個不一樣的地方就是

0:10:46.180,0:10:48.760
我們前面有了 reward

0:10:48.760,0:10:51.760
我們會把每一個 example

0:10:51.760,0:10:55.200
在我們假設把它當成一個分類問題的話

0:10:55.200,0:10:57.940
我會把這個分類問題的每一個 example

0:10:57.940,0:11:01.980
前面都乘上 R(τ^n)

0:11:01.980,0:11:03.660
每一個分類問題前面

0:11:03.660,0:11:06.140
我們都乘上 R(τ^n)

0:11:10.060,0:11:13.760
那這件事情到底是什麼意思呢？

0:11:13.760,0:11:17.300
把每一筆的 training data 都乘上 R(τ^n)

0:11:17.300,0:11:19.540
到底是什麼意思呢？

0:11:19.540,0:11:22.220
如果簡單的想

0:11:22.220,0:11:26.520
假設現在 R(τ^1) 的值是 2

0:11:26.520,0:11:29.200
假設 R(τ^2) 的值是 1

0:11:29.200,0:11:32.160
我們說，把每一筆 training data 前面

0:11:32.160,0:11:34.060
都 weighted by R(τ^n)

0:11:34.060,0:11:35.120
意思就是說

0:11:35.120,0:11:38.860
我們現在把 input s (上標1, 下標1)

0:11:38.860,0:11:40.598
output 要是 left 這件事情

0:11:40.598,0:11:43.240
這個 example 複製兩次

0:11:43.240,0:11:47.560
因為它的 reward 是 2，所以就複製兩次

0:11:47.560,0:11:50.140
那 input s (上標 2, 下標 1)

0:11:50.140,0:11:51.840
output fire 這件事情

0:11:51.840,0:11:53.120
它的 reward 只有  1

0:11:53.120,0:11:54.800
所以我們就複製一次

0:11:54.800,0:11:58.240
然後 ，拿這個東西去 train 我們的 network 

0:11:58.240,0:12:01.600
然後你就可以 update 一次你的 network 參數

0:12:01.600,0:12:03.540
然後再重新去 sample 得到

0:12:03.540,0:12:05.340
再重新 sample 得到新的 training data

0:12:05.340,0:12:07.120
再用這個方法去 update 參數

0:12:07.120,0:12:09.440
再去 sample data，再去 update 參數

0:12:09.440,0:12:11.960
就結束了這樣

0:12:11.960,0:12:14.940
這樣有沒有覺得很簡單呢

0:12:14.940,0:12:18.100
這樣有沒有覺得實做很簡單呢

0:12:18.100,0:12:20.400
你唯一需要的

0:12:20.400,0:12:24.720
你唯一需要的， 你只有你需要在 learn classify 的時候

0:12:24.720,0:12:26.680
給你的 training 的 example

0:12:26.680,0:12:28.060
給它 weight

0:12:28.060,0:12:30.220
你唯一需要改程式的

0:12:30.220,0:12:31.860
只有這個部分

0:12:31.860,0:12:34.940
那甚至 Keras，如果我沒有記錯的話

0:12:34.940,0:12:36.820
它其實有支援這個功能

0:12:36.820,0:12:38.080
那其他部分

0:12:38.980,0:12:42.180
你根本就不用就改了你的 code

0:12:42.180,0:12:46.260
這樣大家了解我的意思嗎？

0:12:47.100,0:12:49.420
所以，其實有人會覺得說

0:12:49.420,0:12:50.740
Reinforcement learning 很難

0:12:50.740,0:12:51.720
沒有很難

0:12:51.720,0:12:55.120
你不用改 code 就可以做 Reinforcement learning

0:12:55.120,0:12:58.100
只是很花時間，可能就是真的

0:12:58.100,0:13:00.440
因為，你每一次收集完 data 以後

0:13:00.440,0:13:02.940
你都要解一次分類的問題

0:13:02.940,0:13:04.080
train 一次 neural network

0:13:04.080,0:13:06.580
然後再去收集 data ，再 train 一次 neural network

0:13:06.580,0:13:08.060
跟我們之前做的分類的問題都不一樣

0:13:08.060,0:13:08.920
之前做分類問題

0:13:08.920,0:13:11.040
data 收集好，就在那一邊 train 完一次

0:13:11.040,0:13:12.400
就結束了

0:13:12.400,0:13:14.140
但是，在  Reinforcement learning 問題裡面

0:13:14.140,0:13:16.700
等於你要 train 你的 network 很多次

0:13:16.700,0:13:19.160
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
