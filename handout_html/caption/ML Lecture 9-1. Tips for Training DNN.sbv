0:00:00.000,0:00:03.640
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:03.640,0:00:07.500
我要講的是幾個 deep learning 的 tip

0:00:08.000,0:00:11.860
其實，這一段本來是要在 CNN 之前講的啦

0:00:11.860,0:00:15.520
那在 CNN 那段裡面，我們留下了兩個問題

0:00:15.520,0:00:20.120
第一個是，在 CNN 裡面

0:00:20.120,0:00:22.420
有 max pooling 這樣的架構

0:00:22.420,0:00:26.360
但是，max pooling 這樣的架構顯然不能微分阿

0:00:26.360,0:00:27.800
你把它放在一個 network 裡面

0:00:27.800,0:00:31.020
你在做 Gradient Descent，你在微分它的時候

0:00:31.020,0:00:33.040
你到底是怎麼處理？

0:00:33.040,0:00:36.500
那第二個問題，是我們剛才看到的

0:00:36.500,0:00:38.140
L1 的 Regularization

0:00:38.140,0:00:41.000
但是，我們還沒有解釋它是甚麼東西

0:00:41.000,0:00:44.580
那這個，我們都會在這份投影片裡面解釋

0:00:46.780,0:00:50.720
那本來是要先講這份投影片，再講 CNN 的啦

0:00:50.720,0:00:53.240
但是，因為要講作業三的關係

0:00:53.240,0:00:55.400
所以，就先講了 CNN

0:00:55.400,0:00:59.280
講完這份投影片以後，之前的一些問題就可以被解決

0:00:59.280,0:01:02.600
首先，這邊呢，最重要的一個觀念是

0:01:02.600,0:01:05.660
Deep learning 的 recipe

0:01:05.660,0:01:08.640
如果你在訓練一個 deep learning 2的 network 的時候

0:01:08.640,0:01:10.100
你在做 deep learning 的時候

0:01:10.100,0:01:13.200
它的流程，應該是甚麼樣子的

0:01:13.200,0:01:17.080
那我們都知道說，deep learning 是 3 個 step

0:01:17.080,0:01:21.640
define function 、define 你的 function set

0:01:21.640,0:01:23.740
define 你的 network 的 structure

0:01:23.740,0:01:25.700
決定你的 loss function

0:01:25.700,0:01:29.160
接下來，你就可以用 
Gradient Descent 去做 optimization

0:01:29.160,0:01:31.180
做完這些事情以後

0:01:31.180,0:01:33.700
你會得到一個 neural network

0:01:33.700,0:01:36.140
得到一個好的 neural network

0:01:36.140,0:01:39.920
接下來，你要做甚麼樣的事情呢？

0:01:39.920,0:01:42.000
接下來，你要做甚麼樣的事情呢？

0:01:42.580,0:01:45.120
其實，你第一件要檢查的事情是

0:01:45.140,0:01:48.420
這個 neural network 在你的 training set 上

0:01:48.420,0:01:50.680
有沒有得到好的結果

0:01:50.680,0:01:54.340
不是 testing set 哦，你要先檢查這個 neural network

0:01:54.340,0:01:57.700
在你的 training set 上，有沒有得到好的結果

0:01:57.700,0:01:59.780
如果，沒有的話

0:01:59.780,0:02:02.500
你就回頭去看看說，在這 3 個 step

0:02:02.500,0:02:04.600
這裡面，是不是哪邊出了問題

0:02:04.600,0:02:07.640
你可以做甚麼樣的修改，讓你在 training set 上

0:02:07.640,0:02:09.520
能夠得到好的結果

0:02:09.520,0:02:13.460
那這邊這個先檢查 training set 的 performance

0:02:13.460,0:02:16.640
其實是 deep learning 一個非常 unique 的地方

0:02:16.640,0:02:19.140
如果你想想看其他的方法，比如說

0:02:19.140,0:02:20.280
你今天如果用的是

0:02:20.280,0:02:23.500
雖然這些方法我們都還沒講過，但你或多或少都知道

0:02:23.500,0:02:25.580
比如說，k nearest neighbor

0:02:26.120,0:02:27.780
或者是 decision tree

0:02:27.780,0:02:30.600
其實像 k nearest neighbor 或 decision tree 這種方法

0:02:30.600,0:02:34.720
你做完以後，你其實會不太想檢查你 training set 的結果

0:02:34.720,0:02:36.820
因為，在 training set 上的 performance 正確率就是 100

0:02:36.820,0:02:39.740
你做完 decision tree 或做完 k nearest neighbor

0:02:39.740,0:02:42.620
得到正確率就是 100，沒有甚麼好檢查的

0:02:42.620,0:02:45.020
所以，有人說 deep learning

0:02:45.020,0:02:47.380
看這個 model 裡面這麼多參數

0:02:47.380,0:02:49.160
感覺一臉很容易 overfitting 的樣子

0:02:49.160,0:02:51.000
我跟你講，這個 deep learning 的方法

0:02:51.000,0:02:52.140
它才不容易 overfitting

0:02:52.140,0:02:54.160
我們說的 overfitting 就是在 training set 上

0:02:54.160,0:02:56.400
performance 很好，但 testing set 上 performance

0:02:56.400,0:02:57.300
沒有那麼好嘛

0:02:57.300,0:03:00.380
像這 k nearest neighbor, decision tree，它們一做下去

0:03:00.380,0:03:02.020
在 training set 上正確率都是 100

0:03:02.020,0:03:03.740
在 training set 上正確率都是 100

0:03:03.740,0:03:06.100
這個才是非常容易 overfitting

0:03:06.100,0:03:07.960
而對 deep learning 來說

0:03:07.960,0:03:09.920
overfitting 往往不是你會最

0:03:09.920,0:03:12.060
不是說，deep learning 沒有 overfitting 的問題

0:03:12.060,0:03:14.600
而是說，在 deep learning 裡面，overfitting

0:03:14.600,0:03:16.460
不是第一個你會遇到的問題

0:03:16.460,0:03:19.020
你第一個會遇到的問題，是你在 training 的時候

0:03:19.020,0:03:21.760
它並不是像 k nearest neighbor 這種方法一樣

0:03:21.760,0:03:24.200
你一 train 就可以得到非常好的正確率

0:03:24.200,0:03:26.120
它有可能在 training set 上

0:03:26.120,0:03:28.640
根本沒有辦法給你一個好的正確率

0:03:28.640,0:03:31.100
所以，這個時候你要回頭去檢查說

0:03:31.100,0:03:33.020
在前面的 step 裡面

0:03:33.020,0:03:34.780
要做什麼樣的修改

0:03:34.780,0:03:37.820
好讓你在 training set 上可以得到好的正確率

0:03:38.640,0:03:41.660
假設現在，幸運的是你已經

0:03:41.660,0:03:44.520
在 training set 上得到好的 performance 了

0:03:44.520,0:03:46.860
你要用 deep learning 在 training set 上

0:03:46.860,0:03:49.240
得到 100% 的正確率，是沒那麼容易的

0:03:49.240,0:03:53.160
但可能你在 MNIST 上得到一個 99.8% 的正確率

0:03:53.480,0:03:57.760
接下來，你就把你的 network apply 到 testing set 上

0:03:57.760,0:03:59.600
testing set 上的 performance 才是我們

0:03:59.600,0:04:02.260
最後真正關心的 performance

0:04:02.260,0:04:06.660
那你現在把你的結果 apply 到 testing set 上

0:04:07.060,0:04:09.560
那在 testing set 上 performance 怎麼樣呢？

0:04:09.560,0:04:12.900
如果現在得到的結果是 NO 的話

0:04:12.900,0:04:15.820
那就是 Overfitting

0:04:15.820,0:04:17.680
這個情況才是 Overfitting

0:04:17.680,0:04:20.560
你在 training set 上得到好的結果

0:04:20.560,0:04:24.820
但是，在 testing set 上得到的是不好的結果

0:04:24.820,0:04:28.100
這個時候，這個情況呢

0:04:28.100,0:04:29.600
才叫做 Overfitting

0:04:29.840,0:04:32.880
那你要回過頭去，做某一些事情

0:04:32.880,0:04:35.640
然後，試著去解決 overfitting 這個 problem

0:04:35.640,0:04:37.980
但有時候，你加了新的 technique

0:04:37.980,0:04:40.700
去想要 overcome overfitting 這個 problem 的時候

0:04:40.700,0:04:44.640
你其實反而會讓 training set 上的結果變壞

0:04:44.640,0:04:47.000
所以，你在做這一步的修改以後

0:04:47.000,0:04:50.580
你要先回頭去檢查說，training set 上的結果

0:04:50.580,0:04:53.740
是怎麼樣的，如果 training set 上的結果變壞的話

0:04:53.740,0:04:54.880
你要從頭呢

0:04:54.880,0:04:59.680
去對你的 network training 的 process 做一些調整

0:05:00.240,0:05:03.160
那如果你同時在 training set 還有你手上的 testing set

0:05:03.160,0:05:04.440
都得到好的結果的話

0:05:04.440,0:05:08.020
最後，你就可以把你的系統真正用在 application 上面

0:05:08.020,0:05:09.440
你就成功了

0:05:10.380,0:05:12.160
那這邊有一個重點就是

0:05:12.160,0:05:15.760
不要看到所有不好的 performance

0:05:15.760,0:05:18.140
就說是 overfitting

0:05:18.600,0:05:21.040
舉例來說，這個是文獻上的圖

0:05:21.040,0:05:24.720
但我在現實生活中，也常常看到這樣子的狀況

0:05:24.720,0:05:26.440
在 testing set 上面

0:05:26.440,0:05:28.680
這個是 testing data 的結果

0:05:28.680,0:05:32.860
橫坐標，是 model 參數 update 的次數

0:05:32.860,0:05:34.280
所以，你做 Gradient Descent 的時候

0:05:34.280,0:05:36.040
你 update 幾次參數

0:05:36.040,0:05:39.120
縱座標，是 error rate，所以越低越好

0:05:39.280,0:05:42.400
那如果我們現在表示一個 20 層的 network

0:05:42.400,0:05:43.640
它是黃線

0:05:43.640,0:05:47.260
這個 56 層的 neural network，它是紅線

0:05:47.260,0:05:50.660
那你會發現說，這個 56 層的 network

0:05:50.660,0:05:53.380
它的 error rate 比較高，它的 performance 比較差

0:05:53.380,0:05:56.840
20 層的 neural network，它的 performance 是比較好的

0:05:56.840,0:05:59.980
那有些人看到這個圖，就會馬上得到一個結論

0:05:59.980,0:06:03.580
說 56 層太多了，參數太多了

0:06:03.580,0:06:06.760
56 層果然沒有必要，這個是 overfitting

0:06:06.760,0:06:08.520
但是，真的是這樣子嗎？

0:06:08.520,0:06:12.100
你在說，現在得到的結果是 overfitting 之前

0:06:12.100,0:06:15.480
你要先檢查一下你在 training set 上的結果

0:06:15.480,0:06:17.860
對某些方法來說，你不用檢查這件事

0:06:17.860,0:06:20.140
比如說 k nearest neighbor 或 decision tree

0:06:20.140,0:06:21.980
你不用檢查這件事

0:06:21.980,0:06:23.560
但是，對 neural network 來說

0:06:23.560,0:06:25.560
你是需要檢查這件事情的

0:06:25.560,0:06:29.080
為甚麼呢？因為有可能你在 training set 上得到的結果

0:06:29.080,0:06:30.700
是這個樣子

0:06:30.700,0:06:32.480
是這個樣子的

0:06:32.480,0:06:34.740
橫軸一樣是參數 update 的次數

0:06:34.740,0:06:36.680
縱軸是 error rate

0:06:36.680,0:06:38.980
如果我們比較 20 層的 neural network 跟

0:06:38.980,0:06:40.600
56 層的 neural network 的話

0:06:40.600,0:06:41.840
你會發現說

0:06:41.840,0:06:45.640
在 training set 上 ，20 層的 neural network

0:06:45.640,0:06:48.280
它的 performance 本來就比 56 層好

0:06:48.280,0:06:50.820
在 training set 上 ，56 層的 neural network

0:06:50.820,0:06:54.620
它的 performance 是比較差的

0:06:54.620,0:06:56.020
是比較差的

0:06:56.020,0:06:58.120
那為甚麼會這樣子呢？

0:06:58.120,0:07:00.480
你想想看你在做 neural network training 的時候

0:07:00.480,0:07:02.720
有太多太多的問題

0:07:02.720,0:07:05.020
可以讓你的 training 的結果是不好的

0:07:05.020,0:07:08.460
比如說，我們有 local minimum 的問題

0:07:08.460,0:07:10.600
有 saddle point 的問題，有 plateau 的問題

0:07:10.600,0:07:11.840
有種種的問題

0:07:11.840,0:07:14.720
所以，有可能這個 56 層的 neural network

0:07:14.720,0:07:18.480
你 train 的時候，它就卡在一個 local minimum 的地方

0:07:18.480,0:07:21.420
所以，它得到了一個差的參數

0:07:21.420,0:07:23.500
所以，這個並不是 overfitting

0:07:23.500,0:07:27.020
是在 training 的時候，就沒有 train 好

0:07:27.020,0:07:31.280
那有人會說，這個叫做 underfitting 

0:07:31.280,0:07:33.940
我覺得這個可能不叫做 underfitting 

0:07:33.940,0:07:37.160
但是這個只是名詞定義的問題啦，你要怎麼說都行

0:07:37.160,0:07:40.660
但是，在我的心裡面，underfitting 的意思是說

0:07:40.660,0:07:45.000
這個 model 的 complexity

0:07:45.000,0:07:47.680
這個 model 的參數不夠多，所以

0:07:47.680,0:07:50.380
它的能力不足以解出這個問題

0:07:50.380,0:07:53.020
但對這個 56 層的 neural network 來說

0:07:53.020,0:07:54.960
雖然它得到比較差的 performance

0:07:54.960,0:07:57.180
但假如這個 56 層的 network

0:07:57.180,0:08:00.000
它其實是在這個 20 層的 network 後面

0:08:00.000,0:08:03.100
後面再另外堆 36 層的 network

0:08:03.100,0:08:07.100
那它的參數，其實是比 20 層的 network 還多的

0:08:07.100,0:08:09.960
理論上，20 層的 network 可以做到的事情

0:08:09.960,0:08:12.080
56 層的 network 一定可以做到

0:08:12.080,0:08:13.420
你前面已經有那 20 層

0:08:13.420,0:08:17.500
你前面那 20 層就做跟 20 層 network 一樣的事情

0:08:17.500,0:08:20.740
後面那 36 層就甚麼事都不幹，就都是 identity 就好了

0:08:20.740,0:08:24.140
你明明可以做到跟 20 層一樣的事情

0:08:24.140,0:08:25.860
你為甚麼做不到呢？

0:08:25.860,0:08:27.880
但是，因為會有很多的問題就是

0:08:27.880,0:08:30.300
讓你沒有辦法做到

0:08:30.300,0:08:32.860
所以，這個 56 層的 network 呢

0:08:32.860,0:08:36.280
它比 20 層差，並不是因為它能力不夠

0:08:36.280,0:08:41.860
它只要前 20 層都跟它一樣，後面都是 identity

0:08:42.560,0:08:45.000
明明就可以跟 20 層一樣好

0:08:45.000,0:08:47.000
但它卻沒有得到這樣的結果

0:08:47.000,0:08:49.660
所以，它能力是夠的，所以我覺得這不是 underfitting

0:08:49.660,0:08:53.040
它這個就是沒有 train 好這樣子

0:08:53.040,0:08:58.300
那我還不知道有沒有什麼名詞，專門指稱這個問題

0:08:58.300,0:09:02.060
所以，它其實就是像這個小智的噴火龍一樣

0:09:02.060,0:09:04.460
它等級是夠的，但它就不想要打這樣子

0:09:07.660,0:09:10.800
所以，在 deep learning 的文獻上

0:09:10.800,0:09:13.860
如果，當你讀到一個方法的時候

0:09:13.860,0:09:16.820
你永遠要想一下說，這個方法

0:09:16.820,0:09:19.580
它是要解什麼樣的問題

0:09:19.580,0:09:23.040
因為在 deep learning 裡面，有兩個問題

0:09:23.040,0:09:25.040
一個是 training set 上的 performance 不好

0:09:25.040,0:09:27.440
一個是 testing set 上的 performance 不好

0:09:27.440,0:09:29.140
當只有一個方法 propose 的時候

0:09:29.140,0:09:33.360
它往往就是針對這兩個問題的其中一個

0:09:33.360,0:09:35.000
來做處理

0:09:35.000,0:09:38.540
舉例來說，你等一下能會聽到一個方法叫做 dropout

0:09:38.640,0:09:41.240
dropout 或許大家或多或少都會知道，它是一個

0:09:41.240,0:09:44.300
很有 deep learning 特色，很潮的一個方法

0:09:46.120,0:09:48.520
那很多人就會說，哦，這麼潮的方法，所以

0:09:48.520,0:09:51.720
我今天只要看到 performance 不好，我就很快 dropout

0:09:51.720,0:09:54.220
但是，你只要仔細想一下 dropout 是甚麼時候用的

0:09:54.220,0:09:57.660
dropout 是你在 testing 的結果不好的時候

0:09:57.660,0:10:00.080
你才會 apply dropout

0:10:00.080,0:10:02.020
你的 testing data 結果好的時候

0:10:02.020,0:10:03.580
你是不會 apply dropout

0:10:03.580,0:10:05.580
就是說，dropout 是

0:10:05.580,0:10:07.780
你在 testing 結果不好的時候，才 apply dropout

0:10:07.780,0:10:10.440
如果你今天的問題是你 training 的結果不好

0:10:10.440,0:10:13.100
你 apply dropout，你只會越 train 越差而已

0:10:13.100,0:10:16.660
所以，不同的方法，對治甚麼樣不同的症狀

0:10:16.660,0:10:18.760
你是必須要在心裡想清楚的

0:10:18.760,0:10:22.260
那我們這邊就休息 10 分鐘，等一下再繼續講，謝謝

0:10:22.300,0:10:27.340
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:10:27.400,0:10:29.280
各位同學大家好

0:10:29.280,0:10:30.860
我們來上課吧

0:10:31.320,0:10:33.000
來上課吧

0:10:33.440,0:10:36.560
等一下呢，我們剛才講說

0:10:36.560,0:10:39.400
這個在 deep learning 的 recipe 裡面

0:10:39.400,0:10:41.540
在 train deep learning 的時候有兩個問題

0:10:41.540,0:10:44.020
所以，等一下呢，我們就是要

0:10:44.020,0:10:47.380
這兩個問題，分開來討論

0:10:47.380,0:10:49.800
看看當你遇到這兩個問題的時候呢

0:10:49.800,0:10:51.940
有甚麼樣解決的方法

0:10:54.360,0:10:57.160
首先，如果你今天的

0:10:57.160,0:10:59.720
training 在 training 的結果上不好的時候

0:10:59.720,0:11:01.120
你可能可以看看說

0:11:01.120,0:11:04.360
是不是你在做 network 架構設計的時候

0:11:04.360,0:11:06.100
是不是設計不好

0:11:06.100,0:11:09.520
舉例來說，你可能用的 activation function

0:11:09.520,0:11:12.080
是比較不好的 activation function

0:11:12.080,0:11:14.460
是對 training 比較不利的 activation function

0:11:14.460,0:11:16.940
你可能會換一些新的 activation function

0:11:16.940,0:11:19.040
它可以給你比較好的結果

0:11:19.700,0:11:21.260
那我們知道說

0:11:21.260,0:11:23.280
在 1980 年代的時候

0:11:23.280,0:11:25.900
比較常用的 activation function

0:11:25.900,0:11:28.400
是一個 sigmoid function

0:11:28.400,0:11:32.300
那我們之前有稍微試著 reason 一下

0:11:32.300,0:11:34.880
為甚麼要用 sigmoid function

0:11:36.320,0:11:39.620
今天如果我們用的是 sigmoid function 的時候

0:11:39.620,0:11:42.520
在過去，其實你會發現說

0:11:42.780,0:11:46.020
deeper 並不一定 imply better

0:11:46.020,0:11:49.460
這個是在 MNIST 上面的結果啦

0:11:49.460,0:11:51.220
在手寫數字辨識上的結果

0:11:51.220,0:11:53.560
當你 layer 越來越多的時候呢

0:11:53.560,0:11:57.700
你的 accuracy，一開始持平，後來就掉下去了

0:11:57.700,0:12:01.060
在你的 layer 是 9 層、10 層的時候，整個結果就崩潰啦

0:12:01.060,0:12:02.980
那有人看到這個圖，就會覺得說

0:12:02.980,0:12:06.280
9 層、10 層參數太多了，overfitting

0:12:07.080,0:12:09.060
這個，不是 overfitting

0:12:09.060,0:12:12.060
為甚麼呢？首先呢，我們說你要

0:12:12.060,0:12:15.360
檢查你現在 performance 不好是不是來自於 overfitting

0:12:15.360,0:12:18.260
你要看你 training set 的結果嘛，對不對？

0:12:18.260,0:12:21.180
那這個線，是 training set 的結果

0:12:21.180,0:12:23.440
所以，這個不是 overfitting

0:12:23.440,0:12:25.720
這個是 training 的時候，就 train 壞掉了

0:12:25.720,0:12:29.140
不信的話，我們實際來用 Keras 實做一下

0:12:31.140,0:12:33.360
一個原因是這樣子

0:12:33.960,0:12:35.640
一個原因是

0:12:35.640,0:12:38.660
這個原因叫做 Vanishing 的 Gradient

0:12:38.660,0:12:42.760
這個原因是這樣，當你把 network 疊得很深的時候

0:12:42.760,0:12:44.740
在 input 的幾個 layer

0:12:44.740,0:12:47.380
在最靠近 input 的地方呢

0:12:47.380,0:12:50.300
你的這個 Gradient

0:12:50.300,0:12:54.560
你的這些參數，對最後 loss function 的微分

0:12:54.560,0:12:56.160
會是很小的

0:12:56.160,0:12:58.740
而在比較靠近 output 的地方呢

0:12:58.740,0:13:01.460
它的微分值，會是很大的

0:13:01.460,0:13:06.080
因此，當你設定同樣的 learning rate 的時候

0:13:06.080,0:13:08.460
你會發現說，靠近 input 的地方

0:13:08.460,0:13:10.600
它參數的 update，是很慢的

0:13:10.600,0:13:14.720
靠近 output 的地方，它參數的 update 是很快的

0:13:14.940,0:13:16.660
所以，你會發現說呢

0:13:16.660,0:13:19.320
在 input 幾乎還是 random 的時候

0:13:19.320,0:13:22.100
output 就已經 converge 了

0:13:22.480,0:13:25.480
在 input layer

0:13:25.480,0:13:27.580
在靠近 input 地方的這些參數

0:13:27.580,0:13:29.120
它還是 random 的時候

0:13:29.120,0:13:32.360
output 的地方，就已經根據這些 random 的

0:13:32.360,0:13:33.620
random 的結果呢

0:13:33.620,0:13:35.640
找到了一個 local minimum

0:13:35.640,0:13:38.220
然後，它就 converge 了

0:13:38.220,0:13:39.700
這個時候，你會發現說

0:13:39.700,0:13:44.300
你這個參數的 loss 下降的速度呢

0:13:44.300,0:13:46.280
變得很慢，你就覺得說

0:13:46.280,0:13:48.460
卡在 local minimum 什麼之類的

0:13:48.460,0:13:51.260
就傷心地把程式停掉了

0:13:51.260,0:13:53.700
這個時候，你得到的結果是很差的，為什麼呢？

0:13:53.700,0:13:57.220
因為這個 converge，是幾乎 base on random 的參數

0:13:57.220,0:13:59.380
那幾乎 base on random 的 output

0:13:59.380,0:14:02.780
然後去 converge，所以得到的結果，是很差的

0:14:02.780,0:14:05.560
那為甚麼會有這個現象發生呢？

0:14:05.560,0:14:07.640
為甚麼會有這個現象發生呢？

0:14:07.640,0:14:11.760
如果你自己把 Backpropagation 的式子寫出來的話

0:14:11.760,0:14:15.120
你可以很輕易地發現說，用 sigmoid function

0:14:15.120,0:14:17.180
會導致這件事情的發生

0:14:17.180,0:14:19.700
但是，我們今天不看 Backpropagation 的式子

0:14:19.700,0:14:21.560
我們其實從直覺上來想

0:14:21.560,0:14:24.400
你也可以了解為什麼這件事情發生

0:14:24.400,0:14:26.320
怎麼用直覺來想

0:14:26.320,0:14:28.940
一個參數的 Gradient 的值應該是多少呢？

0:14:28.940,0:14:30.520
我們知道說

0:14:30.520,0:14:36.260
某一個參數 w 對 total cost C 的偏微分阿

0:14:36.260,0:14:39.660
意思就是說，它的直覺的意思就是說

0:14:39.660,0:14:43.220
當我今天把某一個參數做小小的變化的時後

0:14:43.220,0:14:47.400
它對這個 cost 的影響是怎麼樣

0:14:47.400,0:14:50.140
了解嗎？就是我們可以把一個參數

0:14:50.140,0:14:53.700
做小小的變化，然後觀察它對 cost 的變化

0:14:53.700,0:14:56.120
以此來決定說，這個參數

0:14:56.120,0:14:58.440
它的 Gradient 的值有多大

0:14:59.260,0:15:02.180
所以，怎麼做呢？我們就把

0:15:02.320,0:15:05.760
第一個 layer 裡面的某一個參數

0:15:05.760,0:15:11.760
加上 △w，看看對 network 的 output

0:15:11.760,0:15:15.700
和它的 target 之間的 loss

0:15:15.700,0:15:17.240
有甚麼樣的影響

0:15:17.240,0:15:19.020
那你會發現說

0:15:19.660,0:15:22.260
如果我今天這個 △w 很大

0:15:22.260,0:15:24.600
通過 sigmoid function 的時候

0:15:24.600,0:15:27.320
這個 output 呢，是會變小的

0:15:27.320,0:15:31.740
也就是說，改變了某一個參數的 weight

0:15:31.740,0:15:36.600
對某一個 neuron 的 output 的值會有影響

0:15:36.600,0:15:40.380
但是，這個影響是會衰減的

0:15:40.380,0:15:42.080
為甚麼這麼說呢？

0:15:42.080,0:15:45.120
因為，假設你用的是 sigmoid function

0:15:45.120,0:15:48.200
我們知道 sigmoid function 形狀就長這樣

0:15:48.200,0:15:50.340
那 sigmoid function 它會把

0:15:50.340,0:15:52.940
負無窮大到正無窮大之間的值

0:15:52.940,0:15:56.960
都硬壓到 0~1 之間

0:15:57.920,0:16:02.860
也就是說，如果你有很大的 input 的變化

0:16:03.300,0:16:05.640
通過 sigmoid function 以後

0:16:05.640,0:16:08.520
它 output 的變化，會是很小的

0:16:08.520,0:16:13.220
所以，就算今天你這個 △w 有很大的變化

0:16:13.220,0:16:16.200
造成 sigmoid function 的 input 有很大的變化

0:16:16.200,0:16:18.880
對 sigmoid function 來說，它的 output 的變化

0:16:18.880,0:16:20.780
是會衰減的

0:16:20.780,0:16:23.480
而每通過一次 sigmoid function

0:16:23.480,0:16:25.740
變化就衰減一次

0:16:25.740,0:16:27.900
所以，當你的 network 越深

0:16:27.900,0:16:29.980
它衰減的次數就越多

0:16:29.980,0:16:33.660
直到最後，它對 output 的影響是非常小的

0:16:33.660,0:16:35.760
也就是說，你在 input 的地方

0:16:35.760,0:16:38.200
改一下你的參數

0:16:38.200,0:16:39.560
對 output 的地方

0:16:39.560,0:16:42.480
它最後 output 的變化，其實是很小的

0:16:42.480,0:16:45.520
因此，最後對 cost 的影響也很小

0:16:45.520,0:16:49.360
因此，就造成說，在靠近 input 的那些 weight

0:16:49.360,0:16:54.280
它對它這個 Gradient 的值是小的

0:16:54.280,0:16:57.780
那怎麼解決這個問題呢？

0:16:57.780,0:16:59.380
有人就說

0:16:59.380,0:17:03.360
原來比較早年的做法是去 train RBM

0:17:03.360,0:17:07.120
去做這個 layer-wise 的 training

0:17:07.120,0:17:09.560
也就是說，你先認好一個 layer

0:17:10.340,0:17:13.040
就因為我們現在說，如果你把所有的這個

0:17:13.040,0:17:16.220
network 兜起來，那你做 Backpropagation 的時候

0:17:16.220,0:17:18.540
第一個 layer 你幾乎沒有辦法被挑到嘛

0:17:18.540,0:17:22.300
所以，用 RBM 做 training 的時候，它的精神就是

0:17:22.300,0:17:24.100
我先把一個 layer train 好

0:17:24.100,0:17:26.080
再 train 第二個，再 train 第三個

0:17:26.080,0:17:28.060
最後，你在做 Backpropagation 的時候

0:17:28.060,0:17:30.900
雖然說，第一個 layer 幾乎沒有被 train 到

0:17:30.900,0:17:31.580
那無所謂

0:17:31.580,0:17:34.040
一開始在 pre-train 的時候，就把它 pre-train 好了

0:17:34.040,0:17:38.760
所以，這就是 RBM 為什麼做 pre-train 可能有用的原因

0:17:38.760,0:17:41.280
那後來有人說，其實

0:17:41.280,0:17:42.960
後來有人發現說

0:17:42.960,0:17:46.260
其實，我記得 Hinton 跟 Pengel 都

0:17:46.260,0:17:49.480
幾乎在同樣的時間，不約而同地提出同樣的想法

0:17:49.480,0:17:51.480
改一下 activation function

0:17:51.480,0:17:54.700
可能就可以 handle 這個問題了

0:17:54.700,0:17:58.180
所以，現在比較常用的 activation function

0:17:58.180,0:18:02.520
叫做 Rectified Linear Unit，它的縮寫是 ReLU

0:18:02.520,0:18:04.640
會常看到有人叫它 ReLU

0:18:04.640,0:18:06.740
那這個 activation function 它長這樣子

0:18:07.060,0:18:09.940
這個 z 是 activation function 的 input

0:18:09.940,0:18:12.460
a 是 activation function 的 output

0:18:12.460,0:18:16.060
如果今天 activation function 的 input 大於 0 的時候

0:18:16.060,0:18:17.260
input = output

0:18:17.260,0:18:20.340
如果 activation function 的 input 小於 0 的時候

0:18:20.340,0:18:21.980
output 就是 0

0:18:21.980,0:18:25.320
那選擇這樣的 activation function 有甚麼好處呢？

0:18:25.320,0:18:28.420
有以下幾個理由，第一個理由是

0:18:28.420,0:18:31.780
它比較快，跟 sigmoid function 比起來

0:18:31.780,0:18:33.400
它的運算是快很多的

0:18:33.400,0:18:35.420
sigmoid function 裡面還有 exponential

0:18:35.420,0:18:37.240
那個是很慢的，那如果你是

0:18:37.240,0:18:39.640
用這個方法的話，它是快得多的

0:18:39.640,0:18:41.820
如果你看這個

0:18:41.820,0:18:44.900
我記得是 Pengel 寫得原始的 paper 的話呢

0:18:44.900,0:18:46.140
裡面會告訴你說

0:18:46.140,0:18:48.680
這個 activation function 的想法其實

0:18:48.680,0:18:50.700
是有一些生命上的理由的

0:18:50.700,0:18:55.300
那它把這樣的 activation 跟一些生物上的觀察呢

0:18:55.300,0:18:57.060
結合在一起

0:18:57.060,0:18:59.000
那 Hinton 有說過說

0:18:59.000,0:19:01.220
ReLU 這樣的 activation function

0:19:01.220,0:19:05.340
其實，等同於是無窮多的 sigmoid function

0:19:05.340,0:19:06.900
疊加的結果

0:19:06.900,0:19:08.080
無窮多的 sigmoid function

0:19:08.080,0:19:10.720
它們的 bias 都不一樣，疊加的結果會變成 ReLU

0:19:10.720,0:19:11.980
的 activation function

0:19:11.980,0:19:14.100
但它最重要的理由是

0:19:14.100,0:19:17.980
它可以 handle Vanishing gradient 的這個問題

0:19:17.980,0:19:20.440
它怎麼  handle Vanishing gradient 這個問題呢？

0:19:20.440,0:19:26.160
我們來看一下，這個是一個 ReLU 的 neural network

0:19:26.200,0:19:28.960
這是一個 ReLU 的 neural network

0:19:28.960,0:19:30.740
它裡面的每一個 activation function

0:19:30.740,0:19:32.960
都是 ReLU 的 activation function

0:19:32.960,0:19:34.120
那我們知道說

0:19:34.120,0:19:35.380
ReLU 的 activation function

0:19:35.380,0:19:37.580
它作用在兩個不同的 region

0:19:37.580,0:19:41.600
一個 region 是當 activation function 的 input

0:19:41.600,0:19:43.680
大於 0 的時候，input = output

0:19:43.680,0:19:44.980
另外一個 region 是

0:19:44.980,0:19:46.880
activation function 的 input 小於 0

0:19:46.880,0:19:48.380
所以，output 就是 0

0:19:48.380,0:19:51.900
所以，現在每一個 ReLU 的 activation function

0:19:51.900,0:19:54.040
它作用在兩個不同的 region

0:19:54.040,0:19:56.140
一個 region 是

0:19:56.140,0:19:57.860
每一個 activation function 的

0:19:57.860,0:20:02.640
一個可能是 activation function 的 output 就是 0

0:20:02.640,0:20:03.700
另外一個可能是

0:20:03.700,0:20:05.700
activation function 的 input = output

0:20:05.700,0:20:07.080
當 input = output 的時候

0:20:07.080,0:20:09.100
其實，這個 activation function 就是 linear 的

0:20:09.100,0:20:10.480
就是 linear 的

0:20:11.280,0:20:15.400
那對那些 output 是 0 的 neuron 來說

0:20:15.400,0:20:18.540
它其實對整個 network 是一點影響都沒有的阿

0:20:18.540,0:20:20.460
它 output 是 0，所以

0:20:20.460,0:20:22.420
它根本就不會影響最後 output 的值

0:20:22.420,0:20:24.940
所以，假如有一個 neuron 它 output 是 0 的話

0:20:24.940,0:20:27.420
你根本可以把它從 network 裡面整個拿掉

0:20:27.420,0:20:29.680
把它從 network 裡面整個拿掉

0:20:29.680,0:20:33.940
當你把這些 output 是 0 的 network 拿掉

0:20:33.940,0:20:37.680
剩下的 neuron，就都是 input = output，是 linear 的時候

0:20:37.680,0:20:41.760
你整個 network，不就是一個很瘦長的

0:20:41.760,0:20:43.320
linear network 嗎？

0:20:43.320,0:20:46.680
你整個 network，其實就變成是 linear 的 network

0:20:47.800,0:20:49.440
那這個時候

0:20:49.440,0:20:52.220
我們剛才說

0:20:52.220,0:20:53.980
我們的 Gradient 會遞減

0:20:54.000,0:20:57.380
是因為通過 sigmoid function 的關係

0:20:57.380,0:21:00.260
sigmoid function 會把比較大的 input

0:21:00.260,0:21:01.600
變成比較小的 output

0:21:01.600,0:21:03.440
但是，如果你是 linear 的話

0:21:03.440,0:21:04.660
input = output

0:21:04.660,0:21:08.160
你就不會有那個 activation function 遞減的問題了

0:21:09.220,0:21:12.080
講到這邊，有沒有人有問題呢？

0:21:13.260,0:21:15.780
講到這邊，我有一個問題

0:21:18.280,0:21:22.380
現在如果我用 ReLU 的時候，整個 network

0:21:22.380,0:21:24.020
都變成 linear 的阿

0:21:24.020,0:21:27.440
可是，我們要的不是一個 linear 的 network 阿

0:21:27.440,0:21:30.260
我們之所以用 deep learning，就是因為我們

0:21:30.260,0:21:32.820
不想要我們的 function 是 linear 的

0:21:32.820,0:21:35.820
我們希望它是一個 non-linear，一個比較複雜的 function

0:21:35.820,0:21:37.520
所以，我們用 deep learning

0:21:37.520,0:21:39.500
當我們用 ReLU 的時候

0:21:41.220,0:21:43.380
它不就變成一個 linear 的 function 了嗎？

0:21:43.380,0:21:44.980
這樣都不會有問題嗎？

0:21:44.980,0:21:46.840
這樣不是變得很弱嗎？

0:21:49.040,0:21:51.560
其實是這樣子的，這整個 network 呢

0:21:51.560,0:21:54.540
整體來說，它還是 non-linear 的

0:21:54.540,0:21:55.700
大家聽得懂嗎？

0:21:55.700,0:22:00.560
當你的每一個 neuron 做 operation

0:22:00.560,0:22:04.420
當每一個 neuron，它 operation 的 region 是一樣的時候

0:22:04.420,0:22:05.600
它是 linear 的

0:22:05.600,0:22:08.020
但是

0:22:08.020,0:22:11.540
也就是說，如果你對 input 做小小的改變

0:22:11.540,0:22:14.140
不改變 neuron 的 operation 的 region

0:22:14.140,0:22:16.060
它是一個 linear 的 function

0:22:16.060,0:22:18.360
但是，如果你對 input 做比較大的改變

0:22:18.480,0:22:22.900
你改變了 neuron 的 operation region 的話

0:22:22.900,0:22:25.920
它就變成是 non-linear 的

0:22:26.460,0:22:28.840
這樣大家可以接受嗎？

0:22:32.780,0:22:34.860
好那有另外一個問題

0:22:35.080,0:22:37.580
這個也是我常常被問到的問題

0:22:37.580,0:22:40.040
這個不能微分阿

0:22:40.040,0:22:41.260
這不能微分

0:22:41.260,0:22:43.560
這樣你不覺得很苦惱嗎？

0:22:43.560,0:22:48.200
我們之前說，我們做 Gradient Descent 的時候

0:22:48.200,0:22:51.100
你需要對你的 loss function 做微分

0:22:51.100,0:22:52.100
意思就是說

0:22:52.100,0:22:54.040
你要對你的 neural network 是可以做微分的

0:22:54.040,0:22:55.940
你的 neural network 要是一個可微的 function

0:22:56.460,0:22:58.520
ReLU 不可微阿

0:22:58.520,0:23:00.220
至少這個點是不可微的

0:23:00.220,0:23:01.380
那怎麼辦呢？

0:23:01.380,0:23:03.580
其實，實作上你就這個樣子啦

0:23:04.360,0:23:06.720
當你的 region 在這個地方的時候

0:23:06.720,0:23:08.500
gradient 微分就是 1

0:23:08.500,0:23:10.480
region 在這個地方的時候，微分就是 0

0:23:11.340,0:23:14.880
反正不可能 input 正好是 0 嘛，就不要管它

0:23:14.880,0:23:16.640
結束這樣

0:23:18.260,0:23:20.460
那我們來實際試一下

0:23:20.460,0:23:23.260
如果我們把 activation function 換成 ReLU 的時候

0:23:23.260,0:23:24.800
會得到甚麼樣的結果

0:23:24.800,0:23:27.180
比如說，我們就這樣子

0:23:27.180,0:23:29.780
把 sigmoid 換成 ReLU

0:23:34.580,0:23:38.040
把 sigmoid 換成 ReLU，那我們剛才用 sigmoid 的時候

0:23:38.040,0:23:40.620
training 和 testing 的 accuracy 都很差

0:23:40.620,0:23:42.940
我們就簡單的把它換成 ReLU

0:23:42.940,0:23:44.460
甚麼其他事都沒做

0:23:46.960,0:23:51.000
沒換嗎？等我一下

0:24:02.280,0:24:05.520
那 ReLU 其實還有種種的變數

0:24:05.520,0:24:09.420
那有人覺得說，如果是 ReLU 的時候

0:24:09.420,0:24:11.280
如果是原來的 ReLU

0:24:11.280,0:24:13.140
它在 input 小於 0 的時候

0:24:13.140,0:24:15.320
output 會是 0，這個時候微分是 0

0:24:15.320,0:24:17.920
你就沒有辦法 update 你的參數了

0:24:17.920,0:24:19.460
所以，我們應該讓

0:24:19.460,0:24:23.940
在 input 小於 0 的時候，output 還是有一點點的值

0:24:23.940,0:24:26.380
也就是 input 小於 0 的時候，output 是

0:24:26.380,0:24:28.560
input 乘上 0.01

0:24:28.560,0:24:31.860
這個東西叫做 Leaky ReLU

0:24:32.420,0:24:34.400
那這個時候，有人就會問說

0:24:34.400,0:24:39.460
為甚麼是 0.01，為甚麼不是 0.07, 0.08 之類的呢？

0:24:39.460,0:24:42.800
所以，就有人提出了 Parametric ReLU

0:24:42.800,0:24:45.440
他說，在負的這邊呢

0:24:45.440,0:24:52.160
(output) a = (input) z*α

0:24:52.160,0:24:54.680
α 是一個 network 的參數

0:24:54.680,0:24:57.780
它可以透過 training data 被學出來

0:24:57.780,0:25:01.320
甚至每一個 neuron 都可以有不同的 α 的值

0:25:02.760,0:25:07.000
那又會有人問說，為甚麼一定要是 ReLU 這個樣子呢？

0:25:07.000,0:25:08.560
可不可以是別的樣子

0:25:08.560,0:25:12.660
所以，後來又有一個更進階的想法，
叫做 Maxout network

0:25:12.660,0:25:14.580
那在 Maxout network 裡面呢

0:25:14.580,0:25:20.600
你就是讓你的 network 自動學它的 activation function

0:25:20.600,0:25:23.980
那因為現在 activation function 是自動學出來的

0:25:23.980,0:25:29.520
所以 ReLU 就只是 Maxout network 的一個 special case

0:25:29.520,0:25:33.060
Maxout network 它可以學出 
ReLU 這樣的 activation function

0:25:33.060,0:25:36.440
但是，它也可以是其他的 activation function

0:25:36.440,0:25:37.960
用 training data 來決定說

0:25:37.960,0:25:40.860
現在的 activation function 應該要長甚麼樣子

0:25:41.580,0:25:43.620
Maxout network 長甚麼樣子呢？

0:25:43.620,0:25:45.720
假設現在有 input

0:25:45.720,0:25:48.260
一個 2 dimension 的 vector，[x1, x2]

0:25:48.260,0:25:50.100
然後，我們就把

0:25:50.100,0:25:52.800
x1, x2 乘上不同的 weight

0:25:52.800,0:25:55.300
變成一個 value, 5

0:25:55.300,0:25:57.780
然後，再乘上不同 weight 得到 7

0:25:57.780,0:26:00.760
再乘上不同 weight 得到 -1，再乘上不同 weight 得到 1

0:26:00.760,0:26:02.120
那本來這些值呢

0:26:02.120,0:26:03.920
應該要通過 activation function

0:26:03.920,0:26:07.100
不管是 sigmoid function 還是 ReLU

0:26:07.100,0:26:08.920
得到另外一個 value

0:26:08.920,0:26:11.680
但是，現在在 Maxout network 裡面

0:26:11.680,0:26:13.980
現在在 Maxout network 裡面呢

0:26:13.980,0:26:15.600
我們做的事情是這樣子

0:26:16.600,0:26:18.920
你把這些 value

0:26:18.920,0:26:21.360
group 起來，你把這些 value group 起來

0:26:21.360,0:26:24.820
哪些 value 應該被 group 起來這件事情是

0:26:24.820,0:26:26.380
事先決定的

0:26:26.380,0:26:29.940
比如說，現在，這兩個 value 是一組

0:26:29.940,0:26:31.440
這兩個 value 是一組

0:26:31.440,0:26:34.100
那你在同一個組裡面

0:26:34.100,0:26:36.040
選一個值最大的當作 output

0:26:36.040,0:26:38.040
比如說，這個組就選 7

0:26:38.040,0:26:39.660
這個組就選 1

0:26:39.660,0:26:41.400
那這件事情呢

0:26:42.060,0:26:44.940
其實就跟 Max Pooling 一樣對不對

0:26:44.940,0:26:47.660
只是我們現在不是在 image 上做 Max Pooling 

0:26:47.660,0:26:50.620
我們是在在一個 layer 上做 Max Pooling 

0:26:50.620,0:26:52.060
我們把 layer 裡面的

0:26:52.540,0:26:54.980
本來要放到 neuron 裡面的

0:26:54.980,0:26:57.900
這個 activation function

0:26:57.900,0:27:01.220
我們本來要把它放到 neuron 的 activation function

0:27:01.220,0:27:03.000
的這個 input 的值 group 起來

0:27:03.000,0:27:05.740
然後，只選 max 當作 output

0:27:05.740,0:27:07.580
然後，就不用 activation function 了

0:27:07.580,0:27:09.460
就不加 activation function

0:27:09.460,0:27:11.200
得到的值是 7 跟 1

0:27:11.200,0:27:13.620
那你可以想說，這個東西呢

0:27:13.620,0:27:15.580
就是一個 neuron

0:27:15.580,0:27:18.280
只是它的 output 是一個 vector，而不是一個值

0:27:20.340,0:27:22.680
那接下來這個 7 跟 1 呢

0:27:22.680,0:27:24.260
就乘上不同的 weight

0:27:24.260,0:27:26.600
就得到另外一排不同的值

0:27:26.940,0:27:30.440
然後，你一樣把它們做 grouping

0:27:30.440,0:27:34.040
你一樣從每個 group 裡面選最大的值

0:27:34.040,0:27:36.880
1 跟2 就選 2，4 跟3 就選 4

0:27:38.580,0:27:40.660
其實，在實作上

0:27:40.660,0:27:44.440
幾個 element 要不要放在同一個 group 裡面，這件事情

0:27:44.440,0:27:45.640
是你可以自己決定的

0:27:45.640,0:27:49.240
就跟 network structure 一樣，是你自己需要調的參數

0:27:49.240,0:27:51.720
所以，你可以不是兩個 element 放一組

0:27:51.720,0:27:53.880
你可以是 3 個、4 個、5 個都可以

0:27:53.880,0:27:55.240
這個是你自己決定的

0:27:57.920,0:28:01.020
我們現在先說，Maxout network

0:28:01.020,0:28:05.720
它是有辦法做到跟 ReLU 一模一樣的事情

0:28:05.720,0:28:09.360
它可以模仿 ReLU 這個 activation function

0:28:09.360,0:28:10.820
怎麼做呢？

0:28:10.820,0:28:14.560
我們知道說，假設我們這邊有一個 ReLU 的 neuron

0:28:14.560,0:28:16.780
它的 input 就一個 value x

0:28:17.980,0:28:22.200
你會把 x 乘上這個 neuron 的 weight, w

0:28:22.200,0:28:24.520
再加上 bias, b

0:28:24.520,0:28:29.260
然後，通過 activation function, ReLU 得到 a

0:28:29.260,0:28:34.100
所以，現在如果我們看 x 跟 a 的關係

0:28:34.100,0:28:35.700
是什麼樣子呢？

0:28:36.120,0:28:38.060
假設 x 是橫軸

0:28:38.060,0:28:41.320
那這個 x 是橫軸

0:28:41.320,0:28:43.440
假設 y 軸是這個 z 的話

0:28:43.440,0:28:46.000
它就是 w*x + b

0:28:46.000,0:28:49.900
z 跟 x 之間的關係是 linear 的

0:28:49.900,0:28:51.140
是 linear 的，是這個樣子

0:28:51.900,0:28:53.340
那如果你選 a 呢

0:28:53.340,0:28:56.580
a 跟 z有甚麼樣的關係呢？

0:28:56.580,0:29:00.240
因為現在通過的是 ReLU 的 activation function

0:29:00.240,0:29:04.320
所以，如果你今天 z 的值大過 0 的時候

0:29:04.320,0:29:05.500
a = z

0:29:05.500,0:29:07.100
z 的值小於 0 的時候

0:29:07.100,0:29:08.400
a 就是 0

0:29:08.400,0:29:11.980
所以，a 跟 x 的關係是這個樣子

0:29:11.980,0:29:16.200
在這個地方，a = z；在這個地方，a = 0

0:29:16.200,0:29:19.480
所以，我們今天用 ReLU 的 activation function

0:29:19.480,0:29:22.980
它 input 和 output，x 和 a 之間的關係是長這樣子

0:29:23.680,0:29:26.320
如果我們今天用 Maxout network

0:29:26.320,0:29:29.080
用 Maxout network，你把 w

0:29:29.080,0:29:34.660
你把 input, x 乘上 weight, w 再加上 bias，得到 z1

0:29:35.140,0:29:38.940
你再把 x 乘上另外一組 weight

0:29:39.420,0:29:40.760
加上另外一個 bias

0:29:40.760,0:29:43.180
得到 z2，那我今天假設說

0:29:43.180,0:29:46.920
另外一個 weight 跟另外一個 bias 都是 0，所以 z2 = 0

0:29:46.920,0:29:50.420
然後，你做 Max Pooling 

0:29:50.420,0:29:53.240
你就可以選 z1, z2 其中一個比較大的呢

0:29:53.240,0:29:54.900
當作 a

0:29:54.900,0:29:59.380
現在，如果我們看 z1 跟 x 之間的關係

0:29:59.380,0:30:02.400
我們得到的是藍色這條線

0:30:02.900,0:30:07.240
如果我們看 x 跟 z2 之間的關係

0:30:07.240,0:30:11.720
我們得到的是水平這條線，因為 z2 總是 0

0:30:11.720,0:30:14.480
如果 z2 前面接的 weight 跟 bias 都是 0

0:30:14.480,0:30:17.380
z2 總是 0，所以它是紅色的這條線

0:30:17.680,0:30:20.440
那我們現在做的是 Maxout

0:30:20.440,0:30:25.040
我們是在 z1, z2 裡面選一個大的當作 output a

0:30:25.040,0:30:27.860
所以，如果今天 x 是在這個 region 的時候

0:30:27.860,0:30:30.240
你的 a 就會等於 z1，是這個 region

0:30:30.920,0:30:33.340
如果今天 x 是在這個 region 的時候

0:30:33.340,0:30:37.160
你的 a 就會等於比較大的 z2，所以是這個 region

0:30:37.700,0:30:40.700
那今天你只要把這個 w 跟這個 b

0:30:40.700,0:30:42.520
等於這個 w 跟這個 b

0:30:42.520,0:30:44.080
你就可以讓

0:30:44.080,0:30:46.740
這個 ReLU 的 input 和 output 的關係

0:30:46.740,0:30:49.540
等於這個 Maxout network 的 input 和 output 的關係

0:30:49.540,0:30:51.420
所以，由此可知， 就是

0:30:51.420,0:30:55.140
ReLU 是 Maxout network 可以做到的事情

0:30:55.140,0:30:58.400
只要它設定出正確的參數

0:30:58.400,0:31:01.240
但是，Maxout network 它也可以做出

0:31:01.240,0:31:04.100
更多的、不同的 activation function

0:31:04.100,0:31:08.020
比如說，現在假設這兩個 weight 不是 0，而是 w', b'

0:31:08.020,0:31:09.200
那會怎樣呢？

0:31:09.200,0:31:11.960
就得到藍色這條線 z1

0:31:11.960,0:31:14.660
跟紅色這條線 z2

0:31:14.660,0:31:16.780
因為 w', b' 是不一樣的值，所以

0:31:16.780,0:31:19.280
它可能是另外一條斜直線，長的是這樣子

0:31:19.880,0:31:21.800
接下來，你做 Max Pooling 的時候

0:31:21.800,0:31:24.040
你會在 z1, z2 裡面選一個大的

0:31:24.040,0:31:27.260
所以，在這個範圍內，你選了

0:31:28.120,0:31:29.320
你選了 z1

0:31:29.320,0:31:32.020
在這個範圍內，你選了 z2

0:31:32.020,0:31:35.100
所以你就得到了一個不一樣的 activation function

0:31:35.100,0:31:37.340
而這個 activation function 長甚麼樣子

0:31:37.340,0:31:41.280
是由 network 的參數 w, b, w', b' 決定的

0:31:41.280,0:31:43.840
所以，這個 activation function 它是一個

0:31:43.840,0:31:45.760
Learnable 的 activation function

0:31:45.760,0:31:48.680
它是一個可以根據 data 去 
generate 出來的 activation function

0:31:48.680,0:31:50.880
每一個 neuron 根據不同的 weight

0:31:50.880,0:31:52.780
它可以有不同的 activation function

0:31:53.760,0:31:55.520
那 ReLU 是這樣子

0:31:55.520,0:31:58.640
它可以做出任何的

0:31:58.640,0:32:03.340
piecewise 的 linear 的 convex activation function

0:32:03.340,0:32:06.980
如果你看一下它的性質，你就不難理解這件事情

0:32:06.980,0:32:10.200
那至於這個 piecewise 的 linear function 裡面

0:32:10.200,0:32:12.300
有多少個 piece

0:32:12.300,0:32:16.180
這決定於你現在把多少個 element 放在一個 group

0:32:16.180,0:32:17.920
假如說兩個 element 一個 group

0:32:17.920,0:32:20.400
那你可以有長這樣子的 activation function

0:32:20.400,0:32:21.340
是 ReLU

0:32:21.340,0:32:24.460
你可以有一個 activation function 它的作用就是取

0:32:24.460,0:32:25.720
絕對值

0:32:25.720,0:32:28.980
假設你是 3 個 element 一個 group

0:32:28.980,0:32:31.440
你可以有長這樣子的 activation function

0:32:31.440,0:32:33.980
你也可以有長這樣子的 activation function 等等

0:32:34.920,0:32:37.940
那接下來我們要面對另外一個問題，就是

0:32:37.940,0:32:39.580
這個東西怎麼 train

0:32:39.580,0:32:41.280
這個東西怎麼 train

0:32:41.280,0:32:42.760
這裡面有個 Max 阿

0:32:42.760,0:32:45.520
它不能微分阿

0:32:45.520,0:32:47.980
這個東西怎麼 train

0:32:47.980,0:32:50.280
這個做法是這樣子的

0:32:50.940,0:32:54.600
假設現在這個 z1 跟

0:32:54.600,0:32:58.460
假設這邊這兩個值，比較大的是

0:32:58.460,0:32:59.960
上面這個值

0:32:59.960,0:33:04.220
我們現在把這個 group 裡面比較大的值，用框框框起來

0:33:04.220,0:33:05.560
用框框框起來

0:33:05.560,0:33:08.100
那比較大的值

0:33:08.100,0:33:11.160
就會等於這個 max operation 的 output

0:33:11.160,0:33:13.040
就會等於 max operation 的 output

0:33:13.040,0:33:16.680
所以，這個值等於這個值，這個值等於這個值

0:33:16.680,0:33:19.000
這個值等於這個值，這個值等於這個值

0:33:19.000,0:33:23.980
所以，max operation 其實
在這邊就是一個 linear 的 operation

0:33:23.980,0:33:27.340
這是 linear，這是 linear，只是它

0:33:27.340,0:33:29.940
會選擇在前面這個 group 裡面的

0:33:29.940,0:33:34.720
它只接給前面這個 group 裡面的某一個 element

0:33:36.600,0:33:38.900
也就是說，也就是說其實呢

0:33:38.900,0:33:43.040
那這些沒有被接到的 element，它就沒用啦

0:33:43.040,0:33:44.760
它就不會影響 network 的 output 啦

0:33:44.760,0:33:47.300
所以，你就可以把它拿掉，你就可以把它拿掉

0:33:49.340,0:33:53.700
所以，其實當我們在做 Maxout 的時候

0:33:53.700,0:33:55.880
當你給它一個 input 的時候

0:33:55.880,0:34:01.260
你其實也是得到一個比較細長的 linear network

0:34:01.260,0:34:03.860
所以，你在 train 的時候，你 train 的就是

0:34:03.860,0:34:07.140
這個比較細長的 linear network 裡面的參數

0:34:07.140,0:34:11.140
你就是去 train 這些連到這一個 element 的這些參數

0:34:11.140,0:34:15.520
連到這個 element 的這些參數

0:34:15.520,0:34:19.700
假設我給你一個這樣子的 linear network，
你當然知道它是怎麼 train 的

0:34:19.700,0:34:22.880
用 Backpropagation train 就好，你知道它是怎麼 train 的

0:34:22.880,0:34:25.620
但這個時候呢，你就會有一個問題

0:34:29.040,0:34:31.760
沒被 train 到的 element 怎麼辦呢？

0:34:31.760,0:34:34.960
如果某一個這個 element，它不是最大的值

0:34:34.960,0:34:37.640
那它連接的那些 weight

0:34:37.640,0:34:39.320
就不會被 train 到了嗎？

0:34:39.320,0:34:40.900
你做 Backpropagation 的時候

0:34:40.900,0:34:43.900
你只會 train 在這個圖上的

0:34:43.900,0:34:46.240
比較深顏色的這些實線

0:34:46.240,0:34:48.180
你不會 train 到這個 weight 阿

0:34:48.340,0:34:50.680
這個 weight 不就沒被 train 到了嗎？

0:34:50.680,0:34:52.500
怎麼辦呢？

0:34:52.500,0:34:54.620
這看起來，表面上是一個問題

0:34:54.620,0:34:57.180
但實作上，它不是一個問題

0:34:57.180,0:34:58.220
為甚麼呢？

0:34:58.220,0:35:01.260
因為當你 input 不同的

0:35:01.260,0:35:04.020
當你給它不同的 input 的時候

0:35:04.020,0:35:06.660
你得到的這些 z 的值是不一樣的

0:35:06.660,0:35:08.680
你給它不同 input 的時候

0:35:08.680,0:35:11.220
max 的值，是不一樣的

0:35:11.220,0:35:14.480
所以，每一次你給它不同的 input 的時候

0:35:14.480,0:35:17.320
這個 network 的 structure 都是不一樣的

0:35:17.320,0:35:19.900
因為我們有很多很多筆 training data

0:35:19.900,0:35:21.780
而 network 的 structure 不斷地變換

0:35:21.780,0:35:25.640
所以，最後每一個 weight 在實際上都會被 train 到

0:35:25.640,0:35:27.440
Maxout 就是這麼做

0:35:27.440,0:35:29.240
Maxout network 就是這麼做

0:35:29.240,0:35:31.300
所以，如果我們回到 Max Pooling 

0:35:31.300,0:35:34.100
Max Pooling跟 Maxout 是一模一樣的 operation 阿

0:35:34.100,0:35:35.900
只是換一個說法而已，對不對

0:35:35.900,0:35:38.040
所以，你會 train Maxout

0:35:38.040,0:35:40.620
你就會 train Max Pooling，這是一模一樣的作法

0:35:40.620,0:35:43.700
講到這邊大家有沒有甚麼問題呢？

0:35:44.940,0:35:47.160
沒有的話，那

0:35:48.040,0:35:50.360
另外一個我們要講的是這個

0:35:50.360,0:35:52.260
adaptive 的 learning rate

0:35:52.260,0:35:55.860
其實 adaptive 的 learning rate，我們之前已經有講過了

0:35:55.860,0:35:59.080
我們之前有講過這個 Adagrad

0:35:59.280,0:36:00.960
我們之前有講過 Adagrad

0:36:00.960,0:36:03.280
我們說 Adagrad 的做法就是

0:36:03.280,0:36:07.780
我們現在每一個 parameter 都要有不同的 learning rate

0:36:07.780,0:36:11.220
而這個 learning rate 是怎麼
給它這麼 adaptive 的 learning rate 呢？

0:36:11.220,0:36:13.160
我們就把一個固定的 learning rate η

0:36:13.160,0:36:19.900
除掉這一個參數過去所有 gradient 值的平方和，開根號

0:36:19.900,0:36:23.200
把這項除以平方和開根號

0:36:23.200,0:36:26.020
就得到新的 parameter

0:36:26.020,0:36:28.460
那這個 Adagrad 它的精神就是說

0:36:28.460,0:36:32.140
如果我們今天考慮兩個參數，w1, w2

0:36:32.140,0:36:35.640
如果 w1 是在

0:36:35.640,0:36:36.960
這個方向上

0:36:36.960,0:36:41.480
如果 w1 在這個方向上，它平常 gradient 都比較小

0:36:41.480,0:36:44.840
那它是比較平坦的，給它比較大的 learning rate

0:36:44.840,0:36:46.860
反過來說，在這個方向上

0:36:46.860,0:36:48.380
平常 gradient 都是比較大的

0:36:48.380,0:36:51.940
所以，它是比較陡峭的，所以給它比較小的 learning rate

0:36:55.800,0:36:58.660
但是，實際上呢，我們面對的問題

0:36:58.660,0:37:04.320
有可能是比 Adagrad 可以處理的問題更加複雜的

0:37:04.320,0:37:10.720
也就是說，我們之前在做這個 Linear Regression 的時候

0:37:10.720,0:37:13.680
我們看到的這個 optimization 的 function

0:37:13.680,0:37:15.660
loss function 是這樣子 convex的形狀

0:37:15.660,0:37:18.240
但實際上，當我們在做 deep learning 的時候

0:37:18.240,0:37:22.440
這個 loss function 它可以是任何形狀，你知道嗎？

0:37:22.440,0:37:24.800
它可以是任何形狀

0:37:24.800,0:37:28.420
比如說，它可以是這樣，怪異的月形的形狀

0:37:28.420,0:37:34.420
如果當今天你的 error surface 是這個形狀的時候

0:37:34.420,0:37:38.160
那你會遇到的問題是，就算是同一個方向上

0:37:38.160,0:37:42.720
你的 learning rate 也比需要能夠快速地變動

0:37:42.720,0:37:45.120
就我們剛才在做 convex function 的時候

0:37:45.120,0:37:46.400
在每個方向上

0:37:46.400,0:37:48.080
這個方向很平坦，就一直很平坦

0:37:48.080,0:37:50.260
這個方向很陡峭，就一直很陡峭

0:37:50.260,0:37:52.880
但是，如果今天在更複雜的問題的時候

0:37:52.880,0:37:56.040
有可能，你考慮 w1 改變是在這個方向

0:37:56.040,0:37:57.960
在某個區域

0:37:57.960,0:38:01.020
它很平坦，所以它需要比較小的 learning rate

0:38:01.020,0:38:03.200
但是，到了另外一個區域

0:38:03.200,0:38:04.740
它又突然變得很陡峭

0:38:04.740,0:38:07.520
這個時候，它需要比較大的 learning rate

0:38:07.520,0:38:11.680
所以，真正要處理 deep learning 的問題，用 Adagrad

0:38:11.680,0:38:16.420
可能是不夠的，你需要更 dynamic 的調整

0:38:16.420,0:38:18.100
這個 learning rate 的方法

0:38:18.100,0:38:21.820
所以，這邊有一個 Adagrad 的進階膽，叫 RMSProp

0:38:21.820,0:38:24.600
RMSProp，我覺得是一個滿神奇的方法

0:38:24.600,0:38:26.720
因為你好像，找不到它的 paper

0:38:26.720,0:38:28.420
因為這個在 Hinton 的那個

0:38:28.420,0:38:30.580
MOOC 的 course 裡面，他提出來

0:38:30.580,0:38:34.300
他在他的線上課程裡面提出一個方法

0:38:34.300,0:38:39.020
大家要 cite 的時候，要 cite 那個線上課程的連結

0:38:40.660,0:38:42.000
這招還真的有用

0:38:42.000,0:38:44.380
這個 RMSProp 是這樣子做的

0:38:44.900,0:38:48.080
我們現在把

0:38:48.080,0:38:50.580
這個固定的 learning rate

0:38:50.580,0:38:53.540
除掉一個值，我們稱之為 σ

0:38:53.540,0:38:55.900
這個 σ 是甚麼呢？

0:38:55.900,0:38:58.380
在第一個時間點

0:38:58.380,0:39:02.800
這個 σ 就是你第一個算出來的 gradient 的值 g^0

0:39:04.340,0:39:06.820
那在第二個時間點呢？

0:39:06.820,0:39:10.780
在第二個時間點，你算出一個新的 gradient, g^1

0:39:10.780,0:39:13.300
這個時候

0:39:13.300,0:39:15.740
你的 σ 的值

0:39:15.740,0:39:18.300
新的 σ 的值，σ^1 呢

0:39:18.300,0:39:25.400
就是原來的 σ 值的平方，乘上 α

0:39:25.400,0:39:29.160
再加上新的 g 的值，(g^1)^2

0:39:29.160,0:39:31.600
再乘上 (1 - α)

0:39:31.600,0:39:34.640
而這個 α 的值是

0:39:34.640,0:39:36.820
你可以自由去調的

0:39:36.820,0:39:39.680
也就是我們原來在

0:39:39.680,0:39:42.260
或是我們再來看下一個例子

0:39:42.260,0:39:43.360
我們現在有一個

0:39:43.360,0:39:45.400
在下一個時間點，我們又算出 g^2

0:39:45.400,0:39:47.400
我們得到 σ^2

0:39:47.400,0:39:50.500
σ^2 怎麼算的呢？它是把原來的 σ^1

0:39:50.500,0:39:52.600
取平方乘上 α

0:39:52.600,0:39:55.720
再加上 (1 - α) 乘上 (g^2)^2

0:39:55.720,0:39:58.080
再開根號，得到這個 σ^2

0:39:58.080,0:40:00.680
那跟原來的 Adagrad 不一樣的地方是

0:40:00.680,0:40:04.300
原來的 Adagrad 你在這邊分母放的值

0:40:04.300,0:40:08.140
就是把 g^0, g^1, g^2 都取平方和開根號

0:40:08.140,0:40:11.960
但是，在這邊的時候

0:40:11.960,0:40:14.740
在 RMSProp 裡面呢，這個 σ^1

0:40:14.740,0:40:17.260
它裡面包含了 g^0 跟 g^1

0:40:17.260,0:40:18.900
那這邊也包含了 g^2

0:40:18.900,0:40:21.620
所以，它根號裡面也同樣包含了 g^0, g^1, g^2

0:40:21.620,0:40:22.700
就跟 Adagrad 一樣

0:40:22.700,0:40:25.520
但是，你現在可以給它乘上 weight, α

0:40:25.520,0:40:27.900
或者是 (1 - α)

0:40:27.900,0:40:29.900
所以，你可以調整說

0:40:29.900,0:40:31.580
我比較傾向

0:40:31.580,0:40:32.860
你可以調整這個 α 的值

0:40:32.860,0:40:34.800
這個 α 的值就也是像 learning rate 阿

0:40:34.800,0:40:38.220
也是你要手動設的值，當然你就設個 0.9 之類的

0:40:38.220,0:40:40.780
你可以手動去調這個

0:40:40.780,0:40:43.600
α 的值，讓它說

0:40:43.600,0:40:47.740
如果你把這個 α 的值設的小一點

0:40:47.740,0:40:52.040
那意思就是說，你傾向於相信新的 gradient

0:40:52.040,0:40:54.500
所告訴你的，這個 error surface

0:40:54.500,0:40:56.860
平滑或陡峭的程度

0:40:56.860,0:40:59.200
傾向於相信新的 gradient

0:40:59.200,0:41:02.140
比較無視於舊的 gradient 提供給你的 information

0:41:02.140,0:41:06.060
這樣大家應該可以瞭解這個結果

0:41:06.060,0:41:09.300
所以，在第 t 個時間點，你算出來的 σ

0:41:09.300,0:41:11.740
就是把 (σ^(t-1))^2 乘上 α

0:41:11.740,0:41:15.040
加上 (1 - α) 乘上在第 t 個時間點算出來的

0:41:15.040,0:41:17.380
gradient 的平方

0:41:17.380,0:41:20.300
所以，當你做 RMSProp 的時候

0:41:20.300,0:41:23.320
你一樣是在這算 gradient 的 zooming square

0:41:23.320,0:41:26.200
但是，你可以給

0:41:26.200,0:41:28.400
現在已經看到的 gradient 比較大的 weight

0:41:28.400,0:41:30.920
給過去看到的 gradient 比較小的 weight

0:41:33.120,0:41:35.920
除了 learning rate 的問題以外

0:41:35.920,0:41:38.040
我們知道說在做 deep learning 的時候

0:41:38.040,0:41:40.700
大家都會說，我們會卡在 local minimum

0:41:40.700,0:41:43.500
那我之前也有講過說，我們不見得是卡在 local minimum

0:41:43.500,0:41:45.000
也有可能卡在 saddle point

0:41:45.000,0:41:48.420
甚至，你有可能卡在 plateau 的地方

0:41:48.420,0:41:51.360
大家聽到這個問題都非常的擔心

0:41:51.360,0:41:54.080
覺得說，哇！這個做 deep learning 呢

0:41:54.080,0:41:55.340
是非常困難的

0:41:55.340,0:41:57.580
因為你可能胡亂做一下就一大推的問題

0:41:57.780,0:41:59.060
那其實呢

0:41:59.060,0:42:01.340
Yann LeCun 他在 07 年的時候

0:42:01.340,0:42:04.100
他有一個滿特別的說法

0:42:04.100,0:42:05.880
07 年的時候就講過這件事情

0:42:05.880,0:42:10.200
它說你不用擔心 local minimum 的問題

0:42:10.200,0:42:14.080
我不知道這件事情有多確切

0:42:14.080,0:42:15.140
我沒有 verify 過

0:42:15.140,0:42:18.540
但是，如果你有甚麼 verify 的結果的話

0:42:18.540,0:42:20.120
你可以跟我分享一下

0:42:20.120,0:42:22.820
Yann LeCun 的說法，他是這樣說的，他說

0:42:22.820,0:42:25.920
其實，在這個 error surface 上

0:42:25.920,0:42:27.200
沒有太多 local minimum

0:42:27.200,0:42:28.940
所以，你不用太擔心

0:42:28.940,0:42:31.080
為甚麼呢？他說

0:42:31.080,0:42:33.660
你要是一個 local minimum

0:42:33.660,0:42:34.980
你在每一個 dimension

0:42:34.980,0:42:37.540
都必須要是這樣子的形狀對不對？

0:42:37.540,0:42:40.740
都要是一個山谷的谷底

0:42:40.740,0:42:42.720
每一個 dimension 都要是山谷的谷底

0:42:42.720,0:42:46.380
我們假設這個山谷的谷底出現的機率是 p 好了

0:42:46.380,0:42:47.860
山谷的谷底出現的機率是 p

0:42:47.860,0:42:51.060
因為我們的 network 有非常非常多的參數

0:42:51.060,0:42:53.320
所以，假設有 1000 個參數

0:42:53.320,0:42:55.380
你每一個參數都要是山谷的谷底

0:42:55.380,0:42:57.440
那機率就是 p^1000

0:42:57.440,0:42:59.460
你的 network 越大

0:42:59.460,0:43:02.100
參數越多，這個出現的機率就越低

0:43:02.100,0:43:03.920
所以呢，local minimum

0:43:03.920,0:43:05.740
在一個很大的 neural network 裡面

0:43:05.740,0:43:07.600
其實沒有你想像的那麼多

0:43:07.600,0:43:10.020
一個很大的 neural network，它看起來其實是

0:43:10.020,0:43:13.260
其實搞不好是很平滑的，根本沒有太多 local minimum

0:43:13.260,0:43:16.480
所以，當你走走走，走到一個你覺得是

0:43:16.480,0:43:18.480
local minimum 的地方，卡住的時候

0:43:18.480,0:43:22.260
它八成就是 local minimum，或是很接近 local minimum

0:43:22.260,0:43:25.160
給大家參考

0:43:25.160,0:43:29.120
那你有甚麼特別的想法，再告訴我

0:43:31.500,0:43:34.820
有一個 heuristic 的方法

0:43:34.820,0:43:38.000
可以稍微處理一下，我們上述說的

0:43:38.000,0:43:41.360
我們剛才講的 local minimum 還有 plateau 的問題

0:43:41.360,0:43:46.120
這個方法，你可以說是從真實的世界，得到一些靈感

0:43:46.120,0:43:48.160
我們知道在真實的世界裡面

0:43:48.160,0:43:51.780
如果這個是一個地形，是一個山坡

0:43:51.780,0:43:56.080
你把一個球從左上角丟下來，把它滾下來

0:43:56.080,0:43:58.840
然後，它滾滾滾，它滾到 plateau 的地方呢

0:43:58.840,0:44:02.880
它不會停下來阿，因為有慣性嘛，它還會繼續往前

0:44:02.880,0:44:06.620
它就算是走到上坡的地方，假設這個波沒有很陡

0:44:06.620,0:44:07.880
因為慣性的關係

0:44:07.880,0:44:10.500
它搞不好走走走，還是可以翻過這個山坡

0:44:10.500,0:44:12.180
結果它就可以走到了

0:44:12.660,0:44:15.800
比這個 local minimum 還要好的地方

0:44:15.800,0:44:17.580
那所以我們

0:44:17.580,0:44:19.040
要做的事情就是把

0:44:19.040,0:44:21.480
這個慣性這個特性呢

0:44:21.480,0:44:23.580
塞到 Gradient Descent 裡面去

0:44:23.580,0:44:28.800
那這件事情，就叫做 momentum

0:44:29.540,0:44:32.880
這個東西怎麼做呢？我們先很快地秒複習一下

0:44:32.880,0:44:34.680
一般的 Gradient Descent

0:44:34.680,0:44:37.520
一般的 Gradient Descent 是怎麼做的呢？

0:44:37.520,0:44:38.800
我們是這樣子做的

0:44:38.800,0:44:40.840
這個是選一個初始的值

0:44:40.840,0:44:43.100
然後，計算一下它的 gradient

0:44:43.100,0:44:44.620
它的 gradient 是這個方向

0:44:44.620,0:44:46.920
那我們就走 gradient 的反方向

0:44:46.920,0:44:49.180
乘上一個 learning rate η

0:44:49.180,0:44:51.340
得到 θ^1，再算 gradient

0:44:51.340,0:44:52.820
再走一個新的方向

0:44:52.820,0:44:55.140
再算 gradient、再走一個方向；
再算 gradient、再走一個方向

0:44:55.140,0:44:55.860
以此類推

0:44:55.860,0:44:58.760
一直到 gradient = 0 的時候，或 gradient 趨近 0 的時候

0:44:58.760,0:45:00.100
我們就停止

0:45:00.100,0:45:03.020
當我們加上 momentum 的時候我們是怎麼做的呢？

0:45:03.020,0:45:05.780
當我們加上 momentum 的時候

0:45:05.780,0:45:09.880
我們每一次移動的方向

0:45:09.880,0:45:13.300
不再是只有考慮 gradient

0:45:14.000,0:45:19.520
而是我們現在的 gradient

0:45:19.520,0:45:22.900
加上在前一個時間點

0:45:22.900,0:45:23.960
移動的方向

0:45:23.960,0:45:26.160
這樣聽起來可能很抽象，所以

0:45:26.160,0:45:28.600
我們實際地來看一下，它是怎麼運作的

0:45:28.600,0:45:31.140
一樣選一個初始值 θ^0

0:45:31.140,0:45:33.120
一樣選一個初始值 θ^0

0:45:33.540,0:45:39.420
然後，我們用一個值 v 去記錄

0:45:39.420,0:45:41.380
我們在前一個時間點

0:45:41.380,0:45:42.800
移動的方向

0:45:42.800,0:45:45.500
v 記錄我們前一個時間點移動的方向

0:45:45.500,0:45:48.340
因為現在是初始值，之前沒有移動過，所以

0:45:48.340,0:45:50.900
前一個時間點移動的方向是 0

0:45:50.900,0:45:53.980
接下來計算在 θ^0 地方的 gradient

0:45:53.980,0:45:56.420
現在算出 θ^0 的 gradient

0:45:56.420,0:45:58.380
算出來是紅色這個箭頭

0:45:59.620,0:46:02.880
然後，我們現在要移動的方向

0:46:02.880,0:46:05.100
並不是紅色箭頭告訴我們的方向

0:46:05.100,0:46:07.820
而是，前一個時間點的

0:46:07.820,0:46:09.280
movement v^0

0:46:09.280,0:46:13.340
再加上 negative 的 gradient

0:46:13.600,0:46:17.500
然後，我們得到現在要移動的方向 v^1

0:46:17.500,0:46:19.620
所以，到這邊就好像是慣性一樣

0:46:19.620,0:46:24.540
如果我們之前走的方向是 v^0

0:46:25.200,0:46:28.140
那今天有一個新的 gradient，並不會

0:46:28.140,0:46:31.380
讓你參數 update 的方向完全轉向

0:46:31.380,0:46:33.960
它會改變你的方向

0:46:33.960,0:46:36.200
但是，因為有慣性的關係，所以

0:46:36.200,0:46:39.520
原來走的方向還是有一定程度的影響

0:46:39.520,0:46:43.100
那我們或許看下一個例子，會比較清楚

0:46:43.100,0:46:45.800
我們現在在上一個時間點移動的方向呢

0:46:45.800,0:46:47.940
是 v^1

0:46:49.180,0:46:52.720
接下來，再計算一下 gradient

0:46:52.720,0:46:55.860
計算一下 gradient，就是紅色的箭頭

0:46:55.860,0:46:59.380
接下來要決定說，在第二個時間點

0:46:59.380,0:47:01.180
我們要走的方向是甚麼樣？

0:47:01.180,0:47:03.260
第二個時間點要走的方向是

0:47:03.260,0:47:05.360
過去走的方向 v^1

0:47:05.360,0:47:08.240
減掉 leaning rate 乘上 gradient

0:47:08.240,0:47:11.300
如果我們看這個圖上的話，gradient 會告訴我們說

0:47:12.140,0:47:13.840
要走這個方向

0:47:13.840,0:47:17.040
負的 η 乘上 gradient，要走這個方向

0:47:17.040,0:47:19.300
但是，前面的 movement

0:47:19.300,0:47:21.840
是綠色的箭頭，它是這個方向

0:47:21.840,0:47:22.940
這個方向

0:47:22.940,0:47:25.000
我們會把這個 movement 乘上一個 λ

0:47:25.000,0:47:26.360
那這個 λ 其實也是一個

0:47:26.360,0:47:28.940
跟 learning rate 一樣，是手要調的參數

0:47:28.940,0:47:32.660
它告訴你說，現在這個慣性這件事情，影響力有多大

0:47:32.660,0:47:34.560
λ 大的話，慣性影響力就大

0:47:34.560,0:47:36.220
λ 小的話，慣性影響力就大

0:47:36.220,0:47:38.620
總之，慣性告訴我們走這邊

0:47:38.620,0:47:40.280
gradient 告訴我們走這邊

0:47:40.280,0:47:43.980
這兩個合起來呢，就走了一個新的方向，就是這邊

0:47:43.980,0:47:46.620
這個就是 v2，所以，以此類推

0:47:46.620,0:47:49.020
新的 gradient 告訴我們走這邊

0:47:49.020,0:47:50.580
慣性告訴我們

0:47:50.580,0:47:53.820
新的 gradient 告訴我們走紅色這個虛線的方向

0:47:53.820,0:47:56.460
慣性告訴我們走綠色虛線的方向

0:47:56.460,0:47:58.740
合起來最後就是走藍色的方向

0:47:59.360,0:48:01.080
然後，update 參數以後

0:48:01.080,0:48:03.040
gradient 告訴我們走紅色這個虛線的方向

0:48:03.040,0:48:06.360
慣性告訴我們走綠色虛線的方向

0:48:06.360,0:48:08.780
合起來就是走藍色的方向

0:48:10.060,0:48:14.020
那你可以用另外一個方法來理解這件事情

0:48:14.020,0:48:16.280
其實，v^i

0:48:16.280,0:48:20.040
你在每一個時間點移動的 movement

0:48:20.040,0:48:25.320
你在第 i 個時間點移動的步伐 v^i 呢

0:48:25.320,0:48:28.760
移動的量、方向，v^i 呢

0:48:28.760,0:48:33.480
其實就是過去所有算出來的 gradient 的總和

0:48:33.480,0:48:35.840
為甚麼這麼說呢？

0:48:35.840,0:48:38.480
我們知道 v^0 = 0

0:48:39.140,0:48:41.440
v^1 呢，v^1 在這邊

0:48:41.440,0:48:45.820
v^1 是 λ*(v^0) - η*(θ^0) 的 gradient

0:48:45.820,0:48:47.260
而 v^0 = 0

0:48:47.260,0:48:51.760
所以，v^1 = -η * gradient

0:48:51.760,0:48:52.920
所以是這個樣子

0:48:52.920,0:48:56.520
那 v^2 呢，v^2 我們就把

0:48:56.520,0:48:58.900
v^1 是負的 gradient

0:48:58.900,0:49:04.800
v^1 是負的 η * gradient 代進去

0:49:04.800,0:49:07.980
我們把 v^1 代到這邊，再乘上 λ

0:49:07.980,0:49:10.320
再減掉 η，乘以 θ^1 的 gradient

0:49:10.320,0:49:11.560
得到結果就是這樣

0:49:11.560,0:49:14.920
你得到的結果就是你把 θ^0 的地方的 gradient

0:49:14.920,0:49:16.660
減掉 λ * η

0:49:16.660,0:49:20.580
再減掉 η * θ^1 的 gradient

0:49:20.580,0:49:23.160
你得到 v^2

0:49:23.160,0:49:27.020
所以，v^2 裡面同時有在 θ^0 算出來的 gradient

0:49:27.020,0:49:29.100
同時有在 θ^1 的地方算出來的 gradient

0:49:29.100,0:49:32.700
只是這兩個 gradient，它的 weight 是不一樣的

0:49:32.700,0:49:35.620
如果你 λ 都設小於 0 的值的話呢

0:49:35.620,0:49:40.340
越之前的 gradient，它的 weight 就越小

0:49:40.340,0:49:42.720
越之前的 gradient，就越不去理它

0:49:42.720,0:49:45.880
你越在意現在的 gradient，但是過去的 gradient

0:49:45.880,0:49:50.260
也會對你現在要 update 的方向有一定程度的影響力

0:49:50.260,0:49:53.240
這個，就是 momentum

0:49:53.240,0:49:56.880
如果你看數學式子不太喜歡的話

0:49:56.880,0:49:58.800
那我們就從直覺上來看一下

0:49:58.800,0:50:01.760
到底加入 Momentum 以後，是怎麼運作的

0:50:02.060,0:50:03.880
在加入 Momentum 以後呢

0:50:03.880,0:50:06.220
每一次移動的方向

0:50:06.220,0:50:13.000
是 negative 的 gradient 加上 momentum

0:50:13.000,0:50:14.720
建議我們要走的方向

0:50:14.720,0:50:18.440
Momentum 其實就是上一個時間點的 movement

0:50:19.880,0:50:22.780
所以，現在假設我們有一個

0:50:22.780,0:50:25.720
假設我們初始的參數是在這個位置

0:50:25.720,0:50:28.280
那 gradient 建議我們往右走

0:50:28.280,0:50:30.800
所以，最後就往右移動

0:50:31.300,0:50:34.000
那如果說之後移到這個位置

0:50:34.000,0:50:37.620
gradient 建議我們往右走

0:50:37.900,0:50:41.780
而 momentum 也會建議我們往右走

0:50:41.780,0:50:44.600
因為我們是從左邊這邊移過來的嘛

0:50:44.600,0:50:47.580
所以，前一個步伐我們是向右的

0:50:47.580,0:50:49.700
如果你考慮 momentum 的時候呢

0:50:49.700,0:50:50.780
我們也會向右

0:50:50.780,0:50:53.320
所以，你把 gradient 建議我們走的方向

0:50:53.320,0:50:55.220
跟 momentum 建議我們走的方向合起來

0:50:55.220,0:50:57.260
你就得到這個藍色的線

0:50:58.000,0:51:00.900
所以你會繼續向右，如果我們今天走到

0:51:00.900,0:51:02.740
local minimum 的地方

0:51:02.740,0:51:05.040
走到 local minimum 的地方，gradient 是 0

0:51:05.040,0:51:07.860
所以，gradient 會告訴你說就停在這裡吧

0:51:07.860,0:51:11.480
但是，momentum 會告訴你說，之前是從右邊走過來的

0:51:11.480,0:51:13.580
所以，你仍然應該要繼續往右走

0:51:13.580,0:51:15.800
也就是綠色箭頭的方向

0:51:15.800,0:51:17.960
所以，最後你參數 update 的方向

0:51:17.960,0:51:19.520
仍然會繼續向右

0:51:19.520,0:51:22.040
甚至你可以樂觀地期待說

0:51:22.040,0:51:25.420
如果今天在往右的時候，走到這個地方

0:51:25.420,0:51:28.220
gradient 要求我們向左走

0:51:28.220,0:51:31.500
現在左邊如果是算微分的話呢

0:51:31.500,0:51:32.980
如果考慮 gradient 的話呢

0:51:32.980,0:51:35.600
參數應該往左移動

0:51:35.600,0:51:37.600
但是，momentum 建議我們

0:51:37.600,0:51:39.120
繼續向右走

0:51:39.120,0:51:41.700
因為你是從左邊過來的

0:51:41.700,0:51:43.200
因為你是從左向右過來的

0:51:43.200,0:51:45.140
所以 momentum 建議你繼續向右走

0:51:45.140,0:51:47.360
如果今天 momentum 其實比較強的話

0:51:47.360,0:51:50.560
你最後，就還是會向右走

0:51:50.560,0:51:52.880
所以，你有一定的可能性，你可以

0:51:52.880,0:51:55.420
有可能你可以跳出

0:51:55.420,0:51:57.760
local minimum，如果這個 local minimum 不深的話

0:51:57.760,0:52:01.720
你有可能藉由慣性的力量呢

0:52:01.720,0:52:06.280
跳出 local minimum，然後走到比較好的 global minimum

0:52:06.280,0:52:08.360
比較低的 local minimum

0:52:08.820,0:52:13.060
那如果你今天把 RMSProp 加上 momentum 的話

0:52:13.060,0:52:15.860
其實，你就得到 Adam 這樣

0:52:15.860,0:52:17.560
現在如果你沒有甚麼 prefer 的話

0:52:17.560,0:52:20.020
你就先學 Adam 就是了，那我發現在

0:52:20.020,0:52:23.140
作業二裡面，其實還滿多人 implement Adam 的

0:52:23.140,0:52:26.400
大家太強了，都自己 implement Adam 在作業二

0:52:26.400,0:52:28.640
我想這些你們都很熟了

0:52:28.640,0:52:30.320
沒什麼特別好講的

0:52:30.320,0:52:33.400
你可能看這個 algorithm

0:52:33.400,0:52:35.780
哇！感覺好像有點複雜

0:52:35.780,0:52:39.380
但是，好多人 implement 這個東西

0:52:39.380,0:52:46.140
其實，Adam 就是 RMSProp 加上 momentum

0:52:46.140,0:52:48.940
這兩個東西綜合起來，就是 Adam

0:52:48.940,0:52:52.960
我們非常非常快地來看一下這個式子

0:52:52.960,0:52:54.740
在這個式子裡面呢

0:52:54.740,0:52:57.640
在這個式子裡面，一開始要先初始一個東西叫做 m0

0:52:57.640,0:53:00.280
m0 就是 momentum

0:53:00.280,0:53:02.420
就是前一個時間點的 movement

0:53:02.420,0:53:05.860
那這邊有另外一個值叫做 v0

0:53:05.860,0:53:09.800
v0 就是我們剛才在 RMSProp 裡面看到的那個 σ

0:53:09.800,0:53:13.600
這個東西就是之前的 gradient 的 zooming square

0:53:13.600,0:53:17.720
之前算出來的 gradient 的平方和

0:53:17.720,0:53:19.460
就是 v0

0:53:19.460,0:53:23.800
你看，它先算一下 gradient，就是 gt

0:53:24.380,0:53:27.080
然後，根據 gt 呢

0:53:27.080,0:53:32.380
你就可以算出 mt，也就是現在要走的方向

0:53:32.380,0:53:35.420
現在要走的方向，是考慮過去要走的方向

0:53:35.420,0:53:36.920
再加上 gradient

0:53:36.920,0:53:42.640
接下來，算一下要放在分母的地方的 vt

0:53:42.640,0:53:46.520
這個 vt 是過去、前一個時間點的 vt

0:53:46.520,0:53:49.780
加上 gradient 的平方，等一下要開根號

0:53:49.780,0:53:51.260
這邊呢，它做了一個

0:53:51.260,0:53:55.160
跟原來 RMSProp 跟 momentum 裡面沒有的東西

0:53:55.160,0:53:58.160
叫做 bias correction

0:53:58.160,0:54:01.520
它會把 mt 跟 vt 都除上一個值

0:54:01.520,0:54:04.000
都除上一個值，那這個值本來比較大

0:54:04.000,0:54:05.260
那後來呢

0:54:05.260,0:54:09.560
這個值本來比較小，那後來呢

0:54:09.560,0:54:11.820
會越來越接近 1

0:54:11.820,0:54:15.620
至於為甚麼要這麼做，
他的 paper 裡面會告訴你他的理由

0:54:16.100,0:54:18.180
最後，你在 update 的時候

0:54:18.180,0:54:22.760
你把 momentum 建議你的方向，mt\head

0:54:22.760,0:54:29.400
去乘上 learning rate α，再除掉 RMSProp

0:54:29.400,0:54:34.080
就是 RMSProp normalize 以後，建議的 learning rate

0:54:34.080,0:54:35.940
最後，得到你 update 的方向

0:54:35.940,0:54:37.680
這個就是 Adam 這樣

0:54:37.680,0:54:39.680
那我猜你應該沒有聽得太懂

0:54:39.680,0:54:42.240
不過沒有關係，因為在 toolkit 裡面，只是打幾個

0:54:42.240,0:54:45.000
指令而已，我們就先在這邊休息 10 分鐘

0:54:45.000,0:54:46.740
等一下再繼續，謝謝

0:54:56.680,0:54:59.220
我們來上課吧

0:54:59.220,0:55:02.800
我們剛才講的，就是說

0:55:02.800,0:55:08.120
如果今天你在 training data 上的結果不好的話怎麼辦

0:55:08.120,0:55:10.000
那等一下我要講的呢？

0:55:10.000,0:55:14.340
如果今天你已經在 training data 上得到夠好的結果

0:55:14.340,0:55:17.740
但是，你在 testing data 上的結果仍然不好

0:55:17.740,0:55:19.780
那你有甚麼可行的方法

0:55:19.780,0:55:22.640
等一下會很快介紹 3 個方法

0:55:22.640,0:55:24.060
一個是 Early Stopping

0:55:24.060,0:55:25.380
Regularization 跟 Dropout

0:55:25.380,0:55:29.040
Early Stopping 跟 Regularization 是很 typical 的作法

0:55:29.040,0:55:32.380
他們不是 specific design for deep learning 的

0:55:32.380,0:55:35.000
這是一個很傳統、typical 的作法

0:55:35.000,0:55:38.380
那 Dropout 是一個滿有 deep learning 特色的做法

0:55:38.380,0:55:41.060
那在講 deep learning 的時候，需要講一下

0:55:41.620,0:55:45.120
我們來講一下 Early Stopping，
Early Stopping 是甚麼意思呢？

0:55:45.220,0:55:48.780
我們知道說，隨這你的 training

0:55:48.780,0:55:53.180
你的 total loss，如果你今天的 learning rate 調的對的話

0:55:53.180,0:55:54.940
你的 total loss 通常會越來越小

0:55:54.940,0:55:58.640
那有可能你 rate 沒有設好，loss 變大也是有可能的

0:55:58.640,0:56:01.180
那你想像 learning rate 調的很好的話

0:56:01.180,0:56:03.940
那你在 training set 上的 loss 應該是逐漸變小的

0:56:04.220,0:56:05.600
但是，因為我們知道說

0:56:05.600,0:56:07.760
training set 跟 testing set 他們的 distribution

0:56:07.760,0:56:09.020
並不完全一樣

0:56:09.020,0:56:13.280
所以，有可能當你的 training 的 loss 逐漸減小的時候

0:56:13.280,0:56:17.620
你的 testing data 的 loss 卻反而上升了

0:56:17.620,0:56:18.640
這是有可能的

0:56:18.640,0:56:23.520
所以，理想上，假如你知道 testing data 的 loss 的變化

0:56:23.520,0:56:27.960
你應該停在不是 training set 的 loss 最小

0:56:27.960,0:56:31.100
而是 testing set 的 loss 最小的地方

0:56:31.100,0:56:33.380
你在 train 的時候，你不要一直 train 下去

0:56:33.380,0:56:36.380
你可能 train 到這個地方的時候，就停下來了

0:56:36.380,0:56:39.740
但是實際上，我們不知道 testing set 阿

0:56:39.740,0:56:42.780
你根本不知道你 testing set 的 error 是甚麼阿

0:56:42.780,0:56:46.160
所以，我們其實會用 validation set

0:56:46.160,0:56:48.740
來 verify 這件事情

0:56:49.580,0:56:52.280
所以，有的地方我可能需要稍微講得清楚一點

0:56:52.280,0:56:55.500
就是這邊的 testing set 阿，並不是指

0:56:55.500,0:56:57.160
真正的 testing set

0:56:57.160,0:56:59.960
它指的是你有 label data 的 testing set

0:56:59.960,0:57:02.520
比如說，如果你今天是在做作業的時候

0:57:02.520,0:57:04.780
這邊的 testing set 可能指的是

0:57:04.780,0:57:06.120
Kaggle 上的 public set

0:57:06.120,0:57:10.760
或者是，你自己切出來的 validation set

0:57:10.760,0:57:13.760
希望大家可以知道我的意思

0:57:13.760,0:57:15.580
這個只是名詞用的不同而已

0:57:17.420,0:57:22.340
但是，你不會知道真正的 testing set 的變化

0:57:22.340,0:57:25.220
所以，其實我們會切一個 validation set

0:57:25.220,0:57:26.320
來 verify 說

0:57:26.320,0:57:29.380
甚麼時候用 validation set 模擬 testing set

0:57:29.380,0:57:30.360
來看說甚麼時候呢

0:57:30.360,0:57:32.560
這個 validation set 的 loss 最小的時候

0:57:32.560,0:57:34.360
你的 training 就停下來

0:57:34.360,0:57:37.360
那其實在 Kaggle 裡面，就可以支援你做這件事啊

0:57:37.360,0:57:40.820
所以，你就自己看一下 documentation

0:57:40.820,0:57:45.140
那 Regularization 是甚麼呢？

0:57:45.140,0:57:51.400
我們重新定義了那個我們要去 minimize 的 loss function

0:57:51.400,0:57:55.620
我們原來有一個 loss function

0:57:55.620,0:57:58.020
我們要 minimize 的 loss function

0:57:58.020,0:58:00.540
是 define 在你的 training data 上的

0:58:00.540,0:58:04.560
比如說要 minimize square error 
或 minimize cross entropy

0:58:04.560,0:58:06.480
那在做 Regularization 的時候呢

0:58:06.480,0:58:09.820
我們會加另外一個 Regularization 的 term

0:58:09.820,0:58:13.360
這個 Regularization 的 term 呢

0:58:13.360,0:58:15.380
這個 Regularization 的 term 呢

0:58:15.380,0:58:18.620
比如說，它可以是你的參數的 L2 norm

0:58:18.620,0:58:19.880
甚麼意思呢？

0:58:19.880,0:58:22.960
假設現在我們的參數 θ 裡面，它是

0:58:22.960,0:58:26.080
一群參數，w1, w2 等等有一大堆的參數

0:58:26.080,0:58:29.340
那這個 θ 的 L2 norm 呢，就是

0:58:29.340,0:58:33.700
你把你的 model 裡面的每一個參數都取平方

0:58:33.700,0:58:37.640
然後加起來，就是這個 θ2

0:58:37.640,0:58:41.300
那因為我們現在用 L2 norm 來做 Regularization

0:58:41.300,0:58:44.640
所以，這件事稱之為 L2 的 Regularization

0:58:44.640,0:58:48.140
那我們之前有講過說，在做 Regularization 的時候呢

0:58:48.140,0:58:52.500
一般我們是不會考慮 bias 這項

0:58:52.500,0:58:56.180
因為我們之前有講過說，加 Regularization  的目的

0:58:56.180,0:58:59.840
是為了要讓我們的 function 更平滑

0:58:59.840,0:59:01.320
而 bias 這件事情

0:59:01.320,0:59:04.260
通常跟 function 的平滑程度是沒有關係的

0:59:04.260,0:59:07.060
所以，通常我們在算 Regularization 的時候

0:59:07.060,0:59:09.060
不會把 bias 考慮進來

0:59:10.580,0:59:13.340
那如果我們把

0:59:13.340,0:59:15.980
L2 的 Regularization 放在這邊

0:59:15.980,0:59:18.780
我們會得到怎麼樣的結果呢

0:59:18.780,0:59:21.060
如果做微分的話，會得到怎麼樣的結果呢

0:59:21.060,0:59:23.840
如果我們把這個新的 objective function

0:59:23.840,0:59:26.160
我們把新的這個 loss function，也就是 L'

0:59:26.160,0:59:31.240
等於 L 加上 parameter 的 2 norm

0:59:31.240,0:59:33.180
做 gradient 的話呢

0:59:33.180,0:59:37.000
我們會得到 L' 對某一個參數 w 的偏微分

0:59:37.000,0:59:39.240
等於 L 對某個參數的偏微分

0:59:39.240,0:59:42.660
加上 λ 乘上某一個參數

0:59:42.660,0:59:45.200
因為這一項是

0:59:45.200,0:59:47.080
所有參數的平方和

0:59:47.080,0:59:50.660
所以，把這項對某個參數 w 做偏微分

0:59:50.660,0:59:53.680
你得到的結果就是 w

0:59:53.680,0:59:57.640
所以，你現在 update 參數的式子

0:59:57.640,0:59:58.940
會變成這樣

0:59:58.940,1:00:02.580
本來我們 update 的式子是把原來的參數

1:00:02.580,1:00:07.880
減掉 η 乘上 w 對

1:00:07.880,1:00:11.640
loss function 的偏微分 就得到新的參數

1:00:11.640,1:00:13.900
那現在這個 loss function 呢

1:00:13.900,1:00:16.740
這個 L'，這個 ∂L'/∂w

1:00:16.740,1:00:18.780
你可以換成，這個樣子

1:00:18.780,1:00:21.220
那你把這一項塞到這個地方

1:00:21.220,1:00:23.980
你把這一項塞到這個地方

1:00:24.240,1:00:25.840
那你會發現說呢

1:00:25.840,1:00:28.500
這邊有出現原來的參數

1:00:28.500,1:00:31.800
這邊也有出現原來的參數

1:00:31.800,1:00:35.160
所以，你可以把這幾項整理在一起

1:00:35.160,1:00:36.780
就變成這樣

1:00:36.780,1:00:39.640
你把這一項、這一項提出來

1:00:39.640,1:00:42.460
變成 (1 - η*λ) * w^t

1:00:42.460,1:00:49.300
再減掉你的參數對原來的 loss function 的 gradient

1:00:49.300,1:00:52.360
所以，如果根據這個式子，你就會發現說

1:00:52.360,1:00:55.000
其實在 update 參數的時候

1:00:55.000,1:00:57.020
每一次在 update 之前

1:00:57.020,1:01:01.180
你就把參數先乘個 (1 - η*λ)

1:01:01.180,1:01:04.900
也就是，每次你在 update 你的參數之前

1:01:04.900,1:01:07.560
通常你這邊的 η 就是你的 learning rate

1:01:07.560,1:01:09.720
它是一個很小的值

1:01:09.720,1:01:12.500
那這個 λ 通常會設一個很小的值，比如說

1:01:12.500,1:01:15.220
0.001 之類的

1:01:15.220,1:01:17.800
所以，η*λ 就是一個很小的值

1:01:17.800,1:01:20.660
(1 - η*λ) 通常是一個接近 1 的值

1:01:20.660,1:01:25.240
比如說 0.99，所以，今天你看這個 update 的式子的話

1:01:25.240,1:01:29.320
如果我們不管原來的 loss function 怎麼寫

1:01:29.320,1:01:30.720
只看這個 update 式子的話

1:01:30.720,1:01:33.860
等於你在 update 參數的時候，你做的事情是

1:01:33.860,1:01:38.620
每次 update 參數之前，就不分青紅皂白，先乘個 0.99

1:01:39.240,1:01:41.680
也就是說，你每次都會讓你的參數

1:01:41.680,1:01:42.980
越來越接近 0

1:01:42.980,1:01:46.240
不一定是越來越小，因為如果今天 w 是負的

1:01:46.240,1:01:49.320
w 是負的，負的乘上 0.99

1:01:49.320,1:01:52.360
它就變大了，它就接近 0，對不對

1:01:52.360,1:01:55.420
所以，今天每一個參數在 update 之前

1:01:55.420,1:01:59.040
都乘上一個小於 1 的值，所以它每次都越來越靠近 0

1:01:59.040,1:02:00.340
越來越靠近 0

1:02:00.480,1:02:02.060
那有人就會想說

1:02:02.060,1:02:04.960
越來越靠近 0，最後不就通通變 0 嗎？

1:02:04.960,1:02:07.140
這很崩潰阿，聽起來就不 make sense

1:02:07.140,1:02:09.280
那不會最後所有的參數都變 0

1:02:09.280,1:02:11.700
為甚麼？因為你還有後面這一項阿

1:02:11.700,1:02:13.340
沒有後面這一項

1:02:13.340,1:02:16.240
每一次 update 參數就越來越小，最後通通變 0

1:02:16.240,1:02:18.300
但是，問題就是後面還有這個

1:02:18.300,1:02:20.360
從微分那邊得到這一項

1:02:20.380,1:02:25.220
那這一項，會跟前面這一項，最後取得平衡

1:02:25.220,1:02:28.460
所以，並不會最後所有的參數都變成 0

1:02:29.200,1:02:31.540
因誤，如果我們使用

1:02:31.540,1:02:33.560
L2 的 Regularization 的時候

1:02:33.560,1:02:37.300
我們每次都會讓 weight 小一點、小一點、小一點

1:02:37.300,1:02:40.260
所以，這招叫做 Weight Decay

1:02:41.180,1:02:42.860
那其實 是這樣子

1:02:42.860,1:02:45.500
在 deep learning 裡面

1:02:45.500,1:02:49.960
Regularization 雖然有幫助，但是它的重要性

1:02:49.960,1:02:54.140
跟其他方法，比如說 SVM 比起來，並沒有那麼高

1:02:54.140,1:02:57.360
Regularization 幫助往往沒有那麼顯著

1:02:57.360,1:02:59.100
我覺得有一個可能的原因是

1:02:59.100,1:03:01.780
如果你看前面的 Early Stopping

1:03:01.780,1:03:06.020
我們可以決定說，甚麼時候 training 應該要被停下來

1:03:06.020,1:03:09.620
因為，我們現在在做這個 neural network 的時候

1:03:09.620,1:03:11.640
通常初始參數的時候，我們都是從

1:03:11.640,1:03:15.360
一個很小的、接近 0 的值開始初始參數

1:03:15.360,1:03:18.320
初始的時候，都是給它一個很小的、接近 0 的值

1:03:18.320,1:03:20.380
那你在做 update 的時候

1:03:20.380,1:03:24.360
通常就是讓參數離 0 越來越遠、越來越遠

1:03:24.360,1:03:27.780
而做 Regularization 這件事情

1:03:27.780,1:03:30.940
它要達到的目的，就是希望我們的參數

1:03:30.940,1:03:32.800
不要離 0 太遠

1:03:32.800,1:03:35.580
那我們參數不要離 0 太遠

1:03:35.580,1:03:38.220
加上 Regularization 所造成的效果

1:03:38.220,1:03:40.800
跟減少 update 次數

1:03:40.800,1:03:42.220
所造成的效果

1:03:42.220,1:03:43.720
其實，可能是很像的

1:03:43.720,1:03:46.200
但你今天做 Early Stopping，減少 update 次數

1:03:46.200,1:03:47.920
其實也會避免你的參數

1:03:47.920,1:03:50.800
離那些接近 0 的值太遠

1:03:50.800,1:03:54.280
那跟 Regularization 做的事情可能是很接近的

1:03:54.280,1:03:55.440
所以在 neural network 裡面

1:03:55.440,1:03:57.780
Regularization 雖然有幫助，但沒有那麼重要

1:03:57.780,1:04:00.040
沒有重要到說，比如說你看像 SVM

1:04:00.040,1:04:03.440
它是 explicitly 把 Regularization 這件事情

1:04:03.440,1:04:05.820
寫在它的 objective function 裡面

1:04:05.820,1:04:08.360
對不對，因為在做 SVM 的時候

1:04:08.360,1:04:11.480
它其實是要解一個 compass optimization problem

1:04:11.480,1:04:12.400
所以，實際上

1:04:12.400,1:04:15.280
它解的時候，並不一定會有 iteration 的過程

1:04:15.280,1:04:17.780
它一步就解出那個最好的結果了

1:04:17.780,1:04:24.160
它不像 deep learning 裡面有 Early Stopping 這件事

1:04:24.160,1:04:25.840
SVM 裡面，沒有 Early Stopping 這件事

1:04:25.840,1:04:27.060
一步就走到結果了

1:04:27.060,1:04:28.600
所以，你沒有辦法

1:04:28.600,1:04:31.320
用 Early Stopping 防止它離你太遠

1:04:31.320,1:04:33.320
所以你必須要把 Regularization

1:04:33.320,1:04:35.280
explicitly 加到你的 loss function 裡面去

1:04:38.060,1:04:43.160
那如果我們看 L1 的 Regularization

1:04:43.160,1:04:46.220
有人就會問說，為甚麼一定是平方，能不能用別的

1:04:46.220,1:04:48.400
當然可以用別的，比如說，你可以做

1:04:48.400,1:04:51.340
L1 的 Regularization，你可以把你的

1:04:51.340,1:04:54.740
Regularization 換成你的參數的 1 norm

1:04:54.740,1:04:56.980
也就是換成你參數裡面

1:04:56.980,1:04:58.880
換成你這個參數的集合裡面

1:04:58.880,1:05:01.000
每一個參數的絕對值的和

1:05:01.000,1:05:05.260
所以，如果我們把這一項換掉的話

1:05:05.260,1:05:07.840
如果我們把這一項從 2 norm 換成 1 norm 的話

1:05:07.840,1:05:09.380
會得到麼事呢？

1:05:09.940,1:05:12.220
你的第一個問題可能就是

1:05:12.220,1:05:14.960
絕對值不能微分阿

1:05:14.960,1:05:16.020
不能微分阿

1:05:16.020,1:05:18.480
給你一個最簡單的回答就是

1:05:18.480,1:05:21.360
這個東西 implement 在 Keras, TensorFlow 都沒有問題

1:05:21.360,1:05:23.880
所以，顯然是可以微分這樣

1:05:23.880,1:05:26.040
那實際的回答是這個樣子的

1:05:26.040,1:05:29.900
這個東西阿

1:05:29.900,1:05:32.780
它是取絕對值對不對？

1:05:32.780,1:05:36.180
那取絕對值，input 和 output 的關係不就長這樣子嗎？

1:05:36.180,1:05:38.960
不就是一個 V 的形狀嗎？

1:05:38.960,1:05:41.500
然後，在 V 的一邊

1:05:41.500,1:05:45.140
微分值是 1，在另外一邊微分值是 -1

1:05:45.140,1:05:47.940
那不能微的地方，其實只有在 0 的地方而已

1:05:47.940,1:05:50.060
就不要管它這樣子

1:05:51.240,1:05:54.240
真的出現、真的走到 0 的時候

1:05:54.240,1:05:56.520
你就胡亂給它一個值，比如說，給它 0 就好了

1:05:59.300,1:06:00.660
所以說

1:06:00.660,1:06:02.560
如果你把這一項

1:06:02.560,1:06:04.720
對 w 做微分的時候

1:06:04.720,1:06:06.340
你得到的結果是怎麼樣呢？

1:06:06.340,1:06:10.120
如果今天 w 是正的

1:06:10.120,1:06:12.240
那微分出來就是 +1

1:06:12.240,1:06:15.780
w 是負數，微分出來就是 -1

1:06:15.780,1:06:20.740
所以，我們這邊寫了一個 w 的 sign function

1:06:20.740,1:06:23.620
w 的 sign function 意思就是說，如果 w 是正數的話

1:06:23.620,1:06:25.260
這個 function output 就是 +1

1:06:25.260,1:06:29.080
w 是負數的話，這個 function output 就是 -1

1:06:29.700,1:06:32.420
如果我們把這一項

1:06:32.420,1:06:36.360
塞到參數 update 的式子裡面，會有甚麼結果呢？

1:06:36.360,1:06:37.840
就變成這樣

1:06:37.840,1:06:41.040
那我們可以把這個展開來

1:06:41.040,1:06:42.180
就變成這樣

1:06:42.180,1:06:44.240
那這個式子告訴我們甚麼？

1:06:44.240,1:06:48.140
這個式子告訴我們說，我們每一次在 update 參數的時候

1:06:48.140,1:06:50.520
我們每一次 update 參數的時候

1:06:50.520,1:06:51.980
我們就不管三七二十一

1:06:51.980,1:06:56.000
都一定要去減一個 η*λ，再乘一個

1:06:56.000,1:06:57.440
w 的 sign

1:06:57.500,1:07:00.780
也就是說，如果今天 w 是正的

1:07:00.780,1:07:03.300
w 是正的，這項就是 +1

1:07:03.300,1:07:05.740
所以，就變成是減一個 positive 的值

1:07:05.740,1:07:07.640
就會讓你的參數變小

1:07:07.640,1:07:09.980
如果 w 是負的，這一項是 -1

1:07:09.980,1:07:13.980
那就變成是加一個值，就會讓你的參數變大

1:07:13.980,1:07:16.200
也就是說，只要你的參數是正的

1:07:16.200,1:07:19.980
就減掉一些，只要你的參數是負的

1:07:19.980,1:07:21.020
就加上一些

1:07:21.020,1:07:23.300
那不管那個參數原來的值是多少

1:07:23.300,1:07:25.780
所以，如果你把這個 L1

1:07:25.780,1:07:29.240
跟 L2 做一下比較的話

1:07:29.240,1:07:32.560
他們同樣是讓參數變小，但是他們做的事情

1:07:32.560,1:07:34.600
是略有不同的

1:07:34.600,1:07:37.880
因為如果你是用 L1 的時候，每次都減掉固定的值

1:07:37.880,1:07:39.280
你用 L2 的時候

1:07:39.280,1:07:42.660
你是每一次都乘上一個小於 1 固定的值

1:07:42.660,1:07:46.320
所以，比如說，今天 w 是一個很正的值的話

1:07:46.320,1:07:47.960
比如說，它是一百萬

1:07:47.960,1:07:53.440
那你乘上一個 0.99，你其實把 w 減掉一個很大的值

1:07:53.440,1:07:55.820
但是，對 L1 來說

1:07:55.820,1:07:57.220
它每次減掉的值都是固定的

1:07:57.220,1:08:01.700
不管 w 是一百萬還是 0.1，w 減掉的值都是固定的

1:08:01.700,1:08:04.640
所以，對 L1 來說，對 L2 來說

1:08:04.640,1:08:06.860
只要 w 有出現很大的值

1:08:06.860,1:08:10.460
這個很大的 w

1:08:10.460,1:08:12.660
它下降很快，它很快就會變得很小

1:08:12.660,1:08:14.160
在 learning 的 process 中

1:08:14.160,1:08:16.460
但是，如果你今天是

1:08:16.460,1:08:18.420
L1 的話，那就不一樣了

1:08:18.420,1:08:21.540
如果 w 有很大的值，它的下降速度跟其他

1:08:21.540,1:08:22.840
很小的 w 是一樣的

1:08:22.840,1:08:24.620
所以，透過 L1 的 training 以後

1:08:24.620,1:08:26.920
你有可能認出來的 model 裡面

1:08:26.920,1:08:29.580
還是有一些很大很大的值

1:08:29.580,1:08:32.540
但是，如果我們考慮很小的值的話

1:08:32.540,1:08:34.760
對 L2 來說，很小的值

1:08:34.760,1:08:40.520
比如說 0.1, 0.01 阿，它的下降速度就很慢

1:08:40.520,1:08:42.980
所以，在 L 裡面，它會

1:08:42.980,1:08:47.280
train 出來的結果，它會保留很多接近 0 的值

1:08:47.280,1:08:51.880
那 L1 呢，它每次到下降一個固定的 value

1:08:51.880,1:08:53.200
那在 L1 裡面呢

1:08:53.200,1:08:56.020
它不會保留很多很小的值

1:08:56.020,1:09:00.260
所以，如果你用 L1 做 training 的時候呢，你得到的結果

1:09:00.260,1:09:01.580
就是會比較 sparse

1:09:01.580,1:09:04.420
那比較 sparse 的意思是說你 train 出來的參數裡面

1:09:04.420,1:09:06.300
有很多接近 0 的值

1:09:06.300,1:09:08.380
但是，也有很大的值

1:09:08.380,1:09:11.500
不像如果是 L2 的話，你 train 出來的結果

1:09:11.500,1:09:14.760
你的值是平均的都比較小

1:09:14.760,1:09:16.420
所以，他們 train 出來的結果是

1:09:16.420,1:09:18.740
略有差異的，那我們剛才

1:09:18.740,1:09:21.660
我們剛才在講 cn 的時候，有講過

1:09:21.660,1:09:25.500
L1 就是要產生一個 image 的時候，有產生 L1

1:09:25.500,1:09:27.240
那在剛才那個 task 裡面呢

1:09:27.240,1:09:29.680
L1 是比較適合的

1:09:29.680,1:09:32.320
因為我想要看到 sparse 的結果

1:09:32.320,1:09:37.040
我有試過用 L2，但是結果就沒有 L1 看起來那麼明顯

1:09:37.040,1:09:38.960
雖然 L1 看起來也沒有很明顯啦

1:09:38.960,1:09:40.180
但 L1 看起來的結果

1:09:40.180,1:09:42.620
是還比較像是一個 digit

1:09:43.660,1:09:47.480
那這邊就胡亂講一個東西，Weight Decay

1:09:47.480,1:09:50.320
我們在人腦裡面也會做 Weight Decay，對不對

1:09:50.320,1:09:53.140
這個是從、我記得龍騰的生物課本上有這個圖

1:09:53.980,1:09:57.840
這個是剛出生的時候，嬰兒的神經是這樣

1:09:57.840,1:10:00.560
6 歲的時候，有很多很多的神經

1:10:00.560,1:10:03.880
但是，到 14 歲的時候，神經間的連結

1:10:03.880,1:10:06.360
又減少了，所以

1:10:06.360,1:10:08.620
neural network 也會跟我們人

1:10:08.620,1:10:10.820
有一些很類似的事情，如果有一些 weight

1:10:10.820,1:10:12.620
你都沒有去 update 它

1:10:12.620,1:10:14.340
那它每次都會越來越小

1:10:14.340,1:10:16.340
最後就接近 0 就不見了

1:10:16.340,1:10:19.540
這跟人腦的運作，是有異曲同工之妙

1:10:20.640,1:10:22.900
那最後我們要講一下 dropout

1:10:22.900,1:10:25.400
我們先講 dropout 是怎麼做的

1:10:25.400,1:10:29.180
然後，才講為甚麼這樣做

1:10:29.420,1:10:31.040
dropout 是怎麼做的呢？

1:10:31.040,1:10:34.440
它是這樣，在 training 的時候

1:10:34.440,1:10:37.740
training 的時候，每一次我們要 update 參數之前

1:10:37.740,1:10:42.060
我們都對每一個 neuron，其實也包括 input 的地方

1:10:42.060,1:10:45.000
input 的 input layer 裡面的每一個 element

1:10:45.000,1:10:46.300
也算是一個 neuron

1:10:46.300,1:10:48.980
我們對 network 裡面的每一個 neuron

1:10:48.980,1:10:53.760
做 sampling，那這個 sampling 是要決定說

1:10:53.760,1:10:58.500
這個 neuron 要不要被丟掉，每個 neuron 有 p% 的機率

1:10:58.500,1:11:00.300
會被丟掉

1:11:01.380,1:11:04.440
那如果一個 neuron 被 sample 到要丟掉的時候

1:11:04.440,1:11:07.540
那你知道這個 neuron 要被丟掉了，那跟它相連的 weight

1:11:07.780,1:11:11.520
也失去作用，也都被丟掉，所以就變這樣

1:11:11.520,1:11:12.980
所以，做完這個 sample 以後

1:11:12.980,1:11:17.220
你的 network 的 structure 就變瘦了，變得比較細長

1:11:17.960,1:11:22.780
然後，你再去 train 這個比較細長的 network

1:11:22.780,1:11:26.100
而要注意一下，這個 sampling

1:11:26.100,1:11:29.580
是每次 update 參數之前，都要做一次

1:11:29.580,1:11:31.320
所以，每一次 update 參數的時候

1:11:31.320,1:11:35.160
你拿來 training 的那個 network structure 是不一樣的

1:11:35.160,1:11:38.440
每一次你都要重新做一次 sample，所以，你每一次

1:11:38.440,1:11:40.460
在做重新 sample 的時候

1:11:40.460,1:11:43.360
你得到的這個結果，會是不一樣的

1:11:45.360,1:11:47.180
那 testing 的時候呢

1:11:47.180,1:11:52.300
當你在 training 的時候，使用 dropout 的時候

1:11:52.300,1:11:54.480
你的 performance 是會變差的

1:11:54.480,1:11:56.100
了解我意思嗎？就是

1:11:56.180,1:11:59.260
因為本來如果你不要 dropout 的話

1:11:59.260,1:12:01.140
本來好好的做，不要 dropout 的話

1:12:01.140,1:12:06.620
你在 MNIST 上，剛剛可以把正確率做個 100% 這樣

1:12:06.620,1:12:08.240
但是，如果你加 dropout 的時後

1:12:08.240,1:12:09.640
因為你的神經元在 train 的時候

1:12:09.640,1:12:11.180
有時候莫名其妙就會不見

1:12:11.180,1:12:13.600
所以，你在 training 的時候，
有時候 performance 是會變差的

1:12:13.600,1:12:14.940
本來可以 train 到 100%

1:12:14.940,1:12:17.520
它就會變成只剩下 98%

1:12:17.520,1:12:18.680
有一些 neuron 不見了嘛

1:12:18.680,1:12:21.500
所以，當你加了 dropout 的時候

1:12:21.500,1:12:23.020
你在 training 上會看到的結果變差

1:12:23.020,1:12:25.420
但是，dropout 它真正要做的事情是

1:12:25.420,1:12:28.140
它就是要讓你 training 的結果變差

1:12:28.140,1:12:30.080
但是 testing 的結果是變好的

1:12:30.080,1:12:32.760
也就是，如果你今天遇到的問題是你 training 做得不夠好

1:12:32.760,1:12:34.660
你再加 dropout，你就是越做越差這樣子

1:12:35.320,1:12:36.880
那在 testing 的時候怎麼做呢？

1:12:36.880,1:12:39.380
在 testing 的時候要注意兩件事

1:12:39.380,1:12:42.400
第一件事情就是 testing 的時候不加 dropout

1:12:42.400,1:12:44.940
testing 的時候就是所有的 neuron 都要用

1:12:44.940,1:12:46.140
不做 dropout

1:12:46.340,1:12:49.720
另外一個地方是

1:12:49.720,1:12:51.460
在 testing 的時候

1:12:51.460,1:12:54.140
假設你的 dropout rate

1:12:54.140,1:12:56.280
在 training 的時候，dropout rate 是 p%

1:12:56.280,1:13:00.320
那在 testing 的時候，所有 weight 都要乘 (1 - p%)

1:13:01.080,1:13:04.740
也就是說，假設現在 dropout rate 是 50%

1:13:04.740,1:13:07.280
那我們在 training 的時候，learn 出來的 weight

1:13:07.280,1:13:08.500
等於 1

1:13:08.500,1:13:11.080
那 testing 的時候，你要把那個 weight

1:13:11.080,1:13:12.440
設 0.5

1:13:12.440,1:13:17.060
那有沒有很奇怪，我看很多人都皺眉頭這樣子

1:13:17.060,1:13:21.680
這個步驟非常地神妙

1:13:21.680,1:13:22.940
我覺得第一個想出來這個人

1:13:22.940,1:13:27.280
這個 Hinton，若憑空想出來這個想法真的非常神妙

1:13:27.280,1:13:29.900
那你自己在 implement dropout 的時候阿

1:13:29.900,1:13:32.460
過去，在還沒有那麼多 toolkit 的時候，常常有人說

1:13:32.840,1:13:35.500
拿給我看一個程式，說我做 dropout 了

1:13:35.500,1:13:37.400
它都沒有進步，老師你看看怎麼辦

1:13:37.400,1:13:41.740
我看就說，你忘了除這個 0.5，難怪沒有進步

1:13:41.740,1:13:43.500
不過現在 toolkit 都會自動幫你除 0.5

1:13:43.500,1:13:45.500
所以，就不用再擔心這件事情了

1:13:46.480,1:13:51.140
那為甚麼 dropout 有用，直覺的想法是這樣子

1:13:51.720,1:13:54.780
training 的時候，會丟掉一些 neuron

1:13:54.780,1:13:58.820
就好像是你要練輕功的時候，你會在腳上綁一些重物

1:13:59.320,1:14:01.500
然後，你實際上戰鬥的時候

1:14:01.500,1:14:03.500
實際上 testing 的時候，是沒有 dropout 的

1:14:03.500,1:14:04.560
實際上 test 的時候

1:14:04.560,1:14:06.880
你就把重物拿下來，所以就會變得很強

1:14:07.220,1:14:09.660
這個是小李，他平常都綁一個重物

1:14:09.660,1:14:12.680
只有在，我記得是要貫徹自己的忍道的時候

1:14:12.680,1:14:14.680
他才會拿下來

1:14:14.680,1:14:16.460
還是打輸我愛羅就是了

1:14:19.760,1:14:23.000
另外一個直覺的理由是這樣子

1:14:24.380,1:14:28.920
一個 neural network 裡面的每一個 neuron 就是一個學生

1:14:28.920,1:14:31.380
那大家被連結在一起

1:14:31.380,1:14:34.260
就是大家聽到要做 final project

1:14:34.260,1:14:37.080
那你知道說，在一個團隊裡面

1:14:37.080,1:14:40.540
總是有人會擺爛，就是它是會 dropout 的

1:14:40.540,1:14:43.500
所以，假設說你覺得你的隊友

1:14:43.500,1:14:45.060
其實是會擺爛的

1:14:45.060,1:14:47.440
所以，這個時候你就會想要好好做

1:14:47.440,1:14:50.000
實際上，你就會想要去 carry 他

1:14:50.000,1:14:51.700
但是，實際上最後在 testing 的時候

1:14:51.700,1:14:53.840
大家都有好好做，沒有人需要被 carry

1:14:53.840,1:14:57.120
那因為每個人都做是更有利，所以，結果是更好的

1:14:59.360,1:15:01.800
所以，在 testing 的時候，不用 dropout

1:15:01.800,1:15:04.360
另外，我想解釋的就是

1:15:04.360,1:15:07.520
直覺的需要解釋的就是

1:15:07.520,1:15:11.000
為甚麼 dropout rate 50% 的時候，就要乘 0.5

1:15:11.220,1:15:15.160
為甚麼 training 跟 testing 的 weight 是不一樣的呢？

1:15:15.160,1:15:17.960
照理說 training 用甚麼 weight 就要用在 testing 上阿

1:15:17.960,1:15:20.520
你這樣 training 跟 testing 的時候居然是用不同的 weight

1:15:20.520,1:15:21.620
為甚麼這樣呢？

1:15:21.620,1:15:23.220
直覺的理由是這樣

1:15:24.980,1:15:27.440
假設現在 dropout rate 是 50%

1:15:27.780,1:15:32.920
那在 training 的時候，你的期望總是會

1:15:32.920,1:15:34.840
丟掉一半的 neuron

1:15:34.840,1:15:37.060
對每一個 neuron 來說

1:15:37.060,1:15:39.800
總是期望說它有一半的 neuron

1:15:39.800,1:15:41.620
是不見的，是沒有 input 的

1:15:41.620,1:15:44.660
所以，你現在如果認好一組 weight

1:15:44.660,1:15:47.340
假設你在這個情況下，認好一組 weight

1:15:47.340,1:15:50.480
但是，在 testing 的時候，是沒有 dropout 的阿

1:15:50.480,1:15:53.000
所以，對同一組 weight 來說

1:15:53.000,1:15:56.000
假如你在這邊用這組 weight 得到 z

1:15:56.000,1:15:58.280
跟在這邊用這組 weight 得到 z'

1:15:58.280,1:16:00.920
它們得到的值，其實是會差兩倍的，對不對

1:16:00.920,1:16:04.040
因為在這個情況下，你總是會有一半的 input 不見

1:16:04.040,1:16:06.520
在這個情況下，你所有的 input 都會在

1:16:06.520,1:16:10.820
而你用同一組 weight 的話，變成 z' 就是 z 的兩倍了

1:16:10.820,1:16:12.400
這樣變成 training跟 testing 不 match

1:16:12.400,1:16:13.960
你 performance反而會變差

1:16:13.960,1:16:15.400
所以，怎麼辦？

1:16:15.400,1:16:17.780
把所有 weight 都乘 0.5 阿

1:16:17.940,1:16:24.500
乘 0.5  以後，做一下 normalization

1:16:26.080,1:16:30.220
把所有 weight 都乘 0.5，這樣 z 就會等於 z'

1:16:30.220,1:16:32.540
就是這麼回事

1:16:32.540,1:16:35.120
把這個 weight 乘上一個值以後，

1:16:35.120,1:16:38.040
反而會讓 training 跟 testing 是比較 match 的

1:16:38.040,1:16:41.600
這個是比較直觀上的結果，如果你要

1:16:41.600,1:16:44.200
更正式講的話，其實 dropout 有很多理由

1:16:44.200,1:16:47.640
這個東西還是一個可以探討的問題

1:16:47.700,1:16:51.720
在文獻上找到很多不同的觀點來解釋

1:16:51.720,1:16:53.820
為甚麼 dropout 會 work

1:16:53.820,1:16:56.160
那我覺得我比較能接受的是

1:16:56.160,1:16:59.840
dropout 是一種終極的 ensemble 的方法

1:16:59.840,1:17:01.800
甚麼是 ensemble 的方法呢？

1:17:01.800,1:17:03.680
ensemble 的方法在比賽的時候常用

1:17:03.680,1:17:05.200
ensemble 的方法意思是說

1:17:05.200,1:17:08.500
我們有一個很大的 training set

1:17:08.500,1:17:10.640
那你每次從 training set 裡面

1:17:10.640,1:17:13.640
只 sample 一部分的 data 出來

1:17:13.640,1:17:15.760
只 sample 一部分的 data 出來

1:17:15.760,1:17:19.700
記得我們在講 bias 跟 variance 的 trade off 的時候

1:17:19.700,1:17:24.340
我們不是以講過說，打靶有兩種狀況，一種是

1:17:24.340,1:17:27.420
你的 bias 大，所以你打不準

1:17:27.420,1:17:30.380
一種是你的 variance 很大，所以你打得準

1:17:30.380,1:17:32.500
如果你今天有一個很複雜的 model

1:17:32.500,1:17:34.340
很笨重、很大的 model 的時候

1:17:34.340,1:17:38.020
它往往是 bias 準，但 variance 很大

1:17:38.020,1:17:40.520
但是，如果你這個笨重的 model 有很多個

1:17:40.520,1:17:42.500
雖然它 variance 很大，最後平均起來

1:17:42.500,1:17:43.900
結果就很準

1:17:43.900,1:17:46.960
對不對，所以今天只 ensemble 做的事情

1:17:46.960,1:17:49.620
其實，就是要利用這個特性

1:17:49.620,1:17:54.380
我們 train 很多個 model

1:17:54.380,1:17:57.480
我們把原來的 training data 裡面 sample 出很多 subset

1:17:57.480,1:17:59.240
然後，train 很多個 model

1:17:59.240,1:18:01.680
然後，每一個 model 你甚至可以是 structure 不一樣

1:18:01.680,1:18:04.880
雖然說，每一個 model 他們可能

1:18:04.880,1:18:07.320
variance 很大，但是

1:18:07.320,1:18:10.500
如果他們都是很複雜的 model 的時候，平均起來

1:18:11.360,1:18:13.020
這個 bias 就很小

1:18:13.020,1:18:15.740
所以，你真正在 testing 的時候

1:18:15.740,1:18:18.640
train 了一把 model，然後在 testing 的時候

1:18:18.640,1:18:22.440
丟一筆 training data 近來，它通過所有的 model

1:18:22.440,1:18:25.640
得到一大堆的結果，再把這一大堆的結果平均起來

1:18:25.640,1:18:27.200
當作我們最後的結果

1:18:27.200,1:18:30.740
那如果你的 model 很複雜的畫，這一招往往有用

1:18:30.740,1:18:33.040
那 random forest 也是實踐這個方法的

1:18:33.040,1:18:36.360
一個實踐這個精神的一個方法

1:18:36.360,1:18:38.400
如果你用一個 decision tree，它就很弱

1:18:38.400,1:18:41.180
胡亂做它就會 overfitting，那如果你用 random forest

1:18:41.180,1:18:43.100
它就沒有那麼容易 overfitting

1:18:44.660,1:18:48.080
那為甚麼說 dropout 是一個終極的 ensemble 的方法呢

1:18:48.400,1:18:51.000
我們知道在做 dropout 的時候，我們每次

1:18:51.000,1:18:54.140
我們每一次要 update 參數的時候

1:18:54.140,1:18:56.540
就你拿一個 minibatch 出來，要 update 參數的時候

1:18:56.540,1:18:58.740
你都會做一次 sample

1:18:58.740,1:19:01.980
所以，你拿第一個 minibatch 的時候

1:19:01.980,1:19:03.480
你 train 的 network 長這樣子

1:19:03.480,1:19:04.860
你拿第二個 minibatch 的時候

1:19:04.860,1:19:06.020
你 train 的 network 可能長這樣

1:19:06.020,1:19:08.460
你拿第三個長這樣，你拿第四個長這樣

1:19:08.460,1:19:10.200
所以，在做 dropout 的時候

1:19:10.200,1:19:13.660
你等於是一個終極的 ensemble 的方式

1:19:13.660,1:19:16.380
你是在 train，假設你有 M 個 neuron

1:19:16.380,1:19:18.440
每一個 neuron 可以 drop 或不 drop

1:19:18.440,1:19:22.020
所以你可能的 neuron 的數目有 2^M 個

1:19:22.020,1:19:23.540
當你在做 dropout 的時候

1:19:23.540,1:19:28.240
你等於是在 train 這 2^M 個 neuron

1:19:28.240,1:19:32.920
你每次都只用一個 minibatch 的 data

1:19:32.920,1:19:35.980
去 train 一個 network，你用這個 minibatch 裡面的 data

1:19:35.980,1:19:38.100
用 minibatch 可能就 100 筆 data 嘛

1:19:38.100,1:19:43.200
你用這些 data 去 train 這些 network

1:19:43.200,1:19:47.000
那總共有 2^M 個可能的 network

1:19:47.000,1:19:50.680
當然因為你最後 update 的次數是有限的

1:19:50.680,1:19:54.180
你可能沒有辦法把 2^M 個 network 每個都 train 一遍

1:19:54.180,1:19:57.080
但是，你可能就 train 了好多好多的參數

1:19:57.080,1:19:58.220
好多好多的 network

1:19:58.220,1:20:01.000
你有做幾次 update 參數，你就 train 幾次 network

1:20:01.000,1:20:04.240
但是，每個 network 就只用一個 batch 來 train

1:20:04.240,1:20:06.020
那每一個 network 用一個 batch 來 train

1:20:06.020,1:20:08.200
可能會讓人覺得很不安

1:20:08.200,1:20:11.700
一個 batch 才 100 筆 data，怎麼 train 一個 network 呢

1:20:11.700,1:20:14.780
那沒有關係，因為這些不同的 network 之間的參數

1:20:14.780,1:20:16.980
是 shared，也就是說

1:20:16.980,1:20:19.820
這一個 network 的這一個參數

1:20:19.820,1:20:21.920
就是這個 network 的這個參數

1:20:21.920,1:20:25.800
就是它的這個參數，這 4 個參數其實是同一個參數

1:20:25.800,1:20:28.160
所以，雖然說一個 network

1:20:28.160,1:20:31.020
的 structure，它只用一個 batch train

1:20:31.020,1:20:34.980
但是一個 weight，它可能用好多個 batch 來 train

1:20:34.980,1:20:37.840
比如說，這個 weight，它在這 4 個 batch

1:20:37.840,1:20:40.760
裡面，在這 4 個 batch 做 dropout 的時候

1:20:40.760,1:20:42.440
都沒有把這個 weight 丟掉

1:20:42.440,1:20:45.960
那這個 weight，就是拿這 4 個 batch 合起來 train 的結果

1:20:47.540,1:20:49.900
所以，當你做 dropout 的時候

1:20:49.900,1:20:54.280
你就是 train 了一大把的 network structure

1:20:54.280,1:20:57.060
理論上，每一次 update 參數的時候

1:20:57.060,1:20:58.320
你都 train 了一個 network 出來

1:20:59.020,1:21:00.620
那 testing 的時候呢？

1:21:00.620,1:21:03.540
按照 ensemble 這個方法的邏輯應該就是

1:21:03.540,1:21:06.180
你把那一大把的 network 通通拿出來

1:21:06.180,1:21:09.920
然後，你把你的 testing data 丟到那一把

1:21:09.920,1:21:11.100
network 裡面去

1:21:11.100,1:21:13.400
每一個 network 都給你吐一個結果

1:21:13.400,1:21:15.440
然後，把所有的結果平均起來

1:21:15.440,1:21:16.880
就是最終的結果

1:21:16.880,1:21:19.620
但是，在實作上你沒辦法這麼做，因為

1:21:19.620,1:21:21.880
這一把 network 實在太多了

1:21:21.880,1:21:23.300
這一把 network 實在太多了

1:21:23.300,1:21:25.260
你沒有辦法把它都通通都拿出來

1:21:25.260,1:21:28.200
你沒有辦法每一個都丟一個 input 進去

1:21:28.200,1:21:29.900
去看看它 output 是什麼，再平均起來

1:21:29.900,1:21:31.020
這樣運算量太大

1:21:31.020,1:21:34.440
所以，dropout 最神奇的地方是它告訴你說

1:21:35.280,1:21:38.060
當你把一個完整的 network 不做 dropout

1:21:38.280,1:21:42.220
但是，把它的 weight 乘上 (1 - p%)

1:21:42.220,1:21:47.340
然後，你把這個東西，把你的 training data 丟進去

1:21:47.340,1:21:49.560
然後，得到它的 output 的時候

1:21:49.560,1:21:51.080
神奇的就是

1:21:51.080,1:21:53.180
這個 ensemble 的結果

1:21:53.180,1:21:56.480
跟這一個，把 weight 乘上 (1 - p%) 的結果

1:21:56.480,1:21:58.660
是可以 approximate 的

1:21:58.660,1:21:59.920
是可以 approximate 的

1:21:59.920,1:22:04.480
那你可能會想說，何以見得呢？

1:22:04.480,1:22:08.000
我們來舉一個例子

1:22:08.000,1:22:12.220
我們來 train 一個很簡單的 network，它就只有一個 neuron

1:22:12.220,1:22:15.000
它的 activation function 是 linear 的

1:22:15.000,1:22:18.840
我這邊就不考慮 bias

1:22:18.840,1:22:21.440
我們這邊有一個 neuron

1:22:21.440,1:22:24.780
然後，它的 input 是 x1, x2

1:22:24.780,1:22:27.040
然後，x1, x2 分別乘上

1:22:27.040,1:22:28.520
經過 training 以後

1:22:28.520,1:22:30.560
經過 dropout training 以後

1:22:30.560,1:22:31.900
你算出來的 weight 是 w1, w2

1:22:31.900,1:22:35.860
所以，它的 output 就是 w1*x1 + w2*x2

1:22:35.860,1:22:39.480
這個 neuron 沒有 activation function

1:22:39.480,1:22:41.220
或 activation function 是 linear 的

1:22:41.220,1:22:44.620
如果我們今天要做 ensemble 的話

1:22:44.620,1:22:48.000
theoretically 就是這麼做，對不對

1:22:48.000,1:22:51.620
每一個 neuron，我們做 dropout 的時候

1:22:51.620,1:22:53.880
你不會 drop 那個 output 的 neuron

1:22:53.880,1:22:56.420
你只會 drop hidden layer 跟 input 的 neuron

1:22:56.420,1:22:58.940
那這邊每一個 neuron，它可以

1:22:59.020,1:23:02.280
它可以被 drop，或不被 drop

1:23:02.280,1:23:04.940
對不對，所以我們總共有 4 種 structure

1:23:04.940,1:23:07.020
一個是通通沒被 drop

1:23:07.020,1:23:11.320
一個是 drop x1、一個是 drop x2,
一個是 x1, x2 都被 drop 掉

1:23:11.320,1:23:14.180
那你最後得到的 output 呢，這個 network

1:23:14.180,1:23:17.340
假設你 input x1, x2，這個 network 給我們的

1:23:17.340,1:23:19.820
就是 w1*x1 + w2*x2

1:23:19.820,1:23:22.340
同樣的 input，但是 x1 被 drop 掉

1:23:22.340,1:23:24.920
你得到的 output 是 w2*x2，這邊是 w1*x1

1:23:24.920,1:23:26.600
這邊給我們的 output 是 0

1:23:26.600,1:23:28.360
我們要做 ensemble

1:23:28.360,1:23:31.340
所以你要把這 4 個 network 
它的 output 通通都 average 起來

1:23:31.340,1:23:33.740
通通都 average 起來，那你 average 起來的結果

1:23:33.740,1:23:37.120
是不是就是，這邊有 4 個值嘛

1:23:37.120,1:23:39.520
然後，w1*x1 出現兩次

1:23:39.520,1:23:40.780
w2*x2 出現兩次

1:23:40.780,1:23:44.420
所以，得到的結果是 1/2* w1*x1 + 1/2*w2*x2，對不對

1:23:45.720,1:23:48.900
那我們現在做的事情是把

1:23:48.900,1:23:52.200
這兩個 weight 都乘 1/2

1:23:52.200,1:23:55.860
我可以把 w1*1/2, w2*1/2

1:23:55.860,1:23:57.520
同樣 input x1, x2

1:23:57.520,1:24:03.320
那得到的 output 也同樣是 1/2*w1*x1 + 1/2*w2*x2

1:24:03.320,1:24:08.400
所以，這邊想要呈現的是說，在這個最簡單的 case 裡面

1:24:09.220,1:24:11.540
ensemble 這件事情

1:24:11.540,1:24:14.840
用不同的 network structure 做 ensemble 這件事情

1:24:14.840,1:24:19.300
跟我們把 weight multiply 一個值

1:24:19.300,1:24:22.220
而不做 ensemble 所得到的 output

1:24:22.220,1:24:23.620
其實是一樣的

1:24:23.620,1:24:27.180
那你可能會說，這個例子這麼簡單

1:24:27.180,1:24:30.080
所以，這個例子上會 work，我想也是很直覺的阿

1:24:30.080,1:24:32.680
大概小學生都知道說，這個是 equivalent

1:24:32.680,1:24:36.460
但是，比如說，這邊是 sigmoid function

1:24:36.460,1:24:38.760
或是它是很多個 layer，它會 work 嗎？

1:24:38.760,1:24:41.680
結論就是不會 equivalent

1:24:41.680,1:24:43.120
就是不會 equivalent

1:24:43.120,1:24:45.140
只有是 linear 的 network

1:24:45.140,1:24:47.900
ensemble 才會等於 multiply 一個 weight

1:24:47.900,1:24:51.300
左邊跟右邊要相等的前提是你的 network 要是 linear 的

1:24:51.300,1:24:53.320
但是，network 不是 linear 的阿

1:24:53.320,1:24:54.600
所以，他們其實不 equivalent

1:24:54.600,1:24:58.380
這個就是 dropout 最後一個很神奇的地方

1:24:58.380,1:25:01.480
雖然不 equivalent，但是最後結果還是會 work 這樣

1:25:01.480,1:25:07.480
所以，根據這個結論，有人有一個想法是說

1:25:07.480,1:25:10.820
既然 dropout 在 linear 的時候

1:25:10.820,1:25:13.600
linear 的 network 上，ensemble 才會

1:25:13.600,1:25:15.700
等於前一個 weight

1:25:15.700,1:25:19.420
所以，今天如果我的 network 很接近 linear 的話

1:25:19.420,1:25:23.600
應該 dropout performance 會比較好，比如說

1:25:23.600,1:25:26.580
怎麼做 network 會比較接近 linear，比如說你用 ReLU

1:25:26.580,1:25:28.760
比如說，你用 Maxout network

1:25:28.760,1:25:29.740
他們是很接近 linear 的

1:25:29.740,1:25:32.400
相對於 sigmoid，它們是比較接近 linear 的

1:25:32.400,1:25:36.720
所以 dropout 確實在用 ReLU 或 Maxout network 的時候

1:25:36.720,1:25:39.100
它的 performance 是確實比較好的

1:25:39.100,1:25:42.860
比如說，你去看 Maxout network 的 paper 的話

1:25:42.860,1:25:45.260
它裡面也有 point 這一點，它的

1:25:45.260,1:25:47.960
Maxout 跟 dropout 加起來的記錄量

1:25:47.960,1:25:50.400
是比 sigmoid function 還要大的，那這個是

1:25:50.400,1:25:53.340
作者相當自豪的一點

1:25:53.340,1:25:56.780
這邊我要講的其實

1:25:56.780,1:25:59.380
就是這樣啦

1:25:59.380,1:26:02.800
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
