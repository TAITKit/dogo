0:00:00.000,0:00:01.060
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:01.060,0:00:04.880
Reinforcement Learning 其實是一個很大的題目

0:00:05.480,0:00:10.400
所以在下面加了一個 subtitle : 學一些皮毛

0:00:10.500,0:00:13.680
剩下時間我們就講一些皮毛

0:00:13.680,0:00:16.700
那這年頭講到 Deep Reinforcement Learning

0:00:16.700,0:00:19.540
大家就會覺得說很興奮，為什麼呢？

0:00:19.600,0:00:24.120
因為，在 15 年 2 月的時候

0:00:24.120,0:00:26.800
Kreeger 先生在Nature 上面發了一篇

0:00:26.800,0:00:29.200
用 Reinforcement Learning 的方法

0:00:29.300,0:00:33.200
來玩 Atari 的小遊戲，都可以痛電人類

0:00:33.300,0:00:36.200
然後，後來在 16 年的春天呢

0:00:36.300,0:00:42.500
又有大家都耳熟能詳的 AlphaGo，也是可以痛電人類

0:00:42.700,0:00:45.560
David Silver 就有說，他覺得說

0:00:45.640,0:00:50.220
AI 就是 Reinforcement Learning 加 Deep learning

0:00:50.220,0:00:52.140
Reinforcement Learning 加 Deep learning 就是

0:00:52.140,0:00:54.660
Deep Reinforcement Learning

0:00:54.660,0:00:57.800
所以，這個東西現在講起來大家都覺得很興奮

0:00:57.900,0:01:01.600
那這個 Reinforcement Learning 是甚麼呢？

0:01:01.700,0:01:05.460
在 Reinforcement Learning 裡面呢，也會有

0:01:05.500,0:01:09.860
一個 Agent，跟一個 Environment

0:01:09.860,0:01:11.100
這樣講可能有一點抽象

0:01:11.200,0:01:12.900
等一下會舉比較具體的例子

0:01:13.000,0:01:21.000
告訴大家說這個 Agent 跟 Environment 它們分別可以是些甚麼

0:01:21.200,0:01:23.620
我找個喉糖出來吃一下這樣子

0:01:23.620,0:01:28.100
你可能會覺得說為什麼我一直咳嗽都不會好

0:01:28.200,0:01:29.880
但是，這其實是沒有甚麼關係的

0:01:29.880,0:01:34.800
我記得我高三的時候，不知道怎麼回事一直咳嗽咳嗽，咳到大二

0:01:36.100,0:01:38.700
後來，就好了

0:01:44.100,0:01:48.100
好，那，

0:01:48.280,0:01:53.180
這個 Agent 呢，它會有 Observation，它會去看這個世界，

0:01:53.320,0:01:57.400
看到世界的某些種種的變化

0:01:57.500,0:02:00.900
那這個 Observation 又叫做 State

0:02:01.100,0:02:02.860
你在看 Deep Reinforcement Learning 的時候，

0:02:02.900,0:02:05.600
你常常會看到一個詞，叫做 State

0:02:05.800,0:02:08.000
其實這個 State，就是 Observation

0:02:08.100,0:02:11.500
這個 State 這個詞呀，我覺得總是很容易讓人誤導

0:02:11.600,0:02:19.900
當你聽到 State這個詞，你總是會想好像是一個，它翻譯應該翻譯成狀態，而這個狀態感覺是系統的狀態

0:02:20.000,0:02:25.500
不是，這個 State 是環境的狀態。這樣，大家了解我的意思嗎?

0:02:25.600,0:02:29.100
所以，我覺得用 Observation 這個詞或許是更貼切的

0:02:29.200,0:02:32.500
就是，你的 Machine 所看到的東西。

0:02:32.600,0:02:38.200
所以，這個 State 其實指的是這個環境的狀態，也就是你的 Machine 所看到的東西。

0:02:38.300,0:02:43.600
所以，在這個 Reinforcement Learning 領域才會有這種胖 DP 的這種作法

0:02:43.700,0:02:49.600
所謂胖 DP 就是 Part your observe 的 state，就是我們 State 只能觀察到一部分的情況

0:02:49.700,0:02:54.840
如果今天這個 State 是 machine 本身的 State，那怎麼會有那種 State 我會不知道的情況

0:02:54.900,0:02:59.520
那就是因為這個 State 其實是，所以如果你把 State 當作 machine 的 State，你就會搞不清楚 Partial observation

0:02:59.840,0:03:03.300
Partial Observation State 的那套想法到底是在幹麻。

0:03:03.400,0:03:09.220
今天就是因為 State 就是環境的 State，所以機器是有可能沒有辦法看到整個環境所有的狀態

0:03:09.370,0:03:13.650
所以才會有這個 Partial observation State 的這個想法

0:03:13.650,0:03:18.400
總之我今天沒有要講那個，但是這個 State 呀其實就是 Observation。

0:03:18.400,0:03:23.400
如果你以後有有機會看看文獻的話，你再看看我說得對不對。

0:03:23.500,0:03:29.210
好，那 Machine 呢，會做一些事情，它做的事情就叫做 Action

0:03:29.430,0:03:33.000
那它做的這些事情，會影響環境，

0:03:33.000,0:03:36.600
會跟環境產生一些互動，

0:03:36.600,0:03:39.400
對環境造成一些影響

0:03:39.500,0:03:44.500
那它因為對環境造成的一些影響，它會得到 Reward。

0:03:46.000,0:03:50.500
這 Reward 就會告訴它，它的影響是好的，還是不好的。

0:03:50.500,0:03:53.600
那這邊舉一個抽象的例子，比如說

0:03:53.600,0:03:59.080
機器看到一杯水，然後它就 take 一個 action，

0:03:59.470,0:04:01.470
action 就把水打翻了，那

0:04:02.330,0:04:07.030
Environment 它就得到一個 negative 的 reward，因為人告訴它說不要這麼做，

0:04:07.120,0:04:11.000
所以它就得到一個負向的 reward。

0:04:11.000,0:04:13.900
接下來呢，因為水被打翻了，

0:04:14.000,0:04:17.020
在 Reinforcement Learning 裡面，這些發生的事情都是連續的

0:04:17.220,0:04:22.600
因為水被打翻了，所以接下來它看到的 Observation 就是水被打翻的狀態。

0:04:22.600,0:04:26.200
看到水被打翻了，它決定 take 另外一個 action，

0:04:26.300,0:04:34.000
它決定要把它擦乾淨，人覺得它做得很對，它就得到一個 positive reward。

0:04:35.200,0:04:37.680
那，機器要做的事情，它生來的目標就是

0:04:37.950,0:04:41.000
它要去學習採取那些 action

0:04:41.100,0:04:44.800
它根據過去的得到的 positive reward 還有 negative reward，

0:04:44.900,0:04:52.300
它去學習採取那些可以讓 reward 被 
maximize 的那些 action，這個就是它存在的目標

0:04:52.400,0:04:58.870
如果我們用 AlphaGo 為例子的話，一開始 Machine 的 Observation 是甚麼?

0:04:58.990,0:05:06.200
Machine 的 Observation 就是棋盤，那棋盤你可以用一個 19*19 的 matrix 來描述它

0:05:06.300,0:05:09.700
所以如果是 AlphaGo 它的 Observation 就是棋盤

0:05:09.800,0:05:13.510
然後接下來呢，它要 take 一個 action，它 take 的 action 是甚麼呢?

0:05:13.770,0:05:17.600
它 take 的 action 就是落子的位置

0:05:17.700,0:05:23.650
它 take 的 action 就是放一個棋子到棋盤上，落一子這樣

0:05:24.010,0:05:27.530
下在這裡，下在 3 之 3。

0:05:29.580,0:05:34.190
接下來呢，在圍棋這個遊戲裡面

0:05:34.330,0:05:38.990
你的 Environment 是甚麼，你的 Environment 其實就是你的對手，

0:05:39.230,0:05:43.840
所以，你落子呀，落子在不同的位置，

0:05:43.980,0:05:48.630
你就會影響你的對手的反應，總之你落子以後，

0:05:48.820,0:05:51.200
你的對手會有反應

0:05:51.300,0:05:58.560
你看到的這個 observation 呢，就變了。假設說，你的對手呢落一個白子在這個地方，你的 observation就變了

0:05:58.630,0:06:02.790
機器看到另外一個 observation 以後，它又要決定它的 action

0:06:03.410,0:06:08.260
所以它再 take 一個 action，它再採取某一個行動，它再落子在另外一個位置。

0:06:08.440,0:06:12.800
所以下圍棋呢，用機器下圍棋呢，就是這麼一回事。

0:06:13.800,0:06:18.500
那今天在圍棋這個 case 裡面，它是個還蠻困難的 Reinforcement Learning 的 task，

0:06:18.600,0:06:22.600
因為在多數的時候，你得到的 reward 都是零，

0:06:22.700,0:06:27.720
因為你落子下去，通常是甚麼事也沒發生

0:06:28.110,0:06:29.500
你得到的 reward 就是零。

0:06:29.600,0:06:33.100
只有在你贏了，或者是輸了的時候，

0:06:33.200,0:06:34.200
你才會得到 reward。

0:06:34.300,0:06:41.000
如果你贏了，你就得到 reward 是 1，如果是輸了，你就得到 reward 是 -1

0:06:41.100,0:06:47.240
所以做 Reinforcement Learning 困難的地方就是，有時候你的 reward 是很?

0:06:47.300,0:06:52.100
只有少數的 action，只有在少數的情況你才能夠會得到 reward

0:06:52.460,0:06:57.250
所以它的難點就是機器怎麼在只有少數的 action 會得到 reward 的情況下

0:06:57.320,0:07:02.600
卻發覺正確的 action，這是一個很困難的問題。

0:07:03.200,0:07:07.940
對 machine 來說呢，它要怎麼學習下圍棋呢，

0:07:08.020,0:07:12.660
它就是不斷地找某一個對手一直下一直下，有時候輸、有時候贏

0:07:12.700,0:07:17.000
接下來，它就是調整它看到的 observation 和 action 之間的關係。

0:07:17.100,0:07:21.600
它裡面有一個 model，它會調整它看到 observation 時，它要採取甚麼 action，

0:07:21.700,0:07:26.000
它會調整那個 model，讓它得到的 rewards 可以被 maximize。

0:07:27.400,0:07:33.260
那我們可以比較一下，如果今天要下圍棋的時候，用 Supervised learning 和 un-supervised learning

0:07:33.420,0:07:36.800
你得到的結果會有怎麼樣的差別

0:07:37.000,0:07:40.000
你的 training 的方法有怎麼樣的差別

0:07:40.100,0:07:42.400
如果是 supervised learning 的話

0:07:42.500,0:07:47.100
那你就是告訴機器說，看到這樣子的盤勢，

0:07:47.200,0:07:50.000
你就落子在這個位置；

0:07:50.000,0:07:53.900
看到另一個盤勢，你就落子在另外一個位置。

0:07:55.000,0:07:58.060
那 Supervised learning 會不足的地方是，

0:07:58.170,0:08:03.110
當我們會用 Reinforcement Learning 的時候，往往是你不知道

0:08:03.200,0:08:09.100
連人都不知道正確答案是甚麼，所以在這個 task，你不太容易做 Supervised learning。

0:08:09.200,0:08:16.700
因為，在圍棋裡面，看到這個盤勢到底下一個位置最好的點是哪裡，其實有時候人也不知道

0:08:16.800,0:08:18.060
那機器可以看著棋譜學，

0:08:18.120,0:08:25.000
那棋譜上面的這個應對不見得是最 optimal

0:08:25.100,0:08:29.700
所以用 Supervised learning 可以學出一個會下圍棋的 Agent

0:08:29.800,0:08:33.290
但它可能不是真正最厲害的 Agent。

0:08:33.460,0:08:37.760
如果用 Supervised learning 就是 machine 從一個老師那邊學，

0:08:37.940,0:08:44.100
那老師會告訴它說，每次看到這樣子的盤勢，你要下在甚麼樣的位置；看到那樣子的盤勢，你要下在甚麼樣的位置。

0:08:44.200,0:08:46.200
這個是 Supervised Learning。

0:08:46.300,0:08:52.740
如果是 Reinforcement Learning ，就是讓機器呢，就不管它，它就找某一個人

0:08:52.890,0:08:58.100
去跟它下圍棋，然後下一下以後，如果贏了，它就得到 positive reward。

0:08:58.200,0:08:59.800
輸了，就得到 negative reward。

0:08:59.900,0:09:07.620
贏了它就知道說，之前的某些下法，可能是好，但是沒有人告訴它

0:09:07.990,0:09:13.900
甚麼樣的下法，在這幾百步裡面，哪幾步是好的，哪幾步是不好的，沒有人告訴它這件事

0:09:14.000,0:09:16.100
它要自己想辦法去知道。

0:09:16.200,0:09:20.600
在 Reinforcement Learning 裡面，你是從過去的經驗去學習，

0:09:20.700,0:09:24.000
但是，沒有老師告訴你說甚麼是好的，甚麼是不好的。

0:09:24.100,0:09:26.600
Machine 要自己想辦法。

0:09:26.900,0:09:30.300
其實在做 Reinforcement Learning 下圍棋的這個 task 裡面，

0:09:30.400,0:09:34.800
Machine 需要大量的 training 的 examples

0:09:34.800,0:09:40.500
它可能要下三千萬盤以後，它才能夠變得很厲害。

0:09:40.500,0:09:43.000
但是因為沒有人可以跟 machine 下三千萬盤，

0:09:43.100,0:09:48.400
所以大家都知道 AlphaGo 的解法，就是任兩個 machine，然後它們自己互下。

0:09:50.400,0:09:54.300
我們知道 AlphaGo 其實是先做 Supervised learning，讓 machine 學得不錯了以後，

0:09:54.400,0:09:58.000
再讓它去做 Reinforcement Learning。

0:10:01.900,0:10:04.600
Reinforcement Learning 也可以被用在 chat-bot 上面

0:10:04.700,0:10:05.800
怎麼用呢?

0:10:05.800,0:10:11.300
我們之前其實也有講過 chat-bot 是怎麼做的，learn一個sequence-to-sequence model，

0:10:11.400,0:10:16.200
input 是一句話，output 就是機器人回答

0:10:16.300,0:10:19.100
如果你用 supervised learning learn 一個 chat-bot

0:10:19.300,0:10:20.100
你就是告訴 machine 說，

0:10:20.590,0:10:25.600
如果有人跟你說 "Hello"，你就要講 "Hi"

0:10:25.700,0:10:28.500
如果有人跟你說 "Bye bye"，你就要說 "Goodbye"，

0:10:28.600,0:10:30.700
這個是 Supervised learning 的 learn 法。

0:10:30.800,0:10:38.300
如果是 Reinforcement Learning 的 learn 法，就是讓 machine 胡亂去跟人講話，講一講以後，人最後就生氣了

0:10:38.300,0:10:45.820
Machine 就知道說，它某句話可能講得不太好，但是沒有人告訴它，它到底哪句話講得不好

0:10:45.950,0:10:49.400
它要自己去想辦法發覺這件事情。

0:10:49.600,0:10:55.100
這個想法聽起來很 crazy，但是真的有 chat-bot 是這樣 learn 的。

0:10:55.200,0:11:00.100
這個怎麼做呢，因為你要讓 machine 去跟人一直講話，

0:11:00.200,0:11:08.100
學習看出人生氣了，或者是沒有生氣，然後去學怎麼跟人對話

0:11:08.200,0:11:12.000
這個學習太慢了，你可能要講好幾百萬次以後

0:11:12.100,0:11:14.100
你要跟好幾百萬人對話以後才會學會

0:11:14.200,0:11:19.400
但是如果一個 Agent 要跟好幾百萬人對話的話，大家都會很煩，沒有人要跟它對話。

0:11:19.400,0:11:22.800
所以怎麼辦呢，就用 AlphaGo style 的講法，

0:11:23.000,0:11:25.980
它任兩個 Agent，讓它們互講。

0:11:26.260,0:11:31.060
任兩個 Chat-bot 互講，可能都亂講，

0:11:31.280,0:11:32.200
有一個說 "See you"，另外一個說 "See you"

0:11:32.200,0:11:36.800
然後另外一個再說 "See you" ，陷入如窮地 loop。

0:11:36.800,0:11:42.200
然後就亂講，就讓兩個 chat-bot 去對話

0:11:42.300,0:11:47.900
然後它對話完以後，還是需要有人去告訴它說，它們講的好呢，還是不好

0:11:47.900,0:11:53.700
所以如果是在圍棋裡面比較簡單，因為圍棋的輸贏是很明確的

0:11:53.800,0:11:56.700
贏了就是 positive，輸了就是 negative

0:11:56.800,0:11:59.500
那輸贏你就寫個程式來判讀就好了

0:11:59.810,0:12:06.200
可是如果是對話的話就很麻煩，因為你可以讓兩個 machine 去互相對話，

0:12:06.300,0:12:09.470
它們兩個可以對話好幾百次，好幾百萬次，但是

0:12:09.560,0:12:16.200
問題就是你不知道這個對話，沒有人告訴那兩個 machine 說你們現在聊天到底還是聊得好還是聊得不好。

0:12:16.600,0:12:20.100
所以這個算是一個尚待克服的問題，

0:12:20.300,0:12:24.400
那這個在文獻上的方法是，這方法可能不見得是最好的方法，

0:12:24.580,0:12:34.400
它說，就訂個 rule，人去寫些規則，這規則其實在 paper 裡面寫得是，也是蠻簡單的，就蠻簡單的幾條規則

0:12:34.500,0:12:39.270
然後這幾條規則會去檢查，我看過去這兩個 Agent 對話的紀錄，

0:12:39.300,0:12:46.100
如果講得好的話，就給它 positive 的 reward，講得不好，就給它 negative 的 reward

0:12:46.200,0:12:50.050
講得好或不好，就是人自己主觀訂的，所以不知道人訂得好不好

0:12:51.500,0:12:58.700
然後 machine 就從它這個 rewards 裡面去學怎麼樣講才是好的

0:12:58.800,0:13:08.100
其實我可以在這邊做個預言，就是我覺得接下來就會有人用 game 來 learn 這個 chat-bot 了

0:13:08.200,0:13:11.760
雖然現在還沒有看到，但我相信很快就會有人幹這麼一件事。

0:13:11.950,0:13:14.800
這個怎麼做呢，你就 learn 一個 discriminator，

0:13:14.900,0:13:19.600
然後這個 discriminator 它會看真正人的對話和那兩個 machine 的對話

0:13:19.600,0:13:24.600
然後就判斷說你們現在這兩個 Agent 的對話，像不像人

0:13:24.700,0:13:25.600
如果像的話，

0:13:27.500,0:13:29.600
它會去抓說像人還是不像人

0:13:29.800,0:13:35.900
接下來呢，那兩個 Agent 的對話它們就會去想要騙過那個 discriminator， 讓它講得越來越像人。

0:13:36.000,0:13:41.400
那個 discriminator 判斷它說像人或不像人的這個結果就是 reward

0:13:41.500,0:13:47.000
它等於是用 discriminator 自動 learn 出給 reward 的方式

0:13:47.100,0:13:49.600
我相信很快就會有人做這麼一件事了

0:13:52.480,0:13:57.260
其實這個 Reinforcement Learning 有很多的應用，今天它特別適合的應用就是，

0:13:57.560,0:14:02.350
如果有一個 task ，人也不知道怎麼做，那你人不知道怎麼做就沒有 labeled data，

0:14:02.540,0:14:04.900
這個時候，用 Reinforcement Learning 是最適合的。

0:14:05.200,0:14:13.600
比如說在語音實驗室裡面，我們有做讓 machine 學會做 Interactive retrieval

0:14:13.600,0:14:16.470
所謂 Interactive retrieval 意思是說，有一個搜尋系統，

0:14:16.690,0:14:20.500
Machine 跟它說想要找尋一個跟 US President 有關的事情

0:14:20.600,0:14:24.600
那 machine 可能覺得說，這 US President 太廢了，

0:14:24.700,0:14:30.000
很多人都是美國總統，你到底是要知道跟美國總統甚麼有關的事情呢?

0:14:30.500,0:14:34.000
這 machine 會反問它一個問題，要求它 modify

0:14:34.000,0:14:35.700
它說它要找跟川普有關的事情

0:14:35.800,0:14:39.400
那 machine 反問它說，你要找的是不是跟選舉有關的事情等等

0:14:39.500,0:14:45.800
但是，machine 要反問甚麼問題，這個人也不知道，我們人也不知道要問甚麼樣的問題才是好

0:14:45.900,0:14:48.800
但是，你可以用 Reinforcement Learning 的方式，

0:14:48.900,0:14:52.900
來讓 machine 學說，問甚麼樣的問題，它可以得到最高的 reward。

0:14:53.000,0:14:58.900
那你的 reward 可能就是，最後搜尋的結果，使用者覺得越好，就是 reward 越高。

0:14:58.900,0:15:05.200
但是，每一次 machine 只要每問一個問題，它就會得到一個 negative 的 reward

0:15:05.200,0:15:08.300
因為每問一個問題，對人來說，就是 extra 的 effort

0:15:08.400,0:15:10.300
所以，應該要有一個 negative reward

0:15:11.600,0:15:18.400
Reinforcement Learning 還有很多 applications，比如說開一台直升機，開一個無人車，或者是

0:15:18.500,0:15:27.100
據說最近 DeepMind 用 Reinforcement Learning 的方法，來幫 Google 的 server 節電

0:15:28.500,0:15:34.900
現在也有人拿 Reinforcement Learning 來讓 machine 產生句子

0:15:37.120,0:15:43.200
在很多 task 裡面，machine 都需要產生句子，比如說 summarization，或者是 translation。

0:15:43.200,0:15:47.700
那這種產生句子的 Task，有時候還蠻麻煩的，為什麼?

0:15:47.800,0:15:51.800
因為有時候，machine 產生出來的句子，它是好的

0:15:51.940,0:15:55.900
但是，可是卻跟答案不一樣。

0:15:55.960,0:15:57.460
因為 translation 有很多種呀，

0:15:57.500,0:16:01.720
有一個標準答案是那樣，但是並不代表說 machine 現在產生出來的跟標準答案不一樣，它一定是壞的

0:16:02.200,0:16:07.700
所以這個時候，你如果可以引入 Reinforcement Learning 的話呢，其實是會有幫助的。

0:16:08.900,0:16:13.440
那 Reinforcement Learning 最常用的 application 就是

0:16:14.280,0:16:18.200
現在最常用的 application 就是打電玩

0:16:18.300,0:16:21.300
打電玩的 applications 現在已經滿坑滿谷

0:16:23.200,0:16:28.100
如果你想要玩的話，現在都有現成的 environment

0:16:28.200,0:16:33.100
可以讓你在現成的 environment 上面去玩

0:16:33.200,0:16:38.000
一個呢，叫做 Gym，這都是 Open AI 公司開發的。

0:16:38.600,0:16:42.200
這個 Gym 比較舊，最近他們又開了一個 Universe

0:16:42.300,0:16:45.800
Universe 裡面有很多那種 3D 的遊戲。

0:16:45.900,0:16:48.800
那每次講說讓 machine 玩遊戲，

0:16:50.100,0:16:56.600
就會有個問題說，可是 machine 不是本來就已經會玩遊戲了嗎?

0:16:56.700,0:17:01.900
在那些遊戲裡面，不是本來就已經有一個 AI  了嗎?

0:17:02.000,0:17:09.400
但是，現在你要讓 machine 用 Reinforcement Learning 的方法，去學玩遊戲，跟那些已經內建的 AI

0:17:09.500,0:17:12.200
其實是不一樣的。

0:17:12.300,0:17:20.200
因為，machine 它學怎麼玩這個遊戲，其實是跟人一樣的，

0:17:20.300,0:17:28.000
它是坐在螢幕前的，也就是說它看到的東西，並不是從那個程式裡面去擷取甚麼東西出來，

0:17:28.100,0:17:30.300
它看到的東西就是那個螢幕畫面，

0:17:30.400,0:17:33.400
它看到的東西跟人一樣就是 pixels，

0:17:33.500,0:17:38.000
當你用 machine 來玩，用 Reinforcement Learning 讓 machine 學習玩這些電玩的時候

0:17:38.100,0:17:41.700
Machine 看到的，就是 pixels。

0:17:41.800,0:17:46.700
然後再來呢，它要 take 哪個 action，它看到這個畫面，它要做甚麼事情

0:17:46.800,0:17:52.300
它自己決定了，並不是人寫程式告訴它說，if 你看到這個東西，then 你做甚麼

0:17:52.300,0:17:54.800
它是自己學出來的。

0:17:55.900,0:18:07.110
舉例來說，你可以讓 machine 玩 Space invader，space invader 就是叫小蜜蜂還是大黃蜂，反正這是 translation 不太重要

0:18:08.300,0:18:14.100
我們等一下舉例的時候都用這個來作例子

0:18:14.200,0:18:17.800
都用 Space invader 來作例子，我們可以稍微解說一下這個遊戲，

0:18:17.800,0:18:24.300
在這個遊戲裡面，你可以 take 的 action 有三個，就是左、右移動，跟開火，

0:18:29.040,0:18:33.560
那它怎麼玩這個 video game 呢? 整個 scenario 是這樣，

0:18:35.740,0:18:40.390
首先呢，machine 會看到一個 observation，

0:18:40.710,0:18:44.500
這個 observation 就是螢幕的畫面，

0:18:44.600,0:18:46.800
也就是螢幕畫面裡面的 pixels

0:18:46.900,0:18:54.300
那開始的 observation 我們就叫它 S1，所以一開始 machine 看到一個 S1

0:18:54.400,0:19:02.600
那這個 S1 其實就是一個 matrix，那這個 matrix 其實就是每一個 pixel 用一個 vector 來描述它

0:19:02.700,0:19:05.300
所以這邊應該是一個三維的 tensor

0:19:05.400,0:19:09.990
這是一個 matrix，但它是有顏色的，所以它三維

0:19:10.000,0:19:14.560
好，那 machine 看到這個畫面以後，它要決定它要 take 哪一個 action，

0:19:14.600,0:19:18.180
它現在只有三個 action 可以選擇，比如說它決定要 "往右移"

0:19:20.000,0:19:24.550
那每次 machine take 一個 action 以後，它會得到一個 reward

0:19:25.030,0:19:29.150
但是因為只是往右移，這個 reward 是甚麼

0:19:29.330,0:19:34.900
就是左上角的這個分數，就是它的 reward，那往右移不會得到任何的 reward，

0:19:35.000,0:19:37.300
所以得到的 reward r1 是 0

0:19:40.000,0:19:49.400
Machine take 完這個 action 以後，它的 action 會影響了環境，machine 看到的 observation 就不一樣

0:19:49.500,0:19:53.400
現在 machine 看到的 observation 叫做 s2，

0:19:53.500,0:19:56.800
那有點不一樣，因為它自己往右移了。

0:19:58.200,0:20:00.800
當然這些外星人，也會稍微移動一點，

0:20:02.400,0:20:05.500
不過這個跟 machine take 的 action 是沒有關係的，

0:20:05.800,0:20:09.420
但是，有時候環境的變化本來就會跟 action 沒有關係

0:20:09.420,0:20:14.400
有時候環境的變化會是純粹隨機的，跟 machine take 的 action 是沒有關係的

0:20:15.240,0:20:19.620
那看到 s2

0:20:19.620,0:20:24.520
這邊講一下通常環境會有 random 的變化

0:20:24.520,0:20:28.340
環境這個 random 的變化跟 machine take 的 action 是沒有甚麼關係的

0:20:28.340,0:20:30.660
比如說這邊突然多出一個子彈

0:20:30.660,0:20:35.940
這些外星人甚麼時候要放出來我覺得應該就是隨機的

0:20:36.620,0:20:41.160
然後 machine 看到 s2 以後他要決定 take 哪一個 action

0:20:41.160,0:20:47.980
這個是 a2 假設他決定他要射擊了

0:20:47.980,0:20:50.460
假設他成功殺了一隻外星人

0:20:50.460,0:20:53.080
他就會得到一個 reward

0:20:53.080,0:20:56.540
那我發現殺不同外星人其實得到分數不一樣

0:20:56.540,0:20:59.180
假設他殺了一個五分的外星人

0:20:59.380,0:21:04.280
那他看到的 observation 就變少了一隻外星人

0:21:04.280,0:21:06.900
這個是第三個 observation

0:21:06.900,0:21:12.720
這個 process 就一直進行下去

0:21:12.720,0:21:17.300
直到某一天在第 T 個回合的時候

0:21:17.300,0:21:19.660
machine take action aT

0:21:19.660,0:21:30.220
然後他得到的 reward rT 進入了另外一個 state

0:21:30.240,0:21:34.720
這個 state 是個 terminal 的 state

0:21:34.720,0:21:36.700
它會讓這個遊戲結束

0:21:36.700,0:21:40.940
在這個 Space Invader 這個遊戲裡面

0:21:40.940,0:21:44.320
terminal state 就是你被殺死就結束了

0:21:44.320,0:21:47.460
所以 machine 可能 take 一個 action 比如說往左移

0:21:47.460,0:21:50.620
那得到 reward 0 不小心撞到 alien 的子彈

0:21:50.620,0:21:55.040
就死了遊戲就結束了

0:21:55.040,0:22:01.620
遊戲的開始到結束叫做一個 episode

0:22:01.620,0:22:06.860
對 machine 來說它要做的事情就是要不斷去玩這個遊戲

0:22:06.860,0:22:13.920
他要學習在怎麼在一個 episode 裡面 maximize 它可以得到的 reward

0:22:13.920,0:22:18.700
maximize 他在整個 episode 裡面可以得到的 total 的 reward

0:22:18.700,0:22:23.180
它必須要在死之前殺最多的外星人

0:22:23.180,0:22:29.420
他要殺最多的外星人而且他要閃避外星人的子彈，讓自己不要那麼容易被殺死

0:22:30.420,0:22:35.220
Reinforcement Learning 的難點在哪裡

0:22:36.840,0:22:39.500
它有兩個難點

0:22:39.500,0:22:46.840
第一個難點是 reward 的出現往往會有 delay

0:22:46.840,0:22:51.620
比如說在 Space Invader 裡面

0:22:51.640,0:22:55.060
其實只有開火這件事情才可能得到 reward

0:22:55.060,0:22:58.100
也就是開火以後才得到 reward

0:22:58.100,0:23:05.500
但是如果 machine 只知道開火以後就得到 reward

0:23:05.520,0:23:08.180
它最後 learn 出來的結果它只會瘋狂開火

0:23:08.180,0:23:12.600
對它來說往左移、往右移沒有任何 reward 它不想做

0:23:12.600,0:23:15.980
實際上往左移、往右移這些 moving

0:23:15.980,0:23:21.420
它對開火能不能夠得到 reward 這件事情是有很關鍵的影響

0:23:21.500,0:23:26.340
雖然往左移、往右移的 action 本身沒有辦法讓你得到任何 reward

0:23:26.340,0:23:28.900
但它可以幫助你在未來得到 reward

0:23:28.900,0:23:32.420
這些事情其實就像規劃未來一樣

0:23:32.420,0:23:37.560
所以 machine 需要有這種遠見，它要有這種 vision

0:23:37.560,0:23:40.600
它才能夠把這些電玩完好

0:23:40.600,0:23:43.180
那其實下圍棋也是一樣

0:23:43.180,0:23:48.000
有時候短期的犧牲最後可以換來最後比較好的結果

0:23:48.000,0:23:50.340
就像是虛子把自己的子堵死一塊

0:23:50.340,0:23:53.000
結果最後反而贏了

0:23:53.000,0:23:58.800
另外一個就是你的 agent 採取的行為

0:23:58.800,0:24:01.980
會影響它之後所看到的東西

0:24:01.980,0:24:09.720
所以 agent 要學會去探索這個世界

0:24:10.700,0:24:17.400
比如說在 Space Invader 裡面你的 agent 只會往左移、往右移

0:24:17.400,0:24:21.860
它從來不開火，他就永遠不知道開火可以得到 reward

0:24:21.860,0:24:27.520
或是它從來沒有試著去擊殺最上面這個紫色的母艦

0:24:27.520,0:24:34.640
它可能從來沒有試著擊殺紫色的母艦，它就永遠不知道擊殺那個東西可以得到很高的 reward

0:24:34.960,0:24:39.660
所以要讓 machine 知道要去 explore 這件事情

0:24:39.660,0:24:41.880
它要去探索它沒有做過的行為

0:24:41.880,0:24:44.540
這個行為可能有好的結果、壞的結果

0:24:44.540,0:24:46.760
但是要探索沒有做過的行為

0:24:46.760,0:24:50.840
在 Reinforcement Learning 裡面也是重要的一件事情

0:24:51.200,0:24:55.480
在下課之前要講一下等一下要講甚麼

0:24:56.320,0:25:00.100
Reinforcement Learning 其實有一個 typical 的講法

0:25:00.100,0:25:04.540
要先講 Markov Decision Process

0:25:04.540,0:25:08.360
但如果先講 Markov Decision Process 的話

0:25:08.360,0:25:10.960
講完 Markov Decision Process 其實就下課了

0:25:10.960,0:25:13.320
你就只聽到 Markov Decision Process

0:25:13.320,0:25:19.040
而且很多課有講 Markov Decision Process
所以我覺得不要從 Markov Decision Process 講起

0:25:19.380,0:25:26.280
在 Reinforcement Learning 裡面很紅的一個方法叫 Deep Q Network

0:25:26.280,0:25:28.960
今天也不講 Deep Q Network

0:25:28.960,0:25:32.120
為甚麼
因為那個東西已經被打趴了

0:25:32.120,0:25:35.480
現在最強的方法叫 A3C

0:25:35.480,0:25:38.120
Deep Reinforcement 已經有點退流行了

0:25:38.120,0:25:45.940
會發現在gym 裡面最強的那些 agent 都是用 A3C
像我剛才看到的例子

0:25:45.940,0:25:49.340
剛剛看到自己玩 Space Invader 的例子就是用 A3C learn

0:25:49.700,0:25:53.580
所以我想說不如直接來講 A3C

0:25:53.580,0:25:58.820
迎頭趕上的概念，直接來講最新的東西

0:25:58.820,0:26:01.600
講 A3C 之前

0:26:01.600,0:26:03.400
需要知道甚麼事情

0:26:03.400,0:26:08.260
需要知道 Reinforcement Learning 的方法

0:26:08.260,0:26:10.120
分成兩大塊

0:26:10.120,0:26:18.000
一個是 Policy-based 的方法
一個是 Value-based 的方法

0:26:18.000,0:26:24.460
Policy-based 的方法應該是比較後來才有的

0:26:24.460,0:26:26.860
應該是先有 Value-based 的方法

0:26:26.860,0:26:33.500
所以一般教科書都是講 Value-based 的方法比較多
講 Policy-based 的方法比較少

0:26:33.500,0:26:37.260
如果你看 Sutton

0:26:37.260,0:26:42.480
有一本 Deep Reinforcement Learning 的 Bible 是 Sutton 寫的

0:26:42.560,0:26:47.760
它在 97 版的教科書裡面

0:26:47.960,0:26:51.320
Policy 的方法講很少

0:26:51.320,0:26:55.200
但它今年又再版

0:26:55.200,0:26:56.820
它還在撰寫中

0:26:56.820,0:27:00.400
我暑假載下來的教科書的內容

0:27:00.400,0:27:03.440
跟最近載下來的內容完全不一樣，差很多

0:27:03.600,0:27:06.220
它最近在改那本教科書

0:27:06.220,0:27:11.180
就有一整個章節在講 Policy Gradient

0:27:13.760,0:27:18.740
在 Policy-based 方法裡面

0:27:18.740,0:27:22.180
會 learn 一個負責做事的 actor

0:27:22.180,0:27:27.140
在 Value-based 的方法裡面會 learn 一個不做事的 critic

0:27:27.140,0:27:29.140
它專門批評，不做事的

0:27:29.140,0:27:35.440
但是要把 actor 跟 critic 加起來叫做 Actor-Critic 的方法

0:27:35.440,0:27:44.400
現在最強的方法就是 Asynchronous Advantage Actor-Critic，縮寫叫 A3C

0:27:44.400,0:27:47.740
所以等一下就講 Actor-Critic 這個方法

0:27:47.740,0:27:51.300
你可能會問最強的 Alpha Go 是用甚麼方法

0:27:51.300,0:27:55.400
如果仔細讀 Alpha Go paper，它是各種方法大雜燴

0:27:55.440,0:27:59.700
它裡面其實有 Policy Gradient 方法、Policy-based 方法

0:27:59.700,0:28:05.060
它也有 Value-based 的方法，它還有 Model-based 的方法，我就沒有講到 Model-based 的方法

0:28:05.060,0:28:09.000
所謂 Model-based 的方法是指一個 Monte Carlo tree search 那一段

0:28:09.000,0:28:11.560
算是 Model-based 的方法

0:28:11.560,0:28:15.680
不過像 Model-based 方法就是要預測未來會發生甚麼事

0:28:15.680,0:28:21.100
有一個對未來事件的理解，預測未來會發生甚麼事

0:28:21.240,0:28:26.420
這種方法應該是只有在棋類遊戲比較有用

0:28:26.420,0:28:28.420
如果是打電玩的話

0:28:28.420,0:28:31.520
就沒有看到 Model-based 的方法有甚麼成功的結果

0:28:31.520,0:28:35.800
打電玩裡面要預測未來會發生的狀況是比較難的

0:28:35.800,0:28:39.460
未來會發生的狀況是難以窮舉

0:28:39.460,0:28:42.120
不像圍棋雖然未來會發生的狀況很多

0:28:42.320,0:28:44.940
但還是可以舉出來

0:28:44.940,0:28:48.280
但是如果是電玩的話

0:28:48.280,0:28:50.080
我就很少看到 Model-based 的方法

0:28:50.080,0:28:54.100
看起來做 Model-based 的方法在電玩上是比較困難的

0:28:54.100,0:28:57.880
以下是一些 reference 如果想學更多的話

0:28:57.880,0:29:00.600
Sutton 的教科書在這裡

0:29:00.600,0:29:03.120
David Silver 有十堂課

0:29:03.120,0:29:09.220
它的內容就是 base on Sutton 的教科書講的

0:29:09.220,0:29:11.580
他講的十堂課，每堂有一個半小時

0:29:11.580,0:29:14.460
他沒講太多 Deep Reinforcement Learning 的東西

0:29:14.460,0:29:17.720
但是他有一個 Deep Reinforcement Learning 的 tutorial

0:29:17.740,0:29:19.720
video lecture 往下找就找到

0:29:19.720,0:29:28.000
另外你可以找到這個 OpenAI、John Schulman 的 lecture

0:29:28.000,0:29:32.680
他 lecture 講的是 Policy-based 的方法

0:29:51.260,0:29:57.060
我們就先來講怎麼學一個 Actor

0:29:57.060,0:30:00.700
所謂的 Actor 是甚麼

0:30:01.280,0:30:08.720
開學的時候就有說過 Machine Learning 在做的事情就是找一個 function

0:30:08.720,0:30:11.300
在 Reinforcement Learning 裡面

0:30:11.300,0:30:17.140
Reinforcement Learning 也是 Machine Learning 的一種
要做的事情也是找一個 function

0:30:21.320,0:30:27.540
我沒有寫錯，我本來想說我應該寫 Actor，但這邊我沒有寫錯

0:30:27.780,0:30:30.120
Actor 就是一個 function

0:30:30.120,0:30:34.940
這個 Actor 通常就寫成 pi，用 pi 來代表這個 function

0:30:34.940,0:30:40.340
這個 function 的 input 就是 machine 看到的 observation

0:30:40.340,0:30:45.840
他的 output 就是 machine 要採取的 action

0:30:45.840,0:30:50.220
observation 就是現在要找的 function 的 input

0:30:50.220,0:30:54.540
action 就是現在要找的 function 的 output

0:30:54.540,0:31:02.160
我們要透過 reward 幫助我們找出這個 function
也就是幫助我們找出 Actor

0:31:02.700,0:31:10.500
在有些文獻上 Actor 又叫作 Policy
所以看到 Policy 的時候他指的就是 Actor

0:31:10.500,0:31:14.660
找這個 function 有三個步驟

0:31:14.660,0:31:18.300
Deep Learning 很簡單的就是三個步驟

0:31:18.300,0:31:25.740
第一個步驟就是決定 function 長甚麼樣子

0:31:25.740,0:31:30.780
決定你的 function space
Neural Network 他決定了一個 function space

0:31:30.780,0:31:34.100
所以 Actor 他可以就是一個 Neural Network

0:31:34.100,0:31:39.800
如果你的 Actor 就是一個 Neural Network 那你就是在做 Deep Reinforcement Learning

0:31:40.580,0:31:43.160
所以這個 Neural Network 的 input

0:31:43.160,0:31:45.200
就是 Machine 看到的 observation

0:31:45.200,0:31:49.860
這 observation 就是一堆 pixel 可以把他用一個 vector 來描述

0:31:49.860,0:31:53.200
或者是用一個 matrix 來描述

0:31:53.200,0:31:58.360
output 就是現在可以採取的 action

0:31:58.360,0:32:03.240
或者是直接看下面這個例子可能會比較清楚

0:32:03.240,0:32:05.260
output 是甚麼

0:32:05.260,0:32:09.580
input 就是 pixel
把 Neural Network 當作 Actor

0:32:09.580,0:32:14.380
他可能不只是一個簡單的 Feed Forward Network 因為你的 input 現在是張 image

0:32:14.380,0:32:17.000
所以裡面應該會有 Convolution Layer

0:32:17.000,0:32:20.360
所以應該是會用 Convolutional Neural Network

0:32:20.400,0:32:21.860
output 的地方呢

0:32:21.860,0:32:25.460
現在有幾個可以採取的 action

0:32:25.460,0:32:29.140
output 就有幾個 dimension

0:32:29.140,0:32:32.420
假設現在在玩 Space Invader 這個遊戲

0:32:32.420,0:32:36.240
可以採取的 action 就是左移、右移跟開火

0:32:36.240,0:32:48.400
那 output layer 就只需要三個 dimension 分別代表左移、右移跟開火

0:32:48.400,0:32:53.300
這個 Neural Network 怎麼決定 Actor 採取哪個 action

0:32:53.300,0:33:01.060
通常做法是這樣，把 image 丟到 Neural Network 裡面去

0:33:01.060,0:33:07.620
他就會告訴你每一個 output 的 dimension 也就是每一個 action 所對應的分數

0:33:07.720,0:33:11.220
你可以採取分數最高的 action

0:33:11.220,0:33:16.100
比如說 left 分數最高
假設已經找好這個 Actor

0:33:16.100,0:33:20.300
machine 看到這個畫面他可能就採取 left

0:33:20.620,0:33:25.820
但是做 Policy Gradient 的時候

0:33:25.820,0:33:30.260
通常會假設 Actor 是 stochastic

0:33:30.260,0:33:33.300
Policy 是 stochastic

0:33:33.300,0:33:37.760
所謂的 stochastic 的意思是你的 Policy 的 output 其實是個機率

0:33:37.760,0:33:42.020
如果你的分數是 0.7、0.2 跟 0.1

0:33:42.020,0:33:45.620
有 70% 的機率會 left

0:33:45.680,0:33:49.900
有 20% 的機率會 right
10% 的機率會 fire

0:33:49.900,0:33:57.760
看到同樣畫面的時候，根據機率，同一個 Actor 會採取不同的 action

0:33:57.760,0:34:01.460
這種 stochastic 的做法其實很多時候是會有好處的

0:34:01.460,0:34:03.660
比如說要玩猜拳

0:34:03.660,0:34:05.260
要玩猜拳的時候

0:34:05.260,0:34:11.540
如果 Actor 是 deterministic，可能就只會出石頭一直輸跟小叮噹一樣

0:34:11.540,0:34:18.060
所以有時候會需要 stochastic 這種 Policy

0:34:18.060,0:34:23.000
在底下的 lecture 裡面都假設 Actor 是 stochastic 的

0:34:24.860,0:34:32.960
用 Neural Network 來當 Actor 有甚麼好處

0:34:34.620,0:34:37.920
傳統的作法是直接存一個 table

0:34:37.920,0:34:42.100
這個 table 告訴我看到這個 observation 就採取這個 action

0:34:42.100,0:34:44.840
看到另外一個 observation 就採取另外一個 action

0:34:44.860,0:34:50.420
但這種作法要玩電玩是不行的

0:34:50.420,0:34:55.240
因為電玩的 input 是 pixel，要窮舉所有可能 pixel 是沒有辦法做到的

0:34:55.240,0:35:00.940
所以一定要用 Neural Network 才能夠讓 machine 把電玩玩好

0:35:00.940,0:35:05.480
用 Neural Network 的好處就是 Neural Network 可以舉一反三

0:35:05.480,0:35:09.300
就算有些畫面完全沒有看過

0:35:09.300,0:35:10.800
machine 從來沒有看過

0:35:13.340,0:35:17.460
因為 Neural Network 的特性
給他 input 一個東西總是會有 output

0:35:17.460,0:35:23.020
就算是他沒有看過的東西
他也有可能得到一個合理的結果

0:35:23.020,0:35:27.680
用 Neural Network 的好處是他比較 generalize

0:35:30.220,0:35:34.680
再來第二個步驟就是要決定一個 function 的好壞

0:35:34.680,0:35:39.280
也就是要決定一個 Actor 的好壞

0:35:39.280,0:35:42.760
在 Supervised Learning 怎麼決定 function 的好壞

0:35:42.760,0:35:48.360
假設給一個 Neural Network 
他的參數假設已經知道就是 theta

0:35:48.360,0:35:53.040
有一堆 training example
假設在做 image classification

0:35:53.040,0:35:56.780
就把 image 丟進去看 output 跟 target 像不像

0:35:56.780,0:35:59.660
如果越像代表 function 越好

0:35:59.660,0:36:02.600
會定義一個東西叫做 Loss

0:36:02.600,0:36:05.880
算每一個 example 的 Loss 合起來就是 Total Loss

0:36:05.880,0:36:09.620
需要找一個參數去 minimize 這個 Total Loss

0:36:09.620,0:36:16.900
其實在 Reinforcement Learning 裡面，一個 Actor 的好壞的定義其實是非常類似的

0:36:16.900,0:36:18.280
怎麼樣類似法

0:36:18.280,0:36:23.300
假設有一個 Actor，Actor 就是一個 Neural Network

0:36:23.300,0:36:27.700
這個 Neural Network 假設他的參數是 theta

0:36:27.700,0:36:32.040
一個 Actor 會用 pi 下標 theta 來表示它

0:36:32.040,0:36:35.060
一個 Actor 是一個 function 他的 input 就是一個 s

0:36:35.060,0:36:40.040
這個 s 就是 machine 看到的、Actor 看到的 observation

0:36:40.040,0:36:43.980
怎麼知道一個 Actor 表現好還是不好

0:36:43.980,0:36:48.100
就讓 Actor 實際的去玩一下這個遊戲

0:36:48.100,0:36:54.600
假設拿 pi( s )
拿參數是 theta 這個 Actor

0:36:54.760,0:36:59.400
實際去玩一個遊戲
他就玩了一個 episode

0:36:59.400,0:37:06.360
他說他看到 s1、take action a1、得到 r1
再看到 s2、take action a2、得到 r2 等等等

0:37:06.360,0:37:11.300
最後就結束了
這個時候玩完遊戲以後

0:37:11.300,0:37:16.460
他得到的 Total Reward 可以寫成 Rθ

0:37:16.460,0:37:20.720
這個 Rθ 就是 r1 + r2 一直加到 rT

0:37:20.720,0:37:28.540
把所有在每一個 step 得到的 reward合起來就是在這一個 episode 裡面得到的 Total Reward

0:37:28.540,0:37:34.120
而 episode 裡面的 Total Reward 才是我們要 maximize 的對象

0:37:34.120,0:37:37.080
我們不是要去 maximize 每一個 step 的 reward

0:37:37.080,0:37:42.840
我們是要去 maximize 整個遊戲玩完會得到的 Total Reward

0:37:43.860,0:37:48.560
但是就算拿同一個 Actor 來玩這個遊戲

0:37:48.560,0:37:53.340
每次玩的時候 Rθ 其實都會是不一樣

0:37:53.340,0:37:56.980
為甚麼？因為兩個原因

0:37:56.980,0:38:04.900
首先 Actor 如果是 stochastic 看到同樣的場景
也會採取不同的 action

0:38:04.900,0:38:07.840
就算是同一個 Actor、同一組參數

0:38:07.840,0:38:11.140
每次玩的時候得到的 Rθ 也會是不一樣

0:38:11.140,0:38:13.700
再來遊戲本身也有隨機性

0:38:13.800,0:38:18.520
就算採取同樣的 action，看到的 observation 每一次也可能都不一樣

0:38:18.660,0:38:20.560
所以遊戲本身也有隨機性

0:38:20.560,0:38:24.520
所以 Rθ 是一個 random variable

0:38:24.780,0:38:30.920
所以我們希望做的事情不是去 maximize 某一次玩遊戲得到的 Rθ

0:38:30.920,0:38:35.700
希望去 maximize 的其實是 Rθ 的期望值

0:38:35.700,0:38:41.540
拿同一個 Actor 玩了千百次遊戲以後

0:38:41.540,0:38:45.940
每次 Rθ 都不一樣但這個 Rθ 的期望值是多少

0:38:45.940,0:38:49.200
這邊用 Rθ bar 來表示它的期望值

0:38:49.200,0:38:53.680
希望這個期望值越大越好

0:38:55.260,0:39:01.460
這個期望值就衡量了某一個 Actor 的好壞

0:39:01.460,0:39:06.740
好的 Actor 他的期望值就應該要比較大

0:39:07.340,0:39:13.720
這個期望值實際上要怎麼計算出來

0:39:13.720,0:39:16.960
你可以這麼做

0:39:16.960,0:39:23.320
假設一場遊戲就是一個 trajectory τ
一場遊戲用 τ 來表示

0:39:23.320,0:39:33.120
τ 是一個 sequence 裡面包含了 state、包含 observation，看到這個 observation 以後 take 的 action

0:39:33.120,0:39:39.980
還有得到的 reward，還有新的 observation、take 的 action、得到的 reward 等等，他是一個 sequence

0:39:41.060,0:39:46.580
R(τ) 代表這個 trajectory 在這場遊戲最後得到的 Total Reward

0:39:46.580,0:39:52.320
把所有的小 r summation 起來就是 total 的 reward

0:39:52.540,0:39:57.980
當我們用某一個 Actor 去玩這個遊戲的時候

0:39:57.980,0:40:02.620
每一個 τ 都會有一個出現的機率

0:40:02.800,0:40:06.860
這個大家可以想像嗎

0:40:06.960,0:40:17.660
就是 τ 代表某一種可能的從遊戲開始到結束的過程

0:40:17.660,0:40:23.140
他代表某一種過程
這個過程有千千百百種

0:40:23.220,0:40:28.700
但是當你選擇了一個 Actor 去玩這個遊戲的時候

0:40:28.700,0:40:32.640
你可能只會看到某一些過程

0:40:32.640,0:40:36.680
某一些過程特別容易出現
某一些過程比較不容易出現

0:40:36.680,0:40:41.240
比如說現在 Actor 是一個很智障的 Actor

0:40:41.240,0:40:44.440
他看到敵人的子彈就要湊上去被自殺

0:40:44.440,0:40:51.700
你看到的每一個 τ 都是你自己控制的太空船挪一下
然後就去自殺了

0:40:51.700,0:40:57.760
當你選擇 Actor 的時候就會有一些 τ 特別容易出現

0:40:57.760,0:41:00.740
只有某一些遊戲的過程特別容易出現

0:41:00.740,0:41:06.500
每一個遊戲出現的過程可以用機率來描述他

0:41:06.500,0:41:09.560
這邊寫一個 P( τ | θ )

0:41:09.560,0:41:15.740
就是當 Actor 的參數是 θ 的時候
τ 這個過程出現的機率

0:41:17.180,0:41:19.420
如果可以接受這樣子的話

0:41:19.420,0:41:26.080
那 Rθ 的期望值，Rθ bar 就寫成

0:41:26.080,0:41:31.400
summation over 所有可能的 τ
所有可能的遊戲進行的過程

0:41:31.400,0:41:36.520
當然這個非常非常的多
尤其如果又是玩電玩

0:41:36.520,0:41:38.600
他是連續的

0:41:38.600,0:41:42.680
他有非常多的可能
這個 τ 是難以窮舉

0:41:42.680,0:41:44.900
現在假設可以窮舉他

0:41:44.900,0:41:48.400
每一個 τ 都有一個機率用 P( τ | θ )

0:41:48.400,0:41:51.240
每一個 τ 都有一個 reward R(τ)

0:41:51.240,0:41:55.600
把這兩個乘起來
再 summation over 所有遊戲可能的 τ 的話

0:41:55.600,0:42:05.100
那就得到了這個 Actor 他期望的這個 reward

0:42:05.100,0:42:12.060
實際上要窮舉所有的 τ 是不可能的
所以怎麼做

0:42:12.060,0:42:17.220
讓 Actor 去玩這個遊戲玩 N 場

0:42:17.220,0:42:20.980
得到 τ1、τ2 到 τN

0:42:20.980,0:42:24.860
這 N 場就好像是 N 筆 training data 這樣子

0:42:27.960,0:42:37.460
玩 N 場這個遊戲就好像是從 P( τ | θ ) sample 出 N 個 τ

0:42:37.700,0:42:40.900
假設某一個 τ 他的機率特別大

0:42:40.900,0:42:46.380
他就特別容易在 N 次 sample 裡面被 sample 出來

0:42:46.380,0:42:51.200
sample 出來的 τ 應該是跟機率成正比的

0:42:51.260,0:42:54.120
當用這個 Actor 玩 N 場遊戲的時候

0:42:54.120,0:42:59.920
就好像是從 P( τ | θ ) 這個機率裡面去做 N 次 sample 一樣

0:42:59.920,0:43:02.620
最後得到的結果是甚麼

0:43:05.640,0:43:15.440
最後就是把 N 個 τ 的 reward 都算出來

0:43:15.440,0:43:22.580
然後再平均起來
就可以拿這一項去近似這一項

0:43:22.580,0:43:26.440
對不對
這個大家應該沒有甚麼問題

0:43:28.420,0:43:34.160
接下來只要記得 summation over N 次 sample 做平均

0:43:34.160,0:43:36.460
對 N 次 sample 做平均

0:43:36.560,0:43:42.140
其實就可以近似從 θ sample τ 出來

0:43:42.460,0:43:44.680
再 summation over 所有的 τ

0:43:44.680,0:43:47.380
summation over 所有的 τ 乘上機率這件事情

0:43:47.380,0:43:51.880
跟 sample N 次這件事情是可以近似的

0:43:52.100,0:43:56.140
接下來就進入最後第三步

0:43:56.140,0:44:01.860
我們知道怎麼衡量一個 Actor

0:44:01.860,0:44:04.560
接下來就是要選一個最好的 Actor

0:44:04.560,0:44:08.360
怎麼選一個最好的 Actor
其實就是用 Gradient Descent

0:44:08.360,0:44:11.180
現在已經有我們的 Objective Function

0:44:11.180,0:44:18.580
我們已經找到目標了
目標就是要最大化這個 Rθ bar

0:44:18.580,0:44:20.760
找一個參數最大化 Rθ bar

0:44:20.760,0:44:23.580
Rθ bar 的式子也有了就寫在這邊

0:44:23.580,0:44:29.280
接下來就可以用 Gradient Ascent 的方法找一個 θ

0:44:29.280,0:44:31.460
讓 Rθ bar 的值最大

0:44:31.460,0:44:35.360
這邊不做 Gradient Descent ，因為 Gradient Descent 要去 minimize 一個東西用 Gradient Descent

0:44:35.360,0:44:38.400
maximize 一個東西用 Gradient Ascent

0:44:38.400,0:44:42.940
怎麼做
很簡單就先隨機的找一個初始的 θ0

0:44:42.940,0:44:53.160
隨機找一個初始的 Actor
然後計算在使用初始的 Actor 的情況下

0:44:53.160,0:44:58.900
你的參數對 Rθ bar 的微分

0:44:58.900,0:45:02.740
算出你的參數對 Rθ bar 的微分

0:45:02.740,0:45:05.860
再去 update 你的參數得到 θ1

0:45:05.860,0:45:09.340
接下來再算 θ1 對 Rθ bar 的微分

0:45:09.340,0:45:12.880
然後再 update θ1 得到 θ2

0:45:12.880,0:45:20.660
用這個 process 最後就可以得到一個可以讓 Rθ bar 最大的 Actor

0:45:20.660,0:45:26.080
當然會有 local optimum 種種問題，就跟做 Deep Learning 的時候是一樣的

0:45:28.960,0:45:33.480
所謂的 Rθ bar 的 gradient 是甚麼

0:45:33.480,0:45:37.820
假設 θ 裡面就是一堆參數，有一堆 weight 有一堆 bias

0:45:37.820,0:45:44.800
就是把所有的 weight、所有的 bias 都對 Rθ bar 做偏微分，把他通通串起來變成個 vector

0:45:44.800,0:45:48.020
就是這個 gradient

0:45:48.020,0:45:53.220
接下來就是實際來運算一下

0:45:53.220,0:45:58.360
如果要計算 Rθ bar 的 gradient

0:45:58.360,0:46:03.960
那 Rθ bar = summation over 所有的 τ 
R( τ ) * P( τ | θ )

0:46:04.080,0:46:09.880
這個 R(τ) 跟 θ 是沒任何關係的

0:46:09.960,0:46:13.760
只有 P( τ | θ ) 跟 θ 才是有關係的

0:46:13.760,0:46:20.400
所以做 gradient 的時候只需要對 P( τ | θ ) 做 gradient 就好

0:46:20.640,0:46:23.560
R(τ) 不需要對 θ 做 gradient

0:46:23.560,0:46:28.820
所以 R(τ) 就算是不可微的

0:46:28.820,0:46:32.140
也沒差因為本來就沒有要微分他

0:46:32.380,0:46:36.400
就算 R(τ) 他是個黑盒子
不知道他的式子

0:46:36.400,0:46:44.440
只知道把 τ 帶進去，R 的 output 會是甚麼也無所謂

0:46:44.440,0:46:51.220
也能夠做
因為我們在這邊完全不需要知道

0:47:01.590,0:47:06.300
這邊就算 R(τ) 不可微

0:47:06.480,0:47:11.280
或者是不知道他的 function 也沒差
因為不需要對它做微分

0:47:11.280,0:47:14.680
根本就不需要知道他長甚麼樣子

0:47:14.680,0:47:19.920
我們只需要知道把 τ 放進去的時候他 output 的值會是多少就行了

0:47:19.920,0:47:23.040
他可以完全徹頭徹尾就是個黑盒子

0:47:23.040,0:47:26.840
實際上對我們來說 R(τ) 也確實徹頭徹尾是個黑盒子

0:47:26.840,0:47:29.760
因為 R(τ) 是取決於

0:47:29.760,0:47:33.580
我們會得到多少 reward
那個 reward 是環境給我們的

0:47:33.580,0:47:36.540
所以我們通常對環境是沒有理解

0:47:36.540,0:47:40.020
比如說在玩 Atari 的遊戲裡面

0:47:40.660,0:47:45.580
reward 是 Atari 的那個程式給我們的

0:47:45.660,0:47:48.360
如果程式是有一些隨機的東西的話

0:47:48.360,0:47:51.160
會根本搞不清楚程式的內容是甚麼

0:47:51.160,0:47:53.480
其實就根本不知道 R(τ) 是長甚麼樣子

0:47:53.480,0:47:55.740
不過反正你不需要知道他長甚麼樣子

0:47:57.580,0:47:59.980
接下來怎麼做

0:47:59.980,0:48:08.860
接下來要做一件事情
做這件事情是為了要讓 P( τ | θ ) 出現

0:48:09.040,0:48:15.120
把 P( τ | θ ) 放在分子的地方也放在分母的地方

0:48:15.120,0:48:17.380
等於甚麼事都沒有做

0:48:17.420,0:48:24.360
接下來這一項會等於這一項

0:48:24.360,0:48:27.500
為甚麼

0:48:28.840,0:48:36.460
dlog(f(x)) / dx = (1 / f(x)) * (df(x) / dx)

0:48:36.460,0:48:40.240
所以對 log  P( τ | θ ) 做 gradient

0:48:40.240,0:48:45.300
等於對 P( τ | θ ) 做 gradient 再除以 P( τ | θ )

0:48:45.300,0:48:49.100
所以這一項就是這一項

0:48:50.660,0:48:59.020
如果你看到 summation over 所有的 τ 再乘上 P( τ | θ ) 的話

0:48:59.160,0:49:04.080
當看到紅色這個框框的時候可以把它換成 sampling

0:49:04.180,0:49:11.360
所以這件事情可以換成拿 θ 玩 N 次遊戲得到 τ1 到 τN

0:49:11.360,0:49:15.740
對 τ1 到 τN 都算出他的 R(τ)

0:49:15.740,0:49:20.540
再 summation over 所有的 sample 出來的結果再取平均

0:49:20.540,0:49:25.880
接下來的問題是怎麼計算這一項

0:49:25.880,0:49:30.500
怎麼計算 log * P( τ | θ ) 的 gradient

0:49:30.500,0:49:36.780
這一項也不難算可以很快地帶過去

0:49:36.780,0:49:40.080
要算這一項要知道 P( τ | θ )

0:49:40.080,0:49:41.740
怎麼算 P( τ | θ )

0:49:41.740,0:49:51.060
首先要知道 P( τ | θ ) = p(s1)  也就是遊戲開始的畫面的出現的機率

0:49:51.060,0:49:54.920
像 Space Invader 我記得每一次開始的畫面都是一樣的

0:49:54.920,0:50:00.760
所以 p(s1) 就是只有某一個畫面的機率是 1
其他都是 0

0:50:00.760,0:50:04.180
有一些遊戲的畫面每次的起始畫面是不一樣的

0:50:04.180,0:50:06.560
這邊需要有個機率

0:50:07.520,0:50:11.600
接下來根據 θ

0:50:11.600,0:50:15.800
在 s1 會有某一個機率採取 a1

0:50:15.800,0:50:22.820
接下來根據在 s1 採取 a1 這件事情

0:50:23.000,0:50:29.600
會得到 r1 然後跳到 s2
這中間是有個機率的，取決於那個遊戲

0:50:29.600,0:50:34.560
接下來在 s2 採取 a2 這個機率取決於你的 model θ

0:50:34.560,0:50:37.220
接下來看到 s2 a2 得到 r2 s3

0:50:37.220,0:50:39.080
這也是取決於那個遊戲

0:50:39.160,0:50:44.000
所以整個畫起來就是這個樣子

0:50:44.400,0:50:51.660
其中某些項跟 agent、Actor 是沒有關係的

0:50:51.660,0:50:57.060
只有畫紅色底線這一項跟 Actor 是有關係的

0:51:00.940,0:51:05.380
接下來就取 log

0:51:05.420,0:51:10.960
取 log 就只是相乘變相加而已

0:51:11.740,0:51:15.560
接下來可以對 θ 做 gradient

0:51:15.560,0:51:17.500
跟 θ 無關的項

0:51:17.500,0:51:24.060
跟 agent 無關的項只取決於遊戲的主機

0:51:24.060,0:51:27.460
遊戲的部分那一項就可以直接被刪掉

0:51:27.460,0:51:31.400
這兩項都跟 gradient 是無關的

0:51:31.400,0:51:34.200
這一項可以刪掉，這一項可以刪掉

0:51:34.200,0:51:38.200
只剩下這個部分

0:51:38.200,0:51:42.100
所以最後算出來的結果就是

0:51:42.100,0:51:44.920
Rθ bar 的 gradient

0:51:45.180,0:51:50.040
它可以被 approximate 甚麼樣子

0:51:55.120,0:52:04.300
sample 出 N 個 τ，每一個 τ 都算出他的 reward 再乘上每一個 τ 的出現的機率的 log 的 gradient

0:52:04.360,0:52:10.740
出現的機率的 log 的 gradient 又可以把他算成是

0:52:10.740,0:52:16.380
summation over 在這個 τ 裡面
所有採取過的 action

0:52:16.380,0:52:19.400
他的機率取 log 的 gradient

0:52:19.400,0:52:22.360
這一項就等於這一項我只是把他置換一下

0:52:22.360,0:52:27.480
把這個 summation 移出去
把這個 R 乘進來

0:52:27.480,0:52:31.780
可以寫成這樣的式子

0:52:31.780,0:52:34.440
這個式子告訴我們甚麼

0:52:34.620,0:52:40.520
這個式子告訴我們，現在要做的事情
假設在 data 裡面

0:52:40.520,0:52:43.960
在 s 上標 n 下標 t 這個 state

0:52:43.960,0:52:50.180
我們曾經採取了 a 上標 n 下標 t 這個 action

0:52:50.180,0:52:54.580
就計算這件事情根據我們的 model 現在發生的機率

0:52:54.800,0:52:59.560
然後把它取 log 然後計算它的 gradient

0:52:59.560,0:53:04.000
這項 gradient 前面會乘上一項
乘上這一項是

0:53:04.120,0:53:09.960
這一個 trajectory 在那一次玩遊戲裡面

0:53:10.040,0:53:17.040
在看到這個 s 產生這個 a 的那一次遊戲裡面總共得到的 Total Reward

0:53:17.040,0:53:23.840
這整個式子其實是非常直覺的

0:53:23.840,0:53:27.820
用這一項去 update model 其實是非常直覺的

0:53:27.820,0:53:33.800
因為它的意思是說
假設某一次玩遊戲的過程中

0:53:33.800,0:53:35.760
在 τn 這次玩遊戲的過程中

0:53:35.820,0:53:42.720
我們在 s 上標 n 下標 t 採取 action a 上標 n 下標 t

0:53:42.720,0:53:48.840
而最後導致的結果是整個遊戲的 R(τ) 是正的

0:53:48.840,0:53:56.000
得到一個正的  reward
就會希望說這個機率是越大越好

0:53:56.000,0:54:01.260
我在某一次玩遊戲的時候
我在看到某一個 observation 的時候

0:54:01.260,0:54:02.740
採取某一個 action

0:54:02.860,0:54:05.740
而最後整個遊戲得到好的結果

0:54:05.740,0:54:10.620
就要調整我們的參數
讓在這個 observation

0:54:10.620,0:54:14.680
採取那個 action 的機率變大

0:54:14.680,0:54:18.980
反之如果在玩遊戲的過程發現

0:54:18.980,0:54:25.560
在某一個 state 採取某一個 action
結果發現得到的 reward 居然是負的

0:54:25.560,0:54:32.260
在之後看到同樣的 state 的時候、同樣的 observation 的時候

0:54:32.260,0:54:39.240
我們就希望採取會讓我們看到 negative reward 的那個 action 它的機率變小

0:54:39.240,0:54:45.520
這整個式子是非常直覺的

0:54:45.520,0:54:49.380
這邊要注意的事情是

0:54:49.380,0:54:54.640
這一項是在某一個時間點 t

0:54:55.040,0:54:57.940
的 observation 採取的某一個 action

0:54:57.940,0:55:02.840
但是我們必須要把它乘上整個 trajectory 的 reward

0:55:02.840,0:55:08.000
而不是採取那個 action 以後所產生的 reward

0:55:08.000,0:55:12.100
這件事情也是非常直覺
直覺想就應該這麼做

0:55:12.100,0:55:16.140
假設現在不是考慮整個 trajectory 的 reward

0:55:16.140,0:55:24.620
而是考慮採取 action a 上標 n 下標 t 以後得到的 reward r 上標 n 下標 t 的話

0:55:24.680,0:55:26.300
那會變成說

0:55:26.300,0:55:30.700
只有 fire 會得到 reward

0:55:30.700,0:55:35.720
其他的 action 只要採取 left 或 right 的移動
得到的 reward 都是 0

0:55:35.720,0:55:41.900
所以 machine 就永遠不會想要讓 left 跟 right 產生的機率增加，它只會讓 fire 機率增加

0:55:41.900,0:55:45.720
所以 learn 出來的 agent 就只會一直在原地開火

0:55:46.640,0:55:51.220
這個式子其實是很直覺的

0:55:52.400,0:56:00.440
這邊還有一個問題就是為甚麼要取 log

0:56:02.260,0:56:11.480
這件事情也是有辦法解釋的

0:56:11.480,0:56:22.660
你看這一項它其實就是對 p 的微分再除掉 p 的這個機率

0:56:22.660,0:56:25.980
它是微分再除掉機率

0:56:25.980,0:56:29.480
你可能會想說加這一項多不自然

0:56:29.480,0:56:33.980
把這一項就直接換成分子這一項不是感覺很好嗎

0:56:33.980,0:56:38.280
為甚麼下面還要除一個 p( a | s ) 的機率

0:56:38.280,0:56:42.940
你想想看這件事情是很有道理

0:56:42.940,0:56:50.340
假設現在讓 machine 去玩 N 次遊戲

0:56:50.340,0:56:56.680
那某一個 state 在第 13 次、第 15 次、第 17 次、第 33 次的遊戲裡面

0:56:56.680,0:57:01.200
看到了同一個 observation

0:57:01.200,0:57:05.280
因為 Actor 其實是 stochastic

0:57:05.280,0:57:09.220
所以它有個機率，所以看到同樣的 s，不見得採取同樣 action

0:57:09.220,0:57:12.940
所以假設在第 13 個 trajectory

0:57:12.940,0:57:16.080
它採取 action a，在第 17 個它採取 b

0:57:16.200,0:57:21.300
在 15 個採取 b
在 33 也採取 b

0:57:21.300,0:57:26.440
然後最後 τ 13 的這個 trajectory 得到的 reward 比較大是 2

0:57:26.500,0:57:28.760
另外三次得到的 reward 比較小

0:57:30.640,0:57:33.920
但實際上在做 update 的時候

0:57:33.920,0:57:38.220
它會偏好那些出現次數比較多的 action

0:57:38.220,0:57:40.800
就算那些 action 並沒有真的比較好

0:57:40.800,0:57:48.600
對不對，因為是 summation over 所有 sample 到的結果

0:57:48.600,0:57:51.220
如果 take action b 這件事情

0:57:51.220,0:57:55.980
出現的次數比較多，就算它得到的 reward 沒有比較大

0:57:56.140,0:57:59.320
machine 把這件事情的機率調高

0:57:59.320,0:58:05.380
也可以增加最後這一項的結果

0:58:05.380,0:58:07.820
雖然這個 action 感覺比較好

0:58:07.820,0:58:11.820
但是因為它很罕見，所以調高這個 action 的機率

0:58:12.020,0:58:20.780
最後也不會對你要 maximize 的對象 Objective 的影響也是比較小的

0:58:20.780,0:58:26.880
machine 就會變成不想要 maximize action a 出現的機率
轉而 maximize action b 出現的機率

0:58:26.880,0:58:29.860
這就是為甚麼這邊需要除掉一個機率

0:58:29.860,0:58:32.740
除掉這個機率的好處就是做一個 normalization

0:58:32.740,0:58:36.040
如果有某一個 action 它本來出現的機率就比較高

0:58:36.040,0:58:38.560
它除掉的值就比較大

0:58:38.560,0:58:41.600
讓它除掉一個比較大的值

0:58:41.600,0:58:44.240
machine 最後在 optimize 的時候

0:58:44.240,0:58:48.120
就不會偏好那些機率出現比較高的 action

0:58:58.860,0:59:03.060
這個聽起來都很 ok，但是這邊有一個問題

0:59:03.060,0:59:04.340
什麼樣的問題

0:59:04.340,0:59:08.420
假設 R(τ) 永遠是正的

0:59:08.420,0:59:10.800
會發生甚麼事呢

0:59:10.800,0:59:13.540
因為像玩 Space Invader

0:59:13.540,0:59:17.400
得到的 reward 都是正的，殺了外星人就得到正的分數

0:59:17.400,0:59:20.400
最糟就是殺不到外星人得到分數是 0

0:59:20.400,0:59:24.300
所以這個 R(τ) 永遠都是正的

0:59:24.300,0:59:27.180
在理想的狀況下

0:59:27.420,0:59:31.360
這件事情不會構成問題
為甚麼

0:59:31.360,0:59:35.100
假設在某一個 state 可以採取三個 action a b c

0:59:35.100,0:59:38.800
那這三個 action 採取的結果

0:59:38.800,0:59:42.580
我們得到的 R(τ) 都是正的

0:59:42.580,0:59:45.040
這個正有大有小

0:59:45.040,0:59:48.940
假設 a 跟 c 的 R(τ) 比較大

0:59:48.940,0:59:52.140
b 的 R(τ) 比較小

0:59:52.140,0:59:55.160
經過 update 以後

0:59:55.160,0:59:58.260
還是會讓 b 出現的機率變小

0:59:58.260,1:00:00.160
a c 出現的機率變大

1:00:00.260,1:00:04.620
因為這邊是個機率，所以它會做 normalization

1:00:04.620,1:00:12.100
或是你為了要讓它機率裡的最後 network 的 output
它是 soft-max layer

1:00:12.100,1:00:15.720
就算是在算 gradient 的時候

1:00:15.720,1:00:18.000
你想讓這三個的機率會增加

1:00:18.000,1:00:20.460
但是增加量比較少的那個

1:00:20.560,1:00:23.400
最後它的機率其實是會減少

1:00:23.400,1:00:26.880
聽起來不太成一個問題

1:00:26.920,1:00:30.580
但實作的時候，我們做的是 sampling

1:00:30.580,1:00:35.840
所以某一個 state 可以採取 a b c 三個 action

1:00:35.840,1:00:39.980
但是有可能只 sample 到 b 這個 action
c 這個 action

1:00:39.980,1:00:42.460
而 a 這個 action 就沒 sample 到

1:00:42.560,1:00:44.240
很有可能 sample 就幾次而已

1:00:44.240,1:00:52.540
所以可能 a 這個 action machine 從來沒玩過它、沒試過它，不知道它的 R(τ) 到底有多大

1:00:52.540,1:00:58.920
這個時候就遇到問題了
因為 b 跟 c 機率都會增加

1:00:58.920,1:01:02.800
a 沒 sample 到，沒 sample 到機率就自動減少

1:01:02.800,1:01:06.600
被 sample 到的在做玩 gradient 後的機率自動就會增加

1:01:06.600,1:01:10.700
這樣就變成問題了

1:01:10.700,1:01:14.220
所以怎麼辦
這邊有一個很簡單的想法就是

1:01:14.220,1:01:18.300
我們希望 R(τ) 有正有負

1:01:18.440,1:01:21.260
不要都是正的
怎麼避免都是正的呢

1:01:21.260,1:01:24.980
要把 R(τ) 減掉一個 bias

1:01:24.980,1:01:30.520
這個 bias 其實要自己設計

1:01:30.520,1:01:33.860
到底應該要放甚麼值
設計一下這個 bias

1:01:33.940,1:01:38.940
讓你的 R(τ) 都是正的
減掉一個正的 bias 讓它有正有負

1:01:38.940,1:01:43.080
如果你的 trajectory 某一個 τn 它是特別好的

1:01:43.160,1:01:49.280
這個 b 叫做 baseline，它好過 baseline 才把那個 action 的機率增加

1:01:49.280,1:01:52.380
小於 baseline 把它 action 的機率減小

1:01:52.380,1:01:57.620
這樣子就不會造成某一個 action 沒被 sample 到它的機率就會變小

1:01:57.620,1:02:01.080
因為快要下課了所以不見得要講玩 Critic

1:02:01.080,1:02:03.100
Critic 是甚麼
Critic 就是

1:02:03.100,1:02:05.300
learn 一個 network 它不做事

1:02:05.300,1:02:08.700
其實也可以從 Critic 得到一個 Actor

1:02:08.700,1:02:13.020
這個東西就是 Q Learning
但今天就沒辦法講 Q Learning

1:02:13.020,1:02:17.360
Critic 就是這樣子
learn 一個 function

1:02:17.360,1:02:26.120
這個 function 可以告訴你
現在看到某一個 observation 的時候

1:02:26.120,1:02:31.040
這個 observation 有多好這樣子

1:02:31.040,1:02:34.820
比如說看到這樣子的 observation

1:02:34.820,1:02:39.020
把它丟到 Critic 裡面去
它可能會 output 一個很大的正值

1:02:39.020,1:02:42.460
因為還有很多 alien 可以殺
所以會得到很高的分數

1:02:42.460,1:02:48.660
看到這個狀況可能較會得到相對比較少的值

1:02:48.660,1:02:52.160
因為 alien 變得比較少而且屏障消失了

1:02:52.160,1:02:57.400
這是屏障，屏障消失了，所以你可能很快就會死，分數就比較少

1:02:57.580,1:03:03.420
總之 Actor 跟 Critic 可以合在一起 train

1:03:03.420,1:03:09.580
合在一起 train 的好處就是簡單講這樣比較強這樣子

1:03:09.580,1:03:14.740
這個不一定要在這堂課解釋完

1:03:14.740,1:03:19.700
永遠可以留到下學期 MLDS 再講這樣子

1:03:19.700,1:03:25.640
最後這後面還有很多

1:03:25.640,1:03:29.620
最後只想 demo 一下到底如果用 Actor Critic 這種方法

1:03:29.620,1:03:31.900
可以做到什麼樣的地步

1:03:32.100,1:03:39.840
現在用 Actor Critic 這些東西大家都在玩 3D 遊戲了

1:03:39.840,1:03:44.400
這個就是 machine 自己 learn 的
然後它會走它沒有看過的迷宮

1:03:44.400,1:03:48.820
它會知道要去吃綠色的平果

1:03:48.820,1:03:53.900
machine 看到的，雖然這是個 3D 遊戲，它看到的就是 pixel，跟我們人一樣

1:03:54.980,1:04:01.500
所以用 A3C 可以硬玩這種遊戲
這個是硬玩一發賽車遊戲

1:04:01.500,1:04:04.480
硬開個賽車

1:04:06.240,1:04:09.340
它看到的就是 pixel 跟人一樣

1:04:11.780,1:04:16.620
我記得它的 reward 是速度，速度越快 reward 就會越高

1:04:16.620,1:04:20.980
所以它會拼命想要加速，那撞到東西就會減速，它會避免撞到東西

1:04:20.980,1:04:24.740
比如說前面有些車然後它就避開

1:04:24.740,1:04:29.620
所以最後的結果是滿驚人的

1:04:29.620,1:04:32.340
這學期上課就上到這邊

1:04:32.340,1:04:37.880
我本來想要講一個感性的結論
但是沒有甚麼感性的結論

1:04:37.880,1:04:40.900
已經快下課了不太好講甚麼感性的結論

1:04:40.900,1:04:42.900
那我想要講一下

1:04:42.960,1:04:46.780
其實這學期有個東西沒有教，知道是甚麼嗎

1:04:46.780,1:04:50.740
是 Machine Learning 裡面比較偏向統計理論的部分

1:04:50.740,1:04:52.840
比如說 VC-dimension

1:04:52.840,1:04:58.700
但是電機系未來會有其他課教 Machine Learning 裡面比較統計的部分

1:04:58.700,1:05:03.680
就我所知王老師有要開統計與機器學習

1:05:03.760,1:05:08.100
但是它會講比較偏向統計理論的東西

1:05:08.100,1:05:11.860
所以這學期內容是沒有有關統計理論的部分

1:05:11.860,1:05:15.780
然後如果覺得我教的太簡單了

1:05:15.780,1:05:17.720
那你就可以去聽王老師的課

1:05:17.720,1:05:21.220
如果你覺得我教的太難了，聽不懂

1:05:21.220,1:05:28.020
沒有關係，余老師要開機器學習導論是給大學生的

1:05:28.100,1:05:31.700
所以以後可以先修機器學習導論

1:05:31.700,1:05:34.520
以後電機系會有很多機器學習的課

1:05:34.520,1:05:40.940
我上課的內容跟軒田的機器學習的基石還有技法

1:05:40.940,1:05:44.360
其實我是盡量有錯開的

1:05:44.360,1:05:46.780
就算你是修過基石和技法

1:05:46.780,1:05:49.800
相信你在這門課裡面應該也是有聽到不少東西

1:05:49.800,1:05:52.460
或者是你之後再去聽基石跟技法

1:05:52.460,1:05:59.280
你會發現我講的東西跟軒田講的東西，其實是我盡量做到沒有太多東西重複

1:05:59.280,1:06:05.100
這學期上課就上課這邊，謝謝大家

1:06:05.200,1:06:21.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
