0:00:00.000,0:00:02.960
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:02.960,0:00:05.200
deep learning 現在非常的熱門

0:00:05.200,0:00:06.980
所以，它可以用在甚麼地方

0:00:06.980,0:00:09.700
我覺得真的還不需要多講

0:00:09.700,0:00:11.940
我覺得大家搞不好都知道得比我更多

0:00:11.940,0:00:14.660
我相信如果你隨便用 deep learning 當作關鍵字

0:00:14.660,0:00:15.960
胡亂 google 一下

0:00:15.960,0:00:21.420
你就可以找到一大堆的、exciting 的 result

0:00:21.760,0:00:23.920
所以，我們就直接用這個圖呢

0:00:23.920,0:00:26.180
來簡單地 summarize 一下這個趨勢

0:00:26.180,0:00:29.300
這個圖呢，是 google 的 Jeff Dean

0:00:29.300,0:00:33.640
它在 sigmoid 的一個 keynote speech 的一張投影片

0:00:33.640,0:00:35.980
那這個圖想要表達的事情是這樣

0:00:35.980,0:00:37.860
橫軸代表時間

0:00:37.860,0:00:42.020
從 2012 的 Q1 到 2016

0:00:42.020,0:00:45.180
縱軸，代表說在 google 內部

0:00:45.180,0:00:47.620
有用到 deep learning 的 project 的數目

0:00:47.620,0:00:49.740
那可以發現說，這個趨勢

0:00:49.740,0:00:53.220
是從幾乎 0 到超過 2000

0:00:53.220,0:00:56.540
所以，這個使用 deep learning 的 project 數目呢

0:00:56.540,0:00:59.440
是指數成長的

0:00:59.440,0:01:01.840
那如果你看它的應用的話

0:01:01.840,0:01:05.560
它有各種不同的應用，涵蓋幾乎你可以想像的領域

0:01:05.560,0:01:08.160
比如說，Android, Apps, drug discovery

0:01:08.160,0:01:11.580
Gmail, Image understanding, 
Natural language understanding

0:01:11.580,0:01:14.160
Speech, 各種不同的應用

0:01:14.160,0:01:17.280
通通有用到 deep learning

0:01:17.280,0:01:21.220
那 deep learning 可以做的應用實在是太多了

0:01:21.220,0:01:23.960
我們這邊就不要花時間來講這些東西

0:01:23.960,0:01:25.620
如果要講這些東西的話呢

0:01:25.620,0:01:27.980
再用兩、三堂課，其實也是講不完的

0:01:27.980,0:01:31.600
那這個隨便 google 就有的東西呢，我們就不要再講了

0:01:31.600,0:01:33.760
我們來稍微回顧一下

0:01:33.760,0:01:35.600
這個 deep learning 的歷史

0:01:35.600,0:01:40.340
在歷史上呢，它是有經過好幾次的沉沉浮浮

0:01:40.640,0:01:42.540
首先，在 1958 年

0:01:42.540,0:01:45.840
有一個技術叫做 Perceptron 被提出來

0:01:45.840,0:01:50.000
Perceptron 這個技術，它也是一個 linear 的 model

0:01:50.000,0:01:51.680
它非常非常像

0:01:51.680,0:01:56.560
我們在前一堂課講的 Logistic Regression

0:01:56.560,0:01:59.800
它只是沒有 sigmoid 的部分

0:01:59.800,0:02:01.640
但它還是 linear 的 model

0:02:01.640,0:02:04.620
那一開始，有人提出這個想法的時候

0:02:04.620,0:02:06.780
提出 Perceptron 這個想法的時候呢

0:02:06.780,0:02:09.340
大家非常非常的興奮

0:02:09.340,0:02:12.680
這個是 Frank Rosenblatt

0:02:12.680,0:02:17.340
在海軍的 project 裡面提出來的

0:02:17.340,0:02:19.660
他一開始提出來的時候，大家覺得非常非常興奮

0:02:19.660,0:02:22.280
那個時候要做 Perceptron 的運算呢

0:02:22.280,0:02:25.820
如果你看 Bishop 的教科書，裡面有一張的機器呢

0:02:25.820,0:02:28.420
看起來是房間那麼大的機器

0:02:28.420,0:02:33.660
那個時候，New York Times 據說還寫了一個報導說

0:02:33.660,0:02:36.220
從此以後，人工智慧就要產生了

0:02:36.220,0:02:39.320
電腦可以自己學習，就像現在這個樣子

0:02:39.320,0:02:42.320
可是後來呢

0:02:42.320,0:02:44.760
MIT 有一個

0:02:44.760,0:02:48.540
有人呢，就寫了一本教科書

0:02:48.540,0:02:51.840
這本教科書的名字就叫做 Perceptron

0:02:51.840,0:02:55.080
這本教科書裡面，他就指出了 linear 的 model

0:02:55.080,0:02:58.100
是有極限的，就像我們在上一張投影片講的

0:02:58.100,0:03:01.360
linear model 有很多事都辦不到

0:03:01.360,0:03:03.740
就像是我們剛才舉的那麼簡單的 example

0:03:03.740,0:03:05.600
它都辦不到

0:03:05.600,0:03:07.800
然後大家希望就破滅了

0:03:07.800,0:03:11.900
然後，當時在 1958 年，剛提出 Perceptron 的時候呢

0:03:11.900,0:03:14.680
有各種驚人的 application

0:03:14.680,0:03:17.060
就像現在 deep learning 一樣，有人就說

0:03:17.060,0:03:18.800
我用了 Perceptron

0:03:18.800,0:03:21.420
結果我可以分辨說，給我一張照片

0:03:21.420,0:03:25.380
那照片裡面有坦克，或是一般的卡車

0:03:25.380,0:03:28.820
那它可以正確地分辨說，那些照片裡面是坦克

0:03:28.820,0:03:30.160
那些照片裡面是卡車

0:03:30.160,0:03:33.200
就算那些坦克呢，被藏在叢林裡面

0:03:33.200,0:03:36.260
就算被樹木蓋住呢，也偵測得出來

0:03:36.260,0:03:39.180
那其他人就覺得說，太厲害了，這就是人工智慧

0:03:39.180,0:03:42.720
可是既然 Perceptron 有這樣的

0:03:42.720,0:03:45.780
後來大家就發現說 Perceptron 其實很多 limitation

0:03:45.780,0:03:51.600
那怎麼可能分辨卡車跟坦克，這麼複雜的 object 呢？

0:03:51.820,0:03:54.980
所以，有人就去把那個 data，再拿出來看了一下

0:03:54.980,0:03:58.220
就發現說，原來卡車跟坦克的照片

0:03:58.220,0:03:59.840
是在不同的日子所拍攝的

0:03:59.840,0:04:02.380
所以，一天是雨天，一天是晴天

0:04:02.380,0:04:04.700
所以，這個照片本身的亮度就不一樣

0:04:04.700,0:04:07.120
所以，Perceptron 它唯一抓到的東西

0:04:07.120,0:04:08.660
就只有亮度而已

0:04:10.400,0:04:14.220
所以大家就崩潰了，這個方法的名字就臭掉了

0:04:17.200,0:04:20.100
後來就有人想說，既然

0:04:20.100,0:04:25.140
一個 Perceptron 不行，我能不能連接很多個 Perceptron

0:04:25.140,0:04:27.160
就像我們剛才講的，Logistic Regression

0:04:27.160,0:04:30.400
都接在一起，它應該就很 powerful

0:04:30.400,0:04:33.920
這個就叫做 Multi-layer 的 Perceptron

0:04:33.920,0:04:37.140
事實上，1980 年代

0:04:37.140,0:04:40.720
Multi-layer 的 Perceptron 它的技術

0:04:40.720,0:04:43.040
基本上在 1980 年代的時候呢

0:04:43.040,0:04:44.860
就都開發得差不多了

0:04:44.860,0:04:47.120
那個時候已經開發完的技術

0:04:47.120,0:04:49.100
其實就跟今天的 deep learning

0:04:49.100,0:04:52.740
是沒有太 significant 的差別的

0:04:53.340,0:04:57.100
然後，有一個關鍵的技術是 1986 年的時候

0:04:57.100,0:05:00.140
Hinton propose 的 Backpropagation

0:05:00.140,0:05:02.440
那其實同時也有很多人 propose Backpropagation

0:05:02.440,0:05:05.600
只是大部分人把這個 credit 歸給 Hinton 的 paper

0:05:05.600,0:05:07.720
他那篇 paper 是比較著名的

0:05:07.720,0:05:10.740
但是，在這個時候遇到的問題就是

0:05:10.740,0:05:14.840
通常超過 3 個 layer 的 neural network

0:05:14.840,0:05:17.520
你就 train 不出好的結果

0:05:17.520,0:05:21.060
通常一個還可以，再多你就 train 不出好的結果了

0:05:21.060,0:05:23.200
那後來在 1989 年

0:05:23.200,0:05:26.060
有人就發現了一個理論

0:05:26.060,0:05:28.300
就是說，一個 hidden layer

0:05:28.300,0:05:29.760
其實就可以

0:05:29.760,0:05:32.860
model 任何可能的 function

0:05:32.860,0:05:35.940
了解嗎？只要一個 neural network 有一個 hidden layer

0:05:35.940,0:05:38.440
它就可以是任何的 function

0:05:38.440,0:05:42.100
它就已經夠強了，所以根本沒有必要

0:05:42.100,0:05:43.940
疊很多個 hidden layer

0:05:43.940,0:05:47.680
所以，這個 Multi-layer 的 Perceptron 的方法，又臭掉了

0:05:47.680,0:05:51.500
然後，大家就比較喜歡做 SVM

0:05:51.500,0:05:56.600
那這個方法就臭掉了，據說那一陣子呢

0:05:56.600,0:05:59.200
Multi-layer 的 Perceptron 這個方法

0:05:59.200,0:06:02.140
也有人叫它 neural network 這個方法

0:06:02.140,0:06:03.980
它就像是一個髒話一樣

0:06:03.980,0:06:07.020
寫在 paper 裡面，那個 paper 一定被 reject

0:06:10.760,0:06:12.980
後來呢

0:06:12.980,0:06:14.660
有人就想到一個突破的點

0:06:14.660,0:06:16.600
這個突破的點，關鍵的地方就是

0:06:16.600,0:06:21.080
把它改個名字，這個方法已經臭掉了

0:06:21.080,0:06:24.000
只好改一個名字，把它改成 deep learning

0:06:24.000,0:06:25.940
整個就潮起來了

0:06:25.940,0:06:33.480
所以我講說，改名字其實是有很大的力量的

0:06:33.480,0:06:35.140
大家現在都不想念博士

0:06:35.140,0:06:38.360
我們把博士改個名字，大家都想唸了

0:06:38.360,0:06:41.140
下次跟系主任建議一下

0:06:41.140,0:06:45.820
很多人覺得說，有一個關鍵的技術是

0:06:45.820,0:06:48.100
應該在 2006 年提的

0:06:48.100,0:06:51.380
用 Restricted Boltzmann Machine 做 initialization

0:06:51.380,0:06:54.720
很多人覺得說，這是個突破這樣

0:06:54.720,0:06:57.300
甚至有一陣子呢

0:06:57.300,0:07:00.640
大家的認知是，到底 deep learning 跟

0:07:00.640,0:07:03.420
1980 年代的 Multi-layer Perceptron

0:07:03.420,0:07:06.420
有何不同呢？它的不同之處在於

0:07:06.420,0:07:09.720
如果你有用 RBM 做 initialization

0:07:09.720,0:07:12.180
你在做 Gradient Descent 的時候，不是要找一個

0:07:12.180,0:07:16.020
初始的值嗎？如果你是用 RBM 找的，叫 deep learning

0:07:16.020,0:07:21.500
你沒有用 RBM 找的，是傳統的、
1980 年代的  Multi-layer Perceptron

0:07:21.500,0:07:24.640
後來呢，大家逐漸意識到說

0:07:24.640,0:07:27.320
因為 RBM 這個方法呢

0:07:27.320,0:07:30.020
非常的複雜，假設沒有

0:07:30.020,0:07:32.000
假設你是 machine learning 的初學者的話

0:07:32.000,0:07:34.160
你看這個 paper，我相信你是看不懂的

0:07:34.160,0:07:36.840
如果我們今天要講 RBM

0:07:36.840,0:07:39.600
要從現在開始

0:07:39.600,0:07:42.660
假設用我們上課那些知識，講到讓你聽懂

0:07:42.660,0:07:46.520
要再另外多講 3 周這樣子

0:07:46.520,0:07:49.920
它是有用到一些比較深的理論

0:07:49.920,0:07:52.420
大家覺得說，哇！這個這麼複雜

0:07:52.420,0:07:55.040
一定就是非常的 powerful 阿

0:07:55.040,0:07:57.280
而且它不是 Neural network base 的方法

0:07:57.280,0:07:58.600
它是 graphical model

0:07:58.600,0:08:01.360
它這個大家覺得說，這個這麼複雜，我們看都看不懂

0:08:01.360,0:08:02.800
這個一定是有用的

0:08:02.800,0:08:05.500
後來大家逐漸試來試去以後，就發現說

0:08:05.500,0:08:07.500
這招其實沒什麼用這樣子

0:08:07.500,0:08:11.340
你可以發現說，如果你讀 deep learning 的文獻

0:08:11.340,0:08:17.220
現在已經不太有人用 RBM 做 initialization 了

0:08:17.220,0:08:19.660
因為這一招帶給我們的幫助呢

0:08:19.660,0:08:21.740
並沒有說特別大

0:08:21.740,0:08:25.720
連 Hinton 自己都知道這點，
他在某篇 paper 裡面提過這件事情

0:08:25.720,0:08:28.960
只是大家可能都沒有注意到那篇 paper 就是了

0:08:29.740,0:08:34.680
所以說，但是他有個最強的地方

0:08:34.680,0:08:36.640
它最強的地方就是它讓大家

0:08:36.640,0:08:40.560
重新、再一次對這個 model 有了興趣

0:08:40.560,0:08:43.180
因為它很複雜，所以大家就會開始想要研究

0:08:43.180,0:08:46.300
deep learning 是什麼樣的東西，花很多力氣去研究

0:08:46.300,0:08:48.740
所以，其實我聽過有一個

0:08:48.740,0:08:51.680
google 的人對 RBM 的評論

0:08:51.680,0:08:54.780
他說這個方法就是石頭湯裡面的石頭

0:08:54.780,0:08:56.800
石頭湯的故事大家聽過嗎？

0:08:56.800,0:08:59.140
就有一個人說，我要煮一碗石頭湯

0:08:59.140,0:09:02.300
有一個士兵，他到一個村莊裡面借宿

0:09:02.300,0:09:06.140
然後，他說：我要煮一碗石頭湯

0:09:06.140,0:09:08.920
然後，大家都說：你要怎麼煮一碗石頭湯？

0:09:08.920,0:09:10.340
我用石頭就可以了

0:09:10.340,0:09:13.460
就用石頭煮一碗湯，如果再加點鹽就更好了

0:09:13.460,0:09:14.220
就加點鹽

0:09:14.220,0:09:16.240
再加點米就更好了，再加點米

0:09:16.240,0:09:18.320
再加點菜就更好了，再加點菜

0:09:18.320,0:09:19.240
然後，就煮了一鍋湯

0:09:19.240,0:09:22.000
大家就覺得好好喝，用石頭就可以煮湯這樣

0:09:22.000,0:09:23.760
但是，那個石頭其實並沒有什麼作用

0:09:23.760,0:09:27.820
所以，其實 RBM 就類似這種東西

0:09:34.020,0:09:38.240
然後，我覺得有一個關鍵的突破是在 09 年的時候

0:09:38.240,0:09:42.780
我們，知道要用 GPU 來加速

0:09:42.780,0:09:46.200
這件事情還頗關鍵的，過去如果你做 deep learning

0:09:46.200,0:09:48.480
train 一次，一周就過去了

0:09:48.480,0:09:50.800
然後，結果實驗又失敗，你就不會想做第 2 次了

0:09:52.240,0:09:55.680
其實，我小時候，其實也不是小時候啦

0:09:55.680,0:09:59.840
我多年前有跟一個學弟試著想要做 deep learning

0:09:59.840,0:10:01.580
可是，那個時候還不知道疊很多層

0:10:01.580,0:10:02.440
那個時候大家都疊一層

0:10:02.440,0:10:05.080
直覺想法就知道說，一定要疊很多層阿

0:10:05.080,0:10:06.200
怎麼會只有疊一層？

0:10:06.200,0:10:08.420
所以想說，我們來把它疊很多層吧

0:10:08.420,0:10:11.440
然後，我們就開始做，train 一次要一周

0:10:11.440,0:10:12.860
train 完以後，結果沒有比較好

0:10:12.860,0:10:14.960
就沒有人想要再繼續做下去了

0:10:14.960,0:10:17.160
本來想找專題生做，專題生也都不想做

0:10:17.160,0:10:20.920
就沒有人要做，這個題目就沒有人要做了

0:10:20.920,0:10:23.980
那個時候我們把他好好做出來的話，我們現在就發了

0:10:27.660,0:10:29.680
不過那時候，我們不知道要用 GPU

0:10:29.680,0:10:32.180
所以，train 一次要一周，想要做出來也是很難

0:10:32.180,0:10:34.600
現在有 GPU 以後，本來要一周的東西

0:10:34.600,0:10:37.940
你可能幾個小時，就可以馬上看到結果了

0:10:37.940,0:10:41.240
那在 11 年的時候

0:10:41.240,0:10:44.500
這個方法被引入到語音辨識裡面

0:10:44.500,0:10:47.780
語音辨識發現說，這招果然很有用

0:10:47.780,0:10:50.460
然後，開始瘋狂地用 deep learning 的技術

0:10:50.460,0:10:51.980
到 12 年的時候呢

0:10:51.980,0:10:53.260
這個 deep learning 的技術

0:10:53.260,0:10:55.760
贏了一個很重要的 image 的比賽

0:10:55.760,0:11:00.700
所以，在 image 那邊的人
也瘋狂地用 deep learning 的技術

0:11:00.700,0:11:03.500
其實，deep learning 的技術並沒有

0:11:03.500,0:11:06.500
很複雜，它其實非常簡單

0:11:06.500,0:11:10.340
我們之前講說，machine learning 就是 3 個 step

0:11:10.340,0:11:14.400
那其實 deep learning 也一樣就是，這 3 個 step

0:11:14.400,0:11:18.040
講說 deep learning 就這 3 個 step

0:11:18.040,0:11:20.760
就好像是在說，把那個大象放進冰箱一樣

0:11:20.760,0:11:22.280
大象放進冰箱，大家知道嗎？

0:11:22.280,0:11:24.500
把門打開，把大象趕進去

0:11:24.500,0:11:27.820
把冰箱門關起來，就把大象放進冰箱了

0:11:27.820,0:11:30.260
3 個 step 聽起來就像這樣子

0:11:31.300,0:11:34.200
deep learning 是很簡單的

0:11:34.200,0:11:39.480
你其實可以非常快的了解，其實你可以秒懂它

0:11:40.660,0:11:42.240
其實在 deep learning 裡面

0:11:42.240,0:11:44.200
我們說，在 machine learning 裡面第一個 step

0:11:44.200,0:11:46.180
就是要 define 一個 function

0:11:46.180,0:11:50.680
這個 function，其實就是一個 Neural network

0:11:51.100,0:11:54.420
那這個 Neural network 是

0:11:54.880,0:11:57.420
是甚麼呢？我們剛才已經講說呢

0:11:57.420,0:12:02.540
我們把這個 Logistic Regression 呢

0:12:02.540,0:12:05.100
前後 content 在一起

0:12:05.100,0:12:07.620
然後把一個 Logistic Regression 稱之為 Neuron

0:12:07.620,0:12:08.980
整個稱之為 Neural Network

0:12:08.980,0:12:11.820
我們其實就得到了一個 Neural Network

0:12:11.820,0:12:14.420
那我們可以用不同的方法

0:12:14.420,0:12:17.320
來連接這些 Neural Network

0:12:17.320,0:12:19.840
我們用不同的方法來連接這些 Neural Network

0:12:19.840,0:12:23.500
我們就得到了不同的 structure

0:12:23.500,0:12:24.980
在這個 Neural Network 裡面

0:12:24.980,0:12:27.440
我們有一大堆的 Logistic Regression

0:12:27.440,0:12:30.500
每個 Logistic Regression，它都有自己的 weight

0:12:30.500,0:12:31.820
跟自己的 bias

0:12:31.820,0:12:34.080
這些 weight 跟 bias 集合起來

0:12:34.080,0:12:36.380
就是這個 network 的 parameter

0:12:36.380,0:12:39.300
我們這邊用 θ 來描述它

0:12:39.300,0:12:42.680
那這些 Logistic Regression

0:12:42.680,0:12:45.320
或這些 Neuron

0:12:45.320,0:12:48.860
我們應該怎麼把它接起來呢

0:12:48.860,0:12:50.600
我們應該怎麼把它接起來呢

0:12:50.600,0:12:52.800
有各種不同的方式

0:12:52.800,0:12:55.980
怎麼連接，其實是你手動去設計的

0:12:55.980,0:12:58.040
是你手動去連接的

0:12:58.040,0:12:59.660
最常見的連接方式呢？

0:12:59.660,0:13:03.220
叫做 Fully Connected Feedforward Network

0:13:03.220,0:13:06.200
在 Fully Connected Feedforward Network 裡面呢

0:13:06.200,0:13:10.900
你就把你的 Neuron 排成一排一排

0:13:10.900,0:13:14.480
這邊有 6 個 Neuron，就兩個兩個一排

0:13:14.480,0:13:17.680
然後，每一組 Neuron，它都有一組 weight

0:13:17.680,0:13:19.200
都有一組 bias

0:13:19.200,0:13:23.400
那這個 weight 跟 bias 是根據 training data 去找出來的

0:13:23.400,0:13:27.880
假設上面這個藍色 Neuron 
它的 weight 是 1, -2，它的 bias 是 1

0:13:27.880,0:13:31.940
下面呢，它的 weight 是 -1, 1，它的 bias 是 0

0:13:32.200,0:13:34.480
假設我們現在的輸入

0:13:34.480,0:13:36.000
是 1 跟 -1

0:13:36.000,0:13:39.100
那這兩個藍色的 Neuron 它的 output 是甚麼呢？

0:13:39.100,0:13:43.600
這個做一下小學生會做的運算，你就可以得到答案

0:13:43.600,0:13:47.560
1*1 + (-1)*(-2)，再加上 bias 1

0:13:47.560,0:13:49.560
通過 sigmoid function 以後

0:13:49.560,0:13:53.000
你得到的結果呢，就是 0.98

0:13:53.000,0:13:57.360
那下面呢，你把 1*(-1)、(-1)*1 + 0

0:13:57.360,0:14:01.040
再通過 sigmoid function 以後，你就得到 0.12

0:14:01.040,0:14:05.540
接下來，假設這一個 structure 裡面的每一個 neuron

0:14:05.540,0:14:09.560
它的 weight 跟 bias，我們都是知道的

0:14:09.560,0:14:12.020
那我們就可以反覆進行剛才的運算

0:14:12.020,0:14:18.360
1 跟 -1 通過這兩個 neuron，變成 0.98 跟 0.12

0:14:18.360,0:14:25.260
再通過這兩個 neuron，變成 0.86 跟 0.11

0:14:25.260,0:14:31.020
再通過這兩個 neuron，得到 0.62 跟 0.83

0:14:31.020,0:14:32.780
所以，輸入 1 跟 -1

0:14:32.780,0:14:34.980
經過一串很複雜的轉換以後呢

0:14:34.980,0:14:38.400
就得到 0.62 跟 0.83

0:14:38.400,0:14:42.480
那如果你輸入是 0 跟 0 的話

0:14:42.480,0:14:45.380
你輸入 0 跟 0 的話，你得到的 output 就是

0:14:45.380,0:14:50.380
經過一番一模一樣的運算，你得到的是 0.51, 0.85

0:14:50.380,0:14:53.320
所以，一個 neural network 你就可以把它看作是

0:14:53.320,0:14:55.160
一個 function

0:14:55.160,0:14:57.240
如果一個 neural network 裡面的參數

0:14:57.240,0:14:59.160
weight 跟 bias 我們都知道的話

0:14:59.160,0:15:01.180
它就是一個 function

0:15:01.180,0:15:05.700
它的 input 是一個 vector，它的 output 是另一個 vector

0:15:05.700,0:15:08.360
舉例來說，我們剛才看到說 input 是 [1, -1]

0:15:08.360,0:15:10.320
output 是 [0.62, 0.83]

0:15:10.320,0:15:15.720
input 是 [0, 0]，output 是 [0.51, 0.85]

0:15:15.720,0:15:19.880
所以，一個 network，如果我們已經把參數設上去的話

0:15:19.880,0:15:21.940
它就是一個 function

0:15:21.940,0:15:24.620
如果我們今天還不知道參數

0:15:24.620,0:15:27.580
我只是定出了這個 network 的 structure

0:15:27.580,0:15:30.760
我只是決定好說，這些 neuron 間

0:15:30.760,0:15:33.400
我們要怎麼連接在一起

0:15:33.400,0:15:35.920
這樣子的一個 network structure

0:15:35.920,0:15:39.440
它其實就是 define 了一個 function set，對不對？

0:15:39.440,0:15:43.820
我們可以給這個 network 設不同的參數

0:15:43.820,0:15:45.300
它就變成不同的 function

0:15:45.300,0:15:47.800
把這些可能的 function 通通集合起來

0:15:47.800,0:15:51.260
我們就得到了一個 function set

0:15:51.260,0:15:53.600
所以，一個 neural network 你還沒有認參數

0:15:53.600,0:15:55.740
你只是把它架構架起來

0:15:55.740,0:15:57.880
你決定這些 neuron 要怎麼連接

0:15:57.880,0:16:01.080
你把連接的圖，畫出來的時候

0:16:01.080,0:16:04.080
你其實就決定了一個 function set

0:16:04.080,0:16:06.040
這跟我們之前做的東西都是一樣的

0:16:06.040,0:16:10.040
我們之前也是做 Logistic Regression, Linear Regression

0:16:10.040,0:16:12.020
我們都是也決定了一個 function set

0:16:12.020,0:16:16.180
那這邊呢，我們也只是換一個方式，來決定 function set

0:16:16.180,0:16:18.760
只是如果我們用 neural network 決定 function set 的時候

0:16:18.760,0:16:21.160
你的 function set 是比較大的

0:16:21.160,0:16:24.220
它包含了很多原來你做 Logistic Regression

0:16:24.220,0:16:27.560
做 Linear Regression 所沒有辦法包含的 function

0:16:29.400,0:16:34.420
那剛才講的是一個比較簡單的例子

0:16:34.420,0:16:36.200
在這個例子裡面呢

0:16:36.200,0:16:38.320
我們把 neuron 分成一排一排的

0:16:38.320,0:16:41.480
然後說每一排的 neuron 都兩兩互相連接

0:16:41.480,0:16:44.800
藍色 neuron 的 output 都接給紅色

0:16:45.280,0:16:48.880
紅色的都接給綠色

0:16:48.880,0:16:52.340
綠色後面沒有別人可以接了，
所以它是整個 network 的輸出

0:16:52.340,0:16:56.700
藍色前面沒有其他人了，
所以它就是整個 network 的輸入

0:16:57.080,0:16:58.740
那 in general 而言呢

0:16:58.740,0:17:00.580
我們可以把 network 畫成這樣

0:17:00.580,0:17:03.120
你有好多好多排的 neuron

0:17:03.120,0:17:06.220
你有第 1 排、第 2 排.......到第 L 排

0:17:06.220,0:17:10.840
很多排 neuron，每一排 neuron 它裡面的 neuron 的數目

0:17:10.840,0:17:16.260
可能很多，比如 1000 個、2000 個阿，這個 scale

0:17:16.260,0:17:19.460
那這邊每一顆球，代表一個 neuron

0:17:19.460,0:17:22.680
在 layer 和 layer 之間的 neuron，是兩兩互相連接的

0:17:22.680,0:17:27.620
layer 1 它的 neuron 的 output 會接給

0:17:27.620,0:17:32.280
每一個 layer 2 的 neuron

0:17:32.280,0:17:36.140
那 layer 2 的 neuron 的 input 就是所有 layer 1 的 output

0:17:36.140,0:17:38.140
就是所有 layer 1 的 output

0:17:38.140,0:17:41.340
因為 layer 和 layer 間，所有的 neuron 都有兩兩連接

0:17:41.340,0:17:44.380
所以它叫 Fully Connected 的 Network

0:17:44.380,0:17:47.120
那因為現在傳遞的方向是從 1 到 2

0:17:47.120,0:17:49.220
從 2 到 3，由後往前傳

0:17:49.220,0:17:52.400
所以它叫做 Feedforward Network

0:17:52.820,0:17:55.040
那整個 network 呢，需要一組

0:17:55.040,0:17:58.840
需要一個 input，這個 input 就是一個 vector

0:17:58.840,0:18:01.760
那對 layer 1 的每一個 neuron 來說

0:18:01.760,0:18:03.360
每一個 neuron，它的 input

0:18:03.360,0:18:06.580
就是 input layer 的每一個 dimension

0:18:06.580,0:18:09.380
就是 input layer 的每一個 dimension

0:18:09.380,0:18:12.140
那最後第 L 的那些 neuron

0:18:12.140,0:18:14.140
它後面沒有接其他東西了

0:18:14.140,0:18:17.940
所以，它的 output 就是整個 network 的 output

0:18:17.940,0:18:21.280
假設第 L 排有 M 個 neuron 的話

0:18:21.280,0:18:24.820
它的 output 就是 y1. y2 到 y(下標 M)

0:18:25.500,0:18:28.700
這邊每一個 layer，它是有一些名字的

0:18:28.700,0:18:31.320
input 的地方，我們叫做 input layer

0:18:31.320,0:18:33.820
嚴格說起來，input 其實不是一個 layer

0:18:33.820,0:18:38.160
它跟其他 layer 不一樣，它不是由其他 neuron 所組成的

0:18:38.160,0:18:40.780
但是，我們也把它當作一個 layer 來看

0:18:40.780,0:18:42.100
所以，叫它 input layer

0:18:42.100,0:18:45.060
output 的地方，我們叫它 output layer

0:18:45.060,0:18:48.820
其餘的部分，就叫做 hidden layer

0:18:48.820,0:18:51.140
那所謂的 deep 是什麼意思呢？

0:18:51.140,0:18:54.220
所謂的 deep 就是有很多 hidden layer

0:18:54.220,0:18:55.540
就叫做 deep

0:18:55.540,0:18:58.660
那有人就會問一個問題，說要幾個 hidden layer

0:18:58.660,0:18:59.660
才叫做 deep learning

0:18:59.660,0:19:02.240
這個就很難說了

0:19:02.240,0:19:05.680
有人會告訴你說，要 3 層以上才叫做 deep

0:19:05.680,0:19:09.040
有人會告訴你說，要 8 層以上才叫做 deep

0:19:09.040,0:19:11.460
所以就看每一個人的定義，都不一樣

0:19:11.460,0:19:13.360
本來沒有 deep learning 這個詞的時候

0:19:13.360,0:19:15.460
大家都說：我在做 neural network

0:19:15.460,0:19:18.700
通常都只有一層，自從有 deep learning 這個詞以後

0:19:18.700,0:19:21.700
有一層的人，都說他在做 deep learning 這樣

0:19:23.780,0:19:26.760
所以，現在基本上只要是 neural network base 的方法呢

0:19:26.760,0:19:29.620
大家都會說是 deep learning 的方法

0:19:30.240,0:19:33.300
那到底可以有幾層呢？

0:19:33.300,0:19:35.100
在 2012 年的時候

0:19:35.100,0:19:38.140
參加 ImageNet 比賽得到冠軍的 AlexNet

0:19:38.140,0:19:41.460
它有 8 層，它的錯誤率是 16.4 %

0:19:41.480,0:19:45.660
大家可能都知道說，它比第二名的好非常多

0:19:45.660,0:19:48.560
第二名的 error rate 是 30%

0:19:48.560,0:19:52.560
到 14 年的時候，VGG 有 19層

0:19:52.560,0:19:54.760
它的 error rate 降到 7%

0:19:54.760,0:19:59.160
GoogleNet 有 22 層，它的 error rate 降到 6.7%

0:19:59.160,0:20:01.300
但是，這個都還不算甚麼

0:20:01.300,0:20:03.580
Residual Network 152 層這樣

0:20:03.580,0:20:07.100
如果它跟 GoogleNet, VGG, AlexNet 比起來

0:20:07.100,0:20:08.980
它大概是長這個樣子

0:20:08.980,0:20:11.740
它的 error rate 是 3.57%

0:20:11.740,0:20:14.920
如果你看 Benchmark Corpus 的話，其實

0:20:14.920,0:20:19.320
它這個 performance 是比人在同一個 test 上做的還要好

0:20:19.320,0:20:20.560
你可能會很懷疑說

0:20:20.560,0:20:23.360
人怎麼可能會在影像辨識上輸給機器呢？

0:20:23.360,0:20:25.240
因為那個 test 其實還滿難的

0:20:25.240,0:20:28.880
它給你一張圖，一張狗的圖

0:20:28.880,0:20:32.240
你光回答狗並不是正確答案，你要回答哈士奇這樣子

0:20:35.080,0:20:38.120
那個 test 其實還蠻難的，但其實為了要公平的比較

0:20:38.120,0:20:42.200
當時，在人跟機器比較的時候

0:20:42.200,0:20:46.260
那些人是有事先看過 training data 的

0:20:46.260,0:20:49.640
也就是說，你有先讓它訓練辨識不同狗的種類

0:20:49.640,0:20:51.460
不同花的種類、不同豬的種類

0:20:51.460,0:20:56.420
只是經過訓練以後，它還是沒有機器那麼強就是了

0:20:58.800,0:21:01.200
那這個是跟 101 做一下比較

0:21:01.200,0:21:02.980
101 在這邊

0:21:04.720,0:21:09.060
這個我們之後再講，那 Residual Network 呢

0:21:09.060,0:21:12.440
它不是一般的 Fully Connected Feedforward Network

0:21:12.440,0:21:14.760
如果你用一般 Fully Connected Feedforward Network

0:21:14.760,0:21:18.140
搞在這個地方，其實結果是會有問題的

0:21:18.140,0:21:21.260
它不是 overfitting，而是 train 都 train 不起來

0:21:21.260,0:21:23.360
所以，你其實要特別的 structure

0:21:23.360,0:21:25.520
才能搞定這麼深的 network

0:21:25.520,0:21:26.920
這個我們之後再講

0:21:27.560,0:21:29.060
那 network 的運作呢

0:21:29.060,0:21:34.420
我們常常會把它用 Matrix Operation 來表示

0:21:34.420,0:21:37.020
怎麼說呢，我們舉剛才的例子

0:21:37.020,0:21:40.420
假設第一個 layer 的兩個 neuron

0:21:40.420,0:21:43.200
他們的 weight 分別是 1, -2, -1, 1

0:21:43.460,0:21:49.360
那你可以把 1, -2, -1, 1 排成一個 matrix

0:21:49.360,0:21:53.240
把這個 1, -2, -1, 1 排成一個 matrix

0:21:54.000,0:21:57.040
那當我們 input 1, -1 要做運算的時候

0:21:57.040,0:22:03.080
我們就是把 1*1、(-1)*(-2)、1*(-1)、(-1)*1

0:22:03.080,0:22:08.040
所以我們就可以把 1 跟 -1 當做一個 vector 排在這邊

0:22:08.040,0:22:11.900
當我們把這個 matrix 跟這個 vector

0:22:11.900,0:22:15.700
做運算的時候，我們算 1*1 + (-2)*(-1)

0:22:15.700,0:22:19.000
就等於是做 1*1、(-1)*(-2)

0:22:19.000,0:22:22.260
當我們做 1*(-1)、(-1)*1 的時候

0:22:22.260,0:22:24.800
就等於是做 1*(-1)、(-1)*1 這樣

0:22:25.380,0:22:28.480
接下來呢，有 bias 1 跟 0

0:22:28.480,0:22:31.720
所以，我們要在後面把 bias 排成一個 vector

0:22:31.720,0:22:33.960
再把這個 vector 加上去

0:22:33.960,0:22:37.200
這個結果，算出來就是 [4, -2]

0:22:37.200,0:22:41.640
就是通過 activation function 之前的值，就是 4, -2

0:22:41.640,0:22:43.560
然後，通過這個 sigmoid function

0:22:43.560,0:22:46.060
然後在這個 neural network 的文件裡面

0:22:46.060,0:22:50.580
我們把這個 function 稱之為 activation function

0:22:50.580,0:22:52.780
事實上，它不見得是 sigmoid function

0:22:52.780,0:22:55.240
因為到現在，大家都換成別的 function

0:22:55.240,0:22:58.900
如果你是從 Logistic Regression 想過來的話

0:22:58.900,0:23:02.800
你會覺得它是一個 sigmoid function，
但現在已經較少用 sigmoid function

0:23:02.800,0:23:05.440
假設我們這邊用的是 sigmoid function 的話呢

0:23:05.440,0:23:08.960
我們就是把 4, -2 丟到 sigmoid function 裡面

0:23:08.960,0:23:12.900
接下來算出來，就是 0.98, 0.12

0:23:12.900,0:23:15.800
所以，一個 neural network，一個 feedforward network

0:23:15.800,0:23:19.980
它的一個 layer 的運算，你從 1, -1 到 0.98, 0.12

0:23:19.980,0:23:22.740
你做的運算就是把 input 1 跟 -1

0:23:22.740,0:23:25.960
1 跟 -1 乘一個 matrix

0:23:25.960,0:23:28.100
加上一個 bias 所成的 vector

0:23:28.100,0:23:31.040
再通過一個 sigmoid function

0:23:31.040,0:23:33.380
得到最後的結果

0:23:33.720,0:23:35.540
所以，以 general 來說

0:23:35.540,0:23:37.420
一個 neural network

0:23:37.420,0:23:40.940
假設我們說第一個 layer 的 weight

0:23:40.940,0:23:43.900
全部集合起來，當作一個 matrix, W^1

0:23:43.900,0:23:45.660
這個 W^1 是一個 matrix

0:23:45.660,0:23:48.220
把它的 bias 全部集合起來

0:23:48.220,0:23:49.800
當作一個 vector, b^1

0:23:50.080,0:23:53.620
把第二個 matrix 的 weight 全部集合起來

0:23:53.620,0:23:55.360
當作 W^2

0:23:55.360,0:23:58.700
把它的每一個 neural 的 bias 集合起來，當作 b^2

0:23:58.700,0:24:01.720
到第 L 個 layer，所有的 weight 集合起來，變成 W^L

0:24:01.720,0:24:04.780
bias 集合起來，變成 b^L

0:24:04.780,0:24:10.400
那你今天給它一個 input x 的時候

0:24:10.400,0:24:13.840
output 的這個 y 要怎麼算出來呢？

0:24:13.840,0:24:19.140
我們假設我們把 x1, x2, 到 xN，接起來，變成一個 x

0:24:19.140,0:24:23.940
那這個 output 的 y 要怎麼把它算出來呢？

0:24:23.940,0:24:25.200
你就這樣算

0:24:26.160,0:24:29.960
x * W^1 + b^1

0:24:29.960,0:24:31.860
再通過 activation function

0:24:31.860,0:24:36.060
然後，你就算出第二排這些 neuron 的 output

0:24:36.060,0:24:40.360
我們稱之為 a^1，你把 x * W^1 + b^1，就得到 a^1

0:24:40.360,0:24:42.420
x * W^1 + b^1

0:24:42.420,0:24:45.780
再通過 activation function 以後，就得到 a^1

0:24:46.180,0:24:48.580
接下來呢，你做一樣的運算

0:24:48.580,0:24:51.700
把 a1 * W^2 + b^2

0:24:51.700,0:24:55.380
再通過 activation function 以後，就得到 a^2

0:24:55.380,0:25:00.140
然後，就這樣一層一層的做下去到最後一層

0:25:00.140,0:25:05.340
你把 a^(L-1) * W^L + b^L 通過 activation function 以後

0:25:05.340,0:25:08.000
得到這個 network 的最終 output y

0:25:08.000,0:25:11.080
所以，整個 neural network 的運算其實就是

0:25:11.080,0:25:15.240
一連串 matrix 的 operation

0:25:15.240,0:25:16.820
就是這個 function

0:25:16.820,0:25:19.520
x 它的 input, output，x 跟 y 的關係

0:25:19.520,0:25:21.880
x 跟 y 的關係，他們是怎麼樣的關係呢？

0:25:21.880,0:25:26.240
你把 x * W^1 + b^1，再通過 activation function

0:25:26.240,0:25:30.560
再把這個 output 再乘上 W^2，加 b^2

0:25:30.560,0:25:33.180
再通過 activation function，再通過 sigmoid function

0:25:33.180,0:25:35.460
最後，到乘上 W^L，加上 b^L

0:25:35.460,0:25:37.720
再通過這個 sigmoid function

0:25:37.720,0:25:39.740
就得到最後的 y

0:25:39.740,0:25:41.860
所以，一個 neural network

0:25:41.860,0:25:45.420
實際上做的事情就是一連串的

0:25:45.420,0:25:48.460
vector 乘上 matrix ，再加上 vector

0:25:48.460,0:25:52.920
就是一連串我們在線性代數就有學過的：矩陣運算

0:25:52.920,0:25:56.000
那把這件事情寫成矩陣運算的好處就是

0:25:56.000,0:25:58.460
你可以用 GPU 加速

0:25:58.460,0:26:01.720
實際上呢，現在一般在用 GPU 加速的時候

0:26:01.720,0:26:04.500
這個 GPU 的加速並不是

0:26:04.500,0:26:08.760
真的有去對 neural network 做甚麼特化

0:26:08.760,0:26:11.740
是說，現在有一些特別的技術有做特化

0:26:11.740,0:26:15.360
但是，如果你 general 是買那種玩遊戲的顯卡的話

0:26:15.360,0:26:19.320
那它是沒有對這個 neural network 做甚麼特化

0:26:19.320,0:26:21.620
那你實際上拿來加速的方式是

0:26:21.620,0:26:23.880
當你需要算矩陣運算的時候

0:26:23.880,0:26:27.220
你就 call 一下 GPU，叫它幫你做矩陣運算

0:26:27.220,0:26:31.520
這會比你用 CPU 來算還要快

0:26:31.520,0:26:35.360
所以，我們在寫 neural network 式子的時候

0:26:35.360,0:26:38.920
我們習慣把它寫成 matrix operation 的樣子

0:26:38.920,0:26:41.320
那裡面如果有需要用到矩陣運算的時候

0:26:41.320,0:26:43.240
就 call GPU 來做它

0:26:44.620,0:26:48.180
那這整個 network，我們怎麼看待呢？

0:26:48.360,0:26:53.960
我們可以把它到 output layer 之前的部分阿

0:26:53.960,0:26:55.880
到 output layer 之前的部分

0:26:55.880,0:27:00.560
看作是一個 feature 的 extractor

0:27:00.560,0:27:03.300
這個 feature extractor 就 replace 我們之前

0:27:03.300,0:27:06.180
要手動做 feature engineering

0:27:06.180,0:27:09.780
做 feature transformation 這件事情

0:27:09.780,0:27:12.860
所以，你把一個 x input，通過很多很多 hidden layer

0:27:12.860,0:27:15.000
在最後一個 hidden layer 的 output

0:27:15.000,0:27:17.840
每一個 neuron 的 output，x1, x2, 到 xK

0:27:17.840,0:27:21.200
你就可以把它想成是一組新的 feature

0:27:21.200,0:27:24.860
那 output layer 做的事情呢？output layer 就是

0:27:24.860,0:27:28.060
一個 Multi-class 的 Classifier

0:27:28.060,0:27:32.580
這個 Multi-class Classifier，它是拿前一個 layer 的 output

0:27:32.580,0:27:34.100
當作 feature

0:27:34.100,0:27:38.540
這個 Multi-class Classifier，
它用的 feature 不是直接從 x 抽出來的

0:27:38.540,0:27:42.460
它是經過很多個 hidden layer，做很複雜的轉換以後

0:27:42.460,0:27:44.620
抽出一組特別好的 feature

0:27:44.620,0:27:48.360
這組好的 feature 可能是能夠被 separable

0:27:48.360,0:27:50.280
經過這一連串的轉換以後呢

0:27:50.280,0:27:53.860
它們可以被用一個簡單的 layer 的

0:27:53.860,0:27:58.740
Multi-class Classifier 就把它分類好

0:27:58.740,0:28:00.780
那我們剛才其實有講過說

0:28:00.780,0:28:05.080
Multi-class Classifier 它要通過一個 sigmoid function

0:28:05.080,0:28:07.880
而不要通過一個 Softmax function，對不對？

0:28:07.880,0:28:09.980
要通過一個 Softmax function

0:28:09.980,0:28:13.040
因為我們把 output layer

0:28:13.040,0:28:17.020
也看作是一個 Multi-class Classifier

0:28:17.020,0:28:19.960
因為我們把 output layer 也看作是
一個 Multi-class Classifier

0:28:19.960,0:28:24.380
所以，我們最後一個 layer 也會加上 Softmax

0:28:24.400,0:28:26.520
一般你在做 neural network 的時候呢

0:28:26.520,0:28:28.780
是會這樣做

0:28:29.340,0:28:33.040
那舉一個不是寶可夢的例子

0:28:33.040,0:28:38.580
不是寶可夢的例子，我之後會示範一下做這個例子

0:28:38.580,0:28:43.220
就 input 一張 image，它是一張手寫的數字

0:28:43.220,0:28:46.620
然後 output 說這個 input 的 image，它對應的是甚麼？

0:28:46.620,0:28:49.680
那在這個問題裡面，你的 input 是一張 image

0:28:49.680,0:28:54.200
但對機器來說，一張 image 就是一個 vector

0:28:54.200,0:28:57.760
假設這是一個解析度 16*16 的 image

0:28:57.760,0:29:00.100
那它有 256 個 pixel

0:29:00.100,0:29:05.900
對 machine 來說，它就是一個 256 維的 vector

0:29:05.900,0:29:08.240
那在這個 image 裡面呢

0:29:08.240,0:29:10.500
每一個 pixel 就對應到其中的 dimension

0:29:10.500,0:29:13.000
所以，右上角這個 pixel 就對應到 x1

0:29:13.000,0:29:16.760
這個就對應到 x2，右下角的就對應到 x256

0:29:16.760,0:29:19.520
如果你可以說，有塗黑的地方就是 1

0:29:19.520,0:29:23.440
那沒有塗黑的地方，它對應的數字就是 0

0:29:23.440,0:29:25.080
那 output 呢？

0:29:25.080,0:29:29.820
neural network 的 output，如果你用 Softmax 的話

0:29:29.820,0:29:34.040
那它的 output 代表了一個 
probability distribution，對不對？

0:29:34.040,0:29:37.600
所以，今天假設 output 是 10 維的話

0:29:37.600,0:29:39.800
就可以把這個 output 看成是

0:29:39.800,0:29:44.360
對應到每一個，你可以把這個 output 看成是

0:29:44.360,0:29:49.100
對應到每一個數字的機率

0:29:49.100,0:29:52.360
y1，代表了 input 這張 image

0:29:52.360,0:29:55.780
根據這個 neural network 判斷，它是屬於 1 的機率

0:29:55.780,0:29:59.980
代表它是屬於 2 的機率，代表它是屬於 0 的機率

0:29:59.980,0:30:02.620
那你就實際上讓 network 幫你算一下說

0:30:02.620,0:30:06.680
每一個 input 加上數字屬於 image 的機率是多少？

0:30:06.680,0:30:09.360
假設屬於數字 2 的機率最大是 0.7

0:30:09.360,0:30:13.020
那你的 machine 就會 output 說

0:30:13.020,0:30:16.200
這張 image 它是屬於數字 2

0:30:17.020,0:30:21.100
那在這個 application 裡面，
假設你要解這個手寫數字辨識的問題

0:30:21.100,0:30:24.760
那你唯一需要的，就是一個 function

0:30:24.760,0:30:27.240
這個 function input 是一個二維的 vector

0:30:27.240,0:30:30.260
output 是一個 10 維的 vector

0:30:30.260,0:30:35.000
而這個 function 就是 neural network

0:30:35.000,0:30:38.640
所以，你只要兜一個 neural network

0:30:38.640,0:30:40.660
你可以用簡單的 feedforward network 就好了

0:30:40.660,0:30:46.280
兜一個 neural network，
它的 input 有 256維、是一張 image

0:30:46.280,0:30:49.220
它的 output 你特別設成 10 維

0:30:49.220,0:30:52.180
這 10 維裡面，每一個 dimension

0:30:52.180,0:30:55.140
都對應到一個數字

0:30:55.140,0:30:59.040
如果你做這樣的設計，input 是 256 維

0:30:59.040,0:31:03.240
output 固定是 10 維的話

0:31:03.240,0:31:06.200
那這一個 network，它其實就

0:31:06.200,0:31:10.740
代表了一個可以拿來做手寫數字的 function

0:31:10.740,0:31:14.040
這個 function set 裡面呢

0:31:14.040,0:31:16.500
這個 network 的 structure 就 define 了一個 function set

0:31:16.500,0:31:17.280
這個 function set 裡面

0:31:17.280,0:31:19.940
每一個 function 都可以拿來做手寫數字辨識

0:31:19.940,0:31:22.860
只是有些做出來結果比較好

0:31:22.860,0:31:24.640
有些做出來結果比較差

0:31:24.640,0:31:26.660
那接下來你要做的事情就是

0:31:26.660,0:31:29.980
用 Gradient Descent 去找一組參數

0:31:29.980,0:31:33.600
去挑一個最適合拿來做手寫數字辨識的 function

0:31:33.600,0:31:35.620
那在這個 process 裡面呢

0:31:35.620,0:31:38.320
我們需要做一些 design

0:31:38.320,0:31:40.240
之前在做 Logistic Regression

0:31:40.240,0:31:42.320
或是 Linear Regression 的時候

0:31:42.340,0:31:46.520
我們對 model 的 structure 是沒有甚麼好設計的

0:31:46.520,0:31:48.640
但是，對 neural network 來說

0:31:48.640,0:31:52.200
我們現在唯一的 constraint 只有 input 要是 256 維

0:31:52.200,0:31:53.880
output 要是 10 維

0:31:54.080,0:31:56.480
但是中間要有幾個 hidden layer

0:31:56.480,0:31:58.700
每一個 hidden layer 要有多少的 neuron

0:31:58.700,0:32:00.660
是沒有限制的

0:32:00.660,0:32:04.340
你必須要自己去設計它

0:32:04.340,0:32:07.100
你必須自己去決定說，我要甚麼 layer

0:32:07.100,0:32:09.780
每個 layer 要有多少的 neuron

0:32:09.780,0:32:12.560
那決定 layer 的數目

0:32:12.560,0:32:15.040
和每一個 layer 的 neuron 數這件事情

0:32:15.040,0:32:18.520
就等於是決定了你的 function set 長甚麼樣子

0:32:18.520,0:32:22.100
你可以想像說，如果我今天決定了一個差的 function set

0:32:22.100,0:32:25.320
那裡面沒有包含任何好的 function

0:32:25.320,0:32:28.160
那你之後在找最好的 function 的時候，就好像是

0:32:28.160,0:32:30.700
大海撈針，結果針不在海裡這樣

0:32:30.700,0:32:32.660
怎麼找都找不到一個好的 function

0:32:32.660,0:32:34.960
所以，決定一個好的 function set

0:32:34.960,0:32:37.800
其實很關鍵，你就決定這個 network 的 structure

0:32:37.800,0:32:39.300
是很關鍵的

0:32:39.300,0:32:43.120
講到這邊，總是會有人問我一個問題

0:32:43.120,0:32:46.620
假設我們今天讓 machine 來聽我的 talk

0:32:46.620,0:32:49.920
然後，叫它 predict 之後會有甚麼問題的話

0:32:49.920,0:32:51.860
它一定可以預測接下來的問題

0:32:51.860,0:32:54.640
那我們到底應該要怎麼決定 layer 的數目

0:32:54.640,0:32:56.940
還有每一個 layer 的 neuron 的數目呢

0:32:56.940,0:32:59.880
這個答案，就是我不知道這樣子

0:32:59.880,0:33:04.120
這個問題很難，就好像問說怎麼成為寶可夢大師一樣

0:33:04.580,0:33:09.540
這只能夠憑著經驗和直覺這樣

0:33:09.540,0:33:11.760
這個 network structure 要長甚麼樣子

0:33:11.760,0:33:15.780
就是憑著直覺還有多方的常識阿

0:33:15.780,0:33:19.960
然後，後續想辦法找一個最後的 network structure

0:33:19.960,0:33:23.960
找 network structure 這件事情，並沒有那麼容易

0:33:23.960,0:33:25.860
它有時候是滿困難的

0:33:25.860,0:33:28.180
有時候甚至需要一些 domain knowledge

0:33:28.180,0:33:31.500
所以，我覺得從非 deep learning 的方法

0:33:31.500,0:33:33.860
到 deep learning 的方法

0:33:33.860,0:33:37.900
我並不認為machine learning 真的變得比較簡單

0:33:37.900,0:33:41.180
而是我們把一個問題轉化成另一個問題

0:33:41.180,0:33:43.780
就本來不是 deep 的 model

0:33:43.780,0:33:44.960
我們要得到好的結果

0:33:44.960,0:33:47.420
你往往需要做 Feature Engineering

0:33:47.420,0:33:51.100
也就是做 Feature Transform，然後找一組好的 feature

0:33:51.100,0:33:53.400
但是，如果今天是做 deep learning 的時候

0:33:53.400,0:33:55.360
你往往不需要找一個好的 feature

0:33:55.360,0:33:57.140
比如說，做影像辨識的時候

0:33:57.140,0:33:59.580
你可以直接把 pixel 丟進去

0:33:59.580,0:34:02.560
過去做影像辨識的時候，你需要對影像抽一些 feature

0:34:02.560,0:34:04.000
抽一些人定的 feature

0:34:04.000,0:34:06.140
這件事情就是 Feature Transform

0:34:06.140,0:34:09.960
但是有 deep learning 之後，你可以直接丟 pixel 硬做

0:34:09.960,0:34:13.620
但是，今天 deep learning 製造了一個新的問題

0:34:13.620,0:34:17.600
它所製造的新的問題就是，
你需要去 design network 的 structure

0:34:17.600,0:34:19.980
就你的問題變成本來抽 feature

0:34:19.980,0:34:23.440
轉化成怎麼抽、怎麼 design network structure

0:34:23.440,0:34:26.760
那我覺得 deep learning 是不是真的好用就 depend on

0:34:26.760,0:34:29.600
你覺得哪一個問題比較容易

0:34:29.600,0:34:32.520
我個人覺得如果是

0:34:32.520,0:34:34.680
語音辨識或是影像辨識的話

0:34:34.680,0:34:37.860
design network structure 可能比 
Feature Engineering 容易

0:34:37.860,0:34:42.680
因為，雖然說我們人工會看、會聽

0:34:42.680,0:34:44.560
我們自己都做得嚇嚇叫

0:34:44.560,0:34:46.960
但是，這件事情它太過潛意識了

0:34:46.960,0:34:48.960
它離我們意識的層次太遠

0:34:48.960,0:34:50.200
我們其實不知道

0:34:50.200,0:34:53.620
我們無法意識到，我們到底是怎麼做語音辨識這件事情

0:34:54.060,0:34:57.500
所以，對人來說你要抽一組好的 feature

0:34:57.500,0:35:01.780
讓機器可以用很方便的用 linear 的方法做語音辨識

0:35:01.780,0:35:04.900
這件事對人來說很難，因為
根本不知道好的 feature 長甚麼樣子

0:35:04.900,0:35:07.980
所以，還不如 design 一個 network structure

0:35:07.980,0:35:09.800
或是嘗試各種 network structure

0:35:09.800,0:35:13.260
讓 machine 自己去找出好的 feature

0:35:13.260,0:35:15.300
這件事情反而變得比較容易

0:35:15.300,0:35:17.460
我覺得對影像來說，也是一樣

0:35:17.460,0:35:20.260
那對其他 case 來說，我覺得就是 case by case

0:35:20.260,0:35:22.760
比如說，你有聽過一個說法是

0:35:22.760,0:35:26.080
deep learning 在 NLP 上面

0:35:26.080,0:35:28.560
覺得 performance 沒有那麼好

0:35:28.560,0:35:31.900
有聽過這個說法的同學舉手一下

0:35:31.900,0:35:34.620
好，沒關係，手放下

0:35:34.620,0:35:38.120
好像沒有太多人聽過這個說法

0:35:38.120,0:35:40.020
那這件事情是這樣

0:35:40.020,0:35:44.440
如果你開語音辨識跟影像辨識的文件

0:35:44.440,0:35:47.060
語音辨識跟影像辨識這兩個 community

0:35:47.060,0:35:49.760
是最早開始用 deep learning 的

0:35:49.760,0:35:52.720
一用下去，進步量就非常驚人

0:35:52.720,0:35:57.920
比如說，電視的錯誤率相對下降了 20% 以上

0:35:58.480,0:36:01.720
那如果是 NLP 的話

0:36:01.720,0:36:04.620
你就會覺得說，它的進步量

0:36:04.620,0:36:06.060
似乎沒有那麼驚人

0:36:06.060,0:36:09.740
甚至很多 NLP 的人，現在仍然認為說 
deep learning 不見得那麼 work

0:36:09.740,0:36:12.560
那我覺得這個，我自己的猜想是

0:36:12.560,0:36:18.020
這個原因就是，人在做 NLP 這件事情

0:36:18.020,0:36:20.900
文字處理來說，人是比較強的

0:36:20.900,0:36:23.360
比如說，叫你說

0:36:23.360,0:36:25.620
叫你設計一個 rule detect 說

0:36:25.620,0:36:28.920
一篇 document 它是正面的情緒，還是負面的情緒

0:36:28.920,0:36:30.960
你可以說我就列表

0:36:30.960,0:36:33.300
列一些正面情緒的詞彙跟負面情緒的詞彙

0:36:33.300,0:36:36.480
然後看這個 document 裡面，
正面情緒的詞彙出現百分之多少

0:36:36.480,0:36:38.860
你可能就可以得到一個不錯的結果

0:36:38.860,0:36:40.180
所以，NLP 這個 task

0:36:40.180,0:36:42.620
對人來說，你比較容易設計 rule

0:36:42.620,0:36:44.960
所以，你設計的那些 ad-hoc 的 rule

0:36:44.960,0:36:47.720
我好像可以給你一個還不錯的結果

0:36:47.720,0:36:51.260
這就是為甚麼 deep learning 相較於 NLP 傳統的方法

0:36:51.260,0:36:53.800
覺得進步沒有那麼地顯著

0:36:53.800,0:36:56.140
但它其實還是有一些進步的

0:36:56.140,0:36:59.800
只是(覺得沒有)其他的領域

0:36:59.800,0:37:04.220
沒有語音和影像處理看起來那麼地顯著

0:37:04.220,0:37:05.340
但還是有進步的

0:37:05.340,0:37:07.540
那我覺得就長久而言呢

0:37:07.540,0:37:09.860
因為文字處理其實也是很困難的問題

0:37:09.860,0:37:14.140
你沒有很多幽微的資訊，可能是人自己也不知道的

0:37:14.140,0:37:15.800
所以，就長久而言呢

0:37:15.800,0:37:18.760
deep learning，讓 machine 自己去學這件事情

0:37:18.760,0:37:21.040
還是可以佔到一些優勢

0:37:21.040,0:37:25.100
只是一下子，眼下看起來進步沒有那麼顯著

0:37:25.100,0:37:28.500
它跟傳統的方法比起來的差異就沒有那麼驚人

0:37:28.500,0:37:30.260
但是還是有進步的

0:37:30.260,0:37:31.540
這是我一些想法

0:37:31.760,0:37:33.460
那再來就是有人會問說

0:37:33.460,0:37:35.620
能不能夠自動學 network 的 structure

0:37:35.620,0:37:38.640
其實是可以的，你可以去問余天立老師

0:37:39.020,0:37:43.340
就是在基因演算法那邊，有很多的 technique

0:37:43.340,0:37:48.360
是可以讓 machine 自動的去找出 network structure

0:37:48.360,0:37:51.740
不過這些方法，目前還沒有非常的普及

0:37:51.740,0:37:54.100
你看到那些非常驚人的應用

0:37:54.100,0:37:55.480
比如說，AlphaGo 什麼的阿

0:37:55.480,0:37:57.540
都不是用這些方法所做出來的

0:37:58.620,0:38:00.320
那還有一個常問的問題就是

0:38:00.320,0:38:04.200
我們能不能自己去設計 network 的 structure

0:38:04.200,0:38:06.060
我可不可以不要 fully connected

0:38:06.060,0:38:09.740
我可不可以說，第一個連到第三個，第二個連到第四個

0:38:09.740,0:38:11.400
就自己亂接，可以

0:38:11.400,0:38:14.480
一個特殊的接法就是 Convolutional Neural Network

0:38:14.480,0:38:17.960
這個我們下一堂課再講

0:38:18.140,0:38:21.060
那接下來，第二步

0:38:21.060,0:38:24.180
第二步跟第三步真的很快，我們等一下就秒講這樣子

0:38:24.180,0:38:25.960
第二步是甚麼呢？

0:38:25.960,0:38:28.960
要定義一個 function 的好壞

0:38:28.960,0:38:30.660
在 Neural Network 裡面

0:38:30.660,0:38:33.820
怎麼決定一組參數它的好壞呢？

0:38:33.820,0:38:36.120
就假設給定一組參數

0:38:36.120,0:38:38.080
我要做手寫數字辨識

0:38:38.080,0:38:42.340
所以，我有一張 image，跟它的 label

0:38:42.340,0:38:45.660
然後，把這張 image 呢，這個 label 告訴我們說

0:38:45.660,0:38:49.600
因為你現在是一個 Multiclass classification 的問題

0:38:49.600,0:38:53.280
所以，今天這個 label 1，告訴我們說

0:38:53.280,0:38:56.240
你現在的 target 是一個 vector

0:38:56.240,0:38:58.520
你現在的 target 是一個 10 維的 vector

0:38:58.520,0:39:02.340
只有在第一維，對應到數字 1 的地方

0:39:02.340,0:39:05.500
它值是 1，其他呢，都是 0

0:39:05.960,0:39:08.860
那你就 input 這張 image 的 pixel

0:39:08.860,0:39:11.680
然後，通過這個 neural network 以後呢

0:39:11.680,0:39:13.240
你會得到一個 output

0:39:13.240,0:39:15.760
那這個 output，我們就稱之為 y

0:39:15.760,0:39:18.960
我把我們的 target，稱之為 y\head

0:39:18.960,0:39:20.820
接下來你要做的事情就是

0:39:20.820,0:39:25.760
計算這個 y 跟 y\head 之間的 cross entropy

0:39:25.760,0:39:29.280
就跟我們在做  Multiclass classification 的時候

0:39:29.280,0:39:30.960
是一模一樣的

0:39:30.960,0:39:34.940
我們就計算 y 跟 y\head 之間的 cross entropy

0:39:34.940,0:39:37.800
然後，接下來我們就是要調整 network 的參數

0:39:37.800,0:39:40.980
去讓這個 cross entropy 越小越好

0:39:41.120,0:39:44.860
當然你整個 training data 裡面，
不會只有一筆 data，你有一大堆的 data

0:39:44.860,0:39:45.840
你有第一筆 data

0:39:45.840,0:39:48.740
那它所算出來的 cross entropy 是 C^1

0:39:48.740,0:39:50.340
第二筆 data 算出來是 C^2

0:39:50.340,0:39:52.280
到第 N 筆 data 算出來是 C^N

0:39:52.280,0:39:55.760
那你會把所有 data 的 cross entropy 全部 sum 起來

0:39:55.760,0:39:58.700
得到一個 total loss，L

0:39:59.780,0:40:03.040
然後，你接下來要做的事情就是

0:40:03.040,0:40:06.040
在 function set 裡面，找一個 function

0:40:06.040,0:40:07.920
它可以 minimize 這個 total loss

0:40:07.920,0:40:11.040
或者是找一組 network 的 parameter

0:40:11.040,0:40:13.200
現在寫成 θ*

0:40:13.200,0:40:16.000
它可以 minimize 這個 total loss

0:40:16.000,0:40:18.620
怎麼解這個問題？

0:40:18.620,0:40:22.540
怎麼找一個 θ* minimize 這個 total loss 呢？

0:40:22.540,0:40:25.680
你用的方法就是 Gradient Descent

0:40:25.680,0:40:29.300
Gradient Descent 大家已經太熟了，沒有甚麼好講的

0:40:29.300,0:40:31.660
實際上，在 deep learning 裡面用 Gradient Descent

0:40:31.660,0:40:35.240
跟 Linear Regression 那邊沒有甚麼差別，就一模一樣

0:40:35.240,0:40:37.560
所以，你要做的事情，只是 function 變複雜而已

0:40:37.560,0:40:40.520
其他東西都是一樣的

0:40:40.520,0:40:41.820
也就是說

0:40:41.820,0:40:43.980
你的 θ 裡面是一大堆的參數

0:40:43.980,0:40:46.060
一大堆的 weight 跟一大堆的 bias

0:40:46.060,0:40:48.560
你先 random 找一個初始值

0:40:48.560,0:40:50.800
random 給每一個數字一個初始值

0:40:50.800,0:40:54.460
接下來呢，去計算一下它的 Gradient

0:40:54.460,0:40:58.660
計算每一個參數，對你 total loss 的偏微分

0:40:58.660,0:41:01.360
那把這些偏微分全部集合起來呢

0:41:01.360,0:41:03.960
叫做 Gradient

0:41:03.960,0:41:07.960
有了這些偏微分以後，你就可以更新你的參數

0:41:07.960,0:41:12.180
你就把所有的參數，都減掉一個 learning rate

0:41:12.180,0:41:15.620
這邊寫成 μ，乘上偏微分的值

0:41:15.620,0:41:17.500
你就得到一組新的參數

0:41:17.500,0:41:19.300
這個 process 就反覆進行下去

0:41:19.300,0:41:23.500
你有了新的參數，再計算一下它的 Gradient

0:41:23.500,0:41:26.340
然後，再根據你的 Gradient

0:41:26.340,0:41:29.480
再更新你的參數，你就得到一組新的參數

0:41:29.480,0:41:32.120
按照這個 process，繼續下去

0:41:32.120,0:41:33.880
這個都是我們講過的東西

0:41:33.880,0:41:35.580
你就可以找到一組好的參數

0:41:35.580,0:41:37.500
你就做完 neural network 的 training 了

0:41:37.500,0:41:41.000
所以，其實就這樣子啦

0:41:41.000,0:41:43.140
deep learning 的 training 就這樣子

0:41:43.140,0:41:45.360
就算是最潮的 AlphaGo

0:41:45.360,0:41:47.100
也是用 Gradient Descent train 的啦！

0:41:47.100,0:41:49.900
所以，大家可能會想像說

0:41:49.900,0:41:51.320
如果是 deep learning 的話

0:41:51.320,0:41:52.900
machine 的 learning 應該是這樣子

0:41:52.900,0:41:54.080
其實上，我們知道說

0:41:54.080,0:41:56.740
Gradient Descent 就是玩世紀帝國，所以其實是這個樣子

0:41:56.740,0:41:59.020
然後，希望你不要覺得太失望

0:41:59.800,0:42:02.280
然後，你可能會問說，那這個 Gradient Descent

0:42:02.280,0:42:03.860
的 function 式子長甚麼樣子呢？

0:42:03.860,0:42:07.820
之前我們都手把手的把那個算式算出來給大家看

0:42:07.820,0:42:09.960
但是，在 neural network 裡面

0:42:09.960,0:42:11.500
因為 function 比較複雜

0:42:11.500,0:42:14.680
所以，如果我們要手把手的算出來給大家看

0:42:14.680,0:42:17.380
是比較難，需要花一些時間

0:42:17.380,0:42:20.500
其實，在現代這個時代

0:42:20.500,0:42:24.300
我還記得幾年前，在做 deep learning 很痛苦

0:42:24.300,0:42:26.360
因為你要自己 implement Backpropagation

0:42:26.360,0:42:31.000
那現在呢，你應該已經沒有人
自己 implement Backpropagation 了

0:42:31.000,0:42:33.420
因為有太多太多太多的 toolkit

0:42:33.420,0:42:35.920
可以幫你算 Backpropagation

0:42:35.920,0:42:38.980
那在作業三，我們要做 deep learning，為了容許你用

0:42:38.980,0:42:40.060
這些 toolkit

0:42:40.060,0:42:43.360
所以，你就算不會算這個微分

0:42:43.360,0:42:47.380
Backpropagation 就是算這個微分的一個比較有效的方式

0:42:47.380,0:42:48.960
因為參數非常多嘛

0:42:48.960,0:42:50.420
你有 million的參數

0:42:50.420,0:42:54.300
你沒有辦法為每個 million 的參數
都算微分，這太花時間了

0:42:54.300,0:42:58.980
Backpropagation 是一個比較有效率的算這個微分的方式

0:42:58.980,0:43:01.320
如果你想要更知道詳情的話

0:43:01.320,0:43:03.420
我之前的 deep learning 課呢

0:43:03.420,0:43:07.040
內容是有錄影的，你可以聽一下這個上課的內容

0:43:07.040,0:43:11.360
如果你不想知道的話呢，其實也沒什麼關係

0:43:11.360,0:43:13.460
我聽過一個傳聞是說

0:43:13.460,0:43:17.640
就是說有某個公司，它在應徵 deep learning 的人

0:43:17.640,0:43:20.760
我就問他說，你們要應徵 deep learning 的人，
會問他什麼問題

0:43:20.760,0:43:22.700
他說，我問她說如何算微分

0:43:22.700,0:43:26.440
75%、號稱是 deep learning 專家的人，其實不會算微分

0:43:26.440,0:43:29.080
所以，大家已經太習慣用 toolkit 了

0:43:29.080,0:43:30.540
也不太會算微分了

0:43:30.540,0:43:35.660
那 toolkit 很多啦，這邊就列一些出來，給大家參考

0:43:37.560,0:43:43.940
那最後，在請助教講作業二之前

0:43:43.940,0:43:46.100
我們有一個最後的問題

0:43:47.740,0:43:50.320
為甚麼我們要 deep learning？

0:43:50.320,0:43:52.760
你可能直覺地說這個答案很簡單啊

0:43:52.760,0:43:55.740
因為越 deep，performance 就越好

0:43:55.740,0:43:58.080
這個是一個很早年的實驗

0:43:58.080,0:44:00.920
這個是 2011 年，Interspeech 裡面的某一篇 paper

0:44:00.920,0:44:03.500
他做的是 word error rate

0:44:03.500,0:44:06.520
word error rate 是越小越好，你會發現

0:44:06.520,0:44:07.480
1 個 hidden layer

0:44:07.480,0:44:10.940
每個 hidden layer，2k 個 neuron，word error rate 是24.2

0:44:10.940,0:44:12.960
那越來越 deep 以後

0:44:12.960,0:44:15.980
它的 performance，error rate 就越來越低

0:44:15.980,0:44:19.940
但是，如果你稍微有一點 machine learning 的常識的話

0:44:19.940,0:44:23.120
這個結果並沒有讓你太 surprise 阿，因為

0:44:23.120,0:44:26.420
本來，model 越多的 parameter

0:44:26.420,0:44:29.680
它 cover 的 function set 就越大

0:44:29.680,0:44:32.560
它的 bias 就越小，對不對？

0:44:32.560,0:44:34.760
你今天如果有越多的 training data

0:44:34.760,0:44:37.360
你有夠多的 training data 去控制它的 variance

0:44:37.360,0:44:41.540
一個複雜的 model，一個參數比較多的 model

0:44:41.540,0:44:44.700
它 performance 比較好，是很正常的阿

0:44:44.700,0:44:48.000
那變 deep 有甚麼特別了不起的地方？

0:44:48.000,0:44:51.700
有一個理論是這樣告訴我們

0:44:51.700,0:44:53.740
這個理論是這樣說的

0:44:53.740,0:44:56.000
任何連續的 function

0:44:56.000,0:45:00.280
假設任何連續的 function，
它的 input 是一個 N 維的 vector

0:45:00.280,0:45:02.680
它 output 是一個 M 維的 vector

0:45:02.680,0:45:07.560
它都可以用一個 hidden layer 的 neural network 來表示

0:45:07.560,0:45:11.220
只要你這個 hidden layer 的 neuron 夠多

0:45:11.220,0:45:15.760
它可以表示成任何的 function

0:45:15.760,0:45:18.040
那既然一個 hidden layer 的 neural network

0:45:18.040,0:45:20.260
它可以表示成任何 function

0:45:20.260,0:45:23.940
而我們在做 machine learning 的時候，
我們需要的東西就只是一個 function 而已

0:45:23.940,0:45:25.680
既然 hidden layer 就可以表示任何 function

0:45:25.680,0:45:27.720
那做 deep 的意義何在呢？

0:45:27.720,0:45:29.500
沒有甚麼特別的意義阿

0:45:29.500,0:45:32.900
所以有人說，deep learning 就只是一個噱頭而已

0:45:32.900,0:45:34.780
就是做 deep 感覺比較潮

0:45:34.780,0:45:37.140
如果你只是把它變寬，變成 fat neural network

0:45:37.140,0:45:38.660
那感覺就太虛弱了

0:45:38.660,0:45:41.560
所以它沒有辦法引起大家的注意

0:45:41.560,0:45:43.240
所以，我們要做 deep learning

0:45:43.240,0:45:47.200
真的是這樣嗎？這個我們在往後的 lecture

0:45:47.200,0:45:48.580
再來告訴大家

0:45:48.580,0:45:50.080
最後就是列一些 reference 阿

0:45:50.080,0:45:53.200
如果你對 deep learning 很有興趣的話，你可以先看一下

0:45:53.200,0:45:55.960
那還有我上學期講 deep learning 的錄影呢

0:45:55.960,0:45:57.000
在這邊也找的到

0:45:57.000,0:46:01.380
那另外呢，我有一個 6 小時的 tutorial 的 slide

0:46:01.380,0:46:04.200
那你在這個連結上，也可以找到

0:46:04.200,0:46:09.940
這個 tutorial ，我 11 月還會在新竹再講一次

0:46:09.940,0:46:12.820
這或許是我人生中最後一次講這個 tutorial

0:46:12.820,0:46:14.900
千萬不要來聽，因為

0:46:14.900,0:46:17.280
為甚麼呢？因為在 tutorial 講的東西

0:46:17.280,0:46:20.680
在未來的課程裡面，都會涵蓋，它就只是一個 subset 而已

0:46:20.680,0:46:25.160
那接下來來我們就先休息 5 分鐘，
然後趕快就助教來換場一下

0:46:25.160,0:46:29.320
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
