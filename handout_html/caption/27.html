<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
0:00:00.000,0:00:01.180<br>
台灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:01.400,0:00:03.280<br>
Transfer learning 指的意思是甚麼？<br>
<br>
0:00:03.280,0:00:05.640<br>
Transfer learning 指的意思是說<br>
<br>
0:00:05.640,0:00:14.460<br>
假設你現在手上有一些跟你現在進行的 task 沒有直接相關的 data<br>
<br>
0:00:14.900,0:00:20.780<br>
能不能用這些沒有直接相關的 data，來幫助我們做一些甚麼事情<br>
<br>
0:00:20.780,0:00:25.920<br>
比如說你現在要做的是，貓跟狗的 classifier<br>
<br>
0:00:26.180,0:00:29.200<br>
那所謂的沒有直接相關的 data，是甚麼意思呢？<br>
<br>
0:00:29.340,0:00:32.260<br>
所謂的沒有直接相關，有很多不同的可能<br>
<br>
0:00:33.180,0:00:37.960<br>
一個可能是，input 的 distribution 是類似的<br>
<br>
0:00:37.960,0:00:42.540<br>
比如說，input distribution 一樣是動物的圖片<br>
<br>
0:00:42.540,0:00:47.140<br>
但是 task 裡的 label 是無關的<br>
<br>
0:00:47.140,0:00:51.780<br>
比如說現在你的另外一些 data 其實是要分大象跟老虎<br>
<br>
0:00:51.780,0:00:56.940<br>
input domain 是像的，但是你的 task 是不一樣的<br>
<br>
0:00:57.540,0:01:02.640<br>
還有另外一個可能是，input domain 不一樣，但 task 是一樣的<br>
<br>
0:01:02.640,0:01:08.620<br>
一樣是做貓跟狗的分類，但是圖片是招財貓和高飛狗的圖片<br>
<br>
0:01:08.800,0:01:14.220<br>
跟原來圖片的 distribution 是非常不像的，但要做的task是一樣的<br>
<br>
0:01:14.740,0:01:21.880<br>
transfer learning 要問的問題就是，我們能不能夠在有一些不相干的 data 的情況下<br>
<br>
0:01:21.880,0:01:26.320<br>
然後來幫助我們現在要做的 task<br>
<br>
0:01:26.320,0:01:29.400<br>
為甚麼我們要考慮 transfer learning 這樣的 task 呢？<br>
<br>
0:01:29.400,0:01:31.460<br>
舉例來說，在 speech recognition 裡面<br>
<br>
0:01:31.820,0:01:35.220<br>
可能你現在要做的事情，是台語的語音辨識<br>
<br>
0:01:35.220,0:01:37.900<br>
但是台語的 data 是很少的<br>
<br>
0:01:38.360,0:01:50.020<br>
但其實，語音的 data 是很容易蒐集的，你隨便去 YouTube 爬一下，就可以爬到一大堆英文、中文、其他語言的 data<br>
<br>
0:01:50.180,0:01:56.460<br>
那我們能不能夠用其他語言的 data，來 improve 台語的語音辨識這件事情<br>
<br>
0:01:56.920,0:02:03.000<br>
或者是如果在 image recognition 裡面，或許是現在有興趣的 task 是做 medical 的image<br>
<br>
0:02:03.000,0:02:09.280<br>
你想要讓機器自動診斷說，有沒有 tumor 之類的，這是現在很流行做的事情<br>
<br>
0:02:09.720,0:02:15.680<br>
可是這種 medical的image 其實是很少的，不可能手上有太多這種 image<br>
<br>
0:02:15.940,0:02:18.820<br>
但是實際上 image 的 data 我們永遠都不缺<br>
<br>
0:02:18.820,0:02:20.580<br>
胡亂網路爬就有一大堆 image<br>
<br>
0:02:20.860,0:02:24.660<br>
或 download MNIST，裡面有超過一百萬張 image<br>
<br>
0:02:24.900,0:02:28.080<br>
有這麼多 image，只是不是 medical 的 image<br>
<br>
0:02:28.500,0:02:34.420<br>
這些其他的 image 對你現在要考慮的 task，有沒有可能有幫助<br>
<br>
0:02:34.840,0:02:38.180<br>
或者是在文件的分析上，你現在要分析的文件<br>
<br>
0:02:38.180,0:02:43.300<br>
是某個很 specific 的 domain，比如說你想要分析的是，某種特別的法律的文件<br>
<br>
0:02:43.800,0:02:50.440<br>
那這種法律的文件或許 data 很少，但是假設你可以從網路上 collect 一大堆的 data<br>
<br>
0:02:50.440,0:02:53.920<br>
那這些 data，有沒有可能是有幫助的<br>
<br>
0:02:54.160,0:02:57.220<br>
那其實 transfer learning 這件事情可能嗎？<br>
<br>
0:02:57.540,0:03:02.200<br>
用不相干的 data、來自其他 domain 的 data<br>
來幫助我們現在的 task 是有可能的嗎<br>
<br>
0:03:02.420,0:03:07.720<br>
是有可能的，因為我們在現實生活中，我們都是不斷的在做 transfer learning<br>
<br>
0:03:07.880,0:03:13.760<br>
比如說，你可能是一個研究生，你可能想要知道研究生應該怎麼樣過日子<br>
<br>
0:03:13.900,0:03:17.360<br>
那怎麼辦呢？你就可以參考"爆漫王"這樣子<br>
<br>
0:03:17.360,0:03:25.820<br>
那在"爆漫王"裡面呢，其實漫畫家就是研究生<br>
責編就等同於指導教授<br>
<br>
0:03:26.160,0:03:32.160<br>
漫畫家每周就要畫分鏡去給責編看，跟責編討論<br>
<br>
0:03:32.160,0:03:36.580<br>
每周跟指導教授進度報告一樣，畫分鏡就是跑實驗<br>
<br>
0:03:36.580,0:03:43.140<br>
目標就是要在 jump 上連載，在 jump 連載就是投稿期刊<br>
<br>
0:03:43.140,0:03:45.200<br>
連 word embedding 都知道這件事情<br>
<br>
0:03:45.800,0:03:54.680<br>
雖然我們沒有研究生的守則，但是從"爆漫王"裡面，我們可以知道身為一個研究生<br>
<br>
0:03:54.680,0:03:56.920<br>
應該要做什麼樣的事情<br>
<br>
0:03:57.400,0:04:04.640<br>
你可能會覺得說，拿漫畫來跟神聖的研究做類比，有點不倫不類<br>
<br>
0:04:04.640,0:04:10.240<br>
我跟你說，漫畫家都是用生命來畫漫畫的，其實比我們做研究認真多了<br>
<br>
0:04:10.680,0:04:15.380<br>
我不知道大家有沒有看過"爆漫王"，我簡介一下"爆漫王"的劇情，用三十個字<br>
<br>
0:04:15.380,0:04:20.800<br>
"爆漫王"就是說有兩個高中生，一個叫真城，一個叫高木<br>
<br>
0:04:20.800,0:04:23.720<br>
他們不知道怎麼回事，就很想當漫畫家，後來就當了漫畫家<br>
<br>
0:04:23.720,0:04:34.080<br>
有一次，因為每週都要連載，實在太累了，真城就累到要住院、開刀<br>
<br>
0:04:34.240,0:04:38.900<br>
住院的時候，他手還握著畫筆畫漫畫，大家都阻止他<br>
<br>
0:04:38.900,0:04:44.700<br>
他女朋友來看他，決定要支持他，他女朋友握著他的手繼續畫下去<br>
<br>
0:04:44.700,0:04:49.060<br>
這個就好像說，你今天在做研究的時候，你覺得每周都進度報告太累了<br>
<br>
0:04:49.060,0:04:52.000<br>
譬如說累到生病住院了，其他人都叫你不要再做研究<br>
<br>
0:04:52.340,0:04:55.820<br>
你女朋友來看你，把你的手壓在鍵盤上，繼續寫 code<br>
<br>
0:04:55.820,0:05:05.940<br>
比如說裡面有一個天才漫畫家，叫新妻惠一兒<br>
<br>
0:05:06.160,0:05:11.860<br>
應該在影射尾田榮一郎，新妻惠一兒想要結束他現在的連載<br>
<br>
0:05:11.940,0:05:15.980<br>
跟總編說要結束現在的連載，他跟總編談一個條件<br>
<br>
0:05:15.980,0:05:19.660<br>
我從現在開始接下來十周，我都要是 jump 人氣第一名<br>
<br>
0:05:19.660,0:05:23.740<br>
如果我做到的話你就要讓我結束連載，如果做不到我就繼續畫到死為止<br>
<br>
0:05:23.740,0:05:27.820<br>
叫你跟指導教授說我做這個題目很膩，我想要畢業<br>
<br>
0:05:27.820,0:05:32.260<br>
指導教授說你接下來連發十篇 paper 都拿 best paper award 我就讓你畢業<br>
<br>
0:05:32.860,0:05:35.900<br>
如果做不到就繼續做到死為止，就是這個概念<br>
<br>
0:05:35.900,0:05:42.760<br>
他們是比我們還要認真很多的，所以我覺得研究生都應該看一下"爆漫王"<br>
<br>
0:05:42.760,0:05:52.000<br>
transfer learning 是這樣子的，transfer learning 有很多很多的方法<br>
<br>
0:05:52.000,0:06:04.240<br>
他是很多方法的集合，你以下聽到的 terminology，這個地方有點混亂<br>
<br>
0:06:04.240,0:06:07.580<br>
不同的文獻，他用的詞彙其實是不一樣的<br>
<br>
0:06:07.580,0:06:11.020<br>
有些方法有人說算是 transfer learning<br>
有些方法有人又說不算是 transfer learning<br>
<br>
0:06:11.020,0:06:12.580<br>
這邊是很混亂的<br>
<br>
0:06:12.580,0:06:17.065<br>
如果你看到我跟別人說的不一樣，並不是我錯，或者是別人錯<br>
<br>
0:06:17.065,0:06:18.560<br>
這個地方就是很混亂<br>
<br>
0:06:18.560,0:06:21.420<br>
你只要知道那個方法是甚麼就好了<br>
<br>
0:06:21.420,0:06:28.580<br>
我們講法是我們現在有一個我們想要做的 task<br>
<br>
0:06:28.580,0:06:33.740<br>
有一些跟這個 task 有關的 data，這個叫做 target data<br>
<br>
0:06:33.740,0:06:39.760<br>
我們有一些 data 是跟這個 data 無關的 data，這個 data 叫做 source data<br>
<br>
0:06:39.760,0:06:51.440<br>
target data 他有可能是有 label，也可能沒有 label<br>
source data 也有可能是有 label，也有可能沒有 label<br>
<br>
0:06:51.840,0:06:58.960<br>
所以現在我們會有四種可能，之後我們會分這四種可能來討論<br>
<br>
0:07:15.340,0:07:32.820<br>
Transfer learning 我們可以分成四個不同的象限來討論他<br>
<br>
0:08:09.600,0:08:15.160<br>
我們的 data 分成兩種，一種是 target data，一種是 source data<br>
<br>
0:08:15.500,0:08:22.740<br>
所謂的 target data 是跟我們現在考慮的 task 是直接相關的<br>
<br>
0:08:23.380,0:08:30.360<br>
source data 跟我們現在考慮的 task 沒有直接的關係<br>
<br>
0:08:30.580,0:08:36.420<br>
甚麼叫做"沒有直接的關係"，這個定義是比較模糊的<br>
<br>
0:08:36.420,0:08:38.660<br>
有很多不同的定義方式<br>
<br>
0:08:40.480,0:08:47.440<br>
雖然 input 都是 image，但 image 的 distribution 是很不一樣的<br>
<br>
0:08:47.780,0:08:53.800<br>
比如說一個是真正的實物 image，一個是動畫的 image<br>
<br>
0:08:54.060,0:08:58.800<br>
他們就差很多，那就可以算是和這個 task 沒有直接相關<br>
<br>
0:08:59.260,0:09:06.640<br>
我們可以分成我們的 target data 有可能是有 label 的，有可能是沒有 label 的<br>
<br>
0:09:07.060,0:09:11.460<br>
source data，也可能同時是有 label 的，也可能是沒有 label 的<br>
<br>
0:09:11.600,0:09:17.360<br>
總共有四種可能，分別對這四種可能的情況，介紹些方法<br>
<br>
0:09:18.360,0:09:27.000<br>
我們先來講，假設我們現在我們的 target data 跟 source data 同時都有 label 的情況下<br>
<br>
0:09:27.000,0:09:36.020<br>
我們可以做甚麼事情<br>
最常見的也最簡單的作法就是 fine-tune 你的 model<br>
<br>
0:09:36.020,0:09:39.720<br>
甚麼意思呢？ 我們先來看一下我們現在的 task<br>
<br>
0:09:39.720,0:09:44.900<br>
在我們 task 裡面，我們的 target 和 source data 通通都有 label<br>
<br>
0:09:44.900,0:09:49.800<br>
target data 我們就用上標 t 來表示<br>
<br>
0:09:49.800,0:09:56.700<br>
target data 裡有我們要找的 function 的 input，x 上標 t，跟他的 output，y 上標 t<br>
<br>
0:09:56.700,0:10:04.700<br>
source data 裡有 function 的 input，x 上標 s，跟 y 上標 s<br>
<br>
0:10:04.700,0:10:12.480<br>
通常我們會假設 target data 的量非常少<br>
<br>
0:10:12.480,0:10:17.560<br>
如果 target data 量很多，就當作一般 machine learning 的 problem<br>
<br>
0:10:17.560,0:10:19.620<br>
直接拿 target data 來 train 你的 model 就好了<br>
<br>
0:10:19.620,0:10:21.920<br>
也不需要做 transfer learning<br>
<br>
0:10:21.920,0:10:28.240<br>
在做 transfer learning 的假設是 target data 的量是非常少的<br>
<br>
0:10:28.520,0:10:34.880<br>
而 source data 是很多的，雖然 source data 跟我們的 task 沒有關係<br>
<br>
0:10:34.880,0:10:37.480<br>
但我們想知道說，在 target data 很少的情況下<br>
<br>
0:10:37.480,0:10:43.360<br>
如果有一大堆不相干的 source data<br>
到底有沒有可能會有幫助<br>
<br>
0:10:43.920,0:10:49.000<br>
如果今天 target data 的量非常少，少到只有幾個 example<br>
<br>
0:10:49.000,0:10:50.480<br>
這個叫做 one-shot learning<br>
<br>
0:10:53.000,0:10:59.380<br>
從名字來看好像只有一個 example，所以叫 one-shot<br>
但有時候也不見得只有一個 example 才能稱之為 one-shot<br>
<br>
0:10:59.380,0:11:05.580<br>
反正意思就是如我 target data 的量非常非常少，你可以說你在做 one-shot learning<br>
<br>
0:11:07.320,0:11:12.320<br>
這樣子的 task 有什麼樣的例子呢<br>
<br>
0:11:13.140,0:11:18.280<br>
語音上最典型的例子呢，就是 speaker 的 adaptation<br>
<br>
0:11:18.280,0:11:24.920<br>
甚麼意思呢？我現在的 target data 是某一個人的聲音<br>
<br>
0:11:24.920,0:11:32.580<br>
我們要辨識某一個人的聲音<br>
這個人的聲音不可能太多的label data<br>
<br>
0:11:32.640,0:11:38.560<br>
不可能對這個人的 audio 去做太多的 transcription<br>
只有他非常少量的 transcription<br>
<br>
0:11:38.560,0:11:45.960<br>
他可能只有對你的 machine 說三句話<br>
只有這三句話的 label 而已<br>
<br>
0:11:46.480,0:11:53.480<br>
但 source data 你有一大堆的 audio data 來自於不同人的<br>
這些 audio data 都有 transcription<br>
<br>
0:11:53.540,0:11:58.000<br>
通常這種 data 你要 collect 上萬小時都是有可能的<br>
<br>
0:11:58.000,0:12:05.340<br>
當然不可能直接用 target data、某一個 speaker 他的 data 去 train 語音辨識系統<br>
<br>
0:12:05.340,0:12:15.100<br>
這樣一定會壞掉，你會希望說，好幾萬個小時的 source data 對這個 task 有甚麼幫助<br>
<br>
0:12:16.360,0:12:19.620<br>
這邊的處理方式可以非常直覺，怎麼做呢<br>
<br>
0:12:19.620,0:12:25.100<br>
你就拿你的 source data 直接去 train 一個 model<br>
<br>
0:12:25.800,0:12:34.540<br>
接下來你去 fine-tune 你的 model ，用這個 target data<br>
<br>
0:12:34.700,0:12:38.360<br>
甚麼叫做用 target data 來 fine-tune 你的 model 呢<br>
<br>
0:12:38.360,0:12:43.040<br>
這個想法非常直覺，你就把你在 source data 上 train 出來的 model<br>
<br>
0:12:43.220,0:12:48.960<br>
當作是 training 的 initial value ，當作是初始值<br>
<br>
0:12:48.960,0:12:52.480<br>
然後再用 target data train 下去就結束了<br>
<br>
0:12:53.680,0:13:02.080<br>
但是這邊可能會遇到的 challenge 是 source data 真的非常少<br>
<br>
0:13:02.080,0:13:06.300<br>
今天就算在 target data 上 train 出一個好的 model<br>
<br>
0:13:06.300,0:13:10.420<br>
當你把它當 model 的 initialization<br>
<br>
0:13:10.420,0:13:14.060<br>
在 source data 再去做 training 可能 train 下去就壞掉<br>
<br>
0:13:14.340,0:13:19.660<br>
所以在 train 的時候要很小心<br>
有很多不同的技巧<br>
<br>
0:13:19.920,0:13:24.020<br>
有一個技巧叫 conservative training<br>
<br>
0:13:24.020,0:13:27.460<br>
conservative training 是說現在有大量的 source data<br>
<br>
0:13:27.460,0:13:33.780<br>
在語音辨識裡面，他就是很多不同 speaker 的聲音都有 transcription<br>
<br>
0:13:33.780,0:13:37.440<br>
可以去 train 一個拿來做語音辨識的 neural network<br>
<br>
0:13:37.780,0:13:40.400<br>
接下來呢，你有 target data<br>
<br>
0:13:40.400,0:13:43.680<br>
target data 是某個 speaker 的聲音跟 transcription<br>
<br>
0:13:43.680,0:13:46.460<br>
你可能只有五句、十句這麼多而已<br>
<br>
0:13:46.460,0:13:51.060<br>
怎麼辦呢？ 如果你直接拿這些 target data 去 train<br>
<br>
0:13:51.060,0:13:57.920<br>
Train 下去就壞掉<br>
你可以說我在 training 的時候，下一些 constrain<br>
<br>
0:13:57.920,0:14:03.060<br>
讓 training 的時候，train 完了新的 model 跟舊的 model 不要差太多<br>
<br>
0:14:03.060,0:14:08.280<br>
我下一個 constrain，下一個 constrain 其實就是加一個 regularization<br>
<br>
0:14:08.280,0:14:12.740<br>
我們在 training 的時候，你會加L1、L2的 regularization<br>
<br>
0:14:12.740,0:14:16.100<br>
在 conservative training 裡面，你會加另外一種不同的 regularization<br>
<br>
0:14:16.100,0:14:24.665<br>
新的 model 的 output 跟舊的 model 的 output 在看到同一筆 data 的時候<br>
<br>
0:14:24.665,0:14:27.520<br>
他們的 output 越接近越好<br>
<br>
0:14:27.520,0:14:33.360<br>
或者是下 constrain 是新的 model 跟舊的 model<br>
<br>
0:14:33.360,0:14:36.340<br>
他們的 L2 norm 差距越小越好<br>
<br>
0:14:36.340,0:14:43.000<br>
總之就是做一些事情，讓新的 model 跟舊的 model 差距不要太大<br>
<br>
0:14:43.000,0:14:46.060<br>
這樣就可以防止 overfitting<br>
<br>
0:14:46.220,0:14:51.020<br>
防止 target data 只有一點點，train 下去就壞掉的情形<br>
<br>
0:14:52.200,0:14:54.900<br>
另外一個方法是 layer的transfer<br>
<br>
0:14:55.080,0:14:58.080<br>
現在有用 source data train 好的一個 model<br>
<br>
0:14:58.080,0:15:01.300<br>
你把這 model 裡的某幾個 layer 拿出來<br>
<br>
0:15:01.300,0:15:06.380<br>
直接 copy 到新的 model 裡面去<br>
<br>
0:15:06.380,0:15:09.640<br>
你把某幾個 layer 直接 copy 到新的 model 裡面去<br>
<br>
0:15:09.900,0:15:17.780<br>
接下來你用你的 source data ( 應該是target data ) 只去 train 沒有 copy 的 layer<br>
<br>
0:15:17.780,0:15:19.980<br>
可能只有保留一個 layer 是沒有 copy 的<br>
<br>
0:15:19.980,0:15:22.380<br>
source data ( 應該是target data ) 就只 train 那個 layer 就好<br>
<br>
0:15:22.580,0:15:28.640<br>
這樣的好處就是 source data ( 應該是target data ) 只需要考慮非常少的參數<br>
<br>
0:15:28.740,0:15:30.380<br>
就可以避免 overfitting 的情形<br>
<br>
0:15:30.620,0:15:34.380<br>
當然如果 source data ( 應該是target data ) 夠多了<br>
<br>
0:15:34.380,0:15:39.920<br>
我還是要 fine-tune 整個 model，也是可以的<br>
<br>
0:15:41.780,0:15:48.880<br>
layer transfer 是一個非常常見的小技巧<br>
<br>
0:15:48.880,0:15:58.540<br>
現在可能問題是；那些 layer 應該被 transfer，那些 layer 不應該被 transfer 呢<br>
<br>
0:15:58.680,0:16:06.220<br>
有趣的是在不同的 task 上面，需要被 transfer 的 layer 往往是不一樣的<br>
<br>
0:16:06.220,0:16:13.060<br>
比如說，在語音辨識上，通常是 copy 最後幾層<br>
<br>
0:16:13.060,0:16:18.600<br>
然後重新 train input 那一層<br>
<br>
0:16:18.600,0:16:31.400<br>
在語音上你可想成每一個人用同樣的發音方式，但因為口腔結構略有差異<br>
<br>
0:16:31.400,0:16:35.780<br>
同樣的發音方式，得到的聲音是不一樣的<br>
<br>
0:16:35.780,0:16:45.580<br>
那 neural network 前幾層是從聲音訊號裡面，得知說話的人的發音方式<br>
<br>
0:16:45.580,0:16:52.100<br>
再根據發音方式，他就可以得到現在說的是哪一個詞彙，就可以得到辨識的結果<br>
<br>
0:16:52.100,0:16:57.280<br>
從這個角度來看，從發音方式到辨識結果，neural network 的後面幾層<br>
<br>
0:16:57.280,0:17:07.000<br>
是跟說話的人沒有關係的，所以是可以被 copy 的<br>
<br>
0:17:07.000,0:17:12.060<br>
不一樣的是，從聲音訊號，到發音方式這一段<br>
<br>
0:17:12.060,0:17:13.560<br>
可能是每個人都不一樣的<br>
<br>
0:17:13.860,0:17:23.920<br>
所以在做語音辨識的時候，常見的作法是把 neural network 的後幾層是 copy 的<br>
<br>
0:17:24.060,0:17:32.080<br>
只有前面可能第一層用 target data、某一個 speaker 的 data train<br>
<br>
0:17:32.900,0:17:36.320<br>
但是在 image 的時候，我發現是不一樣的<br>
<br>
0:17:36.320,0:17:43.260<br>
因為在 image 的時候，通常是 copy 前面幾層，只 train 最後幾層<br>
<br>
0:17:43.560,0:17:53.080<br>
為甚麼呢？因為在 image 你會發現當你在 source domain 上 learn 了一個 network<br>
<br>
0:17:53.080,0:17:57.010<br>
你 learn 到的 CNN 通常前幾層他做的<br>
<br>
0:17:57.010,0:18:00.940<br>
就是 detect 最簡單的 pattern<br>
<br>
0:18:01.060,0:18:09.900<br>
比如說前幾層，做的事情就是 detect 有沒有直線橫線，或者是有沒有簡單的幾何圖形<br>
<br>
0:18:09.900,0:18:13.540<br>
所以在 image 上面，neural network 前幾層 learn 的東西<br>
<br>
0:18:13.540,0:18:20.360<br>
他是可以被 transfer 到其他的 task 上面<br>
<br>
0:18:20.360,0:18:24.340<br>
而最後幾層 learn 的東西往往比較 abstract<br>
<br>
0:18:24.340,0:18:30.180<br>
這個比較 abstract 他就沒有辦法 transfer 到其他的 task 上面去<br>
<br>
0:18:30.180,0:18:37.080<br>
在做影像處理、辨識的時候，反而是 copy 前面幾層<br>
<br>
0:18:37.080,0:18:41.600<br>
然後後面幾層重 train<br>
到底哪些 layer 要被 transfer<br>
<br>
0:18:41.600,0:18:45.920<br>
其實是 case by case<br>
特別是運用之妙，存乎一心<br>
<br>
0:18:47.400,0:18:54.340<br>
這邊是 image 在 layer transfer 上的實驗<br>
<br>
0:18:54.340,0:18:59.940<br>
這是出自 Bengio 在 NIPS,2014 的 paper<br>
<br>
0:18:59.940,0:19:09.620<br>
這個實驗做在 ImageNet 上，這個實驗他把 ImageNet 的 corpus<br>
<br>
0:19:09.620,0:19:16.720<br>
一百二十萬張 image 分成 source 跟 target<br>
<br>
0:19:16.720,0:19:22.500<br>
這個分法是按照 class 來分的<br>
我們知道 ImageNet 的 image<br>
<br>
0:19:22.980,0:19:25.180<br>
一個 typical 的 setup 是有一千個 class<br>
<br>
0:19:25.180,0:19:28.760<br>
把其中五百個 class 歸為 source data<br>
<br>
0:19:28.760,0:19:32.300<br>
把另外五百個 class 歸為 target data<br>
<br>
0:19:33.100,0:19:42.960<br>
橫軸是我們在做 transfer learning 的時候，copy 了幾個 layer<br>
<br>
0:19:42.960,0:19:51.300<br>
Copy 0 個 layer 就代表完全沒有做 transfer learning<br>
<br>
0:19:51.300,0:19:57.120<br>
這是一個 baseline ，就直接在 target data 上面 train 下去<br>
<br>
0:19:57.120,0:20:01.800<br>
縱軸是 top-1 accuracy，所以是越高越好<br>
<br>
0:20:03.200,0:20:06.040<br>
直接 train 下去的結果是白色這個點<br>
<br>
0:20:06.040,0:20:08.440<br>
沒有做 transfer learning 是白色這個點<br>
<br>
0:20:08.700,0:20:15.800<br>
如果今天 copy 前面幾個 layer，只有 train 最後幾個 layer 的時候<br>
<br>
0:20:15.800,0:20:19.640<br>
如果今天 copy 第一個 layer，train 剩下的 layer<br>
<br>
0:20:19.640,0:20:24.520<br>
Copy 前面兩個、三個 layer，在 source data 上 train 一個 model<br>
<br>
0:20:24.520,0:20:29.160<br>
然後 copy 第一個 layer，或 copy 第二個，copy 第三個，一直到 copy 前面七個 layer<br>
<br>
0:20:29.160,0:20:37.080<br>
然後剩下的 layer 再用 target data 去 train，會得到什麼樣的結果<br>
<br>
0:20:37.080,0:20:43.720<br>
今天它的結果是會變差的<br>
<br>
0:20:43.980,0:20:47.520<br>
但是如果只有 copy 前面幾個 layer 的時候<br>
<br>
0:20:47.520,0:20:51.240<br>
只有 copy 第一個 layer 的時候，performance 稍微有點進步<br>
<br>
0:20:51.240,0:20:57.400<br>
不過 copy 前面兩個 layer，performance 幾乎是持平的<br>
<br>
0:20:57.400,0:21:00.740<br>
但是 copy 的 layer 太多，結果是會壞掉<br>
<br>
0:21:01.440,0:21:07.960<br>
這個實驗顯示說在不同的 data 上面，train 出來的 neural network<br>
<br>
0:21:07.960,0:21:10.980<br>
前面幾個 layer 是可以共用的<br>
<br>
0:21:10.980,0:21:13.820<br>
後面幾個可能是沒有辦法共用的<br>
<br>
0:21:13.820,0:21:22.980<br>
上面這條橙色的線是說，如果 copy 完以後，還有 fine-tune 整個 model 的話<br>
<br>
0:21:22.980,0:21:27.620<br>
把第一個 layer，在 source domain 上 train 一個 model<br>
<br>
0:21:27.620,0:21:32.180<br>
然後把第一個 layer copy 過去以後，再用 target domain fine-tune 整個 model<br>
<br>
0:21:32.180,0:21:34.460<br>
包括前面 copy 過的 layer 的話<br>
<br>
0:21:34.460,0:21:37.460<br>
那得到 performance 是橙色這條線<br>
<br>
0:21:37.460,0:21:43.680<br>
在所有的 case 上面都是有進步的<br>
其實這個結果滿 surprised<br>
<br>
0:21:43.680,0:21:47.560<br>
不要忘了，這可是 ImageNet 的 corpus<br>
<br>
0:21:47.560,0:21:52.900<br>
一般在做 transfer learning 的時候，都是假設 target domain 的 data 非常少<br>
<br>
0:21:52.900,0:21:58.640<br>
這邊 target domain 可是有六十萬張，你可能都沒有處理過這麼多的 image<br>
<br>
0:21:58.640,0:22:01.520<br>
這 target domain 的 data 是非常多的<br>
<br>
0:22:01.720,0:22:08.640<br>
但是就算在這個情況下，再多加了另外六十張 image 做 transfer learning<br>
<br>
0:22:08.640,0:22:13.260<br>
其實還是有幫助的<br>
<br>
0:22:13.840,0:22:22.340<br>
這兩條藍色的線跟 transfer learning 比較沒有關係<br>
不過是這篇paper裡面發現一個有趣的現象<br>
<br>
0:22:24.580,0:22:27.860<br>
假設呢，假設呢<br>
他想要做一個對照組<br>
<br>
0:22:27.860,0:22:33.460<br>
在 target domain 上面 learn 一個 model<br>
<br>
0:22:33.460,0:22:40.500<br>
把前幾個 layer copy 起來<br>
<br>
0:22:40.920,0:22:47.560<br>
再用一次 target domain 的 data train 剩下幾個 layer<br>
<br>
0:22:47.560,0:22:52.780<br>
前面幾個 layer 就 fix 住，只 train 後面幾個 layer<br>
<br>
0:22:53.620,0:23:01.260<br>
直覺上這樣做應該跟直接 train 整個 model 沒有太大差別<br>
<br>
0:23:01.260,0:23:06.280<br>
但最後發現假設你 fix 前面幾個 layer<br>
<br>
0:23:06.280,0:23:10.580<br>
先 train 好一個 model，fix 前面幾個 layer<br>
<br>
0:23:10.580,0:23:13.100<br>
接下來只 train 後面幾個 layer<br>
<br>
0:23:13.140,0:23:15.240<br>
接下來重新 train 後面幾個 layer<br>
<br>
0:23:15.240,0:23:20.120<br>
結果有些時候是會壞掉的<br>
<br>
0:23:20.120,0:23:26.600<br>
他的理由是 training 的時候，前面的 layer 跟後面的 layer<br>
<br>
0:23:26.600,0:23:28.580<br>
他們其實是要互相搭配<br>
<br>
0:23:28.580,0:23:34.980<br>
所以如果只 copy 前面的 layer，然後只 train 後面的 layer<br>
<br>
0:23:34.980,0:23:37.360<br>
其實 performance 會是比較差的<br>
<br>
0:23:37.600,0:23:42.420<br>
後面的 layer 就沒有辦法跟前面的 layer 互相搭配，結果有點差<br>
<br>
0:23:42.420,0:23:48.440<br>
如果可以 fine-tune 整個 model 的話，performance 就跟有沒有 transfer learning 是一樣的<br>
<br>
0:23:48.440,0:23:54.080<br>
這是另一個有趣的發現，作者自己對這件事情是滿 surprised<br>
<br>
0:23:54.080,0:23:55.900<br>
這是另外一個實驗結果<br>
<br>
0:23:55.900,0:24:04.880<br>
這個實驗結果是<br>
紅色這條線是前一頁看到的紅色的這條線<br>
<br>
0:24:04.880,0:24:11.260<br>
這邊假設 source 跟 target 是比較沒有關係的<br>
<br>
0:24:11.260,0:24:16.620<br>
把 ImageNet 的 corpus 分成 source data 跟 target data 的時候<br>
<br>
0:24:16.620,0:24:25.680<br>
把自然界的東西，通通當作 source<br>
<br>
0:24:25.680,0:24:29.040<br>
target 通通是人造的東西，桌子、椅子等等<br>
<br>
0:24:29.040,0:24:32.360<br>
這樣 transfer learning 會有什麼樣的影響<br>
<br>
0:24:32.360,0:24:38.040<br>
如果 source 跟 target 的 data 是差很多的<br>
<br>
0:24:38.040,0:24:43.760<br>
在做 transfer learning 的時候，performance 會掉的比較多<br>
<br>
0:24:43.760,0:24:47.740<br>
前面幾個 layer 影響還是比較小的<br>
<br>
0:24:47.740,0:24:58.880<br>
如果只 copy 前面幾個 layer，仍然跟沒有 copy 是持平的<br>
<br>
0:24:58.880,0:25:03.200<br>
這意味著，就算是 source domain 跟 target domain 是非常不一樣的<br>
<br>
0:25:03.200,0:25:06.160<br>
一邊是自然的東西，一邊是人造的東西<br>
<br>
0:25:06.160,0:25:11.040<br>
在 neural network 第一個 layer，他們仍然做的事情很有可能是一樣的<br>
<br>
0:25:11.760,0:25:17.960<br>
綠色的這條線，爛掉的這條線是假設前面幾個 layer 的參數是 random 的<br>
<br>
0:25:17.960,0:25:23.960<br>
會發生甚麼事情呢？得到的結果就是爛掉，這是一個 baseline<br>
<br>
0:25:25.140,0:25:30.720<br>
另外一個要講的是 multitask learning<br>
<br>
0:25:30.720,0:25:39.280<br>
multitask learning 跟 fine-tune 略有不同的地方就是<br>
<br>
0:25:39.820,0:25:45.260<br>
在 fine-tune 裡面我們 care 的是 target domain 做得好不好<br>
<br>
0:25:45.300,0:25:46.980<br>
在 source domain 上 learn 一個 model<br>
<br>
0:25:47.180,0:25:49.540<br>
在 target domain 上 fine-tune<br>
<br>
0:25:49.540,0:25:53.640<br>
care 的是 target domain 做得好不好，fine-tune 以後 source domain 壞掉就算了<br>
<br>
0:25:53.960,0:26:00.300<br>
在 multitask learning 裡面，其實同時 care target domain 跟 source domain 做得好不好<br>
<br>
0:26:00.300,0:26:05.120<br>
care 在這兩個 domain 上，能不能夠同時把它做好<br>
<br>
0:26:05.120,0:26:15.160<br>
如果是用 deep learning base 方法的話，特別適合來做這種 multitask 的 learning<br>
<br>
0:26:15.880,0:26:24.080<br>
可以丟一個 neural network<br>
假設兩個不同的 task，他們用的是同樣的 feature<br>
<br>
0:26:24.080,0:26:30.040<br>
比如說，都做影像辨識，只是影像辨識的 class 不一樣的話<br>
<br>
0:26:30.040,0:26:37.380<br>
我就 learn 一個 neural network，input 就是兩個不同 task 同樣的 feature<br>
<br>
0:26:37.500,0:26:44.760<br>
但是中間會分叉出來，一部分的 network  他的 output 是 task A 的答案<br>
<br>
0:26:44.760,0:26:48.060<br>
一部分的 network 他的 output 是 task B 的答案<br>
<br>
0:26:48.060,0:26:54.720<br>
這麼做的好處是，task A 跟 task B 他們在前面幾個 layer 會是共用的<br>
<br>
0:26:54.720,0:27:01.200<br>
在前面幾個 layer，會同時使用 task A 跟 task B 的 data 去同時 train 前面幾個layer<br>
<br>
0:27:01.200,0:27:06.640<br>
前面幾個 layer 是用比較多的 data train 的，所以可能有比較好的 performance<br>
<br>
0:27:06.640,0:27:14.620<br>
做這件事的前提就是，要確定這兩個 task 有沒有共通性，是不是可以共用前面幾個 layer<br>
<br>
0:27:15.260,0:27:24.460<br>
有一些更 crazy 的 task，現在連 input 都是沒有辦法 share 的<br>
<br>
0:27:24.540,0:27:32.200<br>
今天兩種不同的 task，不同的 input 都用不同的 neural network<br>
<br>
0:27:32.340,0:27:38.420<br>
把它 transform 到同一個 domain 上面去<br>
在同一個 domain 上，再 apply 不同的 neural network<br>
<br>
0:27:38.720,0:27:47.480<br>
一條路去做 task A，一條路去做 task B<br>
中間可能有某幾個 layer 是 share 的<br>
<br>
0:27:47.540,0:27:52.120<br>
如果在這樣的 task 下，也可以做 transfer learning<br>
<br>
0:27:52.120,0:27:54.320<br>
就算是 task A、task B 的 input、output 完全不一樣<br>
<br>
0:27:54.320,0:27:58.360<br>
如果覺得中間幾個 layer 他們有共同的地方<br>
<br>
0:27:58.360,0:28:03.240<br>
還是可以用這樣子的 model 架構來處理<br>
<br>
0:28:03.780,0:28:09.640<br>
Multitask learning 一個很成功的例子是多語言的語音辨識<br>
<br>
0:28:09.640,0:28:15.320<br>
假設現在有一大堆不同語言的 data<br>
<br>
0:28:15.320,0:28:20.080<br>
假設有法文、德文、西班牙文、義大利文還有中文<br>
<br>
0:28:20.080,0:28:25.520<br>
在 train model 的時候，train 你拿來做語音辨識的 neural network 的時候<br>
<br>
0:28:25.520,0:28:31.240<br>
可以 train 一個 model，它同時可以辨識這五種不同的語言<br>
<br>
0:28:31.240,0:28:34.240<br>
而這個 model 前面幾個 layer 他們會共用參數<br>
<br>
0:28:34.240,0:28:38.840<br>
後面幾個 layer，每個語言可能有自己的參數<br>
<br>
0:28:38.840,0:28:48.780<br>
這麼做是合理的，因為雖然是不同的語言，但都是人類說的<br>
<br>
0:28:49.260,0:28:56.440<br>
前面幾個 layer 就可以 share 同樣的資訊、可以共用同樣的參數<br>
<br>
0:28:56.440,0:29:00.820<br>
其實在 translation 也可以用同樣的事情<br>
<br>
0:29:00.820,0:29:09.400<br>
假設要做中翻英、中文翻日文，那可以把這兩個 model 一起 train<br>
<br>
0:29:09.400,0:29:14.160<br>
在一起 train 的時候，在中翻英、中文翻日文<br>
<br>
0:29:14.160,0:29:18.620<br>
都要先把中文的 data 做process<br>
那把中文的 data 先做process<br>
<br>
0:29:18.620,0:29:24.740<br>
那一部分的 neural network 就可以是兩種不同語言的 data 共同使用<br>
<br>
0:29:24.740,0:29:33.400<br>
這個 transfer 到底可以 transfer 的多廣？<br>
<br>
0:29:33.800,0:29:42.380<br>
比如說德文法文都是同樣語系的語言<br>
所以他們是可以 transfer 的<br>
<br>
0:29:42.380,0:29:48.340<br>
但這些歐洲的語言跟中文或許是不能 transfer 的<br>
<br>
0:29:48.340,0:29:54.240<br>
目前在語音的發現是，幾乎所有的語言都可以 transfer<br>
<br>
0:29:54.620,0:30:02.280<br>
過去還有人蒐集了十幾種語言，把他們互相之間兩兩做 transfer<br>
<br>
0:30:02.280,0:30:06.420<br>
做了一個很大的 N x N 的 table，每一個 case 都有進步<br>
<br>
0:30:06.780,0:30:14.460<br>
目前發現大部分的 case，不同人類的語言<br>
就算覺得他們不是非常像，他們也都可以互相 transfer<br>
<br>
0:30:14.960,0:30:23.440<br>
這邊舉的例子是從歐洲的語言 transfer 到中文上面<br>
<br>
0:30:23.440,0:30:29.240<br>
這邊中文的 data 不是太多，可能是十小時<br>
<br>
0:30:29.680,0:30:36.940<br>
橫軸是中文 training 的 data<br>
<br>
0:30:36.940,0:30:41.020<br>
縱軸是辨識的 character 的 error rate<br>
<br>
0:30:41.300,0:30:46.500<br>
一開始 data 很少，假設只有用中文直接 train 一個 model<br>
<br>
0:30:46.500,0:30:50.220<br>
現在 data 很少，Error rate 當然就很大，當 data 越來越多<br>
<br>
0:30:50.220,0:30:54.320<br>
到一百多小時的時候，error rate 就可以壓到 30 以下<br>
<br>
0:30:54.320,0:31:03.960<br>
今天如果有一大堆的歐洲語言，把歐洲語言跟中文一起做 multitask training<br>
<br>
0:31:03.960,0:31:09.620<br>
用這些歐洲語言的 data，來幫助中文 model 的前面幾層，讓他 train 得更好<br>
<br>
0:31:09.620,0:31:12.960<br>
會發現就算是中文 data 很少的情況下<br>
<br>
0:31:12.960,0:31:17.280<br>
有做 transfer learning，可以得到比較好的 performance<br>
<br>
0:31:17.280,0:31:20.720<br>
當中文的 data 越多的時候，中文本身的 performance 越好<br>
<br>
0:31:20.720,0:31:26.340<br>
就算是中文有一百小時的 data，借用一些歐洲語言來的 knowledge<br>
<br>
0:31:26.340,0:31:30.420<br>
對這個辨識還是有微幅的幫助的<br>
<br>
0:31:31.060,0:31:35.260<br>
這邊的好處是，假設做 multitask learning 的時候<br>
<br>
0:31:35.260,0:31:43.200<br>
會發現有一百多個小時，跟你只有這邊大概是五十個小時以內<br>
<br>
0:31:43.480,0:31:48.360<br>
如果有做 transfer learning 的話，你只需要二分之一以下的 data<br>
<br>
0:31:48.360,0:31:52.440<br>
就可以跟原來有兩倍的 data 做的一樣好<br>
<br>
0:31:53.600,0:31:59.920<br>
常常有人擔心說， transfer learning 會不會有負面的效應<br>
<br>
0:31:59.920,0:32:02.160<br>
會不會有 negative 的 transfer 的結果<br>
<br>
0:32:02.220,0:32:08.100<br>
是有可能的，如果兩個 task 不像的話<br>
Transfer 就是 negative 的<br>
<br>
0:32:08.100,0:32:13.300<br>
兩個 task 不像，讓 neural network 同時做兩個 task<br>
反而把結果弄糟了<br>
<br>
0:32:13.940,0:32:22.560<br>
有人總會思考，兩個 task 之間到底能不能夠 transfer<br>
然後 try and error ，這樣很浪費時間<br>
<br>
0:32:22.560,0:32:32.860<br>
所以有人 propose 這個 progressive neural networks<br>
這個 progressive neural networks 其實是很新的作法<br>
<br>
0:32:32.860,0:32:38.520<br>
這個是 16 年的放在 arXiv 上的 paper<br>
會發現它是有很多問題的<br>
<br>
0:32:38.520,0:32:45.980<br>
這個方法是這樣子<br>
現在有個 Task 1，先 train 一個 Task 1 的 neural network<br>
<br>
0:32:45.980,0:32:50.920<br>
藍色這個 neural network<br>
Train 好以後，它的參數就 fix 住了<br>
<br>
0:32:50.920,0:32:59.320<br>
現在要做 Task 2，一樣有一個 neural network<br>
<br>
0:32:59.320,0:33:20.900<br>
但是 Task 2 它的每一個 hidden layer 都會去接前面 Task 1 某一個 hidden layer 的<br>
 output<br>
<br>
0:33:21.700,0:33:38.000<br>
所以在 training 的時候，它的好處是就算 Task 1 跟 Task 2 非常的不像<br>
<br>
0:33:38.000,0:33:42.720<br>
首先，Task 2 的 data 不會去動到 Task 1 的 model<br>
<br>
0:33:42.720,0:33:46.460<br>
所以 Task 1 一定不會比原來更差<br>
<br>
0:33:46.460,0:34:09.760<br>
再來 Task 2 他去借用 Task 1 的參數，但是它可以把這些參數直接設成 0<br>
<br>
0:34:10.040,0:34:18.020<br>
這樣子也不會影響 Task 2 的 performance<br>
最糟的情況下，就跟自己 train 的 performance 是差不多的<br>
<br>
0:34:18.380,0:34:21.880<br>
如果有 Task 3，也就做一樣的事情<br>
<br>
0:34:21.880,0:34:35.500<br>
Task 3會同時從 Task 1 和 Task 2 的 hidden layer 得到 information<br>
<br>
0:34:37.120,0:34:41.020<br>
你可能會覺得這個 model 怪怪的<br>
<br>
0:34:41.020,0:34:46.040<br>
如果有五個 Task，那第五個 Task 不是要同時接前面四個 Task 嗎？<br>
<br>
0:34:46.040,0:34:52.480<br>
對，他是怪怪的。作者也有說怪怪的，等待大家提出更好的想法<br>
<br>
0:34:57.000,0:35:00.540<br>
假設 target data 是 unlabeled<br>
<br>
0:35:00.580,0:35:05.600<br>
而 Source data 是 labeled 時候<br>
<br>
0:35:05.600,0:35:07.160<br>
這邊的 task 是這樣<br>
<br>
0:35:07.760,0:35:13.860<br>
在 source data 有 function 的 input，也有 function 的 output<br>
<br>
0:35:13.860,0:35:17.880<br>
但在 target data 只有 function 的 input，沒有 function 的 output<br>
<br>
0:35:17.880,0:35:24.700<br>
舉例來說，source data 是 MNIST 的 image<br>
<br>
0:35:24.700,0:35:29.060<br>
Target data 是另外一個 corpus，MNIST-M 的 image<br>
<br>
0:35:29.060,0:35:37.400<br>
把 MNIST 的 image 加上一些奇怪的顏色跟背景<br>
<br>
0:35:37.600,0:35:39.380<br>
MNIST 是有label的<br>
<br>
0:35:39.380,0:35:42.260<br>
我們知道每一張 image 對應到哪個 digit<br>
<br>
0:35:42.260,0:35:45.720<br>
但 MNIST-M 沒有 label<br>
<br>
0:35:45.720,0:35:52.260<br>
在這種情況下，通常把 source data 視作 training data<br>
<br>
0:35:52.260,0:35:57.320<br>
把 target data 視作 testing data<br>
<br>
0:35:57.320,0:36:10.320<br>
這邊的問題是 training data 跟 testing data 是非常的 mismatch<br>
<br>
0:36:10.320,0:36:14.680<br>
在這種 source data，MNIST 上 train 出來的一個 model<br>
<br>
0:36:14.680,0:36:20.620<br>
直接 apply 到另外一個 corpus 上面，他也 work 的<br>
<br>
0:36:20.620,0:36:25.980<br>
雖然另外一個 corpus 要做的事情，也是辨識數字零到九<br>
<br>
0:36:25.980,0:36:29.420<br>
但他們 input 的 image 是非常不一樣的<br>
<br>
0:36:29.420,0:36:35.320<br>
要怎麼把 source data 上 learn 出來的 model 也可以 apply target data 上面呢？<br>
<br>
0:36:35.320,0:36:38.960<br>
如果直接 learn 一個 model<br>
<br>
0:36:38.960,0:36:40.160<br>
Input 是一張 image<br>
<br>
0:36:40.160,0:36:44.820<br>
不管 mismatch training 跟 testing 也沒有關係<br>
<br>
0:36:44.820,0:36:47.240<br>
直接 learn 一個 model 下去看看會怎樣<br>
<br>
0:36:47.420,0:36:51.320<br>
會發現結果是會爛掉的<br>
<br>
0:36:51.320,0:36:54.540<br>
如果把一個 neural network 當作是一個 feature 的 extractor<br>
<br>
0:36:54.540,0:37:01.560<br>
Neural network 的前面幾層可以看作是在抽 feature<br>
後面幾層可以看作是在做 classification<br>
<br>
0:37:01.560,0:37:08.720<br>
如果把 neural network 的前面幾層看作是在抽 feature 的話<br>
把 feature 拿來看會發現甚麼事呢<br>
<br>
0:37:08.720,0:37:16.500<br>
會發現不同 domain 的 data，他的 feature 完全不一樣<br>
<br>
0:37:16.500,0:37:23.420<br>
如果把 MNIST 的 feature 丟進去的話，是藍色的這些點<br>
<br>
0:37:23.420,0:37:29.860<br>
藍色的這些點很明顯分成十群<br>
<br>
0:37:29.860,0:37:31.540<br>
零到九，十個數字<br>
<br>
0:37:31.900,0:37:37.880<br>
但如果把另外一個 corpus 的 image 丟進去的話<br>
<br>
0:37:37.880,0:37:40.600<br>
會發現抽出來的 feature 就是紅色這群<br>
<br>
0:37:40.940,0:37:44.820<br>
這邊有把 t-SNE 降維以後的結果<br>
<br>
0:37:44.820,0:37:47.560<br>
反正做出來就是紅色這一群<br>
<br>
0:37:47.560,0:37:50.880<br>
會發現作 feature extraction 的時候<br>
<br>
0:37:50.880,0:37:54.120<br>
原來 source domain 的 image 跟 target domain 的 image<br>
<br>
0:37:54.120,0:37:57.740<br>
他們根本就不在同一個位置裡面<br>
<br>
0:37:57.740,0:37:59.160<br>
抽出來的 feature 完全不一樣<br>
<br>
0:37:59.160,0:38:03.140<br>
後面的 classifier 雖然可以把藍色的部分做好<br>
<br>
0:38:03.140,0:38:05.740<br>
但紅色的部分就無能為力<br>
<br>
0:38:05.740,0:38:15.900<br>
這邊希望做到前面 feature extractor<br>
可以把 domain 的特性去除掉<br>
<br>
0:38:15.900,0:38:23.100<br>
這招叫 domain-adversarial training<br>
等一下會講為甚麼出現 adversarial 這個字<br>
<br>
0:38:23.120,0:38:27.720<br>
之前已經看過 adversarial 這個字<br>
在前面講 GAN 的時候<br>
<br>
0:38:27.720,0:38:32.700<br>
GAN 就是 generative adversarial 的 model<br>
這邊又出現 adversarial<br>
<br>
0:38:32.700,0:38:43.920<br>
這邊 adversarial 跟 GAN 的原理是非常像的<br>
希望 feature extractor 可以把 domain 的特性消掉<br>
<br>
0:38:43.920,0:38:46.400<br>
也就是 feature extractor 的 output<br>
<br>
0:38:46.400,0:38:50.520<br>
不應該是紅色跟藍色的點分成兩群<br>
<br>
0:38:50.520,0:38:53.480<br>
不同的 domain 不應該是分成兩群<br>
<br>
0:38:53.480,0:38:56.680<br>
而是不同的 domain 都應該被混合在一起<br>
<br>
0:38:57.040,0:39:01.520<br>
希望 feature extractor 的 output 是可以把不同 domain 的 image 混在一起<br>
<br>
0:39:01.520,0:39:04.160<br>
也就是把不同 domain 的特性消掉<br>
<br>
0:39:04.440,0:39:06.900<br>
怎麼 learn 這樣的 feature extractor 呢<br>
<br>
0:39:06.900,0:39:11.180<br>
在後面接一個 domain 的 classifier<br>
<br>
0:39:11.180,0:39:15.680<br>
把 feature extractor 的 output 丟給 domain classifier<br>
<br>
0:39:15.680,0:39:20.620<br>
Domain classifier 也是一個 classification 的 task<br>
<br>
0:39:20.780,0:39:25.180<br>
他要做的是根據 feature extractor 給他的 feature<br>
<br>
0:39:25.180,0:39:27.060<br>
判斷這個 feature 來自於哪個 domain<br>
<br>
0:39:27.060,0:39:32.980<br>
在這個 task 就是要分辨這些 feature 來自於 MNIST 的 corpus<br>
<br>
0:39:32.980,0:39:36.720<br>
還是來自於 MNIST-M 的 corpus<br>
<br>
0:39:37.780,0:39:41.440<br>
這件事情就是有一個 generator 的 output<br>
<br>
0:39:41.440,0:39:46.760<br>
然後有一個 discriminator 讓他的架構非常像 GAN<br>
<br>
0:39:47.400,0:39:51.300<br>
但是跟 GAN 不一樣的是<br>
之前在 GAN 的 task 裡<br>
<br>
0:39:51.300,0:39:56.740<br>
Generator 要產生一張 image 騙過 discriminator<br>
這件事很難<br>
<br>
0:39:57.120,0:40:00.740<br>
但是在 domain-adversarial training 裡面<br>
<br>
0:40:00.740,0:40:03.240<br>
要騙過 domain classifier 太簡單了<br>
<br>
0:40:04.740,0:40:11.280<br>
有一個 solution 是不管看到甚麼東西，output 都 0<br>
就騙過 classifier 了<br>
<br>
0:40:11.280,0:40:16.240<br>
所以只 train domain classifier 是不夠的<br>
<br>
0:40:16.240,0:40:18.660<br>
因為 feature extractor 可以輕易的騙過 domain classifier<br>
<br>
0:40:18.660,0:40:22.620<br>
所以要給 feature extractor 增加任務的難度<br>
<br>
0:40:22.620,0:40:24.920<br>
所以 feature extractor 他 output 的 feature<br>
<br>
0:40:24.920,0:40:28.660<br>
不只要同時騙過 domain classifier<br>
<br>
0:40:28.660,0:40:32.320<br>
還要同時讓 label predictor 做的好<br>
<br>
0:40:32.320,0:40:36.380<br>
Label predictor 吃 feature extractor 的 output<br>
<br>
0:40:36.380,0:40:42.200<br>
他的 output 就是十個 class<br>
<br>
0:40:42.360,0:40:48.180<br>
所以 feature extractor 不只要騙過 domain classifier<br>
<br>
0:40:48.180,0:40:52.100<br>
同時還要滿足 label predictor 的需求<br>
<br>
0:40:52.120,0:40:57.040<br>
他抽出來的 feature 不只是要把 domain 的特性消掉<br>
<br>
0:40:57.040,0:41:00.740<br>
同時還要保留原來 digit 的特性<br>
<br>
0:41:01.820,0:41:05.800<br>
如果把這三個 network 放在一起的話<br>
<br>
0:41:05.800,0:41:12.220<br>
實際上它只是一個大型的 neural network 而已<br>
<br>
0:41:12.360,0:41:17.880<br>
他有很多層就像一個在 multitask learning 會用到的 neural network 一樣<br>
<br>
0:41:17.880,0:41:22.100<br>
但是這個 neural network 是個各懷鬼胎的neural network<br>
<br>
0:41:22.100,0:41:28.260<br>
一般的 neural network 的每一個參數，要做的事情都一樣<br>
<br>
0:41:28.260,0:41:32.180<br>
有共同的目標，要 minimize loss<br>
<br>
0:41:32.400,0:41:34.100<br>
要 optimize accuracy<br>
<br>
0:41:34.100,0:41:40.140<br>
但是在這個 neural network 裡，他的參數是各懷鬼胎的<br>
<br>
0:41:40.500,0:41:50.900<br>
藍色這部分 label predictor 要做的是把 class 的分類正確率做的越高越好<br>
<br>
0:41:51.560,0:41:59.200<br>
Domain classifier 要做的是正確的 predict 一個 image 屬於哪個 domain<br>
<br>
0:41:59.540,0:42:07.300<br>
Feature extractor 要做的是同時 improve Label predictor 的 accuracy<br>
<br>
0:42:07.560,0:42:12.220<br>
同時 minimize Domain classifier 的 accuracy<br>
<br>
0:42:12.220,0:42:17.940<br>
所以 feature extractor 是一個不好的隊友<br>
<br>
0:42:17.940,0:42:22.300<br>
Domain classifier 想要做 domain classification<br>
<br>
0:42:22.300,0:42:25.900<br>
Feature extractor 想要捅他隊友一刀<br>
<br>
0:42:25.900,0:42:28.780<br>
他隊友想要選總統但他背後捅他一刀<br>
<br>
0:42:28.780,0:42:30.540<br>
他不想要讓他選總統<br>
<br>
0:42:30.540,0:42:35.480<br>
他想要做的事情跟他隊友要做的事情是相反的<br>
<br>
0:42:35.480,0:42:39.280<br>
他是一個會陷害他隊友的 model<br>
<br>
0:42:39.880,0:42:43.220<br>
Feature extractor 怎麼陷害隊友呢<br>
<br>
0:42:43.220,0:42:48.940<br>
這件事情很容易，只要加一個 gradient reversal 的 layer 就行了<br>
<br>
0:42:48.940,0:42:52.840<br>
也就是說在作 back propagation 的時候<br>
<br>
0:42:53.200,0:43:00.960<br>
Domain classifier 計算 back propagation 不是有 forward 跟 backward 兩個 path 嗎<br>
<br>
0:43:01.460,0:43:08.020<br>
在作 backward path 的時候<br>
Domain classifier 傳給 feature extractor 什麼樣的 value<br>
<br>
0:43:08.020,0:43:16.220<br>
Feature extractor 就把他乘上一個負號<br>
也就是 domain classifier 告訴你某一個 value 應該要上升<br>
<br>
0:43:16.320,0:43:23.260<br>
他就會故意下降<br>
作一個跟 domain classifier 要求相反的事情<br>
<br>
0:43:23.260,0:43:31.440<br>
Domain classifier 因為看不到真正的 image<br>
所以最後一定會 fail 掉<br>
<br>
0:43:31.700,0:43:35.520<br>
他所能看到的東西都是 feature extractor 告訴他的<br>
<br>
0:43:35.520,0:43:42.040<br>
所以最後一定會無法分辨 feature extractor 所抽來的 feature 是來自於哪個 domain<br>
<br>
0:43:42.820,0:43:47.940<br>
問題是，domain classifier 一定要奮力的掙扎<br>
<br>
0:43:47.940,0:43:51.880<br>
這個 model 原理講起來很簡單<br>
<br>
0:43:51.880,0:43:55.800<br>
實際上的 training 可能跟 GAN 一樣是沒有那麼好 train 的<br>
<br>
0:43:55.800,0:44:03.620<br>
Domain classifier 一定要奮力掙扎<br>
<br>
0:44:03.880,0:44:08.660<br>
他一定要努力判斷現在的 feature 是來自於哪個 domain<br>
<br>
0:44:08.660,0:44:11.020<br>
如果 domain classifier 他比較弱、懶惰<br>
<br>
0:44:11.140,0:44:14.280<br>
他一下就放棄不想做了<br>
<br>
0:44:14.280,0:44:17.320<br>
就沒有辦法把前面的 feature extractor 逼到極限<br>
<br>
0:44:17.320,0:44:22.460<br>
就沒有辦法讓 feature extractor 真的把 domain information remove 掉<br>
<br>
0:44:22.460,0:44:25.360<br>
如果 domain classifier 很 weak<br>
<br>
0:44:25.360,0:44:30.080<br>
他一開始就不想做了，他 output 永遠都是 0 的話<br>
<br>
0:44:30.080,0:44:34.940<br>
那 feature extractor 胡亂弄甚麼 feature 都可以騙過 classifier 的話<br>
<br>
0:44:34.940,0:44:38.200<br>
那就達不到把 domain 特性 remove 掉的效果<br>
<br>
0:44:38.200,0:44:43.840<br>
這個 task 一定要讓 domain classifier 奮力掙扎然後才死掉<br>
<br>
0:44:43.840,0:44:47.280<br>
這樣才能把 feature extractor 的潛能逼到極限<br>
<br>
0:44:47.280,0:44:55.840<br>
這個其實是很新的方法<br>
引用的是 ICML,2015 的 paper 跟 JMLR,2016 的 paper<br>
<br>
0:44:57.080,0:45:00.060<br>
這是 paper 一些實驗的結果<br>
<br>
0:45:00.060,0:45:01.840<br>
作不同 domain 的 transfer<br>
<br>
0:45:01.840,0:45:05.900<br>
包括從 MNIST 上 transfer 到 MNIST-M 上<br>
<br>
0:45:05.900,0:45:10.560<br>
從這個數字的 corpus，transfer 到另一個數字的 corpus<br>
<br>
0:45:10.560,0:45:14.100<br>
從這個數字的 corpus，transfer 到 MNIST 上面<br>
<br>
0:45:14.100,0:45:29.360<br>
兩種不同的道路號誌的 data 互相 transfer<br>
<br>
0:45:51.180,0:45:57.560<br>
如果看實驗結果的話<br>
縱軸代表用不同的方法<br>
<br>
0:45:57.560,0:46:02.180<br>
這邊有一個 source only 的方法<br>
直接 source domain 上 train 一個 model<br>
<br>
0:46:02.180,0:46:05.120<br>
然後 test 在 testing domain 上<br>
<br>
0:46:06.000,0:46:10.980<br>
在這四個結果會發現如果只用 source only 的話<br>
<br>
0:46:10.980,0:46:12.600<br>
Performance 是比較差的<br>
<br>
0:46:12.600,0:46:16.440<br>
這邊比較另一個 transfer learning 的方法<br>
<br>
0:46:16.440,0:46:19.520<br>
留給大家自己去參考文獻<br>
<br>
0:46:19.520,0:46:25.740<br>
這 proposed 的方法是剛剛講的 domain-adversarial training<br>
<br>
0:46:25.740,0:46:30.060<br>
如果用 domain-adversarial training 得到的結果是這裡<br>
<br>
0:46:30.240,0:46:31.420<br>
我們現在最下面這行<br>
<br>
0:46:31.420,0:46:35.920<br>
最下面這行是直接拿 target domain 的 data 去做 training<br>
<br>
0:46:35.920,0:46:38.960<br>
會得到 performance 是最下面這個 row<br>
<br>
0:46:38.960,0:46:42.100<br>
這其實是 performance 的 upper bound<br>
<br>
0:46:42.100,0:46:47.300<br>
會發現如果用 source data 跟 target data train 出來的結果是天差地遠的<br>
<br>
0:46:47.300,0:46:50.980<br>
這中間有一個很大的 gap<br>
<br>
0:46:50.980,0:46:57.820<br>
如果用 domain-adversarial training 可以發現有很好的 improvement<br>
<br>
0:46:57.820,0:47:01.900<br>
在不同的 case 上面都有很好的 improvement<br>
<br>
0:47:02.760,0:47:07.940<br>
接下來要講的是 Zero-shot learning<br>
<br>
0:47:07.940,0:47:12.980<br>
在 Zero-shot learning 裡面跟剛才講的 task 是一樣的<br>
<br>
0:47:13.040,0:47:23.920<br>
只有 source data 有 label，target data 沒有 label<br>
在剛才的 task 裡可以把 source data 當 training data<br>
<br>
0:47:23.920,0:47:32.320<br>
Target data 當作 testing data<br>
但實際上 Zero-shot learning 他的 define 又更加嚴苛一點<br>
<br>
0:47:32.320,0:47:39.240<br>
他的 define 是他的 source data、target data，他的 task 是不一樣的<br>
<br>
0:47:39.240,0:47:48.720<br>
比如說，在影像上面，你的 source data 可能是要分辨貓跟狗<br>
<br>
0:47:48.720,0:47:59.380<br>
Source data 裡面可能有貓的 class、狗的 class<br>
但 target data 裡面，image 是草泥馬<br>
<br>
0:47:59.760,0:48:04.600<br>
在 source data 裡是從來都沒有出現過草泥馬的<br>
<br>
0:48:04.600,0:48:08.100<br>
如果 source data 裡面從來都沒有出現過草泥馬的話<br>
<br>
0:48:10.420,0:48:17.840<br>
Machine 有辦法看到他就說是草泥馬嗎？<br>
這實在是太強人所難<br>
<br>
0:48:17.840,0:48:25.220<br>
這個 task 在語音上很早就有 solution<br>
<br>
0:48:25.220,0:48:28.260<br>
語音本來就常常會遇到 Zero-shot learning 的問題<br>
<br>
0:48:28.260,0:48:34.580<br>
假如把不同的 word 都當作一個詞彙、一個 class 的話<br>
<br>
0:48:34.580,0:48:39.760<br>
本來在 training、testing 的時候，就有可能看到不同的詞彙<br>
<br>
0:48:39.980,0:48:42.720<br>
Testing data 本來就有一些詞彙<br>
<br>
0:48:42.720,0:48:45.200<br>
英文的詞彙這麼多，至少十萬個不同詞彙<br>
<br>
0:48:45.200,0:48:50.580<br>
Testing data 本來就有一些詞彙，是在 training 的時候從來沒有看過<br>
<br>
0:48:50.580,0:48:53.060<br>
在語音上怎麼解決這個問題<br>
<br>
0:48:53.060,0:48:58.800<br>
在語音上的作法是不要直接辨識一段聲音屬於哪一個 word<br>
<br>
0:48:58.800,0:49:02.380<br>
辨識的是一段聲音屬於哪個 phoneme<br>
<br>
0:49:02.380,0:49:04.920<br>
如果不知道 phoneme，就想成音標就好<br>
<br>
0:49:04.920,0:49:08.940<br>
所以辨識的單位不要定成 word，而定成 phoneme<br>
<br>
0:49:08.940,0:49:13.820<br>
再做一個 phoneme 跟 table 之間對應關係的表<br>
<br>
0:49:13.820,0:49:15.960<br>
這個東西叫做 lexicon，也就是辭典<br>
<br>
0:49:15.960,0:49:22.540<br>
建一個文字跟 phoneme 對應關係的表根據人的 knowledge<br>
<br>
0:49:22.540,0:49:25.860<br>
在辨識的時候，只要辨識出 phoneme 就好<br>
<br>
0:49:25.860,0:49:30.740<br>
再去查表，這一段 phoneme 對應到哪一個 word<br>
<br>
0:49:30.740,0:49:35.580<br>
這樣就算有一些 word 是沒有出現在 training data 裡面的<br>
<br>
0:49:35.580,0:49:40.760<br>
只要他在你建好的 lexicon 裡面有出現過<br>
<br>
0:49:40.760,0:49:43.640<br>
你的 model 可以正確辨識出現在聲音屬於哪個 phoneme 的話<br>
<br>
0:49:43.640,0:49:46.500<br>
就可以處理這個問題<br>
<br>
0:49:47.860,0:49:59.020<br>
在影像上，可以把每一個 class，用他的 attribute 來表示<br>
<br>
0:49:59.020,0:50:08.380<br>
也就是說，有一個 database，database 裡面有所有不同<br>
<br>
0:50:08.380,0:50:11.700<br>
可能的 object 跟他的特性<br>
<br>
0:50:11.700,0:50:15.200<br>
假設現在要辨識動物<br>
<br>
0:50:15.200,0:50:20.040<br>
但 training data 跟 testing data 他們的動物不一樣<br>
<br>
0:50:20.700,0:50:22.940<br>
但有一個 database<br>
<br>
0:50:22.940,0:50:27.420<br>
這 database 告訴你每一種動物有什麼樣的特性<br>
<br>
0:50:27.420,0:50:32.520<br>
比如說，狗就是毛茸茸、四隻腳、有尾巴<br>
<br>
0:50:32.520,0:50:35.140<br>
魚有尾巴，沒有四隻腳、沒有毛茸茸<br>
<br>
0:50:35.140,0:50:40.420<br>
Chimp，黑猩猩有毛茸茸，沒有四隻腳、沒有尾巴<br>
<br>
0:50:40.420,0:50:43.280<br>
黑猩猩沒有尾巴，猴子才有尾巴<br>
<br>
0:50:45.060,0:50:55.440<br>
Attribute 要定的夠豐富，每一個 class 都要有獨一無二的 attribute<br>
<br>
0:50:55.440,0:50:57.820<br>
如果有兩個 class 他的 attribute 一模一樣的話<br>
<br>
0:50:57.820,0:51:00.120<br>
等於那個方法會 fail 掉<br>
<br>
0:51:00.120,0:51:06.560<br>
在 training 的時候，不直接辨識每一張 image 屬於哪一個 class<br>
<br>
0:51:06.560,0:51:12.800<br>
而是去辨識每一張 image 具備什麼樣的 attribute<br>
<br>
0:51:12.800,0:51:16.720<br>
Neural network learning 的 target 就是看到猩猩的圖<br>
<br>
0:51:16.720,0:51:22.380<br>
就要說這是一個毛茸茸的動物、沒有四隻腳的動物、沒有尾巴的動物<br>
<br>
0:51:22.380,0:51:28.560<br>
看到一個狗的圖，就要說這是毛茸茸的動物、有四隻腳的動物、有尾巴的動物<br>
<br>
0:51:28.860,0:51:33.180<br>
在 testing 的時候，就算來了一張從來沒有看過的 image<br>
<br>
0:51:33.720,0:51:39.820<br>
沒有關係，neural network 的任務也不是要 detect input 這張 image 是屬於哪一種動物<br>
<br>
0:51:39.820,0:51:44.340<br>
只要 input 這張 image，他有甚麼樣的 attribute<br>
<br>
0:51:44.340,0:51:49.220<br>
Input 一張沒有見過的動物，但只要能夠把他的 attribute 找出來<br>
<br>
0:51:49.600,0:51:58.720<br>
就查表看哪一種，在 database，哪一種動物的 attribute 跟 model 的 output 最接近<br>
<br>
0:51:58.720,0:52:03.140<br>
有時候可能沒有一模一樣，也沒有關係，就看誰最接近<br>
<br>
0:52:03.480,0:52:10.160<br>
哪一個動物的 attribute 跟 neural network 的 output 最接近<br>
那個動物就是你要找的<br>
<br>
0:52:11.840,0:52:18.040<br>
有時候 attribute 可能非常複雜<br>
Attribute 的 dimension 可能很大<br>
<br>
0:52:18.040,0:52:27.000<br>
甚至可以做 attribute 的 embedding<br>
也就是說現在有一個 embedding space<br>
<br>
0:52:27.000,0:52:41.240<br>
把 training data 裡的每一張 image 都透過 transform<br>
<br>
0:52:41.240,0:52:48.840<br>
把他變成 embedding space 上的一個點<br>
<br>
0:52:48.840,0:52:54.000<br>
然後把所有的 attribute 也都變成 embedding space 上一個點<br>
<br>
0:52:54.000,0:52:58.020<br>
g 跟 f 都可以是 neural network<br>
<br>
0:52:58.320,0:53:04.160<br>
training 的時候就希望 f 跟 g 越接近越好<br>
<br>
0:53:04.160,0:53:07.840<br>
在 testing 的時候如果有一張沒有看過的 image<br>
<br>
0:53:07.840,0:53:13.720<br>
這個 image 的 attribute embedding 以後，跟哪一個 attribute 最像<br>
<br>
0:53:13.720,0:53:15.640<br>
那就知道他是什麼樣的 image<br>
<br>
0:53:15.640,0:53:19.640<br>
草泥馬翻譯就是 grass-mud horse<br>
<br>
0:53:19.640,0:53:27.540<br>
我們在這邊先休息一下<br>
<br>
0:55:09.460,0:55:16.820<br>
各位同學大家好，我們來上課吧<br>
剛才講到 Attribute embedding<br>
<br>
0:55:16.960,0:55:24.500<br>
image 跟 attribute 其實都可以描述成 vector<br>
<br>
0:55:24.500,0:55:26.120<br>
所以想要做的事情是把<br>
<br>
0:55:26.120,0:55:32.660<br>
image 跟 attribute 投影到同一個空間裡面<br>
<br>
0:55:32.840,0:55:38.380<br>
可以想像成是對 image 的 vector，也就是這邊的 x<br>
<br>
0:55:38.380,0:55:40.540<br>
跟 attribute 的 vector，也就是這邊的 y<br>
<br>
0:55:40.540,0:55:43.520<br>
都做降維，降到同樣的 dimension<br>
<br>
0:55:43.520,0:55:50.420<br>
把 x 通過一個 function f 都變成 embedding space 上面的 vector<br>
<br>
0:55:50.420,0:55:56.000<br>
把 y 通過另外一個 function g 也都變成 embedding space 上面的 vector<br>
<br>
0:55:56.000,0:55:58.720<br>
要怎麼找 f 跟 g 呢<br>
<br>
0:55:58.940,0:56:02.140<br>
f 跟 g 可以說是 neural network<br>
<br>
0:56:02.140,0:56:03.700<br>
input 一張 image，他變成一個 vector<br>
<br>
0:56:03.700,0:56:07.480<br>
或 input 一個 attribute 的 vector，他變成一個 vector<br>
<br>
0:56:07.480,0:56:09.400<br>
f 跟 g 怎麼找呢<br>
<br>
0:56:09.560,0:56:13.820<br>
你的 training target 就是，你希望說，假設我們已經知道說<br>
<br>
0:56:13.820,0:56:19.120<br>
這個 y1 是 x1 的 attribute，y2 是 x2 的 attribute<br>
<br>
0:56:19.120,0:56:28.920<br>
希望找到 f 跟 g 可以讓 x1、y1 投影到 embedding space 以後越接近越好<br>
<br>
0:56:28.920,0:56:32.920<br>
x2、y2投影到 embedding space 以後越接近越好<br>
<br>
0:56:32.920,0:56:36.060<br>
如果已經把 f 跟 g 找出來了<br>
<br>
0:56:36.060,0:56:40.840<br>
假如有一張從沒見過的 image 叫 x3 在你的 testing data 裡<br>
<br>
0:56:41.340,0:56:45.920<br>
他也可以透過 f 變成 embedding space 上的一個 vector<br>
<br>
0:56:45.920,0:56:50.260<br>
接下來就看這個 embedding vector 他跟<br>
<br>
0:56:50.260,0:56:53.700<br>
這邊有一個錯，這邊應該是 x3<br>
<br>
0:56:53.700,0:56:59.360<br>
這個 x3 應該跟 y3 的 embedding 最接近<br>
<br>
0:56:59.360,0:57:02.800<br>
y3 就是他的 attribute<br>
<br>
0:57:02.800,0:57:05.760<br>
再看他對應到哪一個動物<br>
<br>
0:57:05.760,0:57:10.260<br>
比如說他是 grass-mud horse，所以就是草泥馬<br>
<br>
0:57:10.260,0:57:16.880<br>
有時候會遇到一個問題：如果根本沒有 database 呢<br>
<br>
0:57:16.880,0:57:23.000<br>
根本不知道每一個動物他的 attribute 是甚麼，怎麼辦<br>
<br>
0:57:23.000,0:57:24.960<br>
那可以借用 word vector<br>
<br>
0:57:25.380,0:57:31.060<br>
word vector 的每一個 dimension 就代表了現在這個 word<br>
<br>
0:57:31.060,0:57:33.060<br>
他的某種 attribute<br>
<br>
0:57:33.060,0:57:39.340<br>
所以不一定需要有個 database 去告訴你每一個動物的 attribute 是甚麼<br>
<br>
0:57:39.340,0:57:42.780<br>
假設有一組 word vector，這組 word vector<br>
<br>
0:57:42.780,0:57:45.620<br>
你知道每個動物他對應的 word 的 word vector<br>
<br>
0:57:45.620,0:57:49.460<br>
這 word vector 你就會拿一個很大的 corpus<br>
比如說 Wikipedia train 出來<br>
<br>
0:57:49.460,0:57:53.960<br>
就可以把 attribute 直接換成 word vector<br>
<br>
0:57:53.960,0:57:57.520<br>
所以把 attribute 通通換成那個 word 的 word vector<br>
<br>
0:57:57.520,0:58:02.400<br>
再做跟剛才一樣的 embedding，就結束了<br>
<br>
0:58:02.400,0:58:08.080<br>
那其實剛才的 training，這邊可以稍微講一下<br>
<br>
0:58:08.080,0:58:12.000<br>
假設我們的 training的criterion 是說<br>
<br>
0:58:12.000,0:58:15.000<br>
我們要讓 x^n 通過f(x)<br>
<br>
0:58:15.000,0:58:20.840<br>
y^n 通過 g(y)<br>
他的距離越接近越好<br>
<br>
0:58:20.840,0:58:25.800<br>
就是找一個 f 跟 g<br>
Minimize f(x^n) 跟 g(y^n) 的距離<br>
<br>
0:58:25.800,0:58:30.780<br>
這樣子你覺得 ok 嗎？其實這樣子是會有問題<br>
<br>
0:58:30.780,0:58:37.960<br>
因為這樣 model 就只會 learn 到他把所有不同的 x 跟所有不同的 y<br>
<br>
0:58:38.340,0:58:47.320<br>
都投影到同一點，這樣子距離最少，就停住了<br>
所以如果 loss function 這樣定，其實是不行的<br>
<br>
0:58:47.960,0:58:53.960<br>
所以需要稍微重新設計一下 loss function<br>
前面這個 loss function 只有考慮到說<br>
<br>
0:58:54.260,0:58:58.260<br>
如果我們知道 x^n 跟 y^n 是一個 pair，要讓他越接近越好<br>
<br>
0:58:58.260,0:59:06.080<br>
但沒有考慮到的是，如果知道 x^n 跟另外一個 y 不是同一個 pair<br>
<br>
0:59:06.080,0:59:13.860<br>
他們的距離應該要被拉大<br>
前面這個 loss function 沒有考慮到這件事<br>
<br>
0:59:13.860,0:59:20.340<br>
所以應該要改一下這個 loss function<br>
怎麼改這個 loss function 呢？<br>
<br>
0:59:20.340,0:59:35.620<br>
一個可能是把它改成 max 裡面的兩個 element 分別是<br>
0 跟 k – f(x^n) ⋅ g(y^n) 加上<br>
<br>
0:59:35.640,0:59:42.940<br>
max，找一個 m 不等於 n<br>
f(x^n) ⋅ g(y^m)<br>
<br>
0:59:43.300,0:59:54.260<br>
k 是自己 define 的 margin<br>
k 是一個 constant，training 的時候必須要自己 define<br>
<br>
0:59:54.260,1:00:01.600<br>
今天有一個 max，其中一個 element 是 0<br>
另外一個 element 是一個看起來很長的式子<br>
<br>
1:00:01.600,1:00:16.180<br>
他會從 0 跟這個式子裡面選一個最大的<br>
所以這一項的最小值就是 0<br>
<br>
1:00:16.180,1:00:23.500<br>
甚麼時候會等於 0<br>
當你不是零的這一項，他的值小於 0 的時候<br>
<br>
1:00:23.500,1:00:35.060<br>
這個loss就會是0<br>
如果 k – f(x^n) ⋅ g(y^n) 再加上後面這一項<br>
<br>
1:00:35.060,1:00:43.500<br>
max，m 不等於 n，f(x^n) ⋅ g(y^m) < 0 的時候<br>
這一項會是 zero loss<br>
<br>
1:00:43.500,1:00:51.238<br>
再整理一下，把這兩項移到右邊，把左右對調<br>
<br>
1:00:51.238,1:00:54.720<br>
就得到下面這個式子<br>
<br>
1:00:54.720,1:00:59.080<br>
如果看下面這個式子的話就比較清楚這一項的涵義是甚麼<br>
<br>
1:00:59.080,1:01:03.520<br>
下面這個式子的意思是，甚麼時候會有 zero loss 呢<br>
<br>
1:01:03.600,1:01:16.280<br>
當 f(x^n) 跟 g(y^n) 他的 inner product 值大於另外一個<br>
<br>
1:01:16.280,1:01:21.740<br>
假設在所有不是 y^n 的 y 裡面<br>
<br>
1:01:22.080,1:01:23.960<br>
找一個 m 出來<br>
<br>
1:01:23.960,1:01:27.900<br>
這個 m 是跟 f(x^n) 最接近的<br>
<br>
1:01:28.160,1:01:30.900<br>
就算他們的 inner product 最大<br>
<br>
1:01:30.900,1:01:35.220<br>
還是要比正確答案，小一個 k<br>
<br>
1:01:35.220,1:01:40.620<br>
如果 x^n 跟 y^n 之間的 inner product 值要很大<br>
<br>
1:01:40.620,1:01:46.540<br>
要有多大呢？要大過所有其他的 y^m 跟 x^n 的 inner product<br>
<br>
1:01:46.540,1:01:49.700<br>
而且要大過一個 margin，k<br>
<br>
1:01:49.700,1:01:52.040<br>
如果定這個式子<br>
<br>
1:01:52.040,1:01:57.040<br>
不只是把 pair 起來的 attribute 跟 image 拉近<br>
<br>
1:01:57.040,1:02:04.260<br>
同時也要把那些不成 pair 的東西，把它拆散<br>
<br>
1:02:04.260,1:02:07.200<br>
其實還有一個更簡單的 zero-shot learning 的方法<br>
<br>
1:02:07.200,1:02:11.060<br>
叫 Convex combination of semantic embedding<br>
<br>
1:02:11.060,1:02:14.880<br>
這個方法是說，也不要做甚麼 learning<br>
<br>
1:02:14.880,1:02:20.220<br>
假設有一個 off-the-shelf  語音辨識系統<br>
<br>
1:02:20.220,1:02:23.460<br>
跟一個 off-the-shelf 的 word vector<br>
<br>
1:02:23.480,1:02:25.800<br>
這兩個可能不是自己 train，或網路上載下來的，<br>
<br>
1:02:25.800,1:02:26.900<br>
就可以做這件事情<br>
<br>
1:02:28.660,1:02:32.740<br>
把一張圖丟到 neural network 裡面去<br>
<br>
1:02:32.740,1:02:36.820<br>
他沒有辦法決定他是哪一個 class<br>
<br>
1:02:36.820,1:02:43.560<br>
他覺得他有 0.5 的機率是 lion，0.5 的機率是 tiger<br>
<br>
1:02:43.560,1:02:47.280<br>
接下來再去找 lion 跟 tiger 的 word vector<br>
<br>
1:02:47.280,1:02:54.980<br>
把 lion 跟 tiger 的 word vector 用 1:1 的比例混合<br>
<br>
1:02:54.980,1:02:58.340<br>
0.5 tiger 的 vector 加 0.5 lion 的 vector<br>
<br>
1:02:58.420,1:03:00.320<br>
得到另外一個新的 vector<br>
<br>
1:03:00.320,1:03:06.240<br>
再看哪一個 word 的 word vector 跟混合的結果最接近<br>
<br>
1:03:06.320,1:03:12.400<br>
假設是 liger 這個字最接近的話<br>
<br>
1:03:12.400,1:03:14.700<br>
那這個東西就是 liger<br>
<br>
1:03:14.860,1:03:15.880<br>
liger 是甚麼？<br>
<br>
1:03:15.880,1:03:19.640<br>
liger 就是獅虎，老虎跟獅子生下的後代叫 liger<br>
<br>
1:03:19.780,1:03:23.400<br>
這個是 paper 裡面舉的例子<br>
<br>
1:03:23.400,1:03:24.920<br>
在這邊不需要做任何 training<br>
<br>
1:03:24.920,1:03:28.060<br>
只要有一組 word vector，一個語音辨識系統<br>
<br>
1:03:28.060,1:03:30.860<br>
就可以做這樣子的 transfer learning<br>
<br>
1:03:30.980,1:03:36.080<br>
以下是這個方法的實驗結果<br>
我覺得其實頗驚人的<br>
<br>
1:03:36.080,1:03:42.860<br>
來比一下人類跟機器的差別<br>
來看這個，你覺得他是甚麼<br>
<br>
1:03:42.860,1:03:48.460<br>
來問一下大家的意見吧<br>
覺得是海豹的同學舉手<br>
<br>
1:03:50.400,1:03:59.580<br>
覺得是海象的同學舉手<br>
覺得是海獅的同學舉手<br>
<br>
1:04:01.960,1:04:06.360<br>
機器覺得怎樣呢<br>
假設沒有做 transfer learning<br>
<br>
1:04:06.360,1:04:11.740<br>
直接拿一個 CNN<br>
他覺得最有可能是 sea lion，是海獅<br>
<br>
1:04:11.740,1:04:17.420<br>
其他也是奇奇怪怪的<br>
cowboy boot，也滿像 cowboy boot<br>
<br>
1:04:17.420,1:04:32.440<br>
他說是海獅，如果你答海獅，你跟 machine 同一個等級<br>
正確答案 Stellar sea lion，北海獅<br>
<br>
1:04:33.060,1:04:39.040<br>
這 ImageNet 其實是一個很崩潰的 corpus<br>
這是很崩潰的 task<br>
<br>
1:04:39.040,1:04:47.260<br>
在這種 task 上就覺得說機器是有可能贏過人的<br>
我覺得應該不可能有人答得出正確答案<br>
<br>
1:04:48.060,1:04:49.840<br>
DeViSE 是另一個方法<br>
<br>
1:04:49.840,1:04:55.620<br>
是剛才講的把 word vector 跟 image project 同一個 embedding space 上的結果<br>
<br>
1:04:55.620,1:05:00.900<br>
在這邊的結果並沒有很好<br>
比較荒謬的答案比如說 flip-flop<br>
<br>
1:05:05.220,1:05:12.020<br>
這個是剛才講的完全不用 training 的方法<br>
結果很驚人，他的前五名裡面有北海獅<br>
<br>
1:05:12.560,1:05:14.140<br>
而且其他答案也都是海獅<br>
<br>
1:05:14.140,1:05:19.140<br>
California sea lion，Australian sea lion，South American sea lion<br>
<br>
1:05:20.820,1:05:25.080<br>
這邊有舉另外一個例子，剛剛用草泥馬當例子<br>
<br>
1:05:25.080,1:05:32.060<br>
那個例子並不是亂取的，因為 ImageNet 裡面真的有草泥馬<br>
<br>
1:05:32.060,1:05:35.940<br>
這個大家都知道是草泥馬<br>
machine的答案是這樣子<br>
<br>
1:05:36.240,1:05:45.680<br>
第一名是 Tibetan mastiff，是獒犬<br>
<br>
1:05:45.680,1:05:48.140<br>
然後是 titi monkey我也不知道是甚麼<br>
<br>
1:05:48.140,1:05:55.420<br>
然後 Koala，然後 chow-chow 好像是鬆獅狗之類的<br>
<br>
1:05:55.420,1:05:59.080<br>
他其實滿像獒犬的<br>
<br>
1:05:59.220,1:06:02.320<br>
llama 其實就是草泥馬<br>
<br>
1:06:02.320,1:06:04.280<br>
表面上看起來有答對，其實沒有答對<br>
<br>
1:06:04.280,1:06:08.080<br>
因為答案是另外一種 lama<br>
<br>
1:06:08.080,1:06:12.640<br>
如果你有看過草泥馬傳奇的話<br>
<br>
1:06:12.640,1:06:15.540<br>
其實是部落裡面最強的騎手才能騎牠<br>
<br>
1:06:19.140,1:06:22.200<br>
草泥馬有很多種，至少有三種<br>
<br>
1:06:22.380,1:06:25.749<br>
這種是最強的，反正不是 llama 就對<br>
<br>
1:06:25.749,1:06:27.620<br>
llama 的臉是尖的<br>
<br>
1:06:27.620,1:06:30.980<br>
這種的臉是圓的<br>
<br>
1:06:30.980,1:06:34.380<br>
這個方法得到比較怪的結果<br>
<br>
1:06:34.380,1:06:40.900<br>
其實這個 task 有點難，這可能是一個比較失敗的例子<br>
<br>
1:06:40.900,1:06:43.260<br>
這個 network 也沒有得到正確的結果<br>
<br>
1:06:43.260,1:06:49.040<br>
它得到最接近的 domestic llama<br>
<br>
1:07:06.920,1:07:11.760<br>
Zero-shot learning 剛才舉的例子都是影像的例子<br>
<br>
1:07:11.760,1:07:13.940<br>
最後舉一個文字的例子<br>
<br>
1:07:13.940,1:07:21.540<br>
這個結果滿新的，這個結果最近才被放到 arXiv 上面<br>
<br>
1:07:21.540,1:07:22.820<br>
是 google 的 paper<br>
<br>
1:07:22.820,1:07:28.440<br>
現在不只要放 arXiv 還要做漂亮的動畫在部落格上面<br>
<br>
1:07:28.640,1:07:38.940<br>
這個是做 machine translation<br>
<br>
1:07:39.020,1:07:41.020<br>
在 training 的時候<br>
<br>
1:07:41.020,1:07:45.940<br>
machine 看過如何把英文翻成韓文，有這種 data<br>
<br>
1:07:45.940,1:07:50.400<br>
它知道怎麼把韓文翻成英文，有這種 data<br>
<br>
1:07:50.400,1:07:53.820<br>
它知道怎麼把英文翻成日文，有這種 data<br>
<br>
1:07:53.820,1:07:57.360<br>
它知道怎麼把日文翻成英文，有這種 data<br>
<br>
1:07:57.360,1:08:02.720<br>
接下來它從來沒有看過日文翻韓文的 data，但是它可以翻<br>
<br>
1:08:02.720,1:08:07.940<br>
它從來沒有看過韓文翻日文的 data，所以它也可以翻<br>
<br>
1:08:07.940,1:08:09.500<br>
這件事情怎麼做到的<br>
<br>
1:08:09.500,1:08:13.700<br>
如果看部落格的文章，好像有第二版本的 title 很聳動<br>
<br>
1:08:13.700,1:08:17.980<br>
Google 的 machine，發明了自己的 sequence language<br>
<br>
1:08:17.980,1:08:22.540<br>
後來標題就被偷偷改了，後來看到有另外一個版本的標題<br>
<br>
1:08:22.540,1:08:32.680<br>
那篇部落格可能也不是 google 寫的<br>
<br>
1:08:32.680,1:08:41.380<br>
為甚麼 Zero-shot learning 在這個 task 是可行的<br>
<br>
1:08:41.380,1:08:47.520<br>
因為如果用同一個 model 做了不同語言之間的 translation 以後<br>
<br>
1:08:47.520,1:08:50.060<br>
machine 可以學到的事情是怎樣的呢<br>
<br>
1:08:50.060,1:08:54.240<br>
它可以學到對不同語言的 input<br>
<br>
1:08:54.240,1:09:00.860<br>
都把不同語言的不同句子 project 到同一個 space 上面<br>
<br>
1:09:00.980,1:09:04.420<br>
而在這個 space 上面，它是 language independent 的<br>
<br>
1:09:04.420,1:09:12.820<br>
這個 space 上面的位置，只跟這個句子的 semantic 有關<br>
<br>
1:09:12.820,1:09:15.900<br>
舉例來說，這個是 paper 裡面的例子<br>
<br>
1:09:15.900,1:09:20.500<br>
這個 paper 說根據 learn 好的 translator<br>
<br>
1:09:20.500,1:09:25.320<br>
translator 有一個 encoder，它會把 input 的句子變成一個 vector<br>
<br>
1:09:25.320,1:09:31.860<br>
一個 decoder，根據這個 vector，解回前一個句子，就是翻譯的結果<br>
<br>
1:09:31.860,1:09:36.280<br>
如果把不同的語言，都丟到這個 encoder 裡面<br>
<br>
1:09:36.280,1:09:38.060<br>
讓它變成 vector 的話<br>
<br>
1:09:38.060,1:09:43.420<br>
這些不同語言的不同句子，在這個 space 上的分佈有什麼樣的關係呢<br>
<br>
1:09:43.420,1:09:53.520<br>
比如說這三個，英文、韓文和日文的三個句子<br>
<br>
1:09:53.520,1:09:58.540<br>
這三個句子講的是同一件事情、是一樣的意思<br>
<br>
1:09:58.540,1:10:05.520<br>
通過 encoder 的 embedding 以後，他們在 space 上面其實是差不多的位子<br>
<br>
1:10:05.520,1:10:08.300<br>
其實就是在這個位置<br>
<br>
1:10:08.300,1:10:16.080<br>
在這個圖上，不同的顏色代表同樣的意思<br>
<br>
1:10:16.080,1:10:20.620<br>
這些句子雖然是同樣的意思，但可能來自於不同的語言<br>
<br>
1:10:20.620,1:10:24.740<br>
比如說這邊含紅色的這三條線<br>
<br>
1:10:24.960,1:10:30.260<br>
他們在這個 space 上面是在一起，代表同樣的意思<br>
<br>
1:10:30.260,1:10:34.320<br>
但其實他們來自於不同的語言<br>
<br>
1:10:34.320,1:10:37.800<br>
你說這樣子算 machine 發明了一種新語言<br>
<br>
1:10:37.800,1:10:42.680<br>
其實也算是，如果把 embedding space 當作一種新的語言的話<br>
<br>
1:10:42.680,1:10:45.920<br>
machine 做到的就是發現了一種 sequence language<br>
<br>
1:10:45.920,1:10:50.280<br>
對每一種不同的語言，都先轉成只有他自己知道的 sequence language<br>
<br>
1:10:50.280,1:10:53.200<br>
再從這種 sequence language 轉成另外一種語言<br>
<br>
1:10:53.200,1:10:58.080<br>
就算是有某一個發音的 task，input 語言和 output 語言是 machine 沒有看過的<br>
<br>
1:10:58.080,1:11:04.020<br>
它也可以透過這種它自己學出來的 sequence language<br>
來做 translation<br>
<br>
1:11:04.020,1:11:07.720<br>
這邊是更多的 Zero-shot learning 的 paper給大家參考<br>
<br>
1:11:07.720,1:11:11.240<br>
其實 Zero-shot learning 像剛才看到用在 image 分類上面<br>
<br>
1:11:11.240,1:11:14.180<br>
其實現在也可以用在 caption generation 上<br>
<br>
1:11:14.180,1:11:18.020<br>
希望 machine 如果看到一個從來沒看過的東西<br>
<br>
1:11:18.020,1:11:23.560<br>
它也可以用自然語言來描述它現在到底看到甚麼、或看到甚麼動作<br>
<br>
1:11:25.120,1:11:27.720<br>
剩下一點點部分<br>
<br>
1:11:29.740,1:11:32.860<br>
所以你遇到一個狀況是<br>
<br>
1:11:32.860,1:11:36.720<br>
target data 有 label，source data 沒有 label<br>
<br>
1:11:36.720,1:11:39.600<br>
剛剛講的狀況都是 source data 有 label 的狀況<br>
<br>
1:11:39.600,1:11:43.060<br>
有時候會遇到 source data 沒有 label 的狀況<br>
<br>
1:11:43.060,1:11:47.900<br>
但是 target data 可能有 label，可能沒有 label<br>
<br>
1:11:47.900,1:11:52.000<br>
這種 target data 有 label，source data 沒有 label 的狀況<br>
<br>
1:11:52.000,1:11:54.160<br>
叫做 Self-taught learning<br>
<br>
1:11:54.160,1:12:04.720<br>
target data 有 label，source data 沒有 label，這種是 Self-taught learning<br>
<br>
1:12:04.780,1:12:12.680<br>
target data 沒有 label，source data 也沒有 label，這種是 Self-taught clustering<br>
<br>
1:12:12.760,1:12:20.120<br>
在 Self-taught learning 裡面，作者強調這跟一般的 transfer learning 是不一樣的<br>
<br>
1:12:20.340,1:12:23.700<br>
這個名詞都是大家自己定的<br>
<br>
1:12:23.700,1:12:32.280<br>
在 Self-taught learning 作者裡面，它的想法是 transfer learning 是有 label 的<br>
<br>
1:12:32.280,1:12:34.480<br>
在 source domain 是有 labeled data 的才叫 transfer learning<br>
<br>
1:12:34.480,1:12:36.920<br>
Self-taught learning 不算是一種 transfer learning<br>
<br>
1:12:36.920,1:12:40.940<br>
不過後人都把 Self-taught learning 當作是一種 transfer learning<br>
<br>
1:12:40.940,1:12:45.840<br>
這個只是文字上面枝枝節節的小問題<br>
<br>
1:12:48.640,1:12:53.940<br>
有一個要強調的是 Self-taught learning 跟 semi-supervised learning 有一些不一樣的地方<br>
<br>
1:12:53.940,1:12:58.160<br>
Semi-supervised learning 在 learning的時候，也是有一些 labeled data<br>
<br>
1:12:58.160,1:12:59.800<br>
一些 unlabeled data<br>
<br>
1:12:59.800,1:13:05.200<br>
可以說 source data 是 unlabeled data，target data 是 labeled data<br>
<br>
1:13:05.200,1:13:07.420<br>
這也是一種 semi-supervised learning<br>
<br>
1:13:07.420,1:13:12.120<br>
這種 semi-supervised learning 跟一般 semi-supervised learning 有一些不一樣<br>
<br>
1:13:12.120,1:13:20.020<br>
一般 semi-supervised learning 會假設那些 unlabeled data 至少還是跟 labeled data是比較有關係的<br>
<br>
1:13:20.020,1:13:22.960<br>
但在 Self-taught learning 裡面<br>
<br>
1:13:22.960,1:13:30.240<br>
那些 unlabeled data、那些 source data，跟 target data 關係比較遠<br>
<br>
1:13:30.880,1:13:37.440<br>
其實 Self-taught learning 概念很簡單<br>
假設 source data 夠多，雖然它是 unlabeled<br>
<br>
1:13:37.440,1:13:42.980<br>
可以去 learn 一個 feature extractor<br>
在原始的 Self-taught learning paper 裡面<br>
<br>
1:13:42.980,1:13:50.180<br>
他的 feature extractor  是 sparse coding<br>
因為這 paper 比較舊，大概十年前<br>
<br>
1:13:50.360,1:13:52.620<br>
現在也不見得要用 sparse coding 也可以 learn<br>
<br>
1:13:52.620,1:13:53.880<br>
比如說 auto-encoder<br>
<br>
1:13:53.880,1:13:58.120<br>
用 auto-encoder 跟 decoder 來做feature extractor<br>
<br>
1:13:58.120,1:14:01.260<br>
總之，有大量的 data，他們沒有 label<br>
<br>
1:14:01.260,1:14:06.040<br>
可以做的是用這些 data learn 一個好的 feature extractor<br>
<br>
1:14:06.100,1:14:10.040<br>
用這些 data learn 一個好的 representation<br>
<br>
1:14:10.040,1:14:14.880<br>
用這個 feature extractor 在 target data 上面去抽 feature<br>
<br>
1:14:14.880,1:14:17.680<br>
在 Self-taught learning 原始的 paper 裡面<br>
<br>
1:14:17.680,1:14:21.780<br>
其實做了很多 task，這邊不要一一細講<br>
<br>
1:14:21.780,1:14:25.240<br>
這些 task 都顯示 Self-taught learning 是可以得到滿顯著的 improvement<br>
<br>
1:14:25.240,1:14:27.560<br>
台灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
