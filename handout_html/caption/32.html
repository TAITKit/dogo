<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:01.060<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:01.060,0:00:04.880<br>
Reinforcement Learning 其實是一個很大的題目<br>
<br>
0:00:05.480,0:00:10.400<br>
所以在下面加了一個 subtitle : 學一些皮毛<br>
<br>
0:00:10.500,0:00:13.680<br>
剩下時間我們就講一些皮毛<br>
<br>
0:00:13.680,0:00:16.700<br>
那這年頭講到 Deep Reinforcement Learning<br>
<br>
0:00:16.700,0:00:19.540<br>
大家就會覺得說很興奮，為什麼呢？<br>
<br>
0:00:19.600,0:00:24.120<br>
因為，在 15 年 2 月的時候<br>
<br>
0:00:24.120,0:00:26.800<br>
Kreeger 先生在Nature 上面發了一篇<br>
<br>
0:00:26.800,0:00:29.200<br>
用 Reinforcement Learning 的方法<br>
<br>
0:00:29.300,0:00:33.200<br>
來玩 Atari 的小遊戲，都可以痛電人類<br>
<br>
0:00:33.300,0:00:36.200<br>
然後，後來在 16 年的春天呢<br>
<br>
0:00:36.300,0:00:42.500<br>
又有大家都耳熟能詳的 AlphaGo，也是可以痛電人類<br>
<br>
0:00:42.700,0:00:45.560<br>
David Silver 就有說，他覺得說<br>
<br>
0:00:45.640,0:00:50.220<br>
AI 就是 Reinforcement Learning 加 Deep learning<br>
<br>
0:00:50.220,0:00:52.140<br>
Reinforcement Learning 加 Deep learning 就是<br>
<br>
0:00:52.140,0:00:54.660<br>
Deep Reinforcement Learning<br>
<br>
0:00:54.660,0:00:57.800<br>
所以，這個東西現在講起來大家都覺得很興奮<br>
<br>
0:00:57.900,0:01:01.600<br>
那這個 Reinforcement Learning 是甚麼呢？<br>
<br>
0:01:01.700,0:01:05.460<br>
在 Reinforcement Learning 裡面呢，也會有<br>
<br>
0:01:05.500,0:01:09.860<br>
一個 Agent，跟一個 Environment<br>
<br>
0:01:09.860,0:01:11.100<br>
這樣講可能有一點抽象<br>
<br>
0:01:11.200,0:01:12.900<br>
等一下會舉比較具體的例子<br>
<br>
0:01:13.000,0:01:21.000<br>
告訴大家說這個 Agent 跟 Environment 它們分別可以是些甚麼<br>
<br>
0:01:21.200,0:01:23.620<br>
我找個喉糖出來吃一下這樣子<br>
<br>
0:01:23.620,0:01:28.100<br>
你可能會覺得說為什麼我一直咳嗽都不會好<br>
<br>
0:01:28.200,0:01:29.880<br>
但是，這其實是沒有甚麼關係的<br>
<br>
0:01:29.880,0:01:34.800<br>
我記得我高三的時候，不知道怎麼回事一直咳嗽咳嗽，咳到大二<br>
<br>
0:01:36.100,0:01:38.700<br>
後來，就好了<br>
<br>
0:01:44.100,0:01:48.100<br>
好，那，<br>
<br>
0:01:48.280,0:01:53.180<br>
這個 Agent 呢，它會有 Observation，它會去看這個世界，<br>
<br>
0:01:53.320,0:01:57.400<br>
看到世界的某些種種的變化<br>
<br>
0:01:57.500,0:02:00.900<br>
那這個 Observation 又叫做 State<br>
<br>
0:02:01.100,0:02:02.860<br>
你在看 Deep Reinforcement Learning 的時候，<br>
<br>
0:02:02.900,0:02:05.600<br>
你常常會看到一個詞，叫做 State<br>
<br>
0:02:05.800,0:02:08.000<br>
其實這個 State，就是 Observation<br>
<br>
0:02:08.100,0:02:11.500<br>
這個 State 這個詞呀，我覺得總是很容易讓人誤導<br>
<br>
0:02:11.600,0:02:19.900<br>
當你聽到 State這個詞，你總是會想好像是一個，它翻譯應該翻譯成狀態，而這個狀態感覺是系統的狀態<br>
<br>
0:02:20.000,0:02:25.500<br>
不是，這個 State 是環境的狀態。這樣，大家了解我的意思嗎?<br>
<br>
0:02:25.600,0:02:29.100<br>
所以，我覺得用 Observation 這個詞或許是更貼切的<br>
<br>
0:02:29.200,0:02:32.500<br>
就是，你的 Machine 所看到的東西。<br>
<br>
0:02:32.600,0:02:38.200<br>
所以，這個 State 其實指的是這個環境的狀態，也就是你的 Machine 所看到的東西。<br>
<br>
0:02:38.300,0:02:43.600<br>
所以，在這個 Reinforcement Learning 領域才會有這種胖 DP 的這種作法<br>
<br>
0:02:43.700,0:02:49.600<br>
所謂胖 DP 就是 Part your observe 的 state，就是我們 State 只能觀察到一部分的情況<br>
<br>
0:02:49.700,0:02:54.840<br>
如果今天這個 State 是 machine 本身的 State，那怎麼會有那種 State 我會不知道的情況<br>
<br>
0:02:54.900,0:02:59.520<br>
那就是因為這個 State 其實是，所以如果你把 State 當作 machine 的 State，你就會搞不清楚 Partial observation<br>
<br>
0:02:59.840,0:03:03.300<br>
Partial Observation State 的那套想法到底是在幹麻。<br>
<br>
0:03:03.400,0:03:09.220<br>
今天就是因為 State 就是環境的 State，所以機器是有可能沒有辦法看到整個環境所有的狀態<br>
<br>
0:03:09.370,0:03:13.650<br>
所以才會有這個 Partial observation State 的這個想法<br>
<br>
0:03:13.650,0:03:18.400<br>
總之我今天沒有要講那個，但是這個 State 呀其實就是 Observation。<br>
<br>
0:03:18.400,0:03:23.400<br>
如果你以後有有機會看看文獻的話，你再看看我說得對不對。<br>
<br>
0:03:23.500,0:03:29.210<br>
好，那 Machine 呢，會做一些事情，它做的事情就叫做 Action<br>
<br>
0:03:29.430,0:03:33.000<br>
那它做的這些事情，會影響環境，<br>
<br>
0:03:33.000,0:03:36.600<br>
會跟環境產生一些互動，<br>
<br>
0:03:36.600,0:03:39.400<br>
對環境造成一些影響<br>
<br>
0:03:39.500,0:03:44.500<br>
那它因為對環境造成的一些影響，它會得到 Reward。<br>
<br>
0:03:46.000,0:03:50.500<br>
這 Reward 就會告訴它，它的影響是好的，還是不好的。<br>
<br>
0:03:50.500,0:03:53.600<br>
那這邊舉一個抽象的例子，比如說<br>
<br>
0:03:53.600,0:03:59.080<br>
機器看到一杯水，然後它就 take 一個 action，<br>
<br>
0:03:59.470,0:04:01.470<br>
action 就把水打翻了，那<br>
<br>
0:04:02.330,0:04:07.030<br>
Environment 它就得到一個 negative 的 reward，因為人告訴它說不要這麼做，<br>
<br>
0:04:07.120,0:04:11.000<br>
所以它就得到一個負向的 reward。<br>
<br>
0:04:11.000,0:04:13.900<br>
接下來呢，因為水被打翻了，<br>
<br>
0:04:14.000,0:04:17.020<br>
在 Reinforcement Learning 裡面，這些發生的事情都是連續的<br>
<br>
0:04:17.220,0:04:22.600<br>
因為水被打翻了，所以接下來它看到的 Observation 就是水被打翻的狀態。<br>
<br>
0:04:22.600,0:04:26.200<br>
看到水被打翻了，它決定 take 另外一個 action，<br>
<br>
0:04:26.300,0:04:34.000<br>
它決定要把它擦乾淨，人覺得它做得很對，它就得到一個 positive reward。<br>
<br>
0:04:35.200,0:04:37.680<br>
那，機器要做的事情，它生來的目標就是<br>
<br>
0:04:37.950,0:04:41.000<br>
它要去學習採取那些 action<br>
<br>
0:04:41.100,0:04:44.800<br>
它根據過去的得到的 positive reward 還有 negative reward，<br>
<br>
0:04:44.900,0:04:52.300<br>
它去學習採取那些可以讓 reward 被 <br>
maximize 的那些 action，這個就是它存在的目標<br>
<br>
0:04:52.400,0:04:58.870<br>
如果我們用 AlphaGo 為例子的話，一開始 Machine 的 Observation 是甚麼?<br>
<br>
0:04:58.990,0:05:06.200<br>
Machine 的 Observation 就是棋盤，那棋盤你可以用一個 19*19 的 matrix 來描述它<br>
<br>
0:05:06.300,0:05:09.700<br>
所以如果是 AlphaGo 它的 Observation 就是棋盤<br>
<br>
0:05:09.800,0:05:13.510<br>
然後接下來呢，它要 take 一個 action，它 take 的 action 是甚麼呢?<br>
<br>
0:05:13.770,0:05:17.600<br>
它 take 的 action 就是落子的位置<br>
<br>
0:05:17.700,0:05:23.650<br>
它 take 的 action 就是放一個棋子到棋盤上，落一子這樣<br>
<br>
0:05:24.010,0:05:27.530<br>
下在這裡，下在 3 之 3。<br>
<br>
0:05:29.580,0:05:34.190<br>
接下來呢，在圍棋這個遊戲裡面<br>
<br>
0:05:34.330,0:05:38.990<br>
你的 Environment 是甚麼，你的 Environment 其實就是你的對手，<br>
<br>
0:05:39.230,0:05:43.840<br>
所以，你落子呀，落子在不同的位置，<br>
<br>
0:05:43.980,0:05:48.630<br>
你就會影響你的對手的反應，總之你落子以後，<br>
<br>
0:05:48.820,0:05:51.200<br>
你的對手會有反應<br>
<br>
0:05:51.300,0:05:58.560<br>
你看到的這個 observation 呢，就變了。假設說，你的對手呢落一個白子在這個地方，你的 observation就變了<br>
<br>
0:05:58.630,0:06:02.790<br>
機器看到另外一個 observation 以後，它又要決定它的 action<br>
<br>
0:06:03.410,0:06:08.260<br>
所以它再 take 一個 action，它再採取某一個行動，它再落子在另外一個位置。<br>
<br>
0:06:08.440,0:06:12.800<br>
所以下圍棋呢，用機器下圍棋呢，就是這麼一回事。<br>
<br>
0:06:13.800,0:06:18.500<br>
那今天在圍棋這個 case 裡面，它是個還蠻困難的 Reinforcement Learning 的 task，<br>
<br>
0:06:18.600,0:06:22.600<br>
因為在多數的時候，你得到的 reward 都是零，<br>
<br>
0:06:22.700,0:06:27.720<br>
因為你落子下去，通常是甚麼事也沒發生<br>
<br>
0:06:28.110,0:06:29.500<br>
你得到的 reward 就是零。<br>
<br>
0:06:29.600,0:06:33.100<br>
只有在你贏了，或者是輸了的時候，<br>
<br>
0:06:33.200,0:06:34.200<br>
你才會得到 reward。<br>
<br>
0:06:34.300,0:06:41.000<br>
如果你贏了，你就得到 reward 是 1，如果是輸了，你就得到 reward 是 -1<br>
<br>
0:06:41.100,0:06:47.240<br>
所以做 Reinforcement Learning 困難的地方就是，有時候你的 reward 是很?<br>
<br>
0:06:47.300,0:06:52.100<br>
只有少數的 action，只有在少數的情況你才能夠會得到 reward<br>
<br>
0:06:52.460,0:06:57.250<br>
所以它的難點就是機器怎麼在只有少數的 action 會得到 reward 的情況下<br>
<br>
0:06:57.320,0:07:02.600<br>
卻發覺正確的 action，這是一個很困難的問題。<br>
<br>
0:07:03.200,0:07:07.940<br>
對 machine 來說呢，它要怎麼學習下圍棋呢，<br>
<br>
0:07:08.020,0:07:12.660<br>
它就是不斷地找某一個對手一直下一直下，有時候輸、有時候贏<br>
<br>
0:07:12.700,0:07:17.000<br>
接下來，它就是調整它看到的 observation 和 action 之間的關係。<br>
<br>
0:07:17.100,0:07:21.600<br>
它裡面有一個 model，它會調整它看到 observation 時，它要採取甚麼 action，<br>
<br>
0:07:21.700,0:07:26.000<br>
它會調整那個 model，讓它得到的 rewards 可以被 maximize。<br>
<br>
0:07:27.400,0:07:33.260<br>
那我們可以比較一下，如果今天要下圍棋的時候，用 Supervised learning 和 un-supervised learning<br>
<br>
0:07:33.420,0:07:36.800<br>
你得到的結果會有怎麼樣的差別<br>
<br>
0:07:37.000,0:07:40.000<br>
你的 training 的方法有怎麼樣的差別<br>
<br>
0:07:40.100,0:07:42.400<br>
如果是 supervised learning 的話<br>
<br>
0:07:42.500,0:07:47.100<br>
那你就是告訴機器說，看到這樣子的盤勢，<br>
<br>
0:07:47.200,0:07:50.000<br>
你就落子在這個位置；<br>
<br>
0:07:50.000,0:07:53.900<br>
看到另一個盤勢，你就落子在另外一個位置。<br>
<br>
0:07:55.000,0:07:58.060<br>
那 Supervised learning 會不足的地方是，<br>
<br>
0:07:58.170,0:08:03.110<br>
當我們會用 Reinforcement Learning 的時候，往往是你不知道<br>
<br>
0:08:03.200,0:08:09.100<br>
連人都不知道正確答案是甚麼，所以在這個 task，你不太容易做 Supervised learning。<br>
<br>
0:08:09.200,0:08:16.700<br>
因為，在圍棋裡面，看到這個盤勢到底下一個位置最好的點是哪裡，其實有時候人也不知道<br>
<br>
0:08:16.800,0:08:18.060<br>
那機器可以看著棋譜學，<br>
<br>
0:08:18.120,0:08:25.000<br>
那棋譜上面的這個應對不見得是最 optimal<br>
<br>
0:08:25.100,0:08:29.700<br>
所以用 Supervised learning 可以學出一個會下圍棋的 Agent<br>
<br>
0:08:29.800,0:08:33.290<br>
但它可能不是真正最厲害的 Agent。<br>
<br>
0:08:33.460,0:08:37.760<br>
如果用 Supervised learning 就是 machine 從一個老師那邊學，<br>
<br>
0:08:37.940,0:08:44.100<br>
那老師會告訴它說，每次看到這樣子的盤勢，你要下在甚麼樣的位置；看到那樣子的盤勢，你要下在甚麼樣的位置。<br>
<br>
0:08:44.200,0:08:46.200<br>
這個是 Supervised Learning。<br>
<br>
0:08:46.300,0:08:52.740<br>
如果是 Reinforcement Learning ，就是讓機器呢，就不管它，它就找某一個人<br>
<br>
0:08:52.890,0:08:58.100<br>
去跟它下圍棋，然後下一下以後，如果贏了，它就得到 positive reward。<br>
<br>
0:08:58.200,0:08:59.800<br>
輸了，就得到 negative reward。<br>
<br>
0:08:59.900,0:09:07.620<br>
贏了它就知道說，之前的某些下法，可能是好，但是沒有人告訴它<br>
<br>
0:09:07.990,0:09:13.900<br>
甚麼樣的下法，在這幾百步裡面，哪幾步是好的，哪幾步是不好的，沒有人告訴它這件事<br>
<br>
0:09:14.000,0:09:16.100<br>
它要自己想辦法去知道。<br>
<br>
0:09:16.200,0:09:20.600<br>
在 Reinforcement Learning 裡面，你是從過去的經驗去學習，<br>
<br>
0:09:20.700,0:09:24.000<br>
但是，沒有老師告訴你說甚麼是好的，甚麼是不好的。<br>
<br>
0:09:24.100,0:09:26.600<br>
Machine 要自己想辦法。<br>
<br>
0:09:26.900,0:09:30.300<br>
其實在做 Reinforcement Learning 下圍棋的這個 task 裡面，<br>
<br>
0:09:30.400,0:09:34.800<br>
Machine 需要大量的 training 的 examples<br>
<br>
0:09:34.800,0:09:40.500<br>
它可能要下三千萬盤以後，它才能夠變得很厲害。<br>
<br>
0:09:40.500,0:09:43.000<br>
但是因為沒有人可以跟 machine 下三千萬盤，<br>
<br>
0:09:43.100,0:09:48.400<br>
所以大家都知道 AlphaGo 的解法，就是任兩個 machine，然後它們自己互下。<br>
<br>
0:09:50.400,0:09:54.300<br>
我們知道 AlphaGo 其實是先做 Supervised learning，讓 machine 學得不錯了以後，<br>
<br>
0:09:54.400,0:09:58.000<br>
再讓它去做 Reinforcement Learning。<br>
<br>
0:10:01.900,0:10:04.600<br>
Reinforcement Learning 也可以被用在 chat-bot 上面<br>
<br>
0:10:04.700,0:10:05.800<br>
怎麼用呢?<br>
<br>
0:10:05.800,0:10:11.300<br>
我們之前其實也有講過 chat-bot 是怎麼做的，learn一個sequence-to-sequence model，<br>
<br>
0:10:11.400,0:10:16.200<br>
input 是一句話，output 就是機器人回答<br>
<br>
0:10:16.300,0:10:19.100<br>
如果你用 supervised learning learn 一個 chat-bot<br>
<br>
0:10:19.300,0:10:20.100<br>
你就是告訴 machine 說，<br>
<br>
0:10:20.590,0:10:25.600<br>
如果有人跟你說 "Hello"，你就要講 "Hi"<br>
<br>
0:10:25.700,0:10:28.500<br>
如果有人跟你說 "Bye bye"，你就要說 "Goodbye"，<br>
<br>
0:10:28.600,0:10:30.700<br>
這個是 Supervised learning 的 learn 法。<br>
<br>
0:10:30.800,0:10:38.300<br>
如果是 Reinforcement Learning 的 learn 法，就是讓 machine 胡亂去跟人講話，講一講以後，人最後就生氣了<br>
<br>
0:10:38.300,0:10:45.820<br>
Machine 就知道說，它某句話可能講得不太好，但是沒有人告訴它，它到底哪句話講得不好<br>
<br>
0:10:45.950,0:10:49.400<br>
它要自己去想辦法發覺這件事情。<br>
<br>
0:10:49.600,0:10:55.100<br>
這個想法聽起來很 crazy，但是真的有 chat-bot 是這樣 learn 的。<br>
<br>
0:10:55.200,0:11:00.100<br>
這個怎麼做呢，因為你要讓 machine 去跟人一直講話，<br>
<br>
0:11:00.200,0:11:08.100<br>
學習看出人生氣了，或者是沒有生氣，然後去學怎麼跟人對話<br>
<br>
0:11:08.200,0:11:12.000<br>
這個學習太慢了，你可能要講好幾百萬次以後<br>
<br>
0:11:12.100,0:11:14.100<br>
你要跟好幾百萬人對話以後才會學會<br>
<br>
0:11:14.200,0:11:19.400<br>
但是如果一個 Agent 要跟好幾百萬人對話的話，大家都會很煩，沒有人要跟它對話。<br>
<br>
0:11:19.400,0:11:22.800<br>
所以怎麼辦呢，就用 AlphaGo style 的講法，<br>
<br>
0:11:23.000,0:11:25.980<br>
它任兩個 Agent，讓它們互講。<br>
<br>
0:11:26.260,0:11:31.060<br>
任兩個 Chat-bot 互講，可能都亂講，<br>
<br>
0:11:31.280,0:11:32.200<br>
有一個說 "See you"，另外一個說 "See you"<br>
<br>
0:11:32.200,0:11:36.800<br>
然後另外一個再說 "See you" ，陷入如窮地 loop。<br>
<br>
0:11:36.800,0:11:42.200<br>
然後就亂講，就讓兩個 chat-bot 去對話<br>
<br>
0:11:42.300,0:11:47.900<br>
然後它對話完以後，還是需要有人去告訴它說，它們講的好呢，還是不好<br>
<br>
0:11:47.900,0:11:53.700<br>
所以如果是在圍棋裡面比較簡單，因為圍棋的輸贏是很明確的<br>
<br>
0:11:53.800,0:11:56.700<br>
贏了就是 positive，輸了就是 negative<br>
<br>
0:11:56.800,0:11:59.500<br>
那輸贏你就寫個程式來判讀就好了<br>
<br>
0:11:59.810,0:12:06.200<br>
可是如果是對話的話就很麻煩，因為你可以讓兩個 machine 去互相對話，<br>
<br>
0:12:06.300,0:12:09.470<br>
它們兩個可以對話好幾百次，好幾百萬次，但是<br>
<br>
0:12:09.560,0:12:16.200<br>
問題就是你不知道這個對話，沒有人告訴那兩個 machine 說你們現在聊天到底還是聊得好還是聊得不好。<br>
<br>
0:12:16.600,0:12:20.100<br>
所以這個算是一個尚待克服的問題，<br>
<br>
0:12:20.300,0:12:24.400<br>
那這個在文獻上的方法是，這方法可能不見得是最好的方法，<br>
<br>
0:12:24.580,0:12:34.400<br>
它說，就訂個 rule，人去寫些規則，這規則其實在 paper 裡面寫得是，也是蠻簡單的，就蠻簡單的幾條規則<br>
<br>
0:12:34.500,0:12:39.270<br>
然後這幾條規則會去檢查，我看過去這兩個 Agent 對話的紀錄，<br>
<br>
0:12:39.300,0:12:46.100<br>
如果講得好的話，就給它 positive 的 reward，講得不好，就給它 negative 的 reward<br>
<br>
0:12:46.200,0:12:50.050<br>
講得好或不好，就是人自己主觀訂的，所以不知道人訂得好不好<br>
<br>
0:12:51.500,0:12:58.700<br>
然後 machine 就從它這個 rewards 裡面去學怎麼樣講才是好的<br>
<br>
0:12:58.800,0:13:08.100<br>
其實我可以在這邊做個預言，就是我覺得接下來就會有人用 game 來 learn 這個 chat-bot 了<br>
<br>
0:13:08.200,0:13:11.760<br>
雖然現在還沒有看到，但我相信很快就會有人幹這麼一件事。<br>
<br>
0:13:11.950,0:13:14.800<br>
這個怎麼做呢，你就 learn 一個 discriminator，<br>
<br>
0:13:14.900,0:13:19.600<br>
然後這個 discriminator 它會看真正人的對話和那兩個 machine 的對話<br>
<br>
0:13:19.600,0:13:24.600<br>
然後就判斷說你們現在這兩個 Agent 的對話，像不像人<br>
<br>
0:13:24.700,0:13:25.600<br>
如果像的話，<br>
<br>
0:13:27.500,0:13:29.600<br>
它會去抓說像人還是不像人<br>
<br>
0:13:29.800,0:13:35.900<br>
接下來呢，那兩個 Agent 的對話它們就會去想要騙過那個 discriminator， 讓它講得越來越像人。<br>
<br>
0:13:36.000,0:13:41.400<br>
那個 discriminator 判斷它說像人或不像人的這個結果就是 reward<br>
<br>
0:13:41.500,0:13:47.000<br>
它等於是用 discriminator 自動 learn 出給 reward 的方式<br>
<br>
0:13:47.100,0:13:49.600<br>
我相信很快就會有人做這麼一件事了<br>
<br>
0:13:52.480,0:13:57.260<br>
其實這個 Reinforcement Learning 有很多的應用，今天它特別適合的應用就是，<br>
<br>
0:13:57.560,0:14:02.350<br>
如果有一個 task ，人也不知道怎麼做，那你人不知道怎麼做就沒有 labeled data，<br>
<br>
0:14:02.540,0:14:04.900<br>
這個時候，用 Reinforcement Learning 是最適合的。<br>
<br>
0:14:05.200,0:14:13.600<br>
比如說在語音實驗室裡面，我們有做讓 machine 學會做 Interactive retrieval<br>
<br>
0:14:13.600,0:14:16.470<br>
所謂 Interactive retrieval 意思是說，有一個搜尋系統，<br>
<br>
0:14:16.690,0:14:20.500<br>
Machine 跟它說想要找尋一個跟 US President 有關的事情<br>
<br>
0:14:20.600,0:14:24.600<br>
那 machine 可能覺得說，這 US President 太廢了，<br>
<br>
0:14:24.700,0:14:30.000<br>
很多人都是美國總統，你到底是要知道跟美國總統甚麼有關的事情呢?<br>
<br>
0:14:30.500,0:14:34.000<br>
這 machine 會反問它一個問題，要求它 modify<br>
<br>
0:14:34.000,0:14:35.700<br>
它說它要找跟川普有關的事情<br>
<br>
0:14:35.800,0:14:39.400<br>
那 machine 反問它說，你要找的是不是跟選舉有關的事情等等<br>
<br>
0:14:39.500,0:14:45.800<br>
但是，machine 要反問甚麼問題，這個人也不知道，我們人也不知道要問甚麼樣的問題才是好<br>
<br>
0:14:45.900,0:14:48.800<br>
但是，你可以用 Reinforcement Learning 的方式，<br>
<br>
0:14:48.900,0:14:52.900<br>
來讓 machine 學說，問甚麼樣的問題，它可以得到最高的 reward。<br>
<br>
0:14:53.000,0:14:58.900<br>
那你的 reward 可能就是，最後搜尋的結果，使用者覺得越好，就是 reward 越高。<br>
<br>
0:14:58.900,0:15:05.200<br>
但是，每一次 machine 只要每問一個問題，它就會得到一個 negative 的 reward<br>
<br>
0:15:05.200,0:15:08.300<br>
因為每問一個問題，對人來說，就是 extra 的 effort<br>
<br>
0:15:08.400,0:15:10.300<br>
所以，應該要有一個 negative reward<br>
<br>
0:15:11.600,0:15:18.400<br>
Reinforcement Learning 還有很多 applications，比如說開一台直升機，開一個無人車，或者是<br>
<br>
0:15:18.500,0:15:27.100<br>
據說最近 DeepMind 用 Reinforcement Learning 的方法，來幫 Google 的 server 節電<br>
<br>
0:15:28.500,0:15:34.900<br>
現在也有人拿 Reinforcement Learning 來讓 machine 產生句子<br>
<br>
0:15:37.120,0:15:43.200<br>
在很多 task 裡面，machine 都需要產生句子，比如說 summarization，或者是 translation。<br>
<br>
0:15:43.200,0:15:47.700<br>
那這種產生句子的 Task，有時候還蠻麻煩的，為什麼?<br>
<br>
0:15:47.800,0:15:51.800<br>
因為有時候，machine 產生出來的句子，它是好的<br>
<br>
0:15:51.940,0:15:55.900<br>
但是，可是卻跟答案不一樣。<br>
<br>
0:15:55.960,0:15:57.460<br>
因為 translation 有很多種呀，<br>
<br>
0:15:57.500,0:16:01.720<br>
有一個標準答案是那樣，但是並不代表說 machine 現在產生出來的跟標準答案不一樣，它一定是壞的<br>
<br>
0:16:02.200,0:16:07.700<br>
所以這個時候，你如果可以引入 Reinforcement Learning 的話呢，其實是會有幫助的。<br>
<br>
0:16:08.900,0:16:13.440<br>
那 Reinforcement Learning 最常用的 application 就是<br>
<br>
0:16:14.280,0:16:18.200<br>
現在最常用的 application 就是打電玩<br>
<br>
0:16:18.300,0:16:21.300<br>
打電玩的 applications 現在已經滿坑滿谷<br>
<br>
0:16:23.200,0:16:28.100<br>
如果你想要玩的話，現在都有現成的 environment<br>
<br>
0:16:28.200,0:16:33.100<br>
可以讓你在現成的 environment 上面去玩<br>
<br>
0:16:33.200,0:16:38.000<br>
一個呢，叫做 Gym，這都是 Open AI 公司開發的。<br>
<br>
0:16:38.600,0:16:42.200<br>
這個 Gym 比較舊，最近他們又開了一個 Universe<br>
<br>
0:16:42.300,0:16:45.800<br>
Universe 裡面有很多那種 3D 的遊戲。<br>
<br>
0:16:45.900,0:16:48.800<br>
那每次講說讓 machine 玩遊戲，<br>
<br>
0:16:50.100,0:16:56.600<br>
就會有個問題說，可是 machine 不是本來就已經會玩遊戲了嗎?<br>
<br>
0:16:56.700,0:17:01.900<br>
在那些遊戲裡面，不是本來就已經有一個 AI  了嗎?<br>
<br>
0:17:02.000,0:17:09.400<br>
但是，現在你要讓 machine 用 Reinforcement Learning 的方法，去學玩遊戲，跟那些已經內建的 AI<br>
<br>
0:17:09.500,0:17:12.200<br>
其實是不一樣的。<br>
<br>
0:17:12.300,0:17:20.200<br>
因為，machine 它學怎麼玩這個遊戲，其實是跟人一樣的，<br>
<br>
0:17:20.300,0:17:28.000<br>
它是坐在螢幕前的，也就是說它看到的東西，並不是從那個程式裡面去擷取甚麼東西出來，<br>
<br>
0:17:28.100,0:17:30.300<br>
它看到的東西就是那個螢幕畫面，<br>
<br>
0:17:30.400,0:17:33.400<br>
它看到的東西跟人一樣就是 pixels，<br>
<br>
0:17:33.500,0:17:38.000<br>
當你用 machine 來玩，用 Reinforcement Learning 讓 machine 學習玩這些電玩的時候<br>
<br>
0:17:38.100,0:17:41.700<br>
Machine 看到的，就是 pixels。<br>
<br>
0:17:41.800,0:17:46.700<br>
然後再來呢，它要 take 哪個 action，它看到這個畫面，它要做甚麼事情<br>
<br>
0:17:46.800,0:17:52.300<br>
它自己決定了，並不是人寫程式告訴它說，if 你看到這個東西，then 你做甚麼<br>
<br>
0:17:52.300,0:17:54.800<br>
它是自己學出來的。<br>
<br>
0:17:55.900,0:18:07.110<br>
舉例來說，你可以讓 machine 玩 Space invader，space invader 就是叫小蜜蜂還是大黃蜂，反正這是 translation 不太重要<br>
<br>
0:18:08.300,0:18:14.100<br>
我們等一下舉例的時候都用這個來作例子<br>
<br>
0:18:14.200,0:18:17.800<br>
都用 Space invader 來作例子，我們可以稍微解說一下這個遊戲，<br>
<br>
0:18:17.800,0:18:24.300<br>
在這個遊戲裡面，你可以 take 的 action 有三個，就是左、右移動，跟開火，<br>
<br>
0:18:29.040,0:18:33.560<br>
那它怎麼玩這個 video game 呢? 整個 scenario 是這樣，<br>
<br>
0:18:35.740,0:18:40.390<br>
首先呢，machine 會看到一個 observation，<br>
<br>
0:18:40.710,0:18:44.500<br>
這個 observation 就是螢幕的畫面，<br>
<br>
0:18:44.600,0:18:46.800<br>
也就是螢幕畫面裡面的 pixels<br>
<br>
0:18:46.900,0:18:54.300<br>
那開始的 observation 我們就叫它 S1，所以一開始 machine 看到一個 S1<br>
<br>
0:18:54.400,0:19:02.600<br>
那這個 S1 其實就是一個 matrix，那這個 matrix 其實就是每一個 pixel 用一個 vector 來描述它<br>
<br>
0:19:02.700,0:19:05.300<br>
所以這邊應該是一個三維的 tensor<br>
<br>
0:19:05.400,0:19:09.990<br>
這是一個 matrix，但它是有顏色的，所以它三維<br>
<br>
0:19:10.000,0:19:14.560<br>
好，那 machine 看到這個畫面以後，它要決定它要 take 哪一個 action，<br>
<br>
0:19:14.600,0:19:18.180<br>
它現在只有三個 action 可以選擇，比如說它決定要 "往右移"<br>
<br>
0:19:20.000,0:19:24.550<br>
那每次 machine take 一個 action 以後，它會得到一個 reward<br>
<br>
0:19:25.030,0:19:29.150<br>
但是因為只是往右移，這個 reward 是甚麼<br>
<br>
0:19:29.330,0:19:34.900<br>
就是左上角的這個分數，就是它的 reward，那往右移不會得到任何的 reward，<br>
<br>
0:19:35.000,0:19:37.300<br>
所以得到的 reward r1 是 0<br>
<br>
0:19:40.000,0:19:49.400<br>
Machine take 完這個 action 以後，它的 action 會影響了環境，machine 看到的 observation 就不一樣<br>
<br>
0:19:49.500,0:19:53.400<br>
現在 machine 看到的 observation 叫做 s2，<br>
<br>
0:19:53.500,0:19:56.800<br>
那有點不一樣，因為它自己往右移了。<br>
<br>
0:19:58.200,0:20:00.800<br>
當然這些外星人，也會稍微移動一點，<br>
<br>
0:20:02.400,0:20:05.500<br>
不過這個跟 machine take 的 action 是沒有關係的，<br>
<br>
0:20:05.800,0:20:09.420<br>
但是，有時候環境的變化本來就會跟 action 沒有關係<br>
<br>
0:20:09.420,0:20:14.400<br>
有時候環境的變化會是純粹隨機的，跟 machine take 的 action 是沒有關係的<br>
<br>
0:20:15.240,0:20:19.620<br>
那看到 s2<br>
<br>
0:20:19.620,0:20:24.520<br>
這邊講一下通常環境會有 random 的變化<br>
<br>
0:20:24.520,0:20:28.340<br>
環境這個 random 的變化跟 machine take 的 action 是沒有甚麼關係的<br>
<br>
0:20:28.340,0:20:30.660<br>
比如說這邊突然多出一個子彈<br>
<br>
0:20:30.660,0:20:35.940<br>
這些外星人甚麼時候要放出來我覺得應該就是隨機的<br>
<br>
0:20:36.620,0:20:41.160<br>
然後 machine 看到 s2 以後他要決定 take 哪一個 action<br>
<br>
0:20:41.160,0:20:47.980<br>
這個是 a2 假設他決定他要射擊了<br>
<br>
0:20:47.980,0:20:50.460<br>
假設他成功殺了一隻外星人<br>
<br>
0:20:50.460,0:20:53.080<br>
他就會得到一個 reward<br>
<br>
0:20:53.080,0:20:56.540<br>
那我發現殺不同外星人其實得到分數不一樣<br>
<br>
0:20:56.540,0:20:59.180<br>
假設他殺了一個五分的外星人<br>
<br>
0:20:59.380,0:21:04.280<br>
那他看到的 observation 就變少了一隻外星人<br>
<br>
0:21:04.280,0:21:06.900<br>
這個是第三個 observation<br>
<br>
0:21:06.900,0:21:12.720<br>
這個 process 就一直進行下去<br>
<br>
0:21:12.720,0:21:17.300<br>
直到某一天在第 T 個回合的時候<br>
<br>
0:21:17.300,0:21:19.660<br>
machine take action aT<br>
<br>
0:21:19.660,0:21:30.220<br>
然後他得到的 reward rT 進入了另外一個 state<br>
<br>
0:21:30.240,0:21:34.720<br>
這個 state 是個 terminal 的 state<br>
<br>
0:21:34.720,0:21:36.700<br>
它會讓這個遊戲結束<br>
<br>
0:21:36.700,0:21:40.940<br>
在這個 Space Invader 這個遊戲裡面<br>
<br>
0:21:40.940,0:21:44.320<br>
terminal state 就是你被殺死就結束了<br>
<br>
0:21:44.320,0:21:47.460<br>
所以 machine 可能 take 一個 action 比如說往左移<br>
<br>
0:21:47.460,0:21:50.620<br>
那得到 reward 0 不小心撞到 alien 的子彈<br>
<br>
0:21:50.620,0:21:55.040<br>
就死了遊戲就結束了<br>
<br>
0:21:55.040,0:22:01.620<br>
遊戲的開始到結束叫做一個 episode<br>
<br>
0:22:01.620,0:22:06.860<br>
對 machine 來說它要做的事情就是要不斷去玩這個遊戲<br>
<br>
0:22:06.860,0:22:13.920<br>
他要學習在怎麼在一個 episode 裡面 maximize 它可以得到的 reward<br>
<br>
0:22:13.920,0:22:18.700<br>
maximize 他在整個 episode 裡面可以得到的 total 的 reward<br>
<br>
0:22:18.700,0:22:23.180<br>
它必須要在死之前殺最多的外星人<br>
<br>
0:22:23.180,0:22:29.420<br>
他要殺最多的外星人而且他要閃避外星人的子彈，讓自己不要那麼容易被殺死<br>
<br>
0:22:30.420,0:22:35.220<br>
Reinforcement Learning 的難點在哪裡<br>
<br>
0:22:36.840,0:22:39.500<br>
它有兩個難點<br>
<br>
0:22:39.500,0:22:46.840<br>
第一個難點是 reward 的出現往往會有 delay<br>
<br>
0:22:46.840,0:22:51.620<br>
比如說在 Space Invader 裡面<br>
<br>
0:22:51.640,0:22:55.060<br>
其實只有開火這件事情才可能得到 reward<br>
<br>
0:22:55.060,0:22:58.100<br>
也就是開火以後才得到 reward<br>
<br>
0:22:58.100,0:23:05.500<br>
但是如果 machine 只知道開火以後就得到 reward<br>
<br>
0:23:05.520,0:23:08.180<br>
它最後 learn 出來的結果它只會瘋狂開火<br>
<br>
0:23:08.180,0:23:12.600<br>
對它來說往左移、往右移沒有任何 reward 它不想做<br>
<br>
0:23:12.600,0:23:15.980<br>
實際上往左移、往右移這些 moving<br>
<br>
0:23:15.980,0:23:21.420<br>
它對開火能不能夠得到 reward 這件事情是有很關鍵的影響<br>
<br>
0:23:21.500,0:23:26.340<br>
雖然往左移、往右移的 action 本身沒有辦法讓你得到任何 reward<br>
<br>
0:23:26.340,0:23:28.900<br>
但它可以幫助你在未來得到 reward<br>
<br>
0:23:28.900,0:23:32.420<br>
這些事情其實就像規劃未來一樣<br>
<br>
0:23:32.420,0:23:37.560<br>
所以 machine 需要有這種遠見，它要有這種 vision<br>
<br>
0:23:37.560,0:23:40.600<br>
它才能夠把這些電玩完好<br>
<br>
0:23:40.600,0:23:43.180<br>
那其實下圍棋也是一樣<br>
<br>
0:23:43.180,0:23:48.000<br>
有時候短期的犧牲最後可以換來最後比較好的結果<br>
<br>
0:23:48.000,0:23:50.340<br>
就像是虛子把自己的子堵死一塊<br>
<br>
0:23:50.340,0:23:53.000<br>
結果最後反而贏了<br>
<br>
0:23:53.000,0:23:58.800<br>
另外一個就是你的 agent 採取的行為<br>
<br>
0:23:58.800,0:24:01.980<br>
會影響它之後所看到的東西<br>
<br>
0:24:01.980,0:24:09.720<br>
所以 agent 要學會去探索這個世界<br>
<br>
0:24:10.700,0:24:17.400<br>
比如說在 Space Invader 裡面你的 agent 只會往左移、往右移<br>
<br>
0:24:17.400,0:24:21.860<br>
它從來不開火，他就永遠不知道開火可以得到 reward<br>
<br>
0:24:21.860,0:24:27.520<br>
或是它從來沒有試著去擊殺最上面這個紫色的母艦<br>
<br>
0:24:27.520,0:24:34.640<br>
它可能從來沒有試著擊殺紫色的母艦，它就永遠不知道擊殺那個東西可以得到很高的 reward<br>
<br>
0:24:34.960,0:24:39.660<br>
所以要讓 machine 知道要去 explore 這件事情<br>
<br>
0:24:39.660,0:24:41.880<br>
它要去探索它沒有做過的行為<br>
<br>
0:24:41.880,0:24:44.540<br>
這個行為可能有好的結果、壞的結果<br>
<br>
0:24:44.540,0:24:46.760<br>
但是要探索沒有做過的行為<br>
<br>
0:24:46.760,0:24:50.840<br>
在 Reinforcement Learning 裡面也是重要的一件事情<br>
<br>
0:24:51.200,0:24:55.480<br>
在下課之前要講一下等一下要講甚麼<br>
<br>
0:24:56.320,0:25:00.100<br>
Reinforcement Learning 其實有一個 typical 的講法<br>
<br>
0:25:00.100,0:25:04.540<br>
要先講 Markov Decision Process<br>
<br>
0:25:04.540,0:25:08.360<br>
但如果先講 Markov Decision Process 的話<br>
<br>
0:25:08.360,0:25:10.960<br>
講完 Markov Decision Process 其實就下課了<br>
<br>
0:25:10.960,0:25:13.320<br>
你就只聽到 Markov Decision Process<br>
<br>
0:25:13.320,0:25:19.040<br>
而且很多課有講 Markov Decision Process<br>
所以我覺得不要從 Markov Decision Process 講起<br>
<br>
0:25:19.380,0:25:26.280<br>
在 Reinforcement Learning 裡面很紅的一個方法叫 Deep Q Network<br>
<br>
0:25:26.280,0:25:28.960<br>
今天也不講 Deep Q Network<br>
<br>
0:25:28.960,0:25:32.120<br>
為甚麼<br>
因為那個東西已經被打趴了<br>
<br>
0:25:32.120,0:25:35.480<br>
現在最強的方法叫 A3C<br>
<br>
0:25:35.480,0:25:38.120<br>
Deep Reinforcement 已經有點退流行了<br>
<br>
0:25:38.120,0:25:45.940<br>
會發現在gym 裡面最強的那些 agent 都是用 A3C<br>
像我剛才看到的例子<br>
<br>
0:25:45.940,0:25:49.340<br>
剛剛看到自己玩 Space Invader 的例子就是用 A3C learn<br>
<br>
0:25:49.700,0:25:53.580<br>
所以我想說不如直接來講 A3C<br>
<br>
0:25:53.580,0:25:58.820<br>
迎頭趕上的概念，直接來講最新的東西<br>
<br>
0:25:58.820,0:26:01.600<br>
講 A3C 之前<br>
<br>
0:26:01.600,0:26:03.400<br>
需要知道甚麼事情<br>
<br>
0:26:03.400,0:26:08.260<br>
需要知道 Reinforcement Learning 的方法<br>
<br>
0:26:08.260,0:26:10.120<br>
分成兩大塊<br>
<br>
0:26:10.120,0:26:18.000<br>
一個是 Policy-based 的方法<br>
一個是 Value-based 的方法<br>
<br>
0:26:18.000,0:26:24.460<br>
Policy-based 的方法應該是比較後來才有的<br>
<br>
0:26:24.460,0:26:26.860<br>
應該是先有 Value-based 的方法<br>
<br>
0:26:26.860,0:26:33.500<br>
所以一般教科書都是講 Value-based 的方法比較多<br>
講 Policy-based 的方法比較少<br>
<br>
0:26:33.500,0:26:37.260<br>
如果你看 Sutton<br>
<br>
0:26:37.260,0:26:42.480<br>
有一本 Deep Reinforcement Learning 的 Bible 是 Sutton 寫的<br>
<br>
0:26:42.560,0:26:47.760<br>
它在 97 版的教科書裡面<br>
<br>
0:26:47.960,0:26:51.320<br>
Policy 的方法講很少<br>
<br>
0:26:51.320,0:26:55.200<br>
但它今年又再版<br>
<br>
0:26:55.200,0:26:56.820<br>
它還在撰寫中<br>
<br>
0:26:56.820,0:27:00.400<br>
我暑假載下來的教科書的內容<br>
<br>
0:27:00.400,0:27:03.440<br>
跟最近載下來的內容完全不一樣，差很多<br>
<br>
0:27:03.600,0:27:06.220<br>
它最近在改那本教科書<br>
<br>
0:27:06.220,0:27:11.180<br>
就有一整個章節在講 Policy Gradient<br>
<br>
0:27:13.760,0:27:18.740<br>
在 Policy-based 方法裡面<br>
<br>
0:27:18.740,0:27:22.180<br>
會 learn 一個負責做事的 actor<br>
<br>
0:27:22.180,0:27:27.140<br>
在 Value-based 的方法裡面會 learn 一個不做事的 critic<br>
<br>
0:27:27.140,0:27:29.140<br>
它專門批評，不做事的<br>
<br>
0:27:29.140,0:27:35.440<br>
但是要把 actor 跟 critic 加起來叫做 Actor-Critic 的方法<br>
<br>
0:27:35.440,0:27:44.400<br>
現在最強的方法就是 Asynchronous Advantage Actor-Critic，縮寫叫 A3C<br>
<br>
0:27:44.400,0:27:47.740<br>
所以等一下就講 Actor-Critic 這個方法<br>
<br>
0:27:47.740,0:27:51.300<br>
你可能會問最強的 Alpha Go 是用甚麼方法<br>
<br>
0:27:51.300,0:27:55.400<br>
如果仔細讀 Alpha Go paper，它是各種方法大雜燴<br>
<br>
0:27:55.440,0:27:59.700<br>
它裡面其實有 Policy Gradient 方法、Policy-based 方法<br>
<br>
0:27:59.700,0:28:05.060<br>
它也有 Value-based 的方法，它還有 Model-based 的方法，我就沒有講到 Model-based 的方法<br>
<br>
0:28:05.060,0:28:09.000<br>
所謂 Model-based 的方法是指一個 Monte Carlo tree search 那一段<br>
<br>
0:28:09.000,0:28:11.560<br>
算是 Model-based 的方法<br>
<br>
0:28:11.560,0:28:15.680<br>
不過像 Model-based 方法就是要預測未來會發生甚麼事<br>
<br>
0:28:15.680,0:28:21.100<br>
有一個對未來事件的理解，預測未來會發生甚麼事<br>
<br>
0:28:21.240,0:28:26.420<br>
這種方法應該是只有在棋類遊戲比較有用<br>
<br>
0:28:26.420,0:28:28.420<br>
如果是打電玩的話<br>
<br>
0:28:28.420,0:28:31.520<br>
就沒有看到 Model-based 的方法有甚麼成功的結果<br>
<br>
0:28:31.520,0:28:35.800<br>
打電玩裡面要預測未來會發生的狀況是比較難的<br>
<br>
0:28:35.800,0:28:39.460<br>
未來會發生的狀況是難以窮舉<br>
<br>
0:28:39.460,0:28:42.120<br>
不像圍棋雖然未來會發生的狀況很多<br>
<br>
0:28:42.320,0:28:44.940<br>
但還是可以舉出來<br>
<br>
0:28:44.940,0:28:48.280<br>
但是如果是電玩的話<br>
<br>
0:28:48.280,0:28:50.080<br>
我就很少看到 Model-based 的方法<br>
<br>
0:28:50.080,0:28:54.100<br>
看起來做 Model-based 的方法在電玩上是比較困難的<br>
<br>
0:28:54.100,0:28:57.880<br>
以下是一些 reference 如果想學更多的話<br>
<br>
0:28:57.880,0:29:00.600<br>
Sutton 的教科書在這裡<br>
<br>
0:29:00.600,0:29:03.120<br>
David Silver 有十堂課<br>
<br>
0:29:03.120,0:29:09.220<br>
它的內容就是 base on Sutton 的教科書講的<br>
<br>
0:29:09.220,0:29:11.580<br>
他講的十堂課，每堂有一個半小時<br>
<br>
0:29:11.580,0:29:14.460<br>
他沒講太多 Deep Reinforcement Learning 的東西<br>
<br>
0:29:14.460,0:29:17.720<br>
但是他有一個 Deep Reinforcement Learning 的 tutorial<br>
<br>
0:29:17.740,0:29:19.720<br>
video lecture 往下找就找到<br>
<br>
0:29:19.720,0:29:28.000<br>
另外你可以找到這個 OpenAI、John Schulman 的 lecture<br>
<br>
0:29:28.000,0:29:32.680<br>
他 lecture 講的是 Policy-based 的方法<br>
<br>
0:29:51.260,0:29:57.060<br>
我們就先來講怎麼學一個 Actor<br>
<br>
0:29:57.060,0:30:00.700<br>
所謂的 Actor 是甚麼<br>
<br>
0:30:01.280,0:30:08.720<br>
開學的時候就有說過 Machine Learning 在做的事情就是找一個 function<br>
<br>
0:30:08.720,0:30:11.300<br>
在 Reinforcement Learning 裡面<br>
<br>
0:30:11.300,0:30:17.140<br>
Reinforcement Learning 也是 Machine Learning 的一種<br>
要做的事情也是找一個 function<br>
<br>
0:30:21.320,0:30:27.540<br>
我沒有寫錯，我本來想說我應該寫 Actor，但這邊我沒有寫錯<br>
<br>
0:30:27.780,0:30:30.120<br>
Actor 就是一個 function<br>
<br>
0:30:30.120,0:30:34.940<br>
這個 Actor 通常就寫成 pi，用 pi 來代表這個 function<br>
<br>
0:30:34.940,0:30:40.340<br>
這個 function 的 input 就是 machine 看到的 observation<br>
<br>
0:30:40.340,0:30:45.840<br>
他的 output 就是 machine 要採取的 action<br>
<br>
0:30:45.840,0:30:50.220<br>
observation 就是現在要找的 function 的 input<br>
<br>
0:30:50.220,0:30:54.540<br>
action 就是現在要找的 function 的 output<br>
<br>
0:30:54.540,0:31:02.160<br>
我們要透過 reward 幫助我們找出這個 function<br>
也就是幫助我們找出 Actor<br>
<br>
0:31:02.700,0:31:10.500<br>
在有些文獻上 Actor 又叫作 Policy<br>
所以看到 Policy 的時候他指的就是 Actor<br>
<br>
0:31:10.500,0:31:14.660<br>
找這個 function 有三個步驟<br>
<br>
0:31:14.660,0:31:18.300<br>
Deep Learning 很簡單的就是三個步驟<br>
<br>
0:31:18.300,0:31:25.740<br>
第一個步驟就是決定 function 長甚麼樣子<br>
<br>
0:31:25.740,0:31:30.780<br>
決定你的 function space<br>
Neural Network 他決定了一個 function space<br>
<br>
0:31:30.780,0:31:34.100<br>
所以 Actor 他可以就是一個 Neural Network<br>
<br>
0:31:34.100,0:31:39.800<br>
如果你的 Actor 就是一個 Neural Network 那你就是在做 Deep Reinforcement Learning<br>
<br>
0:31:40.580,0:31:43.160<br>
所以這個 Neural Network 的 input<br>
<br>
0:31:43.160,0:31:45.200<br>
就是 Machine 看到的 observation<br>
<br>
0:31:45.200,0:31:49.860<br>
這 observation 就是一堆 pixel 可以把他用一個 vector 來描述<br>
<br>
0:31:49.860,0:31:53.200<br>
或者是用一個 matrix 來描述<br>
<br>
0:31:53.200,0:31:58.360<br>
output 就是現在可以採取的 action<br>
<br>
0:31:58.360,0:32:03.240<br>
或者是直接看下面這個例子可能會比較清楚<br>
<br>
0:32:03.240,0:32:05.260<br>
output 是甚麼<br>
<br>
0:32:05.260,0:32:09.580<br>
input 就是 pixel<br>
把 Neural Network 當作 Actor<br>
<br>
0:32:09.580,0:32:14.380<br>
他可能不只是一個簡單的 Feed Forward Network 因為你的 input 現在是張 image<br>
<br>
0:32:14.380,0:32:17.000<br>
所以裡面應該會有 Convolution Layer<br>
<br>
0:32:17.000,0:32:20.360<br>
所以應該是會用 Convolutional Neural Network<br>
<br>
0:32:20.400,0:32:21.860<br>
output 的地方呢<br>
<br>
0:32:21.860,0:32:25.460<br>
現在有幾個可以採取的 action<br>
<br>
0:32:25.460,0:32:29.140<br>
output 就有幾個 dimension<br>
<br>
0:32:29.140,0:32:32.420<br>
假設現在在玩 Space Invader 這個遊戲<br>
<br>
0:32:32.420,0:32:36.240<br>
可以採取的 action 就是左移、右移跟開火<br>
<br>
0:32:36.240,0:32:48.400<br>
那 output layer 就只需要三個 dimension 分別代表左移、右移跟開火<br>
<br>
0:32:48.400,0:32:53.300<br>
這個 Neural Network 怎麼決定 Actor 採取哪個 action<br>
<br>
0:32:53.300,0:33:01.060<br>
通常做法是這樣，把 image 丟到 Neural Network 裡面去<br>
<br>
0:33:01.060,0:33:07.620<br>
他就會告訴你每一個 output 的 dimension 也就是每一個 action 所對應的分數<br>
<br>
0:33:07.720,0:33:11.220<br>
你可以採取分數最高的 action<br>
<br>
0:33:11.220,0:33:16.100<br>
比如說 left 分數最高<br>
假設已經找好這個 Actor<br>
<br>
0:33:16.100,0:33:20.300<br>
machine 看到這個畫面他可能就採取 left<br>
<br>
0:33:20.620,0:33:25.820<br>
但是做 Policy Gradient 的時候<br>
<br>
0:33:25.820,0:33:30.260<br>
通常會假設 Actor 是 stochastic<br>
<br>
0:33:30.260,0:33:33.300<br>
Policy 是 stochastic<br>
<br>
0:33:33.300,0:33:37.760<br>
所謂的 stochastic 的意思是你的 Policy 的 output 其實是個機率<br>
<br>
0:33:37.760,0:33:42.020<br>
如果你的分數是 0.7、0.2 跟 0.1<br>
<br>
0:33:42.020,0:33:45.620<br>
有 70% 的機率會 left<br>
<br>
0:33:45.680,0:33:49.900<br>
有 20% 的機率會 right<br>
10% 的機率會 fire<br>
<br>
0:33:49.900,0:33:57.760<br>
看到同樣畫面的時候，根據機率，同一個 Actor 會採取不同的 action<br>
<br>
0:33:57.760,0:34:01.460<br>
這種 stochastic 的做法其實很多時候是會有好處的<br>
<br>
0:34:01.460,0:34:03.660<br>
比如說要玩猜拳<br>
<br>
0:34:03.660,0:34:05.260<br>
要玩猜拳的時候<br>
<br>
0:34:05.260,0:34:11.540<br>
如果 Actor 是 deterministic，可能就只會出石頭一直輸跟小叮噹一樣<br>
<br>
0:34:11.540,0:34:18.060<br>
所以有時候會需要 stochastic 這種 Policy<br>
<br>
0:34:18.060,0:34:23.000<br>
在底下的 lecture 裡面都假設 Actor 是 stochastic 的<br>
<br>
0:34:24.860,0:34:32.960<br>
用 Neural Network 來當 Actor 有甚麼好處<br>
<br>
0:34:34.620,0:34:37.920<br>
傳統的作法是直接存一個 table<br>
<br>
0:34:37.920,0:34:42.100<br>
這個 table 告訴我看到這個 observation 就採取這個 action<br>
<br>
0:34:42.100,0:34:44.840<br>
看到另外一個 observation 就採取另外一個 action<br>
<br>
0:34:44.860,0:34:50.420<br>
但這種作法要玩電玩是不行的<br>
<br>
0:34:50.420,0:34:55.240<br>
因為電玩的 input 是 pixel，要窮舉所有可能 pixel 是沒有辦法做到的<br>
<br>
0:34:55.240,0:35:00.940<br>
所以一定要用 Neural Network 才能夠讓 machine 把電玩玩好<br>
<br>
0:35:00.940,0:35:05.480<br>
用 Neural Network 的好處就是 Neural Network 可以舉一反三<br>
<br>
0:35:05.480,0:35:09.300<br>
就算有些畫面完全沒有看過<br>
<br>
0:35:09.300,0:35:10.800<br>
machine 從來沒有看過<br>
<br>
0:35:13.340,0:35:17.460<br>
因為 Neural Network 的特性<br>
給他 input 一個東西總是會有 output<br>
<br>
0:35:17.460,0:35:23.020<br>
就算是他沒有看過的東西<br>
他也有可能得到一個合理的結果<br>
<br>
0:35:23.020,0:35:27.680<br>
用 Neural Network 的好處是他比較 generalize<br>
<br>
0:35:30.220,0:35:34.680<br>
再來第二個步驟就是要決定一個 function 的好壞<br>
<br>
0:35:34.680,0:35:39.280<br>
也就是要決定一個 Actor 的好壞<br>
<br>
0:35:39.280,0:35:42.760<br>
在 Supervised Learning 怎麼決定 function 的好壞<br>
<br>
0:35:42.760,0:35:48.360<br>
假設給一個 Neural Network <br>
他的參數假設已經知道就是 theta<br>
<br>
0:35:48.360,0:35:53.040<br>
有一堆 training example<br>
假設在做 image classification<br>
<br>
0:35:53.040,0:35:56.780<br>
就把 image 丟進去看 output 跟 target 像不像<br>
<br>
0:35:56.780,0:35:59.660<br>
如果越像代表 function 越好<br>
<br>
0:35:59.660,0:36:02.600<br>
會定義一個東西叫做 Loss<br>
<br>
0:36:02.600,0:36:05.880<br>
算每一個 example 的 Loss 合起來就是 Total Loss<br>
<br>
0:36:05.880,0:36:09.620<br>
需要找一個參數去 minimize 這個 Total Loss<br>
<br>
0:36:09.620,0:36:16.900<br>
其實在 Reinforcement Learning 裡面，一個 Actor 的好壞的定義其實是非常類似的<br>
<br>
0:36:16.900,0:36:18.280<br>
怎麼樣類似法<br>
<br>
0:36:18.280,0:36:23.300<br>
假設有一個 Actor，Actor 就是一個 Neural Network<br>
<br>
0:36:23.300,0:36:27.700<br>
這個 Neural Network 假設他的參數是 theta<br>
<br>
0:36:27.700,0:36:32.040<br>
一個 Actor 會用 pi 下標 theta 來表示它<br>
<br>
0:36:32.040,0:36:35.060<br>
一個 Actor 是一個 function 他的 input 就是一個 s<br>
<br>
0:36:35.060,0:36:40.040<br>
這個 s 就是 machine 看到的、Actor 看到的 observation<br>
<br>
0:36:40.040,0:36:43.980<br>
怎麼知道一個 Actor 表現好還是不好<br>
<br>
0:36:43.980,0:36:48.100<br>
就讓 Actor 實際的去玩一下這個遊戲<br>
<br>
0:36:48.100,0:36:54.600<br>
假設拿 pi( s )<br>
拿參數是 theta 這個 Actor<br>
<br>
0:36:54.760,0:36:59.400<br>
實際去玩一個遊戲<br>
他就玩了一個 episode<br>
<br>
0:36:59.400,0:37:06.360<br>
他說他看到 s1、take action a1、得到 r1<br>
再看到 s2、take action a2、得到 r2 等等等<br>
<br>
0:37:06.360,0:37:11.300<br>
最後就結束了<br>
這個時候玩完遊戲以後<br>
<br>
0:37:11.300,0:37:16.460<br>
他得到的 Total Reward 可以寫成 Rθ<br>
<br>
0:37:16.460,0:37:20.720<br>
這個 Rθ 就是 r1 + r2 一直加到 rT<br>
<br>
0:37:20.720,0:37:28.540<br>
把所有在每一個 step 得到的 reward合起來就是在這一個 episode 裡面得到的 Total Reward<br>
<br>
0:37:28.540,0:37:34.120<br>
而 episode 裡面的 Total Reward 才是我們要 maximize 的對象<br>
<br>
0:37:34.120,0:37:37.080<br>
我們不是要去 maximize 每一個 step 的 reward<br>
<br>
0:37:37.080,0:37:42.840<br>
我們是要去 maximize 整個遊戲玩完會得到的 Total Reward<br>
<br>
0:37:43.860,0:37:48.560<br>
但是就算拿同一個 Actor 來玩這個遊戲<br>
<br>
0:37:48.560,0:37:53.340<br>
每次玩的時候 Rθ 其實都會是不一樣<br>
<br>
0:37:53.340,0:37:56.980<br>
為甚麼？因為兩個原因<br>
<br>
0:37:56.980,0:38:04.900<br>
首先 Actor 如果是 stochastic 看到同樣的場景<br>
也會採取不同的 action<br>
<br>
0:38:04.900,0:38:07.840<br>
就算是同一個 Actor、同一組參數<br>
<br>
0:38:07.840,0:38:11.140<br>
每次玩的時候得到的 Rθ 也會是不一樣<br>
<br>
0:38:11.140,0:38:13.700<br>
再來遊戲本身也有隨機性<br>
<br>
0:38:13.800,0:38:18.520<br>
就算採取同樣的 action，看到的 observation 每一次也可能都不一樣<br>
<br>
0:38:18.660,0:38:20.560<br>
所以遊戲本身也有隨機性<br>
<br>
0:38:20.560,0:38:24.520<br>
所以 Rθ 是一個 random variable<br>
<br>
0:38:24.780,0:38:30.920<br>
所以我們希望做的事情不是去 maximize 某一次玩遊戲得到的 Rθ<br>
<br>
0:38:30.920,0:38:35.700<br>
希望去 maximize 的其實是 Rθ 的期望值<br>
<br>
0:38:35.700,0:38:41.540<br>
拿同一個 Actor 玩了千百次遊戲以後<br>
<br>
0:38:41.540,0:38:45.940<br>
每次 Rθ 都不一樣但這個 Rθ 的期望值是多少<br>
<br>
0:38:45.940,0:38:49.200<br>
這邊用 Rθ bar 來表示它的期望值<br>
<br>
0:38:49.200,0:38:53.680<br>
希望這個期望值越大越好<br>
<br>
0:38:55.260,0:39:01.460<br>
這個期望值就衡量了某一個 Actor 的好壞<br>
<br>
0:39:01.460,0:39:06.740<br>
好的 Actor 他的期望值就應該要比較大<br>
<br>
0:39:07.340,0:39:13.720<br>
這個期望值實際上要怎麼計算出來<br>
<br>
0:39:13.720,0:39:16.960<br>
你可以這麼做<br>
<br>
0:39:16.960,0:39:23.320<br>
假設一場遊戲就是一個 trajectory τ<br>
一場遊戲用 τ 來表示<br>
<br>
0:39:23.320,0:39:33.120<br>
τ 是一個 sequence 裡面包含了 state、包含 observation，看到這個 observation 以後 take 的 action<br>
<br>
0:39:33.120,0:39:39.980<br>
還有得到的 reward，還有新的 observation、take 的 action、得到的 reward 等等，他是一個 sequence<br>
<br>
0:39:41.060,0:39:46.580<br>
R(τ) 代表這個 trajectory 在這場遊戲最後得到的 Total Reward<br>
<br>
0:39:46.580,0:39:52.320<br>
把所有的小 r summation 起來就是 total 的 reward<br>
<br>
0:39:52.540,0:39:57.980<br>
當我們用某一個 Actor 去玩這個遊戲的時候<br>
<br>
0:39:57.980,0:40:02.620<br>
每一個 τ 都會有一個出現的機率<br>
<br>
0:40:02.800,0:40:06.860<br>
這個大家可以想像嗎<br>
<br>
0:40:06.960,0:40:17.660<br>
就是 τ 代表某一種可能的從遊戲開始到結束的過程<br>
<br>
0:40:17.660,0:40:23.140<br>
他代表某一種過程<br>
這個過程有千千百百種<br>
<br>
0:40:23.220,0:40:28.700<br>
但是當你選擇了一個 Actor 去玩這個遊戲的時候<br>
<br>
0:40:28.700,0:40:32.640<br>
你可能只會看到某一些過程<br>
<br>
0:40:32.640,0:40:36.680<br>
某一些過程特別容易出現<br>
某一些過程比較不容易出現<br>
<br>
0:40:36.680,0:40:41.240<br>
比如說現在 Actor 是一個很智障的 Actor<br>
<br>
0:40:41.240,0:40:44.440<br>
他看到敵人的子彈就要湊上去被自殺<br>
<br>
0:40:44.440,0:40:51.700<br>
你看到的每一個 τ 都是你自己控制的太空船挪一下<br>
然後就去自殺了<br>
<br>
0:40:51.700,0:40:57.760<br>
當你選擇 Actor 的時候就會有一些 τ 特別容易出現<br>
<br>
0:40:57.760,0:41:00.740<br>
只有某一些遊戲的過程特別容易出現<br>
<br>
0:41:00.740,0:41:06.500<br>
每一個遊戲出現的過程可以用機率來描述他<br>
<br>
0:41:06.500,0:41:09.560<br>
這邊寫一個 P( τ | θ )<br>
<br>
0:41:09.560,0:41:15.740<br>
就是當 Actor 的參數是 θ 的時候<br>
τ 這個過程出現的機率<br>
<br>
0:41:17.180,0:41:19.420<br>
如果可以接受這樣子的話<br>
<br>
0:41:19.420,0:41:26.080<br>
那 Rθ 的期望值，Rθ bar 就寫成<br>
<br>
0:41:26.080,0:41:31.400<br>
summation over 所有可能的 τ<br>
所有可能的遊戲進行的過程<br>
<br>
0:41:31.400,0:41:36.520<br>
當然這個非常非常的多<br>
尤其如果又是玩電玩<br>
<br>
0:41:36.520,0:41:38.600<br>
他是連續的<br>
<br>
0:41:38.600,0:41:42.680<br>
他有非常多的可能<br>
這個 τ 是難以窮舉<br>
<br>
0:41:42.680,0:41:44.900<br>
現在假設可以窮舉他<br>
<br>
0:41:44.900,0:41:48.400<br>
每一個 τ 都有一個機率用 P( τ | θ )<br>
<br>
0:41:48.400,0:41:51.240<br>
每一個 τ 都有一個 reward R(τ)<br>
<br>
0:41:51.240,0:41:55.600<br>
把這兩個乘起來<br>
再 summation over 所有遊戲可能的 τ 的話<br>
<br>
0:41:55.600,0:42:05.100<br>
那就得到了這個 Actor 他期望的這個 reward<br>
<br>
0:42:05.100,0:42:12.060<br>
實際上要窮舉所有的 τ 是不可能的<br>
所以怎麼做<br>
<br>
0:42:12.060,0:42:17.220<br>
讓 Actor 去玩這個遊戲玩 N 場<br>
<br>
0:42:17.220,0:42:20.980<br>
得到 τ1、τ2 到 τN<br>
<br>
0:42:20.980,0:42:24.860<br>
這 N 場就好像是 N 筆 training data 這樣子<br>
<br>
0:42:27.960,0:42:37.460<br>
玩 N 場這個遊戲就好像是從 P( τ | θ ) sample 出 N 個 τ<br>
<br>
0:42:37.700,0:42:40.900<br>
假設某一個 τ 他的機率特別大<br>
<br>
0:42:40.900,0:42:46.380<br>
他就特別容易在 N 次 sample 裡面被 sample 出來<br>
<br>
0:42:46.380,0:42:51.200<br>
sample 出來的 τ 應該是跟機率成正比的<br>
<br>
0:42:51.260,0:42:54.120<br>
當用這個 Actor 玩 N 場遊戲的時候<br>
<br>
0:42:54.120,0:42:59.920<br>
就好像是從 P( τ | θ ) 這個機率裡面去做 N 次 sample 一樣<br>
<br>
0:42:59.920,0:43:02.620<br>
最後得到的結果是甚麼<br>
<br>
0:43:05.640,0:43:15.440<br>
最後就是把 N 個 τ 的 reward 都算出來<br>
<br>
0:43:15.440,0:43:22.580<br>
然後再平均起來<br>
就可以拿這一項去近似這一項<br>
<br>
0:43:22.580,0:43:26.440<br>
對不對<br>
這個大家應該沒有甚麼問題<br>
<br>
0:43:28.420,0:43:34.160<br>
接下來只要記得 summation over N 次 sample 做平均<br>
<br>
0:43:34.160,0:43:36.460<br>
對 N 次 sample 做平均<br>
<br>
0:43:36.560,0:43:42.140<br>
其實就可以近似從 θ sample τ 出來<br>
<br>
0:43:42.460,0:43:44.680<br>
再 summation over 所有的 τ<br>
<br>
0:43:44.680,0:43:47.380<br>
summation over 所有的 τ 乘上機率這件事情<br>
<br>
0:43:47.380,0:43:51.880<br>
跟 sample N 次這件事情是可以近似的<br>
<br>
0:43:52.100,0:43:56.140<br>
接下來就進入最後第三步<br>
<br>
0:43:56.140,0:44:01.860<br>
我們知道怎麼衡量一個 Actor<br>
<br>
0:44:01.860,0:44:04.560<br>
接下來就是要選一個最好的 Actor<br>
<br>
0:44:04.560,0:44:08.360<br>
怎麼選一個最好的 Actor<br>
其實就是用 Gradient Descent<br>
<br>
0:44:08.360,0:44:11.180<br>
現在已經有我們的 Objective Function<br>
<br>
0:44:11.180,0:44:18.580<br>
我們已經找到目標了<br>
目標就是要最大化這個 Rθ bar<br>
<br>
0:44:18.580,0:44:20.760<br>
找一個參數最大化 Rθ bar<br>
<br>
0:44:20.760,0:44:23.580<br>
Rθ bar 的式子也有了就寫在這邊<br>
<br>
0:44:23.580,0:44:29.280<br>
接下來就可以用 Gradient Ascent 的方法找一個 θ<br>
<br>
0:44:29.280,0:44:31.460<br>
讓 Rθ bar 的值最大<br>
<br>
0:44:31.460,0:44:35.360<br>
這邊不做 Gradient Descent ，因為 Gradient Descent 要去 minimize 一個東西用 Gradient Descent<br>
<br>
0:44:35.360,0:44:38.400<br>
maximize 一個東西用 Gradient Ascent<br>
<br>
0:44:38.400,0:44:42.940<br>
怎麼做<br>
很簡單就先隨機的找一個初始的 θ0<br>
<br>
0:44:42.940,0:44:53.160<br>
隨機找一個初始的 Actor<br>
然後計算在使用初始的 Actor 的情況下<br>
<br>
0:44:53.160,0:44:58.900<br>
你的參數對 Rθ bar 的微分<br>
<br>
0:44:58.900,0:45:02.740<br>
算出你的參數對 Rθ bar 的微分<br>
<br>
0:45:02.740,0:45:05.860<br>
再去 update 你的參數得到 θ1<br>
<br>
0:45:05.860,0:45:09.340<br>
接下來再算 θ1 對 Rθ bar 的微分<br>
<br>
0:45:09.340,0:45:12.880<br>
然後再 update θ1 得到 θ2<br>
<br>
0:45:12.880,0:45:20.660<br>
用這個 process 最後就可以得到一個可以讓 Rθ bar 最大的 Actor<br>
<br>
0:45:20.660,0:45:26.080<br>
當然會有 local optimum 種種問題，就跟做 Deep Learning 的時候是一樣的<br>
<br>
0:45:28.960,0:45:33.480<br>
所謂的 Rθ bar 的 gradient 是甚麼<br>
<br>
0:45:33.480,0:45:37.820<br>
假設 θ 裡面就是一堆參數，有一堆 weight 有一堆 bias<br>
<br>
0:45:37.820,0:45:44.800<br>
就是把所有的 weight、所有的 bias 都對 Rθ bar 做偏微分，把他通通串起來變成個 vector<br>
<br>
0:45:44.800,0:45:48.020<br>
就是這個 gradient<br>
<br>
0:45:48.020,0:45:53.220<br>
接下來就是實際來運算一下<br>
<br>
0:45:53.220,0:45:58.360<br>
如果要計算 Rθ bar 的 gradient<br>
<br>
0:45:58.360,0:46:03.960<br>
那 Rθ bar = summation over 所有的 τ <br>
R( τ ) * P( τ | θ )<br>
<br>
0:46:04.080,0:46:09.880<br>
這個 R(τ) 跟 θ 是沒任何關係的<br>
<br>
0:46:09.960,0:46:13.760<br>
只有 P( τ | θ ) 跟 θ 才是有關係的<br>
<br>
0:46:13.760,0:46:20.400<br>
所以做 gradient 的時候只需要對 P( τ | θ ) 做 gradient 就好<br>
<br>
0:46:20.640,0:46:23.560<br>
R(τ) 不需要對 θ 做 gradient<br>
<br>
0:46:23.560,0:46:28.820<br>
所以 R(τ) 就算是不可微的<br>
<br>
0:46:28.820,0:46:32.140<br>
也沒差因為本來就沒有要微分他<br>
<br>
0:46:32.380,0:46:36.400<br>
就算 R(τ) 他是個黑盒子<br>
不知道他的式子<br>
<br>
0:46:36.400,0:46:44.440<br>
只知道把 τ 帶進去，R 的 output 會是甚麼也無所謂<br>
<br>
0:46:44.440,0:46:51.220<br>
也能夠做<br>
因為我們在這邊完全不需要知道<br>
<br>
0:47:01.590,0:47:06.300<br>
這邊就算 R(τ) 不可微<br>
<br>
0:47:06.480,0:47:11.280<br>
或者是不知道他的 function 也沒差<br>
因為不需要對它做微分<br>
<br>
0:47:11.280,0:47:14.680<br>
根本就不需要知道他長甚麼樣子<br>
<br>
0:47:14.680,0:47:19.920<br>
我們只需要知道把 τ 放進去的時候他 output 的值會是多少就行了<br>
<br>
0:47:19.920,0:47:23.040<br>
他可以完全徹頭徹尾就是個黑盒子<br>
<br>
0:47:23.040,0:47:26.840<br>
實際上對我們來說 R(τ) 也確實徹頭徹尾是個黑盒子<br>
<br>
0:47:26.840,0:47:29.760<br>
因為 R(τ) 是取決於<br>
<br>
0:47:29.760,0:47:33.580<br>
我們會得到多少 reward<br>
那個 reward 是環境給我們的<br>
<br>
0:47:33.580,0:47:36.540<br>
所以我們通常對環境是沒有理解<br>
<br>
0:47:36.540,0:47:40.020<br>
比如說在玩 Atari 的遊戲裡面<br>
<br>
0:47:40.660,0:47:45.580<br>
reward 是 Atari 的那個程式給我們的<br>
<br>
0:47:45.660,0:47:48.360<br>
如果程式是有一些隨機的東西的話<br>
<br>
0:47:48.360,0:47:51.160<br>
會根本搞不清楚程式的內容是甚麼<br>
<br>
0:47:51.160,0:47:53.480<br>
其實就根本不知道 R(τ) 是長甚麼樣子<br>
<br>
0:47:53.480,0:47:55.740<br>
不過反正你不需要知道他長甚麼樣子<br>
<br>
0:47:57.580,0:47:59.980<br>
接下來怎麼做<br>
<br>
0:47:59.980,0:48:08.860<br>
接下來要做一件事情<br>
做這件事情是為了要讓 P( τ | θ ) 出現<br>
<br>
0:48:09.040,0:48:15.120<br>
把 P( τ | θ ) 放在分子的地方也放在分母的地方<br>
<br>
0:48:15.120,0:48:17.380<br>
等於甚麼事都沒有做<br>
<br>
0:48:17.420,0:48:24.360<br>
接下來這一項會等於這一項<br>
<br>
0:48:24.360,0:48:27.500<br>
為甚麼<br>
<br>
0:48:28.840,0:48:36.460<br>
dlog(f(x)) / dx = (1 / f(x)) * (df(x) / dx)<br>
<br>
0:48:36.460,0:48:40.240<br>
所以對 log  P( τ | θ ) 做 gradient<br>
<br>
0:48:40.240,0:48:45.300<br>
等於對 P( τ | θ ) 做 gradient 再除以 P( τ | θ )<br>
<br>
0:48:45.300,0:48:49.100<br>
所以這一項就是這一項<br>
<br>
0:48:50.660,0:48:59.020<br>
如果你看到 summation over 所有的 τ 再乘上 P( τ | θ ) 的話<br>
<br>
0:48:59.160,0:49:04.080<br>
當看到紅色這個框框的時候可以把它換成 sampling<br>
<br>
0:49:04.180,0:49:11.360<br>
所以這件事情可以換成拿 θ 玩 N 次遊戲得到 τ1 到 τN<br>
<br>
0:49:11.360,0:49:15.740<br>
對 τ1 到 τN 都算出他的 R(τ)<br>
<br>
0:49:15.740,0:49:20.540<br>
再 summation over 所有的 sample 出來的結果再取平均<br>
<br>
0:49:20.540,0:49:25.880<br>
接下來的問題是怎麼計算這一項<br>
<br>
0:49:25.880,0:49:30.500<br>
怎麼計算 log * P( τ | θ ) 的 gradient<br>
<br>
0:49:30.500,0:49:36.780<br>
這一項也不難算可以很快地帶過去<br>
<br>
0:49:36.780,0:49:40.080<br>
要算這一項要知道 P( τ | θ )<br>
<br>
0:49:40.080,0:49:41.740<br>
怎麼算 P( τ | θ )<br>
<br>
0:49:41.740,0:49:51.060<br>
首先要知道 P( τ | θ ) = p(s1)  也就是遊戲開始的畫面的出現的機率<br>
<br>
0:49:51.060,0:49:54.920<br>
像 Space Invader 我記得每一次開始的畫面都是一樣的<br>
<br>
0:49:54.920,0:50:00.760<br>
所以 p(s1) 就是只有某一個畫面的機率是 1<br>
其他都是 0<br>
<br>
0:50:00.760,0:50:04.180<br>
有一些遊戲的畫面每次的起始畫面是不一樣的<br>
<br>
0:50:04.180,0:50:06.560<br>
這邊需要有個機率<br>
<br>
0:50:07.520,0:50:11.600<br>
接下來根據 θ<br>
<br>
0:50:11.600,0:50:15.800<br>
在 s1 會有某一個機率採取 a1<br>
<br>
0:50:15.800,0:50:22.820<br>
接下來根據在 s1 採取 a1 這件事情<br>
<br>
0:50:23.000,0:50:29.600<br>
會得到 r1 然後跳到 s2<br>
這中間是有個機率的，取決於那個遊戲<br>
<br>
0:50:29.600,0:50:34.560<br>
接下來在 s2 採取 a2 這個機率取決於你的 model θ<br>
<br>
0:50:34.560,0:50:37.220<br>
接下來看到 s2 a2 得到 r2 s3<br>
<br>
0:50:37.220,0:50:39.080<br>
這也是取決於那個遊戲<br>
<br>
0:50:39.160,0:50:44.000<br>
所以整個畫起來就是這個樣子<br>
<br>
0:50:44.400,0:50:51.660<br>
其中某些項跟 agent、Actor 是沒有關係的<br>
<br>
0:50:51.660,0:50:57.060<br>
只有畫紅色底線這一項跟 Actor 是有關係的<br>
<br>
0:51:00.940,0:51:05.380<br>
接下來就取 log<br>
<br>
0:51:05.420,0:51:10.960<br>
取 log 就只是相乘變相加而已<br>
<br>
0:51:11.740,0:51:15.560<br>
接下來可以對 θ 做 gradient<br>
<br>
0:51:15.560,0:51:17.500<br>
跟 θ 無關的項<br>
<br>
0:51:17.500,0:51:24.060<br>
跟 agent 無關的項只取決於遊戲的主機<br>
<br>
0:51:24.060,0:51:27.460<br>
遊戲的部分那一項就可以直接被刪掉<br>
<br>
0:51:27.460,0:51:31.400<br>
這兩項都跟 gradient 是無關的<br>
<br>
0:51:31.400,0:51:34.200<br>
這一項可以刪掉，這一項可以刪掉<br>
<br>
0:51:34.200,0:51:38.200<br>
只剩下這個部分<br>
<br>
0:51:38.200,0:51:42.100<br>
所以最後算出來的結果就是<br>
<br>
0:51:42.100,0:51:44.920<br>
Rθ bar 的 gradient<br>
<br>
0:51:45.180,0:51:50.040<br>
它可以被 approximate 甚麼樣子<br>
<br>
0:51:55.120,0:52:04.300<br>
sample 出 N 個 τ，每一個 τ 都算出他的 reward 再乘上每一個 τ 的出現的機率的 log 的 gradient<br>
<br>
0:52:04.360,0:52:10.740<br>
出現的機率的 log 的 gradient 又可以把他算成是<br>
<br>
0:52:10.740,0:52:16.380<br>
summation over 在這個 τ 裡面<br>
所有採取過的 action<br>
<br>
0:52:16.380,0:52:19.400<br>
他的機率取 log 的 gradient<br>
<br>
0:52:19.400,0:52:22.360<br>
這一項就等於這一項我只是把他置換一下<br>
<br>
0:52:22.360,0:52:27.480<br>
把這個 summation 移出去<br>
把這個 R 乘進來<br>
<br>
0:52:27.480,0:52:31.780<br>
可以寫成這樣的式子<br>
<br>
0:52:31.780,0:52:34.440<br>
這個式子告訴我們甚麼<br>
<br>
0:52:34.620,0:52:40.520<br>
這個式子告訴我們，現在要做的事情<br>
假設在 data 裡面<br>
<br>
0:52:40.520,0:52:43.960<br>
在 s 上標 n 下標 t 這個 state<br>
<br>
0:52:43.960,0:52:50.180<br>
我們曾經採取了 a 上標 n 下標 t 這個 action<br>
<br>
0:52:50.180,0:52:54.580<br>
就計算這件事情根據我們的 model 現在發生的機率<br>
<br>
0:52:54.800,0:52:59.560<br>
然後把它取 log 然後計算它的 gradient<br>
<br>
0:52:59.560,0:53:04.000<br>
這項 gradient 前面會乘上一項<br>
乘上這一項是<br>
<br>
0:53:04.120,0:53:09.960<br>
這一個 trajectory 在那一次玩遊戲裡面<br>
<br>
0:53:10.040,0:53:17.040<br>
在看到這個 s 產生這個 a 的那一次遊戲裡面總共得到的 Total Reward<br>
<br>
0:53:17.040,0:53:23.840<br>
這整個式子其實是非常直覺的<br>
<br>
0:53:23.840,0:53:27.820<br>
用這一項去 update model 其實是非常直覺的<br>
<br>
0:53:27.820,0:53:33.800<br>
因為它的意思是說<br>
假設某一次玩遊戲的過程中<br>
<br>
0:53:33.800,0:53:35.760<br>
在 τn 這次玩遊戲的過程中<br>
<br>
0:53:35.820,0:53:42.720<br>
我們在 s 上標 n 下標 t 採取 action a 上標 n 下標 t<br>
<br>
0:53:42.720,0:53:48.840<br>
而最後導致的結果是整個遊戲的 R(τ) 是正的<br>
<br>
0:53:48.840,0:53:56.000<br>
得到一個正的  reward<br>
就會希望說這個機率是越大越好<br>
<br>
0:53:56.000,0:54:01.260<br>
我在某一次玩遊戲的時候<br>
我在看到某一個 observation 的時候<br>
<br>
0:54:01.260,0:54:02.740<br>
採取某一個 action<br>
<br>
0:54:02.860,0:54:05.740<br>
而最後整個遊戲得到好的結果<br>
<br>
0:54:05.740,0:54:10.620<br>
就要調整我們的參數<br>
讓在這個 observation<br>
<br>
0:54:10.620,0:54:14.680<br>
採取那個 action 的機率變大<br>
<br>
0:54:14.680,0:54:18.980<br>
反之如果在玩遊戲的過程發現<br>
<br>
0:54:18.980,0:54:25.560<br>
在某一個 state 採取某一個 action<br>
結果發現得到的 reward 居然是負的<br>
<br>
0:54:25.560,0:54:32.260<br>
在之後看到同樣的 state 的時候、同樣的 observation 的時候<br>
<br>
0:54:32.260,0:54:39.240<br>
我們就希望採取會讓我們看到 negative reward 的那個 action 它的機率變小<br>
<br>
0:54:39.240,0:54:45.520<br>
這整個式子是非常直覺的<br>
<br>
0:54:45.520,0:54:49.380<br>
這邊要注意的事情是<br>
<br>
0:54:49.380,0:54:54.640<br>
這一項是在某一個時間點 t<br>
<br>
0:54:55.040,0:54:57.940<br>
的 observation 採取的某一個 action<br>
<br>
0:54:57.940,0:55:02.840<br>
但是我們必須要把它乘上整個 trajectory 的 reward<br>
<br>
0:55:02.840,0:55:08.000<br>
而不是採取那個 action 以後所產生的 reward<br>
<br>
0:55:08.000,0:55:12.100<br>
這件事情也是非常直覺<br>
直覺想就應該這麼做<br>
<br>
0:55:12.100,0:55:16.140<br>
假設現在不是考慮整個 trajectory 的 reward<br>
<br>
0:55:16.140,0:55:24.620<br>
而是考慮採取 action a 上標 n 下標 t 以後得到的 reward r 上標 n 下標 t 的話<br>
<br>
0:55:24.680,0:55:26.300<br>
那會變成說<br>
<br>
0:55:26.300,0:55:30.700<br>
只有 fire 會得到 reward<br>
<br>
0:55:30.700,0:55:35.720<br>
其他的 action 只要採取 left 或 right 的移動<br>
得到的 reward 都是 0<br>
<br>
0:55:35.720,0:55:41.900<br>
所以 machine 就永遠不會想要讓 left 跟 right 產生的機率增加，它只會讓 fire 機率增加<br>
<br>
0:55:41.900,0:55:45.720<br>
所以 learn 出來的 agent 就只會一直在原地開火<br>
<br>
0:55:46.640,0:55:51.220<br>
這個式子其實是很直覺的<br>
<br>
0:55:52.400,0:56:00.440<br>
這邊還有一個問題就是為甚麼要取 log<br>
<br>
0:56:02.260,0:56:11.480<br>
這件事情也是有辦法解釋的<br>
<br>
0:56:11.480,0:56:22.660<br>
你看這一項它其實就是對 p 的微分再除掉 p 的這個機率<br>
<br>
0:56:22.660,0:56:25.980<br>
它是微分再除掉機率<br>
<br>
0:56:25.980,0:56:29.480<br>
你可能會想說加這一項多不自然<br>
<br>
0:56:29.480,0:56:33.980<br>
把這一項就直接換成分子這一項不是感覺很好嗎<br>
<br>
0:56:33.980,0:56:38.280<br>
為甚麼下面還要除一個 p( a | s ) 的機率<br>
<br>
0:56:38.280,0:56:42.940<br>
你想想看這件事情是很有道理<br>
<br>
0:56:42.940,0:56:50.340<br>
假設現在讓 machine 去玩 N 次遊戲<br>
<br>
0:56:50.340,0:56:56.680<br>
那某一個 state 在第 13 次、第 15 次、第 17 次、第 33 次的遊戲裡面<br>
<br>
0:56:56.680,0:57:01.200<br>
看到了同一個 observation<br>
<br>
0:57:01.200,0:57:05.280<br>
因為 Actor 其實是 stochastic<br>
<br>
0:57:05.280,0:57:09.220<br>
所以它有個機率，所以看到同樣的 s，不見得採取同樣 action<br>
<br>
0:57:09.220,0:57:12.940<br>
所以假設在第 13 個 trajectory<br>
<br>
0:57:12.940,0:57:16.080<br>
它採取 action a，在第 17 個它採取 b<br>
<br>
0:57:16.200,0:57:21.300<br>
在 15 個採取 b<br>
在 33 也採取 b<br>
<br>
0:57:21.300,0:57:26.440<br>
然後最後 τ 13 的這個 trajectory 得到的 reward 比較大是 2<br>
<br>
0:57:26.500,0:57:28.760<br>
另外三次得到的 reward 比較小<br>
<br>
0:57:30.640,0:57:33.920<br>
但實際上在做 update 的時候<br>
<br>
0:57:33.920,0:57:38.220<br>
它會偏好那些出現次數比較多的 action<br>
<br>
0:57:38.220,0:57:40.800<br>
就算那些 action 並沒有真的比較好<br>
<br>
0:57:40.800,0:57:48.600<br>
對不對，因為是 summation over 所有 sample 到的結果<br>
<br>
0:57:48.600,0:57:51.220<br>
如果 take action b 這件事情<br>
<br>
0:57:51.220,0:57:55.980<br>
出現的次數比較多，就算它得到的 reward 沒有比較大<br>
<br>
0:57:56.140,0:57:59.320<br>
machine 把這件事情的機率調高<br>
<br>
0:57:59.320,0:58:05.380<br>
也可以增加最後這一項的結果<br>
<br>
0:58:05.380,0:58:07.820<br>
雖然這個 action 感覺比較好<br>
<br>
0:58:07.820,0:58:11.820<br>
但是因為它很罕見，所以調高這個 action 的機率<br>
<br>
0:58:12.020,0:58:20.780<br>
最後也不會對你要 maximize 的對象 Objective 的影響也是比較小的<br>
<br>
0:58:20.780,0:58:26.880<br>
machine 就會變成不想要 maximize action a 出現的機率<br>
轉而 maximize action b 出現的機率<br>
<br>
0:58:26.880,0:58:29.860<br>
這就是為甚麼這邊需要除掉一個機率<br>
<br>
0:58:29.860,0:58:32.740<br>
除掉這個機率的好處就是做一個 normalization<br>
<br>
0:58:32.740,0:58:36.040<br>
如果有某一個 action 它本來出現的機率就比較高<br>
<br>
0:58:36.040,0:58:38.560<br>
它除掉的值就比較大<br>
<br>
0:58:38.560,0:58:41.600<br>
讓它除掉一個比較大的值<br>
<br>
0:58:41.600,0:58:44.240<br>
machine 最後在 optimize 的時候<br>
<br>
0:58:44.240,0:58:48.120<br>
就不會偏好那些機率出現比較高的 action<br>
<br>
0:58:58.860,0:59:03.060<br>
這個聽起來都很 ok，但是這邊有一個問題<br>
<br>
0:59:03.060,0:59:04.340<br>
什麼樣的問題<br>
<br>
0:59:04.340,0:59:08.420<br>
假設 R(τ) 永遠是正的<br>
<br>
0:59:08.420,0:59:10.800<br>
會發生甚麼事呢<br>
<br>
0:59:10.800,0:59:13.540<br>
因為像玩 Space Invader<br>
<br>
0:59:13.540,0:59:17.400<br>
得到的 reward 都是正的，殺了外星人就得到正的分數<br>
<br>
0:59:17.400,0:59:20.400<br>
最糟就是殺不到外星人得到分數是 0<br>
<br>
0:59:20.400,0:59:24.300<br>
所以這個 R(τ) 永遠都是正的<br>
<br>
0:59:24.300,0:59:27.180<br>
在理想的狀況下<br>
<br>
0:59:27.420,0:59:31.360<br>
這件事情不會構成問題<br>
為甚麼<br>
<br>
0:59:31.360,0:59:35.100<br>
假設在某一個 state 可以採取三個 action a b c<br>
<br>
0:59:35.100,0:59:38.800<br>
那這三個 action 採取的結果<br>
<br>
0:59:38.800,0:59:42.580<br>
我們得到的 R(τ) 都是正的<br>
<br>
0:59:42.580,0:59:45.040<br>
這個正有大有小<br>
<br>
0:59:45.040,0:59:48.940<br>
假設 a 跟 c 的 R(τ) 比較大<br>
<br>
0:59:48.940,0:59:52.140<br>
b 的 R(τ) 比較小<br>
<br>
0:59:52.140,0:59:55.160<br>
經過 update 以後<br>
<br>
0:59:55.160,0:59:58.260<br>
還是會讓 b 出現的機率變小<br>
<br>
0:59:58.260,1:00:00.160<br>
a c 出現的機率變大<br>
<br>
1:00:00.260,1:00:04.620<br>
因為這邊是個機率，所以它會做 normalization<br>
<br>
1:00:04.620,1:00:12.100<br>
或是你為了要讓它機率裡的最後 network 的 output<br>
它是 soft-max layer<br>
<br>
1:00:12.100,1:00:15.720<br>
就算是在算 gradient 的時候<br>
<br>
1:00:15.720,1:00:18.000<br>
你想讓這三個的機率會增加<br>
<br>
1:00:18.000,1:00:20.460<br>
但是增加量比較少的那個<br>
<br>
1:00:20.560,1:00:23.400<br>
最後它的機率其實是會減少<br>
<br>
1:00:23.400,1:00:26.880<br>
聽起來不太成一個問題<br>
<br>
1:00:26.920,1:00:30.580<br>
但實作的時候，我們做的是 sampling<br>
<br>
1:00:30.580,1:00:35.840<br>
所以某一個 state 可以採取 a b c 三個 action<br>
<br>
1:00:35.840,1:00:39.980<br>
但是有可能只 sample 到 b 這個 action<br>
c 這個 action<br>
<br>
1:00:39.980,1:00:42.460<br>
而 a 這個 action 就沒 sample 到<br>
<br>
1:00:42.560,1:00:44.240<br>
很有可能 sample 就幾次而已<br>
<br>
1:00:44.240,1:00:52.540<br>
所以可能 a 這個 action machine 從來沒玩過它、沒試過它，不知道它的 R(τ) 到底有多大<br>
<br>
1:00:52.540,1:00:58.920<br>
這個時候就遇到問題了<br>
因為 b 跟 c 機率都會增加<br>
<br>
1:00:58.920,1:01:02.800<br>
a 沒 sample 到，沒 sample 到機率就自動減少<br>
<br>
1:01:02.800,1:01:06.600<br>
被 sample 到的在做玩 gradient 後的機率自動就會增加<br>
<br>
1:01:06.600,1:01:10.700<br>
這樣就變成問題了<br>
<br>
1:01:10.700,1:01:14.220<br>
所以怎麼辦<br>
這邊有一個很簡單的想法就是<br>
<br>
1:01:14.220,1:01:18.300<br>
我們希望 R(τ) 有正有負<br>
<br>
1:01:18.440,1:01:21.260<br>
不要都是正的<br>
怎麼避免都是正的呢<br>
<br>
1:01:21.260,1:01:24.980<br>
要把 R(τ) 減掉一個 bias<br>
<br>
1:01:24.980,1:01:30.520<br>
這個 bias 其實要自己設計<br>
<br>
1:01:30.520,1:01:33.860<br>
到底應該要放甚麼值<br>
設計一下這個 bias<br>
<br>
1:01:33.940,1:01:38.940<br>
讓你的 R(τ) 都是正的<br>
減掉一個正的 bias 讓它有正有負<br>
<br>
1:01:38.940,1:01:43.080<br>
如果你的 trajectory 某一個 τn 它是特別好的<br>
<br>
1:01:43.160,1:01:49.280<br>
這個 b 叫做 baseline，它好過 baseline 才把那個 action 的機率增加<br>
<br>
1:01:49.280,1:01:52.380<br>
小於 baseline 把它 action 的機率減小<br>
<br>
1:01:52.380,1:01:57.620<br>
這樣子就不會造成某一個 action 沒被 sample 到它的機率就會變小<br>
<br>
1:01:57.620,1:02:01.080<br>
因為快要下課了所以不見得要講玩 Critic<br>
<br>
1:02:01.080,1:02:03.100<br>
Critic 是甚麼<br>
Critic 就是<br>
<br>
1:02:03.100,1:02:05.300<br>
learn 一個 network 它不做事<br>
<br>
1:02:05.300,1:02:08.700<br>
其實也可以從 Critic 得到一個 Actor<br>
<br>
1:02:08.700,1:02:13.020<br>
這個東西就是 Q Learning<br>
但今天就沒辦法講 Q Learning<br>
<br>
1:02:13.020,1:02:17.360<br>
Critic 就是這樣子<br>
learn 一個 function<br>
<br>
1:02:17.360,1:02:26.120<br>
這個 function 可以告訴你<br>
現在看到某一個 observation 的時候<br>
<br>
1:02:26.120,1:02:31.040<br>
這個 observation 有多好這樣子<br>
<br>
1:02:31.040,1:02:34.820<br>
比如說看到這樣子的 observation<br>
<br>
1:02:34.820,1:02:39.020<br>
把它丟到 Critic 裡面去<br>
它可能會 output 一個很大的正值<br>
<br>
1:02:39.020,1:02:42.460<br>
因為還有很多 alien 可以殺<br>
所以會得到很高的分數<br>
<br>
1:02:42.460,1:02:48.660<br>
看到這個狀況可能較會得到相對比較少的值<br>
<br>
1:02:48.660,1:02:52.160<br>
因為 alien 變得比較少而且屏障消失了<br>
<br>
1:02:52.160,1:02:57.400<br>
這是屏障，屏障消失了，所以你可能很快就會死，分數就比較少<br>
<br>
1:02:57.580,1:03:03.420<br>
總之 Actor 跟 Critic 可以合在一起 train<br>
<br>
1:03:03.420,1:03:09.580<br>
合在一起 train 的好處就是簡單講這樣比較強這樣子<br>
<br>
1:03:09.580,1:03:14.740<br>
這個不一定要在這堂課解釋完<br>
<br>
1:03:14.740,1:03:19.700<br>
永遠可以留到下學期 MLDS 再講這樣子<br>
<br>
1:03:19.700,1:03:25.640<br>
最後這後面還有很多<br>
<br>
1:03:25.640,1:03:29.620<br>
最後只想 demo 一下到底如果用 Actor Critic 這種方法<br>
<br>
1:03:29.620,1:03:31.900<br>
可以做到什麼樣的地步<br>
<br>
1:03:32.100,1:03:39.840<br>
現在用 Actor Critic 這些東西大家都在玩 3D 遊戲了<br>
<br>
1:03:39.840,1:03:44.400<br>
這個就是 machine 自己 learn 的<br>
然後它會走它沒有看過的迷宮<br>
<br>
1:03:44.400,1:03:48.820<br>
它會知道要去吃綠色的平果<br>
<br>
1:03:48.820,1:03:53.900<br>
machine 看到的，雖然這是個 3D 遊戲，它看到的就是 pixel，跟我們人一樣<br>
<br>
1:03:54.980,1:04:01.500<br>
所以用 A3C 可以硬玩這種遊戲<br>
這個是硬玩一發賽車遊戲<br>
<br>
1:04:01.500,1:04:04.480<br>
硬開個賽車<br>
<br>
1:04:06.240,1:04:09.340<br>
它看到的就是 pixel 跟人一樣<br>
<br>
1:04:11.780,1:04:16.620<br>
我記得它的 reward 是速度，速度越快 reward 就會越高<br>
<br>
1:04:16.620,1:04:20.980<br>
所以它會拼命想要加速，那撞到東西就會減速，它會避免撞到東西<br>
<br>
1:04:20.980,1:04:24.740<br>
比如說前面有些車然後它就避開<br>
<br>
1:04:24.740,1:04:29.620<br>
所以最後的結果是滿驚人的<br>
<br>
1:04:29.620,1:04:32.340<br>
這學期上課就上到這邊<br>
<br>
1:04:32.340,1:04:37.880<br>
我本來想要講一個感性的結論<br>
但是沒有甚麼感性的結論<br>
<br>
1:04:37.880,1:04:40.900<br>
已經快下課了不太好講甚麼感性的結論<br>
<br>
1:04:40.900,1:04:42.900<br>
那我想要講一下<br>
<br>
1:04:42.960,1:04:46.780<br>
其實這學期有個東西沒有教，知道是甚麼嗎<br>
<br>
1:04:46.780,1:04:50.740<br>
是 Machine Learning 裡面比較偏向統計理論的部分<br>
<br>
1:04:50.740,1:04:52.840<br>
比如說 VC-dimension<br>
<br>
1:04:52.840,1:04:58.700<br>
但是電機系未來會有其他課教 Machine Learning 裡面比較統計的部分<br>
<br>
1:04:58.700,1:05:03.680<br>
就我所知王老師有要開統計與機器學習<br>
<br>
1:05:03.760,1:05:08.100<br>
但是它會講比較偏向統計理論的東西<br>
<br>
1:05:08.100,1:05:11.860<br>
所以這學期內容是沒有有關統計理論的部分<br>
<br>
1:05:11.860,1:05:15.780<br>
然後如果覺得我教的太簡單了<br>
<br>
1:05:15.780,1:05:17.720<br>
那你就可以去聽王老師的課<br>
<br>
1:05:17.720,1:05:21.220<br>
如果你覺得我教的太難了，聽不懂<br>
<br>
1:05:21.220,1:05:28.020<br>
沒有關係，余老師要開機器學習導論是給大學生的<br>
<br>
1:05:28.100,1:05:31.700<br>
所以以後可以先修機器學習導論<br>
<br>
1:05:31.700,1:05:34.520<br>
以後電機系會有很多機器學習的課<br>
<br>
1:05:34.520,1:05:40.940<br>
我上課的內容跟軒田的機器學習的基石還有技法<br>
<br>
1:05:40.940,1:05:44.360<br>
其實我是盡量有錯開的<br>
<br>
1:05:44.360,1:05:46.780<br>
就算你是修過基石和技法<br>
<br>
1:05:46.780,1:05:49.800<br>
相信你在這門課裡面應該也是有聽到不少東西<br>
<br>
1:05:49.800,1:05:52.460<br>
或者是你之後再去聽基石跟技法<br>
<br>
1:05:52.460,1:05:59.280<br>
你會發現我講的東西跟軒田講的東西，其實是我盡量做到沒有太多東西重複<br>
<br>
1:05:59.280,1:06:05.100<br>
這學期上課就上課這邊，謝謝大家<br>
<br>
1:06:05.200,1:06:21.000<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
