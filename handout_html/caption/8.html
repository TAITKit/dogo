<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:03.060<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
<br>
0:00:03.060,0:00:05.060<br>
好，我們要來上課囉！<br>
<br>
0:00:06.660,0:00:11.520<br>
剩下的時間，我們要來進入新的主題<br>
<br>
0:00:11.520,0:00:14.060<br>
我們要來講分類這件事情<br>
<br>
0:00:14.580,0:00:19.320<br>
在分類這件事情呢，我們要找的是一個 function<br>
<br>
0:00:19.320,0:00:23.440<br>
它的 input 是一個 object x<br>
<br>
0:00:23.440,0:00:27.160<br>
它的 output 是這個 object 屬於哪一個 class<br>
<br>
0:00:27.160,0:00:28.980<br>
屬於 n 個 class 的哪一個<br>
<br>
0:00:28.980,0:00:31.780<br>
那這樣的 task 有很多的 application<br>
<br>
0:00:31.780,0:00:33.780<br>
信手拈來就一大堆<br>
<br>
0:00:33.780,0:00:35.280<br>
比如說，在金融上<br>
<br>
0:00:35.280,0:00:38.300<br>
他們可以用 classification 的 model<br>
<br>
0:00:38.300,0:00:41.520<br>
來決定要不要貸款給某一個人<br>
<br>
0:00:41.520,0:00:44.880<br>
你就找一個 function，它的 input 是某一個人的 income<br>
<br>
0:00:44.880,0:00:48.520<br>
他的 saving、工作啊、他的年紀阿<br>
<br>
0:00:48.520,0:00:51.120<br>
還有他過去 financial 的 record<br>
<br>
0:00:51.120,0:00:53.380<br>
他過去有沒有欠債阿，等等<br>
<br>
0:00:53.380,0:00:57.000<br>
那 output 就是要借錢給他，或是不借錢給他<br>
<br>
0:00:57.000,0:01:01.000<br>
這是 binary classification 的 problem，<br>
也就是 accept 或是 refuse<br>
<br>
0:01:01.000,0:01:03.440<br>
那或者是拿來做醫療的診斷，比如說<br>
<br>
0:01:03.440,0:01:07.460<br>
input 就是某一個人的症狀，還有他的年紀、性別<br>
<br>
0:01:07.460,0:01:09.620<br>
過去就醫的歷史阿，等等<br>
<br>
0:01:09.840,0:01:13.460<br>
或者是來做，手寫的是數字、文字的辨識<br>
<br>
0:01:13.460,0:01:16.580<br>
那 output 就是，他生的是哪一種病<br>
<br>
0:01:16.620,0:01:21.760<br>
自動來做醫療的診斷<br>
<br>
0:01:21.760,0:01:25.280<br>
比如說，你就手寫一個字給機器看<br>
<br>
0:01:25.280,0:01:27.860<br>
看到這張圖，這是 "金" 這樣子<br>
<br>
0:01:28.140,0:01:31.920<br>
你知道，鄉民都叫我 "大金"，這是金這樣子<br>
<br>
0:01:31.920,0:01:34.780<br>
然後，output 就是<br>
<br>
0:01:35.380,0:01:39.060<br>
這個字、這個 image，它是屬於哪一個 class<br>
<br>
0:01:39.060,0:01:41.820<br>
如果你是做中文的手寫辨識的話<br>
<br>
0:01:41.820,0:01:45.380<br>
那中文有至少 8000 個 character<br>
<br>
0:01:45.380,0:01:48.480<br>
那就是一個 8000 個 class 的 classification<br>
<br>
0:01:48.480,0:01:50.500<br>
你的 model 要從這 8000 個 class 裡面<br>
<br>
0:01:50.500,0:01:52.320<br>
選一個 class 當作 output<br>
<br>
0:01:52.320,0:01:56.080<br>
或者是做人臉辨識，input 一個人臉<br>
<br>
0:01:56.080,0:01:58.800<br>
然後告訴他說，這個人臉是誰的<br>
<br>
0:01:59.220,0:02:02.340<br>
好，那我們要用的是怎麼樣的 Example Application 呢？<br>
<br>
0:02:02.340,0:02:04.660<br>
其實這個也是寶可夢的例子啦<br>
<br>
0:02:04.660,0:02:09.440<br>
你可能以為說，我只有前面 predict CP 值那裡<br>
<br>
0:02:09.440,0:02:12.660<br>
其實，我還有很多其他的例子這樣<br>
<br>
0:02:12.660,0:02:16.180<br>
好，我又做了一些有關寶可夢的研究<br>
<br>
0:02:16.240,0:02:17.860<br>
我這個研究是這樣子的<br>
<br>
0:02:17.860,0:02:20.540<br>
我們知道說寶可夢有不同的屬性<br>
<br>
0:02:20.540,0:02:23.120<br>
有幾種呢？有 18 種屬性<br>
<br>
0:02:23.120,0:02:29.360<br>
包括水、火、電阿，還有草、冰......等等<br>
<br>
0:02:29.360,0:02:32.940<br>
總共有 18 種屬性，到第六代為止，總共有 18 種屬性<br>
<br>
0:02:32.940,0:02:36.220<br>
我們現在要做的是一個分類的問題<br>
<br>
0:02:36.220,0:02:38.880<br>
這個分類的問題就是，要找一個 function<br>
<br>
0:02:38.880,0:02:42.160<br>
這個 function 的 input 就是某一隻寶可夢<br>
<br>
0:02:42.160,0:02:45.460<br>
然後，它的 output 就是要告訴你說，這一隻寶可夢<br>
<br>
0:02:45.460,0:02:47.320<br>
它是屬於哪一種 type<br>
<br>
0:02:47.320,0:02:49.260<br>
比如說，input 給它一隻皮卡丘<br>
<br>
0:02:49.260,0:02:50.700<br>
它 output 就是雷<br>
<br>
0:02:50.700,0:02:55.160<br>
input 給它一個傑尼龜 ，它 output 就是水<br>
<br>
0:02:55.160,0:02:58.700<br>
input 給它一個妙蛙種子，output 就是草<br>
<br>
0:02:58.700,0:03:01.420<br>
所以是一個 classification 的問題<br>
<br>
0:03:01.420,0:03:04.080<br>
那要怎麼樣做這個問題呢？<br>
<br>
0:03:04.080,0:03:07.840<br>
你現在的第一個問題，就是怎麼把一隻寶可夢當作<br>
<br>
0:03:07.840,0:03:09.240<br>
function 的 input<br>
<br>
0:03:09.240,0:03:12.600<br>
你要把一個東西當作 function 的 input，它得數值化<br>
<br>
0:03:12.600,0:03:17.900<br>
你要用數字來表示一隻寶可夢，<br>
你才能夠把它放到一個 function 裡面<br>
<br>
0:03:18.180,0:03:21.240<br>
那我們要怎麼把一隻寶可夢用數字來表示呢？<br>
<br>
0:03:21.240,0:03:26.240<br>
一隻寶可夢，其實它有很多的特性<br>
<br>
0:03:26.240,0:03:29.000<br>
這些特性是可以數值化的<br>
<br>
0:03:29.000,0:03:31.560<br>
比如說，它整體的強度<br>
<br>
0:03:31.560,0:03:35.640<br>
我先說一下，這並不是那個 Pokemon go 的那個東西<br>
<br>
0:03:35.640,0:03:37.580<br>
這個是那個 Pokemon 的電玩<br>
<br>
0:03:37.580,0:03:40.240<br>
你聽不懂我講這個，就算了<br>
<br>
0:03:40.240,0:03:41.940<br>
就假設沒聽到這句話<br>
<br>
0:03:41.940,0:03:49.060<br>
那一隻寶可夢，它其實可以用一組數字來描述它的特性<br>
<br>
0:03:49.060,0:03:50.800<br>
這組數字代表什麼呢？<br>
<br>
0:03:50.800,0:03:54.980<br>
比如說，這個寶可夢，它 total 的 strong<br>
<br>
0:03:54.980,0:03:57.980<br>
就是它有多強這樣子，你可以用一組數字來表示它<br>
<br>
0:03:58.120,0:04:00.920<br>
它的生命值，你可以用數字來表示它<br>
<br>
0:04:01.160,0:04:04.600<br>
它的攻擊力，你可以用數字來表示它<br>
<br>
0:04:04.600,0:04:07.320<br>
它的防禦力，你可以用數字來表示它<br>
<br>
0:04:07.320,0:04:11.060<br>
它的特殊攻擊力，就是它用特殊攻擊的時候<br>
<br>
0:04:11.060,0:04:15.600<br>
這我也不知道怎麼解釋， <br>
反正就是另外一個特殊攻擊的攻擊力<br>
<br>
0:04:15.600,0:04:18.220<br>
特殊攻擊的防禦力<br>
<br>
0:04:18.220,0:04:21.540<br>
還有它的速度，速度可以決定說<br>
<br>
0:04:21.540,0:04:25.480<br>
就是兩隻寶可夢相遇的時候，誰可以先攻擊<br>
<br>
0:04:26.180,0:04:28.300<br>
比如說，皮卡丘<br>
<br>
0:04:28.300,0:04:31.420<br>
它整體強的程度是 320<br>
<br>
0:04:31.420,0:04:34.160<br>
它 HP 是 35，攻擊力是 55<br>
<br>
0:04:34.160,0:04:37.100<br>
防禦力是40，特殊攻擊力是 50<br>
<br>
0:04:37.100,0:04:41.800<br>
然後，它的特殊防禦力是 50，速度是 90<br>
<br>
0:04:42.180,0:04:46.200<br>
所以一隻皮卡丘，我們就可以用一個 vector 來描述它<br>
<br>
0:04:46.200,0:04:49.980<br>
這個 vector 裡面總共有 7 個數值<br>
<br>
0:04:49.980,0:04:56.520<br>
所以，1 隻寶可夢，它就是 7 個數字所組成的一個 vector<br>
<br>
0:04:56.520,0:04:59.280<br>
所以 1 隻皮卡丘，可以用這 7 個數字來描述它<br>
<br>
0:04:59.280,0:05:01.300<br>
我們現在要問的問題就是<br>
<br>
0:05:01.300,0:05:04.420<br>
我們能不能把 7 個數字，輸進一個 function<br>
<br>
0:05:04.420,0:05:05.580<br>
這個 function 就告訴我們說<br>
<br>
0:05:05.580,0:05:09.880<br>
它的 output 是哪一種種類的寶可夢<br>
<br>
0:05:09.880,0:05:12.040<br>
那你可能會問這樣的問題<br>
<br>
0:05:12.040,0:05:14.500<br>
這件事情的重要性，到底在哪裡？<br>
<br>
0:05:14.500,0:05:16.960<br>
這件事情，是非常重要的<br>
<br>
0:05:16.960,0:05:19.860<br>
因為當兩隻寶可夢相遇，在決鬥的時候<br>
<br>
0:05:19.860,0:05:22.580<br>
他們之間是有屬性相剋的關係<br>
<br>
0:05:22.580,0:05:25.380<br>
這個是 18*18 的屬性相剋表<br>
<br>
0:05:25.380,0:05:28.940<br>
因為，你知道說，總共有 18 隻寶可夢<br>
<br>
0:05:28.940,0:05:32.700<br>
總共有 18 種 type，所以是 18*18 的屬性相剋表<br>
<br>
0:05:32.700,0:05:36.300<br>
比如說，今天這個格鬥系遇到<br>
<br>
0:05:36.300,0:05:38.360<br>
左邊，這個是攻擊方<br>
<br>
0:05:38.360,0:05:40.040<br>
然後，上面這個是防禦方<br>
<br>
0:05:40.040,0:05:43.240<br>
所以，格鬥系遇到一般的時候，他的攻擊力就 *2<br>
<br>
0:05:43.240,0:05:47.240<br>
這樣，你看得懂這個圖了嗎？所以，這個是很重要的<br>
<br>
0:05:47.240,0:05:48.860<br>
那你可能會問說<br>
<br>
0:05:48.860,0:05:52.740<br>
這個寶可夢屬性，不是寶可夢圖鑑上都有了嗎？<br>
<br>
0:05:52.740,0:05:55.260<br>
綠色寶可夢屬性，有什麼意義呢？<br>
<br>
0:05:55.260,0:05:56.900<br>
這件事情是有很大的意義的<br>
<br>
0:05:56.900,0:05:59.260<br>
因為，你有可能在決鬥中的時候，遇到<br>
<br>
0:05:59.260,0:06:01.560<br>
對方出的是圖鑑上沒有的<br>
<br>
0:06:01.560,0:06:03.020<br>
你沒有見過的寶可夢<br>
<br>
0:06:03.020,0:06:05.240<br>
如果你現在有這個預測的 model 的話<br>
<br>
0:06:05.240,0:06:08.440<br>
你就可以預測說，它出的寶可夢是哪一種屬性的<br>
<br>
0:06:08.440,0:06:11.040<br>
你就可以用正確的屬性來對付它<br>
<br>
0:06:11.040,0:06:14.880<br>
所以，這個是有非常廣泛地運用的<br>
<br>
0:06:14.880,0:06:18.360<br>
而且我發現說，寶可夢圖鑑，其實是有影像辨識的功能<br>
<br>
0:06:18.360,0:06:19.920<br>
它影像辨識的功能很強<br>
<br>
0:06:19.920,0:06:22.800<br>
照一張圖，它就可以告訴你說，它是哪一種寶可夢<br>
<br>
0:06:22.800,0:06:26.360<br>
我們應該要把這個 prediction 的 model <br>
加到寶可夢的圖鑑裡面<br>
<br>
0:06:26.360,0:06:28.420<br>
它就可以幫我們 predict 新的寶可夢<br>
<br>
0:06:28.420,0:06:34.600<br>
那，怎麼完成這個任務<br>
<br>
0:06:36.060,0:06:38.400<br>
首先，我們要先收集 data<br>
<br>
0:06:38.400,0:06:44.980<br>
比如說，你就說，我把寶可夢編號 400 以下的，<br>
當作 training data<br>
<br>
0:06:44.980,0:06:47.180<br>
把編號 400 以上的，當作 testing data<br>
<br>
0:06:47.180,0:06:50.000<br>
你就假設說，因為它的寶可夢其實是越來越多的<br>
<br>
0:06:50.000,0:06:51.860<br>
我記得我小時候只有 150 隻<br>
<br>
0:06:51.860,0:06:55.760<br>
後來發現，越來越多寶可夢，現在已經有 800 隻了<br>
<br>
0:06:55.760,0:06:57.300<br>
所以寶可夢是不斷增加的<br>
<br>
0:06:57.300,0:07:00.880<br>
所以編號比較前面的，是比較早發現的那些寶可夢<br>
<br>
0:07:00.880,0:07:03.800<br>
所以，模擬說，我們已經發現那些寶可的情況下<br>
<br>
0:07:03.800,0:07:05.460<br>
如果看到新的寶可夢的時候<br>
<br>
0:07:05.460,0:07:08.920<br>
所以，你要收集 data，比如說<br>
<br>
0:07:09.540,0:07:11.460<br>
我們能不能夠預測說，它是屬於哪一種屬性的<br>
<br>
0:07:11.460,0:07:14.160<br>
我們的 data 就是一些 pair<br>
<br>
0:07:14.160,0:07:16.300<br>
告訴我們說 function 的 input, output 是甚麼<br>
<br>
0:07:16.300,0:07:18.440<br>
比如說，input 皮卡丘，就要 output 電<br>
<br>
0:07:18.440,0:07:20.320<br>
input 傑尼龜，就要 output 水<br>
<br>
0:07:20.320,0:07:22.740<br>
input 妙蛙種子，就要 output 草<br>
<br>
0:07:23.320,0:07:25.900<br>
那怎麼解這個 classification 的問題？<br>
<br>
0:07:25.900,0:07:27.100<br>
有人會這麼想<br>
<br>
0:07:27.100,0:07:29.000<br>
假設有人沒有學過 classification<br>
<br>
0:07:29.000,0:07:31.220<br>
他學過 Regression，他就說<br>
<br>
0:07:31.220,0:07:34.900<br>
Classification 就當作 Regression 的問題來硬解<br>
<br>
0:07:34.900,0:07:36.120<br>
怎麼硬解呢？<br>
<br>
0:07:36.120,0:07:38.900<br>
我們用 binary 的 classification 來當作例子<br>
<br>
0:07:38.900,0:07:44.340<br>
假設我們現在只要 output：<br>
input 的 x 屬於 class 1 或 class 2<br>
<br>
0:07:44.340,0:07:46.900<br>
那你就把他當作一個 Regression 的問題<br>
<br>
0:07:46.900,0:07:53.060<br>
task 1 就代表說它的 target，也就是 y\head 是 1<br>
<br>
0:07:53.060,0:07:57.420<br>
task 2 就代表說，它的 target 是 -1<br>
<br>
0:07:57.420,0:08:00.700<br>
就這樣，你就當作 Regression 的 problem，<br>
一直 train 下去<br>
<br>
0:08:00.700,0:08:02.740<br>
然後，train 完這個 model 以後呢<br>
<br>
0:08:02.740,0:08:04.080<br>
在 testing 的時候<br>
<br>
0:08:04.080,0:08:07.420<br>
因為 Regression 的 output 不會正好是 0 或是 1 阿<br>
<br>
0:08:07.420,0:08:11.400<br>
它是一個 number，它是一個數值<br>
<br>
0:08:11.400,0:08:14.760<br>
如果這個數值比較接近 1 的話，就說是 class 1<br>
<br>
0:08:14.760,0:08:18.320<br>
如果這個數值比較接近 -1 的話，就說是 class 2<br>
<br>
0:08:18.320,0:08:22.120<br>
所以，你可以想成說，現在就是以 0 為分界<br>
<br>
0:08:22.120,0:08:24.940<br>
如果你的 Regression model output 是大於 0 的話<br>
<br>
0:08:24.940,0:08:26.500<br>
就比較接近 1<br>
<br>
0:08:26.500,0:08:28.380<br>
你的 model 就會說是 class 1<br>
<br>
0:08:28.380,0:08:32.880<br>
如果小於 0 的話，就比較接近 -1，<br>
所以 model 會說是 class 2<br>
<br>
0:08:32.880,0:08:34.720<br>
如果你這麼做的話<br>
<br>
0:08:34.720,0:08:38.820<br>
會遇到什麼樣的問題呢？<br>
<br>
0:08:38.820,0:08:40.120<br>
你會遇到這樣的問題<br>
<br>
0:08:40.780,0:08:43.640<br>
假設說，我們現在的 model 是一個<br>
<br>
0:08:43.640,0:08:46.940<br>
我們現在的 model，input 和 output 的關係<br>
<br>
0:08:46.940,0:08:50.360<br>
y = b + w1*x1 + w2*x2<br>
<br>
0:08:50.360,0:08:55.700<br>
所以，input 這兩個 feature，也就是 x1 跟 x2<br>
<br>
0:08:56.760,0:09:01.140<br>
我們現在有兩個 class，紅色是 class 1、藍色是 class 2<br>
<br>
0:09:01.560,0:09:03.680<br>
那如果你用 Regression 來想的話<br>
<br>
0:09:03.680,0:09:08.500<br>
藍色的那些 object，藍色的那些東西<br>
<br>
0:09:08.500,0:09:10.380<br>
input 到這個 Regression 的 model<br>
<br>
0:09:10.380,0:09:12.660<br>
我們都希望它越接近 1 越好<br>
<br>
0:09:12.660,0:09:15.900<br>
紅色這些東西，input 到 Regression 的 model<br>
<br>
0:09:15.900,0:09:18.400<br>
我們都希望它越接近 -1 越好<br>
<br>
0:09:18.400,0:09:20.900<br>
這件事，可能是做得到的<br>
<br>
0:09:20.900,0:09:22.700<br>
如果你把這些 data<br>
<br>
0:09:22.700,0:09:27.900<br>
真的找出這個 b, w1, w2 的話<br>
<br>
0:09:27.900,0:09:29.600<br>
那你會發現說呢<br>
<br>
0:09:29.600,0:09:35.920<br>
b + w1*x1 + w2*x2 (投影片上寫錯)<br>
<br>
0:09:35.920,0:09:40.120<br>
這個式子等於 0 的線，是綠色這條<br>
<br>
0:09:40.120,0:09:43.420<br>
也就是 class 1 和 class 2 的分界點<br>
<br>
0:09:43.420,0:09:46.660<br>
是在綠色的這條線上<br>
<br>
0:09:46.660,0:09:49.700<br>
這看起來、聽起來滿好的<br>
<br>
0:09:50.180,0:09:52.600<br>
但是，你可能會遇到這樣的問題<br>
<br>
0:09:52.600,0:09:56.120<br>
假設，你今天 class 1 的分佈不是這樣子<br>
<br>
0:09:56.120,0:10:00.540<br>
假設你 class 1 的分佈是這樣子，你就麻煩了<br>
<br>
0:10:00.540,0:10:01.620<br>
為什麼？<br>
<br>
0:10:01.620,0:10:06.460<br>
因為，如果你用綠色的這條線，所代表的 model 的話<br>
<br>
0:10:06.460,0:10:09.580<br>
注意一下，這個是二維的<br>
<br>
0:10:09.580,0:10:14.580<br>
綠色這條線，只是代表說這個 model 的值是 0<br>
<br>
0:10:14.580,0:10:17.160<br>
就是這個 y 的值，是 0<br>
<br>
0:10:17.160,0:10:19.320<br>
你的 Regression 的 output 是 0<br>
<br>
0:10:19.320,0:10:23.440<br>
左上角這邊，代表這個 Regression 的 output 是小於 0<br>
<br>
0:10:23.440,0:10:27.020<br>
右邊上角這邊，代表這個 Regression 的 output 是大於 0<br>
<br>
0:10:27.620,0:10:30.260<br>
那綠色這條線，會有什麼問題呢？<br>
<br>
0:10:30.260,0:10:32.260<br>
綠色這條線的問題就是<br>
<br>
0:10:32.260,0:10:40.380<br>
左上角 < 0，右上角 > 0<br>
<br>
0:10:40.380,0:10:43.220<br>
越偏右下，它的值就越大<br>
<br>
0:10:43.220,0:10:49.000<br>
所以，如果今天是考慮右下角這些點的話<br>
<br>
0:10:49.000,0:10:53.020<br>
它用這個綠色的 model，它做 Regression 的時候<br>
<br>
0:10:53.020,0:10:55.700<br>
它的 output 可能會是遠大於 1 的<br>
<br>
0:10:55.700,0:10:57.700<br>
但是，如果你用 Regression 的話<br>
<br>
0:10:57.700,0:11:01.540<br>
你會希望，藍色的點都越接近 1 越好<br>
<br>
0:11:01.540,0:11:05.980<br>
太大也不好，它要越接近 1 越好<br>
<br>
0:11:05.980,0:11:08.340<br>
太小不行、太大也不行<br>
<br>
0:11:08.340,0:11:12.360<br>
所以變成說，這些遠大於 1 的點<br>
<br>
0:11:12.360,0:11:16.720<br>
它其實對 Regression 來說，是 error，是錯的<br>
<br>
0:11:16.720,0:11:18.780<br>
這些點是不好的<br>
<br>
0:11:18.780,0:11:24.420<br>
所以，你今天如果拿這樣兩群藍色的點跟紅色的點<br>
<br>
0:11:24.420,0:11:26.780<br>
去做 Regression 的時候<br>
<br>
0:11:26.780,0:11:28.820<br>
你得到的線，不會是綠色這條<br>
<br>
0:11:28.820,0:11:32.260<br>
雖然綠色這條，你用直覺看、你用眼睛一看就會知道說<br>
<br>
0:11:32.260,0:11:34.260<br>
它是一個比較好的 boundary<br>
<br>
0:11:34.260,0:11:36.540<br>
但是，如果你用 Regression 刃下去的話<br>
<br>
0:11:36.540,0:11:39.200<br>
不會是綠色這條，它會是紫色這條<br>
<br>
0:11:39.200,0:11:44.420<br>
因為，它會覺得說，我把綠色這條線，往右偏一點<br>
<br>
0:11:44.420,0:11:48.660<br>
這樣的好處就是，這邊這些藍色的點<br>
<br>
0:11:48.660,0:11:50.920<br>
它的值就沒有那麼大<br>
<br>
0:11:50.920,0:11:52.860<br>
它的值就會壓小<br>
<br>
0:11:52.860,0:11:55.220<br>
就讓他們，比較接近 1<br>
<br>
0:11:55.220,0:12:00.000<br>
結果，這樣子的 function 反而對 Regression 來說<br>
<br>
0:12:00.000,0:12:02.400<br>
是一個比較好的 function<br>
<br>
0:12:03.000,0:12:07.440<br>
也就是說，Regression 那個定義 function 好壞的方式<br>
<br>
0:12:07.440,0:12:10.360<br>
就 classification 來說，不適用<br>
<br>
0:12:10.360,0:12:13.080<br>
今天這個 problem，對 Regression 來說<br>
<br>
0:12:13.080,0:12:15.960<br>
紫色的，是一個好的 function<br>
<br>
0:12:16.960,0:12:21.100<br>
但是，顯然對 classification 來說，<br>
綠色的才是一個好的 function<br>
<br>
0:12:21.260,0:12:23.660<br>
但是，如果你當作 Regression 的 problem 來做<br>
<br>
0:12:23.660,0:12:26.640<br>
套用到 Regression，一樣的作法的時候<br>
<br>
0:12:26.640,0:12:28.820<br>
你得到的結果會是不好的<br>
<br>
0:12:28.820,0:12:33.980<br>
因為 Regression 對 model 好壞的定義，<br>
是不適合用在這個地方的<br>
<br>
0:12:33.980,0:12:38.060<br>
所以，如果你用 Regression 的話<br>
<br>
0:12:38.060,0:12:41.200<br>
你的 Regression model 它會懲罰那些<br>
<br>
0:12:41.200,0:12:45.800<br>
太正確、那些 output 值太大的那些點<br>
<br>
0:12:45.800,0:12:47.900<br>
這樣，反而你得到的結果是不好的<br>
<br>
0:12:48.300,0:12:49.960<br>
那，還有另外一個問題<br>
<br>
0:12:49.960,0:12:53.640<br>
其實，這種硬把 Classification 當作 Regression 來做<br>
<br>
0:12:53.640,0:12:57.300<br>
我還看過有人會真的這麼做<br>
<br>
0:12:57.300,0:12:59.520<br>
不果我勸你不要這麼做<br>
<br>
0:12:59.520,0:13:03.020<br>
比如說，你今天有 Multiple class 的話<br>
<br>
0:13:03.020,0:13:07.020<br>
那你可能說，我把 class 1 當作 target 是 1<br>
<br>
0:13:07.020,0:13:08.400<br>
class 2 當作 target 是 2<br>
<br>
0:13:08.400,0:13:10.620<br>
class 3 當作 target 是 3<br>
<br>
0:13:10.620,0:13:12.100<br>
這樣子做事會有問題的<br>
<br>
0:13:12.200,0:13:14.680<br>
因為當你這樣子做的時候，你就假設說<br>
<br>
0:13:14.680,0:13:17.440<br>
class 3 和 class 2 是比較近的<br>
<br>
0:13:17.440,0:13:18.680<br>
它們有某種關係<br>
<br>
0:13:18.680,0:13:20.800<br>
class 2 和 class 1 是比較近的<br>
<br>
0:13:20.800,0:13:22.320<br>
它們有某種關係<br>
<br>
0:13:22.320,0:13:25.440<br>
但是，實際上，如果這種關係不存在的話<br>
<br>
0:13:25.440,0:13:29.940<br>
class 1, class 2, class 3<br>
他們中間並沒有某種特殊的 relation<br>
<br>
0:13:29.940,0:13:32.540<br>
並沒有誰應該跟誰比較有關係的話<br>
<br>
0:13:32.540,0:13:35.920<br>
你這樣子把它當作一個 Regression 的問題來處理<br>
<br>
0:13:35.920,0:13:38.320<br>
你就沒有辦法得到一個好的結果<br>
<br>
0:13:38.820,0:13:42.400<br>
那我們這邊，應該要怎麼做呢？<br>
<br>
0:13:42.400,0:13:45.060<br>
理想上的做法，是這個樣子<br>
<br>
0:13:46.120,0:13:49.160<br>
找一個 function，這個 function 裡面呢<br>
<br>
0:13:49.160,0:13:54.040<br>
這在做 Regression 的時候，它的 output 是 real number<br>
<br>
0:13:54.040,0:13:57.940<br>
對不對？但是在 classification 的時候<br>
<br>
0:13:57.940,0:14:00.580<br>
它的 output 是 discrete<br>
<br>
0:14:00.580,0:14:02.680<br>
它是某一個 class，它是 discrete<br>
<br>
0:14:02.680,0:14:04.920<br>
那我要想辦法讓 model 做到這件事情<br>
<br>
0:14:04.920,0:14:07.040<br>
你當然可以有不同的想法<br>
<br>
0:14:07.040,0:14:08.480<br>
那一個可能的想法是<br>
<br>
0:14:08.480,0:14:11.860<br>
假如是二元分類的問題，是 binary classification 的話<br>
<br>
0:14:11.860,0:14:15.040<br>
我就說，我們要找的 function f 裡面<br>
<br>
0:14:15.040,0:14:16.560<br>
內建另外一個 function g<br>
<br>
0:14:16.560,0:14:20.100<br>
希望它是自動，當然我們的 g <br>
也是根據 training data 被找出來的<br>
<br>
0:14:20.100,0:14:24.480<br>
如果 g 代進 x，x 代進去的值大於 0 的話<br>
<br>
0:14:24.480,0:14:26.500<br>
那就說是 class 1<br>
<br>
0:14:26.500,0:14:29.080<br>
否則呢，就說是 class 2<br>
<br>
0:14:29.520,0:14:31.220<br>
那在 training 的時候<br>
<br>
0:14:31.220,0:14:35.000<br>
我們的 loss 應該怎麼定義才好呢？<br>
<br>
0:14:35.000,0:14:39.360<br>
我們的 loss 應該定義成，我們可以把 loss 定義成呢<br>
<br>
0:14:39.360,0:14:42.480<br>
如果我選了某一個 function f<br>
<br>
0:14:42.480,0:14:44.240<br>
它在我們的 training data 上面<br>
<br>
0:14:44.240,0:14:47.100<br>
predict 錯誤的次數<br>
<br>
0:14:47.100,0:14:48.520<br>
我們當然希望說<br>
<br>
0:14:48.520,0:14:51.580<br>
我們找出來的 function<br>
<br>
0:14:51.580,0:14:59.000<br>
它在 data 上的錯誤次數越小，代表它的 loss 越小<br>
<br>
0:14:59.000,0:15:01.040<br>
你就可以把這個式子寫成這樣<br>
<br>
0:15:01.040,0:15:03.580<br>
summation over 所有的 training example<br>
<br>
0:15:03.580,0:15:09.440<br>
δ (f(x^n) ≠ (y\head)^n)<br>
<br>
0:15:09.440,0:15:14.380<br>
就是如果 f(x^n) 的 output 跟<br>
正確答案 (y\head)^n 不一樣的話<br>
<br>
0:15:14.380,0:15:16.740<br>
這個 δ 就是 1，否則就是 0<br>
<br>
0:15:16.740,0:15:18.740<br>
如果你把它全部 sum 起來的話<br>
<br>
0:15:18.740,0:15:22.460<br>
就是你用這個 function f，在 training data 上面<br>
<br>
0:15:22.460,0:15:25.740<br>
它會分類錯誤的次數<br>
<br>
0:15:25.740,0:15:28.120<br>
那當然希望這個值，越小越好<br>
<br>
0:15:28.120,0:15:32.300<br>
但是，如果要你解這個 function，你現在八成不會<br>
<br>
0:15:32.300,0:15:34.480<br>
因為，我們學過的是 Gradient Descent<br>
<br>
0:15:34.480,0:15:36.080<br>
你可能會用 Gradient Descent 解<br>
<br>
0:15:36.080,0:15:38.240<br>
但是，這個沒辦法微分阿<br>
<br>
0:15:38.240,0:15:42.220<br>
這個就沒辦法微分，這個也沒辦法微分，通通不能微分<br>
<br>
0:15:42.220,0:15:43.400<br>
不知道怎麼做<br>
<br>
0:15:43.400,0:15:46.200<br>
其實，這個是有方法，比如說<br>
<br>
0:15:46.200,0:15:49.700<br>
Perceptron 就是一個方法，SVM 就是一個方法<br>
<br>
0:15:49.700,0:15:53.160<br>
那我們今天先不講這個方法<br>
<br>
0:15:53.160,0:15:56.420<br>
我們今天先來講另外一個 solution<br>
<br>
0:15:56.420,0:16:00.660<br>
這個 solution，我們先用機率的觀點來看待它<br>
<br>
0:16:00.660,0:16:03.120<br>
之後我們會說，這樣的 solution<br>
<br>
0:16:03.120,0:16:07.740<br>
也是跟 machine learning 的 3 個 step，其實是一樣的<br>
<br>
0:16:08.380,0:16:12.640<br>
我們先這樣看，就是有兩個盒子<br>
<br>
0:16:12.640,0:16:16.200<br>
我們來回憶一下，我們國中的時候，機率會問的問題<br>
<br>
0:16:16.720,0:16:19.900<br>
有兩個盒子，盒子一裡面有藍球和綠球<br>
<br>
0:16:19.900,0:16:21.900<br>
盒子二裡面也有藍球跟綠球<br>
<br>
0:16:21.900,0:16:26.220<br>
假設我不告訴你說，我從哪一個盒子裡面，<br>
挑一顆球出來<br>
<br>
0:16:26.220,0:16:29.880<br>
但是，我告訴你說，我從這兩個盒子的某一個盒子裡面<br>
<br>
0:16:29.880,0:16:33.220<br>
隨機抽取一顆球出來，它是藍色的<br>
<br>
0:16:33.220,0:16:38.060<br>
那這顆藍色的球，它從盒子 1<br>
<br>
0:16:38.060,0:16:41.240<br>
跟從盒子 2 抽出來的機率，分別是多少？<br>
<br>
0:16:41.240,0:16:43.340<br>
我相信這個，小學生就可以回答我<br>
<br>
0:16:43.340,0:16:46.680<br>
假如說，你告訴我說<br>
<br>
0:16:46.680,0:16:50.200<br>
你從盒子 1 裡面，抽一顆球的機率是 2/3<br>
<br>
0:16:50.200,0:16:52.660<br>
你從盒子 2 裡面，抽一顆球的機率是 1/3<br>
<br>
0:16:52.660,0:16:57.020<br>
這告訴我說，在盒子 1 裡面，藍球占 4/5<br>
<br>
0:16:57.020,0:16:58.740<br>
綠球占 4/5<br>
<br>
0:16:58.740,0:17:03.080<br>
盒子 2 裡面，藍球占 2/5，綠球占 3/5<br>
<br>
0:17:03.080,0:17:04.740<br>
那你就可以輕易計算說<br>
<br>
0:17:04.740,0:17:07.440<br>
如果我今天得到一顆藍球<br>
<br>
0:17:07.440,0:17:11.480<br>
他從盒子 1 裡面，抽出來的機率<br>
<br>
0:17:11.480,0:17:15.600<br>
這個機率就是，這個國小就應該有教過了<br>
<br>
0:17:16.560,0:17:18.920<br>
至少國中有教過吧，對不對？<br>
<br>
0:17:18.920,0:17:20.780<br>
大雞應該都會，就是<br>
<br>
0:17:20.780,0:17:24.140<br>
Given 一顆藍球，他從 B1 裡面 sample 出來的機率<br>
<br>
0:17:24.140,0:17:27.420<br>
就是這個樣子，這個沒什麼好解釋的<br>
<br>
0:17:27.420,0:17:29.120<br>
我相信大家都秒懂，這樣<br>
<br>
0:17:29.340,0:17:32.300<br>
那這個跟分類，有什麼關係呢？<br>
<br>
0:17:32.460,0:17:37.060<br>
如果我們把盒子換成分類的話<br>
<br>
0:17:37.060,0:17:42.780<br>
把盒子 1 跟盒子 2，換成類別 1 跟類別 2<br>
<br>
0:17:42.780,0:17:45.000<br>
換成類別 1 跟類別 2<br>
<br>
0:17:45.000,0:17:48.940<br>
這個時候呢，給我一個 x<br>
<br>
0:17:49.340,0:17:51.940<br>
就是，我們要分類的那個對象<br>
<br>
0:17:51.940,0:17:54.440<br>
比如說，今天我們的例子就是，分類一隻寶可夢<br>
<br>
0:17:54.440,0:17:58.120<br>
給我一隻寶可夢，他從某一個 class 裡面<br>
<br>
0:17:58.120,0:18:01.300<br>
sample 出來的機率是多少呢？<br>
<br>
0:18:01.840,0:18:03.840<br>
那我們需要知到哪些值？<br>
<br>
0:18:04.040,0:18:06.880<br>
我們需要知道 class 1<br>
<br>
0:18:06.880,0:18:10.600<br>
我從 class 1 裡面，抽一個 x 出來的機率<br>
<br>
0:18:10.860,0:18:15.320<br>
我們要知道從 class 2 裡面，抽一個 x 出來的機率<br>
<br>
0:18:15.320,0:18:20.300<br>
我們要知道說，從 class 1 裡面，抽一個 x 出來<br>
<br>
0:18:20.300,0:18:24.320<br>
從 class 1 裡面，抽出我們現在考慮的這個 x 的機率<br>
<br>
0:18:24.680,0:18:28.260<br>
我們要知道從這個 class 2 裡面<br>
<br>
0:18:28.260,0:18:31.000<br>
抽出我現在考慮的這個 x 的機率<br>
<br>
0:18:31.000,0:18:34.400<br>
如果有這些，當給我們一個 x 的時候<br>
<br>
0:18:34.400,0:18:35.920<br>
有了這 4 個數值<br>
<br>
0:18:35.920,0:18:41.060<br>
我們就可以計算，這個 x 是屬於 x1 的機率<br>
<br>
0:18:41.060,0:18:43.740<br>
怎麼算呢？x 屬於 x1 的機率<br>
<br>
0:18:43.740,0:18:48.140<br>
就是 case 1 本身的機率<br>
<br>
0:18:48.140,0:18:53.200<br>
乘上 case 1 sample 一個 object 出來，是 x 的機率<br>
<br>
0:18:53.200,0:18:56.800<br>
再除掉 case 1 本身的機率<br>
<br>
0:18:56.800,0:19:00.040<br>
乘上 case 1 sample 一個 object 出來，是 x 的機率<br>
<br>
0:19:00.040,0:19:02.420<br>
加上 case 2 本身的機率<br>
<br>
0:19:02.420,0:19:07.960<br>
乘上從 case 2 裡面 sample 出一個 object ，是 x 的機率<br>
<br>
0:19:07.960,0:19:10.280<br>
所以，我們現在的問題就是<br>
<br>
0:19:10.280,0:19:13.040<br>
如果我們知道這個機率的話<br>
<br>
0:19:13.040,0:19:16.640<br>
問題就解決了，因為給我一個寶可夢 x<br>
<br>
0:19:16.640,0:19:20.260<br>
我就可以看說，它從哪一個 case 來的機率最大<br>
<br>
0:19:20.260,0:19:23.100<br>
那機率最大那個 case，就是正確答案<br>
<br>
0:19:23.100,0:19:25.580<br>
那現在的問題是<br>
<br>
0:19:25.580,0:19:27.360<br>
我們如果要算這個值<br>
<br>
0:19:27.360,0:19:30.340<br>
那我們就要算這 4 個值<br>
<br>
0:19:30.340,0:19:33.120<br>
假設我們是考慮一個二元分類問題的話<br>
<br>
0:19:33.120,0:19:35.520<br>
我們就需要算這 4 個值<br>
<br>
0:19:35.520,0:19:37.240<br>
好，那這 4 個值怎麼來呢？<br>
<br>
0:19:37.240,0:19:43.140<br>
我們就希望從我們的 training data，去把這些值估測出來<br>
<br>
0:19:43.720,0:19:48.900<br>
那這一整套想法，叫做 Generative model<br>
<br>
0:19:48.900,0:19:51.520<br>
為甚麼它叫做 Generative model 呢？<br>
<br>
0:19:51.520,0:19:57.080<br>
因為有這個 model 的話，你可以拿它來 generate 一個 x<br>
<br>
0:19:57.080,0:19:58.320<br>
什麼意思呢？<br>
<br>
0:19:58.320,0:20:03.360<br>
你可以計算，某一個 x 出現的機率<br>
<br>
0:20:03.360,0:20:06.420<br>
如果你可以計算每一個 x 出現的機率<br>
<br>
0:20:06.420,0:20:07.920<br>
你就知道 x 的 distribution<br>
<br>
0:20:07.920,0:20:14.380<br>
你就可以用這個 distribution 來產生 x、sample x 出來<br>
<br>
0:20:14.700,0:20:16.440<br>
這一個機率是甚麼呢？<br>
<br>
0:20:16.440,0:20:19.880<br>
這個機率很簡單，它就是你從 c1 裡面<br>
<br>
0:20:19.880,0:20:22.160<br>
挑一個 x 出來的機率<br>
<br>
0:20:22.160,0:20:24.320<br>
乘上 c1 挑出 x 的機率<br>
<br>
0:20:24.320,0:20:27.060<br>
加上你從 case 2 裡面，挑一個 x 的機率<br>
<br>
0:20:27.060,0:20:29.240<br>
乘上 case 2 產生 x 的機率<br>
<br>
0:20:29.240,0:20:31.560<br>
你有這些機率<br>
<br>
0:20:31.560,0:20:33.100<br>
你就可以算這個<br>
<br>
0:20:33.280,0:20:36.200<br>
你就可以算某一個 x 出現的機率<br>
<br>
0:20:36.200,0:20:38.540<br>
你就可以自己產生 x<br>
<br>
0:20:38.540,0:20:42.300<br>
所以，這個東西，叫做 Generative model<br>
<br>
0:20:43.400,0:20:48.020<br>
那我們先來看一下 P(C1) 跟 P(C2)<br>
<br>
0:20:48.020,0:20:52.820<br>
我們先來算一下 P(C1) 跟 P(C2) 它出現的機率<br>
<br>
0:20:52.820,0:20:55.520<br>
那這個機率呢，叫做 Prior<br>
<br>
0:20:55.520,0:21:00.240<br>
他們呢，是比較好算的<br>
<br>
0:21:00.240,0:21:03.580<br>
假設我們今天考慮的這兩個 class 呢<br>
<br>
0:21:03.580,0:21:06.880<br>
分別是水系跟一般系<br>
<br>
0:21:06.880,0:21:09.780<br>
class 1 就是指水系的神奇寶貝<br>
<br>
0:21:09.780,0:21:11.940<br>
class 2 就是指一般系的神奇寶貝<br>
<br>
0:21:11.940,0:21:14.720<br>
另外 16 隻寶可夢，我們就無視它<br>
<br>
0:21:14.720,0:21:17.780<br>
先考慮一個二元分類的問題<br>
<br>
0:21:17.780,0:21:22.960<br>
那我們現在呢，把編號 ID，再圖鑑裡面編號 < 400 的<br>
<br>
0:21:22.960,0:21:26.920<br>
水系的和一般系的，就當作是 training data<br>
<br>
0:21:26.920,0:21:28.540<br>
剩下的當作是 testing data<br>
<br>
0:21:28.540,0:21:30.080<br>
如果你想要做的更嚴謹一點的話<br>
<br>
0:21:30.080,0:21:33.680<br>
你可以再把 training data 裡面<br>
切一份 validation data 出來<br>
<br>
0:21:34.100,0:21:38.040<br>
好，那 training data 裡面呢，總共有 79 隻是水系的<br>
<br>
0:21:38.040,0:21:39.700<br>
總共有 61 隻一般系<br>
<br>
0:21:39.700,0:21:43.800<br>
你可能會問說，為什麼選水系<br>
跟一般系當作二元分類問題<br>
<br>
0:21:43.800,0:21:48.220<br>
因為我其實統計了一下 18 種寶可夢，每個種類的數目<br>
<br>
0:21:48.220,0:21:50.220<br>
水系跟一般系是最多的<br>
<br>
0:21:50.220,0:21:52.740<br>
所以，我們就先選水系跟一般系<br>
<br>
0:21:52.740,0:21:57.140<br>
不過，我試了一下，如果你要把 18 種<br>
都分類正確，好像是做不太出來的<br>
<br>
0:21:57.140,0:22:00.880<br>
因為它們中間，做不太出來這樣子<br>
<br>
0:22:00.880,0:22:03.480<br>
你就先考慮，分兩個 class 就好<br>
<br>
0:22:05.080,0:22:09.300<br>
如果我們現在知道說，training data 裡面<br>
<br>
0:22:09.300,0:22:11.580<br>
79 隻水系、61 隻一般系<br>
<br>
0:22:11.580,0:22:16.980<br>
那從這個第一類裡面<br>
<br>
0:22:16.980,0:22:21.340<br>
從 class 1 裡面，sample 出一隻寶可夢的機率是多少呢<br>
<br>
0:22:21.340,0:22:26.340<br>
是不是就是 79 / (79+61)，算出來是 0.56<br>
<br>
0:22:26.540,0:22:30.480<br>
那從 class 2，sample 出一隻寶可夢的機率<br>
<br>
0:22:30.480,0:22:34.240<br>
就是 61 / (79+61)，就是 0.44<br>
<br>
0:22:34.500,0:22:38.920<br>
這個是比較容易、比較簡單可以理解的<br>
<br>
0:22:38.920,0:22:42.780<br>
好，那再來我們的問題是這樣子<br>
<br>
0:22:42.780,0:22:47.440<br>
怎麼計算說，如果給我某一個 class<br>
<br>
0:22:47.440,0:22:52.060<br>
某一隻寶可夢，是從這個 class sample 出來的機率<br>
<br>
0:22:52.060,0:22:54.820<br>
比如說，如果給你一個海龜<br>
<br>
0:22:54.820,0:22:57.080<br>
我也不知道這個海龜應該叫什麼名字<br>
<br>
0:22:57.080,0:22:59.500<br>
給一隻海龜，它是從<br>
<br>
0:22:59.500,0:23:02.200<br>
就是從水系的神奇寶貝裡面<br>
<br>
0:23:02.200,0:23:04.020<br>
挑一隻神奇寶貝出來<br>
<br>
0:23:04.020,0:23:10.380<br>
它是海龜的機率，到底應該有多大呢？<br>
<br>
0:23:11.100,0:23:13.740<br>
那我們現在的 training data 是長這個樣子<br>
<br>
0:23:13.740,0:23:16.920<br>
屬於水系的神奇寶貝有 79 隻<br>
<br>
0:23:17.160,0:23:21.800<br>
所以有傑尼龜、可達鴨、蚊香蝌蚪之類的<br>
<br>
0:23:21.800,0:23:25.240<br>
這些我是認識的啦，我小時候在卡通有看到<br>
<br>
0:23:25.240,0:23:26.940<br>
這個是編號後面的<br>
<br>
0:23:26.940,0:23:29.140<br>
我小時候卡通沒有看到，所以我也不知道它叫什麼名字<br>
<br>
0:23:29.140,0:23:30.900<br>
它也不在這 79 隻裡面<br>
<br>
0:23:30.900,0:23:33.240<br>
那我到底要怎麼算說<br>
<br>
0:23:35.020,0:23:40.300<br>
從水系的神奇寶貝裡面挑一隻挑出來，是海龜的機率呢<br>
<br>
0:23:40.380,0:23:42.500<br>
你可能會想說<br>
<br>
0:23:42.500,0:23:45.120<br>
這 79 隻神奇寶貝又沒海龜<br>
<br>
0:23:45.120,0:23:47.660<br>
所以挑一隻出來，是海龜的機率根本是 0 阿<br>
<br>
0:23:47.660,0:23:50.240<br>
可是這海龜是水系的<br>
<br>
0:23:50.240,0:23:53.300<br>
我一看它的臉，我就知道它是水系的 XD<br>
<br>
0:23:54.760,0:23:57.580<br>
那它就是水系的，所以<br>
<br>
0:23:57.580,0:24:01.100<br>
你說，它從水系的裡面挑出來是 0 也不對阿<br>
<br>
0:24:01.100,0:24:02.720<br>
所以怎麼辦？<br>
<br>
0:24:02.720,0:24:05.400<br>
首先就是，每一隻神奇寶貝<br>
<br>
0:24:05.400,0:24:08.180<br>
每一隻寶可夢，我們剛才講過說<br>
<br>
0:24:08.180,0:24:11.300<br>
它都用一個向量來描述<br>
<br>
0:24:11.300,0:24:15.820<br>
這個向量裡面的值，就是它的各種的特徵值<br>
<br>
0:24:15.820,0:24:19.680<br>
所以，這個 vector，我們又稱之為一個 feature<br>
<br>
0:24:19.680,0:24:24.020<br>
所以，每一個寶可夢，都是用一堆 feature 來描述它<br>
<br>
0:24:24.920,0:24:27.820<br>
然後呢，我們就真的把那些水系的神奇寶貝<br>
<br>
0:24:27.820,0:24:33.260<br>
水系的寶可夢，它們的防禦力和特殊防禦力畫出來<br>
<br>
0:24:33.260,0:24:36.880<br>
其實，每一隻寶可夢有 7 個不同的數值阿<br>
<br>
0:24:36.880,0:24:41.340<br>
不過 7 個沒辦法畫，我就先畫防禦力跟特殊防禦力就好<br>
<br>
0:24:41.340,0:24:45.800<br>
值得強調的是，這邊的是真正的 data 這樣<br>
<br>
0:24:45.800,0:24:50.680<br>
如果你想要載完整的寶可夢的 data 的話<br>
<br>
0:24:50.680,0:24:53.920<br>
這個投影片最後也有附一個連結<br>
<br>
0:24:53.920,0:24:57.020<br>
你是可以載到完整的 data<br>
<br>
0:24:58.280,0:25:02.280<br>
那我們就把 79 隻寶可夢<br>
<br>
0:25:02.280,0:25:09.180<br>
它的防禦力跟特殊防禦力，都先畫在這張圖<br>
<br>
0:25:09.180,0:25:10.640<br>
二維的平面上<br>
<br>
0:25:10.640,0:25:15.400<br>
所以這二維的平面上，每一個點，就代表了一隻寶可夢<br>
<br>
0:25:15.400,0:25:17.880<br>
比如說，這個點是可達鴨<br>
<br>
0:25:17.880,0:25:20.480<br>
它的防禦力是 48，特殊防禦力是 50<br>
<br>
0:25:20.480,0:25:22.080<br>
這個點是傑尼龜<br>
<br>
0:25:22.080,0:25:25.140<br>
它的防禦力是 65，特殊防禦力是 64<br>
<br>
0:25:25.140,0:25:27.260<br>
可是，現在的問就是<br>
<br>
0:25:27.260,0:25:30.180<br>
如果給我們一個新的點<br>
<br>
0:25:30.180,0:25:35.300<br>
這個點是代表一隻<br>
沒有在我們的 training data 裡面的寶可夢<br>
<br>
0:25:35.300,0:25:39.040<br>
是我們沒有看過的寶可夢，比如這隻海龜<br>
<br>
0:25:39.040,0:25:43.680<br>
它的防禦力是 103，特殊防禦力是 45<br>
<br>
0:25:43.680,0:25:46.100<br>
它的位置大概在這個地方<br>
<br>
0:25:47.820,0:25:51.600<br>
從水系裡面，挑到這隻神奇寶貝，它是水系的機率<br>
<br>
0:25:52.120,0:25:53.780<br>
到底應該是多少<br>
<br>
0:25:53.780,0:25:55.240<br>
那你不可以說它是 0 ，你不能說<br>
<br>
0:25:55.240,0:26:01.020<br>
這個 training data 裡面從來沒有<br>
出現這隻海龜，所以它的機率就是 0<br>
<br>
0:26:01.020,0:26:02.340<br>
這樣顯然是不對的<br>
<br>
0:26:02.340,0:26:04.040<br>
你要想個辦法估測說<br>
<br>
0:26:04.040,0:26:07.500<br>
從這些我們已經有的 神奇寶貝裡面，估測說<br>
<br>
0:26:07.500,0:26:09.980<br>
如果從水系的神奇寶貝裡面<br>
<br>
0:26:09.980,0:26:14.000<br>
挑一隻出來，它是這個海龜的機率，到底有多少？<br>
<br>
0:26:14.780,0:26:18.340<br>
好，那怎麼辦呢？<br>
<br>
0:26:18.340,0:26:20.040<br>
你可以想像說<br>
<br>
0:26:20.040,0:26:22.480<br>
這 79 隻神奇寶貝<br>
<br>
0:26:22.480,0:26:25.700<br>
其實只是冰山的一角<br>
<br>
0:26:25.700,0:26:30.180<br>
就是水系的神奇寶貝，是從一個機率的分布裡面<br>
<br>
0:26:30.180,0:26:33.280<br>
水系神奇寶貝它的防禦力跟特殊防禦力<br>
<br>
0:26:33.280,0:26:38.040<br>
是從一個 Gaussian 的 distribution 裡面，sample 出來的<br>
<br>
0:26:38.040,0:26:41.220<br>
我們只是 sample 了 79 個點以後<br>
<br>
0:26:41.220,0:26:44.200<br>
得到的分佈長這個樣子<br>
<br>
0:26:44.200,0:26:47.420<br>
但是，從 Gaussian 的 distribution 裡面<br>
<br>
0:26:47.420,0:26:50.820<br>
sample 出這個點的機率，不是 0<br>
<br>
0:26:50.820,0:26:56.680<br>
我們假設說，這 79 個點是從一個 Gaussian 的 distribution 裡面 sample 出來的<br>
<br>
0:26:56.740,0:26:58.460<br>
再來，我們要做的事情就是<br>
<br>
0:26:58.460,0:27:02.960<br>
如果給我這 79 個點<br>
<br>
0:27:02.960,0:27:07.360<br>
我們怎麼找到那個 Gaussian 的 distribution<br>
<br>
0:27:07.740,0:27:12.520<br>
那我這邊還做了幾頁投影片要說 Gaussian distribution<br>
<br>
0:27:12.520,0:27:15.120<br>
那我覺得這應該，不用講吧<br>
<br>
0:27:15.120,0:27:18.240<br>
假設你不知道 Gaussian distribution 是什麼的話<br>
<br>
0:27:18.240,0:27:21.380<br>
你就想成它是一個 function<br>
<br>
0:27:21.380,0:27:24.460<br>
這個 function 的 input 就是一個 vector x<br>
<br>
0:27:24.460,0:27:27.820<br>
在這邊呢，代表某一隻寶可夢的數值<br>
<br>
0:27:27.820,0:27:34.520<br>
它的 output 就是這一隻寶可夢，<br>
從這一個 distribution 裡面<br>
<br>
0:27:34.520,0:27:39.180<br>
這一個 x 從這一個 distribution 裡面，<br>
被sample 出來的機率<br>
<br>
0:27:39.180,0:27:41.820<br>
其實嚴格說起來，這個東西並不是機率<br>
<br>
0:27:41.820,0:27:44.460<br>
它是 probability 的 density<br>
<br>
0:27:44.600,0:27:48.920<br>
它跟機率是成正比的，但它並不 exactly 就是機率<br>
<br>
0:27:48.920,0:27:54.440<br>
但是這邊，為了讓大家不要太混亂，<br>
我們就假設它是機率<br>
<br>
0:27:54.440,0:27:57.540<br>
好，那這個機率的分佈呢<br>
<br>
0:27:57.540,0:27:59.620<br>
它是由兩個東西決定<br>
<br>
0:27:59.620,0:28:02.860<br>
一個東西，叫做 mean，這邊寫成 μ<br>
<br>
0:28:02.860,0:28:07.540<br>
另外一個東西，叫做 variance，寫做 Σ，它是一個 matrix<br>
<br>
0:28:07.540,0:28:11.260<br>
mean 是一個 vector，Σ 是一個 matrix<br>
<br>
0:28:11.260,0:28:19.960<br>
所以你把 μ 跟 Σ 代入這個看起來有點複雜的 function<br>
<br>
0:28:19.960,0:28:23.380<br>
那它就會有不同的形狀<br>
<br>
0:28:23.380,0:28:27.120<br>
同樣的 x，如果有不同的 μ 跟 Σ<br>
<br>
0:28:27.120,0:28:31.900<br>
那你代進同樣的 x，它 output 的<br>
機率分布呢，就會是不一樣的<br>
<br>
0:28:31.900,0:28:34.660<br>
下面呢，就舉幾個例子<br>
<br>
0:28:34.660,0:28:37.880<br>
比如說，同樣的 Σ、不同的 μ<br>
<br>
0:28:37.880,0:28:41.880<br>
代表說他們機率分布最高點的地方，是不一樣的<br>
<br>
0:28:41.880,0:28:45.140<br>
比如說，同樣的 μ、不同的 Σ<br>
<br>
0:28:45.140,0:28:47.480<br>
代表說機率分布的最高點是一樣的<br>
<br>
0:28:47.480,0:28:52.920<br>
但是，它們的分布，散的程度是不一樣的<br>
<br>
0:28:53.440,0:28:56.020<br>
那接下來的問題就是<br>
<br>
0:28:56.760,0:29:01.400<br>
我們假設有一個 Gaussian 存在<br>
<br>
0:29:01.400,0:29:04.960<br>
從這個 Gaussian 裡面 ，<br>
<br>
0:29:04.960,0:29:08.440<br>
sample 出這 79 個點<br>
<br>
0:29:08.440,0:29:15.540<br>
那，到底這個 Gaussian長什麼樣子呢？<br>
<br>
0:29:15.540,0:29:18.140<br>
如果我們可以找到這個 Gaussian 的話<br>
<br>
0:29:18.140,0:29:20.300<br>
假設我們可以根據這 79 個點<br>
<br>
0:29:20.300,0:29:23.760<br>
估測出這個 Gaussian 的 μ (mean)<br>
<br>
0:29:23.760,0:29:28.500<br>
應該在這個位置，是 [75, 71.3]<br>
<br>
0:29:29.040,0:29:33.300<br>
它的 Σ 應該是這樣的分布<br>
<br>
0:29:33.300,0:29:38.500<br>
它的 x 跟 y 是有一些 correlation 的<br>
<br>
0:29:38.500,0:29:41.800<br>
但是沒有 x^2 跟 y^2 這邊這麼大<br>
<br>
0:29:41.800,0:29:43.420<br>
大概是長這樣子<br>
<br>
0:29:43.420,0:29:46.740<br>
那給我們一個新的點 x<br>
<br>
0:29:47.160,0:29:49.700<br>
它是我們過去從來沒有看過的點<br>
<br>
0:29:49.700,0:29:52.860<br>
它不在這個 sampling 裡面，<br>
它不在這 79 個 sampling 裡面<br>
<br>
0:29:52.860,0:29:56.140<br>
但是，如果我們知道 μ 跟 x 的話<br>
<br>
0:29:56.340,0:30:00.420<br>
我們就可以把 Gaussian distribution 的 function 寫出來<br>
<br>
0:30:00.420,0:30:03.060<br>
知道 μ 跟 x，我們就可以把這個function 寫出來<br>
<br>
0:30:03.060,0:30:04.780<br>
這個 function 是 depend on μ 跟 x 的<br>
<br>
0:30:04.780,0:30:07.600<br>
所以，我們把它寫成 f (下標 μ 跟 Σ(x))<br>
<br>
0:30:07.600,0:30:09.760<br>
你把這個 x 代進去<br>
<br>
0:30:09.760,0:30:12.600<br>
經過一串複雜的運算以後<br>
<br>
0:30:12.600,0:30:15.040<br>
那你就可以算出呢<br>
<br>
0:30:15.040,0:30:18.740<br>
某一個 x 從這個 Gaussian 裡面<br>
<br>
0:30:18.740,0:30:22.360<br>
從這個 mean 是 μ、它的 covariance matrix <br>
是 Σ 的 Gaussian 裡面<br>
<br>
0:30:22.360,0:30:24.920<br>
被 sample 出來的機率<br>
<br>
0:30:24.920,0:30:28.600<br>
那如果你對這個 function 沒什麼概念的話呢<br>
<br>
0:30:28.600,0:30:29.840<br>
你就可以想像說<br>
<br>
0:30:29.840,0:30:34.280<br>
如果 x 越接近中心點，越接近 μ 這個地方<br>
<br>
0:30:34.280,0:30:37.300<br>
它 sample 出來的機率當然是比較大的<br>
<br>
0:30:37.300,0:30:41.680<br>
像這個 x 在這麼遠的地方，<br>
它 sample 出來的機率就是比較小的<br>
<br>
0:30:43.340,0:30:44.940<br>
那再來有一個問題就是<br>
<br>
0:30:44.940,0:30:48.340<br>
怎麼找這個 μ 跟怎麼找這個 Σ ？<br>
<br>
0:30:48.340,0:30:53.400<br>
這邊用的這個概念呢，叫做 Maximum Likelihood<br>
<br>
0:30:53.660,0:30:57.660<br>
你可以想像說，這 79 個點，其實可以從<br>
<br>
0:30:57.660,0:31:00.900<br>
任何一個 Gaussian 裡面，被 sample 出來<br>
<br>
0:31:00.900,0:31:01.960<br>
對不對？<br>
<br>
0:31:01.980,0:31:05.300<br>
任何一個 Gaussian 都有可能sample 出這 79 個點<br>
<br>
0:31:05.300,0:31:08.100<br>
不管你是 μ 在這個位置<br>
<br>
0:31:08.100,0:31:11.180<br>
然後，它的 covariance matrix 長這個樣子<br>
<br>
0:31:11.180,0:31:13.760<br>
還是 μ 在這個位置<br>
<br>
0:31:13.760,0:31:16.140<br>
它的 covariance 長這個樣子<br>
<br>
0:31:16.140,0:31:19.960<br>
它都有可能sample 出這 79 個點<br>
<br>
0:31:19.960,0:31:23.780<br>
對不對？因為你從 Gaussian 裡面 sample 出一個 point<br>
<br>
0:31:23.780,0:31:27.220<br>
它可以是整個空間上的任何一個點<br>
<br>
0:31:27.220,0:31:30.340<br>
只是有些地方機率很低，有些地方機率很高<br>
<br>
0:31:30.340,0:31:33.440<br>
但沒有一個地方的機率是 exactly 等於 0 的<br>
<br>
0:31:33.440,0:31:36.280<br>
所以，雖然說，右上角這個 Gaussian<br>
<br>
0:31:36.280,0:31:41.160<br>
右上角這個 Gaussian，它 sample 出<br>
左下角這個點的機率很低<br>
<br>
0:31:41.160,0:31:45.560<br>
但是，並不代表說，這個機率是 0<br>
<br>
0:31:45.560,0:31:52.600<br>
但是，雖然說每一個 Gaussian 都<br>
有可能 sample 出這 79 個點<br>
<br>
0:31:52.600,0:31:58.100<br>
但是，他們 sample 出這 79 個點的可能性是不一樣的<br>
<br>
0:31:58.100,0:32:02.320<br>
他們 sample 出這 79 個點的 Likelihood 是不一樣的<br>
<br>
0:32:02.320,0:32:05.040<br>
顯然說，如果你的 Gaussian 是這個的話<br>
<br>
0:32:05.040,0:32:07.880<br>
他 sample 出這 79 個點的 Likelihood 就比較高<br>
<br>
0:32:07.880,0:32:09.280<br>
如果你的 Gaussian 是這個的話<br>
<br>
0:32:09.280,0:32:14.400<br>
他 sample 出這 79 個點的機率是比較低的<br>
<br>
0:32:14.980,0:32:22.400<br>
所以說，今天給我們某一個 Gaussian 的 μ 跟 Σ<br>
<br>
0:32:22.400,0:32:26.600<br>
我們就可以算這個 Gaussian 的 likelihood<br>
<br>
0:32:26.600,0:32:30.160<br>
也就是說，給我一個 Gaussian 的 μ 跟 Σ<br>
<br>
0:32:30.160,0:32:32.080<br>
我們就可以算這個 Gaussian<br>
<br>
0:32:32.080,0:32:36.980<br>
sample 出這 79 個點的機率<br>
<br>
0:32:36.980,0:32:39.600<br>
那這個 likelihood、這個可能性呢<br>
<br>
0:32:39.600,0:32:41.920<br>
我們可以把它寫成這樣一個式子<br>
<br>
0:32:41.920,0:32:43.400<br>
這個可能性呢<br>
<br>
0:32:43.800,0:32:47.480<br>
這邊呢，我們也用了 L，因為我想不到更好的 notation<br>
<br>
0:32:47.480,0:32:50.420<br>
可能會跟 loss function 有點混淆<br>
<br>
0:32:50.420,0:32:53.900<br>
但是，Likelihood 用別的 notation 又很怪<br>
<br>
0:32:53.900,0:32:55.460<br>
還是用 L<br>
<br>
0:32:55.460,0:32:57.660<br>
那這個 L，它的 input 就是<br>
<br>
0:32:57.660,0:33:02.040<br>
Gaussian 的 mean(μ) 跟 covariance (Σ)<br>
<br>
0:33:02.040,0:33:06.880<br>
L 做的事就是把這個 μ 跟 Σ，<br>
代到這個 likelihood 的 function 裡面<br>
<br>
0:33:06.880,0:33:08.700<br>
那它會告訴我們說<br>
<br>
0:33:08.700,0:33:12.500<br>
這個 μ 跟 Σ，它 sample 出這 79 個點的機率<br>
<br>
0:33:12.500,0:33:14.260<br>
到底有多大？<br>
<br>
0:33:14.520,0:33:16.280<br>
它的可能性到底有多大？<br>
<br>
0:33:16.280,0:33:17.660<br>
這個東西怎麼算？<br>
<br>
0:33:17.660,0:33:19.220<br>
這個點，這個東西就是這樣算<br>
<br>
0:33:19.220,0:33:25.900<br>
因為所有的 79 個點是獨立被 sample 出來的<br>
<br>
0:33:25.900,0:33:30.960<br>
所以，今天這個 Gaussian，它 sample 出這 79 個點的機率<br>
<br>
0:33:30.960,0:33:35.000<br>
就是，這個 Gaussian sample 出第 1 個點的機率<br>
<br>
0:33:35.000,0:33:37.320<br>
乘上 sample 出第 2 個點的機率<br>
<br>
0:33:37.320,0:33:38.940<br>
乘上 sample 出第 3 個點的機率<br>
<br>
0:33:38.940,0:33:42.700<br>
一直到 sample 出第 79 個點的機率<br>
<br>
0:33:43.880,0:33:47.800<br>
那所以我們有 79 隻水系的神奇寶貝<br>
<br>
0:33:47.800,0:33:51.520<br>
我們知道，它是從某一個 Gaussian，被 sample 出來的<br>
<br>
0:33:51.520,0:33:54.100<br>
我們接下來要做的事情，就是<br>
<br>
0:33:54.100,0:33:56.280<br>
找到那一個 Gaussian<br>
<br>
0:33:56.500,0:33:58.520<br>
找一個 Gaussian<br>
<br>
0:33:58.520,0:34:03.840<br>
那個 Gaussian，它 sample 出這 79 個點的機率<br>
<br>
0:34:03.840,0:34:05.300<br>
是最大的<br>
<br>
0:34:05.300,0:34:09.320<br>
它 sample 出這 79 個點的 Likelihood 是最大的<br>
<br>
0:34:09.320,0:34:13.120<br>
那這個 Gaussian，我們就當作是<br>
<br>
0:34:13.120,0:34:15.860<br>
sample 出這 79 個點的 Gaussian<br>
<br>
0:34:15.860,0:34:19.060<br>
那這個  Likelihood 最大的 Gaussian 呢<br>
<br>
0:34:19.060,0:34:23.160<br>
我們寫作 (μ*, Σ*)<br>
<br>
0:34:23.160,0:34:24.920<br>
所以，我們現在要做的事情是這樣<br>
<br>
0:34:24.920,0:34:26.700<br>
Likelihood 的 function 寫做這樣子<br>
<br>
0:34:26.700,0:34:28.780<br>
那這每一個 f，如果你想知道的話<br>
<br>
0:34:28.780,0:34:30.620<br>
它很複雜，是寫成這個樣子<br>
<br>
0:34:30.620,0:34:34.440<br>
你就把這個 x 代進去，然後再算出它的 x^2 代進去<br>
<br>
0:34:34.440,0:34:35.460<br>
然後你就算出它<br>
<br>
0:34:35.660,0:34:40.880<br>
然後呢，我們要窮舉所有的 μ<br>
<br>
0:34:40.880,0:34:45.760<br>
窮舉所有的 Σ，看哪一個可以讓上面的 likelihood 的式子最大<br>
<br>
0:34:45.760,0:34:49.700<br>
它就是我們要找的 μ* 跟 Σ*<br>
<br>
0:34:49.700,0:34:53.800<br>
它就是我們認為最有可能產生這 79 個點的 μ* 跟 Σ*<br>
<br>
0:34:53.800,0:34:58.780<br>
我們就當作這 79 個點，是從這個 μ*, Σ* sample 出來的<br>
<br>
0:34:58.780,0:35:02.380<br>
這個東西，怎麼做呢？<br>
<br>
0:35:02.380,0:35:06.460<br>
其實這樣子講，如果你爽的話，你就用微分解一下<br>
<br>
0:35:06.460,0:35:08.120<br>
找那個極值的地方這樣<br>
<br>
0:35:08.120,0:35:11.820<br>
秒解這樣，你也可以背個公式解<br>
<br>
0:35:11.820,0:35:14.200<br>
怎麼秒解，就是<br>
<br>
0:35:14.200,0:35:16.760<br>
哪一個 μ* 可以讓這個最大呢？<br>
<br>
0:35:16.760,0:35:18.520<br>
這個結果是很直覺的<br>
<br>
0:35:18.520,0:35:21.180<br>
就是平均值可以讓它最大<br>
<br>
0:35:21.180,0:35:24.980<br>
所以，你就把 79 個 x 平均起來<br>
<br>
0:35:24.980,0:35:32.400<br>
你就把 79 個 x 當作是 vector 加起來，除 79，就得到 μ*<br>
<br>
0:35:32.480,0:35:34.280<br>
平均就是 μ*<br>
<br>
0:35:34.280,0:35:39.160<br>
如果你不爽的話，你就把這個式子取個微分阿<br>
<br>
0:35:39.160,0:35:42.140<br>
對 μ 取個微分，然後找它微分是 0 的點<br>
<br>
0:35:42.140,0:35:44.060<br>
解出來就是你的 μ*<br>
<br>
0:35:44.920,0:35:50.080<br>
Σ* 是甚麼呢？你先把 μ* 算出來<br>
<br>
0:35:50.080,0:35:55.700<br>
然後對所有的 x^n，你都算 (x^n - μ*)<br>
<br>
0:35:55.700,0:35:59.440<br>
乘 (x^n - μ*)^T (的 transpose)<br>
<br>
0:35:59.440,0:36:06.680<br>
你就算說，假設 x^n 的 mean 是 μ* 的話<br>
<br>
0:36:06.680,0:36:10.460<br>
的 covariance，那你算出來呢，就是 Σ*<br>
<br>
0:36:10.460,0:36:15.340<br>
那如果你不爽的話，就把這些值對 Σ* 做微分<br>
<br>
0:36:15.340,0:36:19.280<br>
對 Σ* 做微分，然後解它微分是 0 的點<br>
<br>
0:36:19.280,0:36:21.180<br>
你就解出來這個<br>
<br>
0:36:21.180,0:36:26.380<br>
有了這些以後<br>
<br>
0:36:26.380,0:36:28.320<br>
我們就真的去算一下<br>
<br>
0:36:28.320,0:36:30.640<br>
這個是真正的結果<br>
<br>
0:36:30.940,0:36:35.120<br>
79 隻水系的神奇寶貝，79 隻水系的寶可夢<br>
<br>
0:36:35.200,0:36:38.300<br>
算出來的 μ 是這樣子<br>
<br>
0:36:38.300,0:36:41.420<br>
算出來的 Σ 是這樣子<br>
<br>
0:36:41.420,0:36:42.580<br>
也就是說呢<br>
<br>
0:36:42.580,0:36:47.120<br>
假設這 79 隻水系的神奇寶貝<br>
<br>
0:36:47.120,0:36:48.480<br>
是從這個 Gaussian sample 出來的話<br>
<br>
0:36:48.480,0:36:51.700<br>
那最有可能 sample 出這 79 個點的 Gaussian<br>
<br>
0:36:51.700,0:36:55.880<br>
它的 mean 是 μ1，它的 covariance 是 Σ1<br>
<br>
0:36:56.240,0:36:59.200<br>
那如果你看 class 2 的話<br>
<br>
0:37:00.320,0:37:03.120<br>
class 2 是一般系的神奇寶貝<br>
<br>
0:37:03.120,0:37:05.560<br>
有幾隻呢？有 61 隻<br>
<br>
0:37:05.560,0:37:09.460<br>
那我們一樣算它的 mean 跟 variance<br>
<br>
0:37:09.460,0:37:12.440<br>
這 61 隻一般系的神奇寶貝<br>
<br>
0:37:12.440,0:37:14.000<br>
最有可能 sample 出它的 Gaussian<br>
<br>
0:37:14.000,0:37:17.920<br>
它的 mean 是長這樣，它的 variance 是長這樣<br>
<br>
0:37:17.920,0:37:19.700<br>
有了這些以後<br>
<br>
0:37:20.380,0:37:24.280<br>
就結束了，我們就可以做分類的問題了<br>
<br>
0:37:24.280,0:37:25.280<br>
怎麼做呢？<br>
<br>
0:37:25.280,0:37:27.180<br>
我們說要做分類的問題<br>
<br>
0:37:27.180,0:37:30.900<br>
我們只要算出 P(C1|x)<br>
<br>
0:37:30.900,0:37:33.660<br>
給我一個 x，它是從 C1 來的機率<br>
<br>
0:37:33.660,0:37:36.060<br>
那這整項可以寫成這樣子<br>
<br>
0:37:36.760,0:37:40.420<br>
只要我們最後算出來這一項，大於 0.5 的話<br>
<br>
0:37:40.420,0:37:43.220<br>
那 x 就屬於 class 1<br>
<br>
0:37:44.760,0:37:49.140<br>
那 P(C1) 很容易算，就是這麼回事<br>
<br>
0:37:49.140,0:37:51.660<br>
那 P(C2) 我們算過，就是這麼回事<br>
<br>
0:37:51.660,0:37:54.940<br>
P(x|C1) 怎麼算呢？<br>
<br>
0:37:54.940,0:37:56.420<br>
我們已經找出<br>
<br>
0:37:56.420,0:37:59.300<br>
我們已經假設說這個東西<br>
<br>
0:37:59.300,0:38:01.620<br>
它就是一個 Gaussian distribution<br>
<br>
0:38:01.620,0:38:05.520<br>
這個 Gaussian distribution 的 mean 跟 variance<br>
<br>
0:38:05.520,0:38:07.880<br>
分別就是 μ1 跟 Σ1<br>
<br>
0:38:07.880,0:38:09.860<br>
這我們剛才已經算出來過了<br>
<br>
0:38:09.860,0:38:11.620<br>
因為我們剛才已經根據<br>
<br>
0:38:11.620,0:38:16.800<br>
class 1 所有的、那 79 隻寶可夢的分布<br>
<br>
0:38:16.800,0:38:18.800<br>
知道說，他們是從一個<br>
<br>
0:38:18.800,0:38:23.840<br>
mean 是 μ1，covariance 是 Σ1 的 <br>
distribution 裡面 sample 出來的<br>
<br>
0:38:23.840,0:38:25.700<br>
那如果是這一項呢？<br>
<br>
0:38:25.700,0:38:31.660<br>
P(x|C2)，那我們也知道說，<br>
它的這個 Gaussian distribution<br>
<br>
0:38:31.660,0:38:35.060<br>
它是從 mean 是 μ^2, covariance 是 Σ^2<br>
<br>
0:38:35.060,0:38:37.380<br>
的 distribution 裡面 sample 出來的<br>
<br>
0:38:37.380,0:38:41.760<br>
有了這些以後，問題就解決了<br>
<br>
0:38:41.760,0:38:43.460<br>
那結果怎麼樣呢？<br>
<br>
0:38:43.460,0:38:44.860<br>
我是真的有做的<br>
<br>
0:38:45.360,0:38:47.840<br>
藍色的點是<br>
<br>
0:38:47.840,0:38:50.940<br>
這個橫軸跟縱軸<br>
<br>
0:38:50.940,0:38:54.140<br>
分別就是防禦力跟特殊防禦力<br>
<br>
0:38:54.560,0:38:59.080<br>
藍色的點，是水系的神奇寶貝的分布<br>
<br>
0:38:59.080,0:39:03.160<br>
紅色的點，是一般系的神奇寶貝的分布<br>
<br>
0:39:03.160,0:39:05.340<br>
看到這個結果，我有點緊張<br>
<br>
0:39:05.340,0:39:09.780<br>
因為覺得分不出來，我用人眼看就知道不太 ok<br>
<br>
0:39:10.320,0:39:13.300<br>
那我們就真的計算一下<br>
<br>
0:39:13.300,0:39:16.620<br>
在這個二維平面上<br>
<br>
0:39:16.620,0:39:20.600<br>
每一個點，我都當作一個 x<br>
<br>
0:39:20.600,0:39:24.980<br>
進去我都可以算一個，它是 C1 的機率對不對？<br>
<br>
0:39:24.980,0:39:30.740<br>
這個圖上的每一個點，我都可以算它是 C1 的機率<br>
<br>
0:39:30.740,0:39:34.440<br>
那這個機率呢，用顏色來表示<br>
<br>
0:39:34.440,0:39:37.720<br>
紅色就代表說，在這個區域呢<br>
<br>
0:39:37.720,0:39:42.360<br>
是 class 1，是水系神奇寶貝的機率是比較大的<br>
<br>
0:39:42.360,0:39:47.320<br>
在這個地方呢，水系神奇寶貝的機率是比較小的<br>
<br>
0:39:47.320,0:39:48.740<br>
你看這很合理嘛<br>
<br>
0:39:48.740,0:39:51.500<br>
因為，水系神奇寶貝在這邊的分布還是比較多<br>
<br>
0:39:51.500,0:39:54.560<br>
在這邊比較多，所以這個地方機率是比較大的<br>
<br>
0:39:55.100,0:39:58.220<br>
那現在，因為我們處理的是分類的問題<br>
<br>
0:39:58.220,0:40:01.220<br>
我們算這個機率，我們是要 output 說是哪一類<br>
<br>
0:40:01.220,0:40:05.520<br>
所以我們說，機率大於 0.5，就是類別一<br>
<br>
0:40:05.520,0:40:09.160<br>
也就是紅色這個區間，就是類別一<br>
<br>
0:40:09.160,0:40:12.400<br>
機率小於 0.5，就是藍色這個區間<br>
<br>
0:40:12.400,0:40:15.660<br>
他們就是類別二，這個是類別一<br>
<br>
0:40:15.660,0:40:17.380<br>
你會發現說<br>
<br>
0:40:17.380,0:40:19.400<br>
如果你看藍色的點的話<br>
<br>
0:40:19.400,0:40:22.320<br>
是比較多藍色的點，在這個紅色的區間<br>
<br>
0:40:22.320,0:40:26.280<br>
那紅色的點，是比較多在這個藍色的區間<br>
<br>
0:40:26.280,0:40:27.640<br>
那也有一些到紅色的區間<br>
<br>
0:40:27.640,0:40:32.260<br>
有點難搞，因為他們中間沒有一個明確的 boundary<br>
<br>
0:40:33.240,0:40:36.680<br>
那把它 apply 到 testing set 上<br>
<br>
0:40:36.680,0:40:40.300<br>
現在，testing set 就是編號大於 400 那些寶可夢<br>
<br>
0:40:40.300,0:40:44.860<br>
把他們整個 class 1 跟 class 2 的寶可夢<br>
畫在這個二維平面上<br>
<br>
0:40:44.860,0:40:46.620<br>
那 boundary 是一樣的<br>
<br>
0:40:46.620,0:40:49.000<br>
這個 boundary 是一樣的<br>
<br>
0:40:49.000,0:40:52.200<br>
你會發現說，分的不甚太好<br>
<br>
0:40:52.200,0:40:54.860<br>
正確率是 47 %<br>
<br>
0:40:55.120,0:40:57.620<br>
那你有一點擔心<br>
<br>
0:40:57.620,0:41:02.000<br>
會不會是這題有可能分不出來，這也是有可能的<br>
<br>
0:41:02.000,0:41:06.080<br>
但是我想說我們現在只看了二維的空間，對不對<br>
<br>
0:41:06.080,0:41:08.180<br>
機器學習厲害的地方就是<br>
<br>
0:41:08.180,0:41:09.820<br>
因為我們讓機器處理這個問題<br>
<br>
0:41:09.820,0:41:14.000<br>
所以高維空間也可以處理，不是只處理二維的空間而已<br>
<br>
0:41:14.000,0:41:15.940<br>
所以我們看一下高維的空間<br>
<br>
0:41:15.940,0:41:18.237<br>
事實上，每一隻神奇寶貝(寶可夢呢)<br>
<br>
0:41:18.237,0:41:21.160<br>
它是分布在一個七維的空間裡面<br>
<br>
0:41:21.160,0:41:25.200<br>
如果只用二維的空間分不出來，<br>
可是搞不好七維分的出來阿<br>
<br>
0:41:25.200,0:41:28.720<br>
就是這個紅色跟藍色搞不好在高維的空間上看到<br>
<br>
0:41:28.720,0:41:30.420<br>
一個是這樣，一個是這樣<br>
<br>
0:41:30.420,0:41:32.560<br>
現在從上面往下看，就覺得疊在一起<br>
<br>
0:41:32.560,0:41:34.800<br>
高維空間上，搞不好是分開的<br>
<br>
0:41:34.800,0:41:36.820<br>
搞不好在七維空間上，是分開的<br>
<br>
0:41:36.820,0:41:41.160<br>
所以，每一個寶可夢都是用七個數值來表示<br>
<br>
0:41:41.160,0:41:44.480<br>
所以，每一個寶可夢都是存在七維空間中的一個點<br>
<br>
0:41:44.480,0:41:49.860<br>
我們一樣可以算 class 跟 class 2 在七維空間中<br>
<br>
0:41:49.860,0:41:55.360<br>
sample 出那些點的 μ1 跟 μ2<br>
<br>
0:41:55.360,0:41:57.320<br>
μ1 跟 μ2 都是七維<br>
<br>
0:41:57.320,0:42:02.980<br>
Σ1 跟 Σ2 都是 7*7 的 matrix<br>
<br>
0:42:02.980,0:42:07.180<br>
然後你就做一發，正確率就 50%，很糟<br>
<br>
0:42:07.180,0:42:10.440<br>
這樣就跟你 random 猜，大概也是這個樣子<br>
<br>
0:42:10.440,0:42:15.900<br>
然後就 so sad 這樣，然後我們下周再看看要怎麼改進它<br>
<br>
0:42:15.940,0:42:17.180<br>
謝謝<br>
<br>
0:42:30.700,0:42:35.020<br>
各位同學大家好，我們就開始上課吧<br>
<br>
0:42:35.020,0:42:38.460<br>
上次我們講到哪裡呢？我們講到說<br>
<br>
0:42:39.800,0:42:46.580<br>
如果我們想要做，寶可夢的屬性的分類的話<br>
<br>
0:42:46.580,0:42:52.380<br>
我們可以假設一個機率模型<br>
<br>
0:42:52.380,0:42:55.640<br>
那我們把這個機率模型裡面呢<br>
<br>
0:42:55.640,0:42:58.600<br>
拆成有 required probability<br>
<br>
0:42:58.600,0:43:01.760<br>
跟每一個 class 自己的 distribution<br>
<br>
0:43:01.760,0:43:06.860<br>
那每一個 class 自己的機率呢，就用 Gaussian 來假設它<br>
<br>
0:43:06.860,0:43:08.940<br>
那經過一番運算<br>
<br>
0:43:08.940,0:43:12.140<br>
我們算出每一個 class 的 required<br>
<br>
0:43:12.140,0:43:16.080<br>
跟每一個 class 的 Gaussian distribution 以後呢<br>
<br>
0:43:16.080,0:43:17.960<br>
做一下<br>
<br>
0:43:21.880,0:43:24.840<br>
講到這邊，做一下之後發現呢<br>
<br>
0:43:24.840,0:43:26.280<br>
結果壞掉了<br>
<br>
0:43:26.280,0:43:29.620<br>
就算是我用了全部寶可夢的 7 個 feature<br>
<br>
0:43:29.620,0:43:32.740<br>
還是壞掉了<br>
<br>
0:43:32.740,0:43:35.120<br>
那怎麼辦呢？<br>
<br>
0:43:42.500,0:43:48.740<br>
那其實呢，當你用這樣子的 <br>
probability generated 的 model 的時候<br>
<br>
0:43:48.740,0:43:53.400<br>
像我在上一堂課裡面用的那種模型<br>
<br>
0:43:53.400,0:43:54.920<br>
是比較少見的<br>
<br>
0:43:54.920,0:43:56.680<br>
其實你不常看到<br>
<br>
0:43:56.680,0:44:02.400<br>
給每一個 Gaussian 都有自己的 mean 跟自己的 variance<br>
<br>
0:44:02.640,0:44:05.340<br>
每一個 Gaussian 都有自己的 mean 跟自己的 variance<br>
<br>
0:44:05.340,0:44:09.940<br>
class 1 有一個 μ1，有一個 Σ1<br>
<br>
0:44:09.940,0:44:12.800<br>
class 2 有一個 μ2、 Σ2<br>
<br>
0:44:12.800,0:44:15.720<br>
比較常見的做法是，不同的 class<br>
<br>
0:44:15.720,0:44:20.640<br>
可以 share 同一個 covariance 的 matrix<br>
<br>
0:44:20.640,0:44:23.360<br>
首先。你想想看，covariance matrix<br>
<br>
0:44:23.360,0:44:26.360<br>
它其實是跟你 input 的 feature size<br>
<br>
0:44:26.360,0:44:30.940<br>
是跟它的平方成正比的<br>
<br>
0:44:30.940,0:44:34.820<br>
所以，covariance matrix 當你的 feature size 很大的時候<br>
<br>
0:44:34.820,0:44:37.880<br>
它的增長呢，其實是可以非常快的<br>
<br>
0:44:37.880,0:44:39.660<br>
所以在這個情況下呢<br>
<br>
0:44:39.660,0:44:45.200<br>
如果你把兩個不同的 Gaussian <br>
都給它不同的 covariance matrix<br>
<br>
0:44:45.200,0:44:48.240<br>
那你的 model 參數可能就太多了，model 參數多<br>
<br>
0:44:48.240,0:44:52.240<br>
variance 就大，也就是容易 overfitting<br>
<br>
0:44:52.240,0:44:56.040<br>
所以，如果我們要有效減少參數的話<br>
<br>
0:44:56.040,0:44:58.300<br>
我們可以給這兩個 class<br>
<br>
0:44:58.300,0:45:02.520<br>
就是屬於水系的神奇寶貝和屬於一般系的神奇寶貝<br>
<br>
0:45:02.520,0:45:09.400<br>
它們的描述這兩個 class 的 feature 分布的 Gaussian<br>
<br>
0:45:09.400,0:45:13.540<br>
故意給他們同樣的 covariance matrix<br>
<br>
0:45:13.540,0:45:16.480<br>
強迫他們共用 covariance matrix<br>
<br>
0:45:16.860,0:45:21.600<br>
這樣子呢，你就只需要比較少的 parameter<br>
<br>
0:45:21.600,0:45:24.540<br>
就可以來 model 這一個模型了<br>
<br>
0:45:25.240,0:45:26.740<br>
這甚麼意思呢？<br>
<br>
0:45:26.740,0:45:32.680<br>
也就是說，現在我們有 79 隻水系的寶可夢<br>
<br>
0:45:32.680,0:45:36.420<br>
我們假設它是從一個 mean 是 μ1<br>
<br>
0:45:36.420,0:45:42.500<br>
covariance 是 Σ 的 Gaussian 所 generate 出來的<br>
<br>
0:45:42.980,0:45:47.380<br>
那另外，這邊是多少隻呢？<br>
<br>
0:45:47.380,0:45:49.460<br>
這邊應該是 61 隻<br>
<br>
0:45:49.460,0:45:53.780<br>
我們這邊給它編號從 80 到 140<br>
<br>
0:45:53.900,0:45:56.860<br>
有另外 61 隻是屬於一般系的寶可夢<br>
<br>
0:45:56.860,0:46:02.260<br>
我們假設這些寶可夢他們的屬性的這個分布呢<br>
<br>
0:46:02.260,0:46:05.320<br>
是從另外一個 Gaussian 所 generate 出來的<br>
<br>
0:46:05.320,0:46:08.920<br>
另外一個 Gaussian，它的 mean 是 μ2<br>
<br>
0:46:08.920,0:46:11.300<br>
但是它的 covariance matrix<br>
<br>
0:46:11.300,0:46:13.880<br>
跟 generate 前一個<br>
<br>
0:46:13.880,0:46:16.720<br>
跟 generate class 1，跟 generate 水屬性的寶可夢<br>
<br>
0:46:16.720,0:46:20.040<br>
他們用的 covariance matrix 是同一個<br>
<br>
0:46:20.040,0:46:24.100<br>
這兩個 class 他們 share 同一個 covariance matrix<br>
<br>
0:46:24.620,0:46:28.520<br>
如果這樣子的話，你怎麼計算 Likelihood 呢？<br>
<br>
0:46:28.520,0:46:34.000<br>
如果你現在要計算，某一組 μ1, μ2 和 Σ<br>
<br>
0:46:34.000,0:46:37.220<br>
generate 這總共兩個 case 合起來<br>
<br>
0:46:37.220,0:46:41.540<br>
140 筆 data 的可能性的話<br>
<br>
0:46:41.540,0:46:44.520<br>
你就像下面這樣計算<br>
<br>
0:46:44.780,0:46:48.520<br>
這個計算方法就是<br>
<br>
0:46:48.520,0:46:54.240<br>
計算如果你今天用 μ1 跟 Σ1 產生 x^1 的機率<br>
<br>
0:46:54.240,0:46:57.820<br>
乘上用 μ1 跟 Σ1 產生 x^2 的機率<br>
<br>
0:46:57.820,0:47:03.460<br>
剛才都一直唸 Σ1 不好意思<br>
<br>
0:47:03.460,0:47:05.580<br>
這邊不是 Σ1，這個是只有 Σ 而已<br>
<br>
0:47:05.580,0:47:09.320<br>
因為這兩個 class，是共用同一個 covariance matrix<br>
<br>
0:47:10.160,0:47:13.660<br>
現在呢，用 μ1 跟 Σ 產生 x^1<br>
<br>
0:47:13.660,0:47:17.480<br>
到用 μ1 跟 Σ 產生 x^79<br>
<br>
0:47:17.480,0:47:24.160<br>
那如果是第一個 class 的 x 用這個方法來產生<br>
<br>
0:47:24.160,0:47:26.760<br>
如果是第一個 class 的 x 呢<br>
<br>
0:47:26.760,0:47:29.700<br>
你就用 μ2 跟 Σ 產生 x^80<br>
<br>
0:47:29.700,0:47:32.140<br>
用 μ2 跟 Σ 產生 x^81<br>
<br>
0:47:32.140,0:47:35.660<br>
到用 μ2 跟 Σ 產生 x^140<br>
<br>
0:47:35.840,0:47:38.320<br>
那在這個式子裡面呢 μ1, μ2<br>
<br>
0:47:38.320,0:47:40.960<br>
你要怎麼算 μ1, μ2 呢？<br>
<br>
0:47:40.960,0:47:46.060<br>
你要怎麼找一個 μ1, μ2 跟 Σ 讓<br>
這個 Likelihood 的 function 最大呢？<br>
<br>
0:47:46.060,0:47:48.180<br>
那 μ1, μ2 的算法<br>
<br>
0:47:48.180,0:47:52.720<br>
跟我們之前沒有把 class 1 跟 class 2 的 covariance<br>
<br>
0:47:52.720,0:47:57.620<br>
tight 在一起的時候，那個算式是一模一樣的<br>
<br>
0:47:57.620,0:48:02.340<br>
你就只要把 class 1 裡面的 x 平均起來就變 μ1<br>
<br>
0:48:02.340,0:48:05.440<br>
class 2 裡面的 x 平均起來就變 μ2<br>
<br>
0:48:05.440,0:48:07.980<br>
唯一不一樣的是<br>
<br>
0:48:09.000,0:48:11.860<br>
(右下角)我們要把它按接受這樣子<br>
<br>
0:48:17.580,0:48:20.600<br>
那唯一不一樣的是<br>
<br>
0:48:20.600,0:48:22.460<br>
Σ 嗯？<br>
<br>
0:48:25.320,0:48:27.420<br>
唯一不一樣的是 Σ<br>
<br>
0:48:27.420,0:48:31.160<br>
因為我們現在 Σ 要同時考慮這兩個 class<br>
<br>
0:48:31.160,0:48:33.040<br>
所以它當然是不一樣的<br>
<br>
0:48:33.040,0:48:35.660<br>
那這個 Σ 的式子應該長甚麼樣子呢？<br>
<br>
0:48:35.660,0:48:39.920<br>
這個 Σ 的式子，這個結果非常的直觀<br>
<br>
0:48:39.920,0:48:42.840<br>
如果你想要看它的推導的話<br>
<br>
0:48:42.840,0:48:46.540<br>
我這邊引用的就是 Bishop 這本教科書<br>
<br>
0:48:46.540,0:48:50.620<br>
以後如果要引用的話，盡量引 Bishop，為甚麼呢？<br>
<br>
0:48:50.620,0:48:53.660<br>
因為它在網路上，有 available 的版本<br>
<br>
0:48:58.060,0:49:01.660<br>
這個 Σ 應該長甚麼樣子呢？<br>
<br>
0:49:01.660,0:49:02.980<br>
這個結果非常的直觀<br>
<br>
0:49:02.980,0:49:06.600<br>
你就把原來我們根據這些 data<br>
<br>
0:49:06.600,0:49:09.880<br>
所算出來的 covariance matrix (Σ^1)<br>
<br>
0:49:09.880,0:49:11.880<br>
跟根據這些 data<br>
<br>
0:49:11.880,0:49:14.260<br>
所算出來的 covariance matrix，Σ^2<br>
<br>
0:49:14.260,0:49:18.440<br>
weighted by 他們 element 的數目<br>
<br>
0:49:18.440,0:49:21.480<br>
你這個 class 1 有 79 個<br>
<br>
0:49:21.480,0:49:23.100<br>
所以你就把 Σ^1 * 79<br>
<br>
0:49:23.100,0:49:26.780<br>
class 1 有 61 個<br>
<br>
0:49:26.780,0:49:29.140<br>
所以你就把 Σ^2 * 61，再取平均<br>
<br>
0:49:29.140,0:49:33.300<br>
你就把原來這兩個 Gaussian，各自算的 covariance matrix<br>
<br>
0:49:33.300,0:49:35.360<br>
加權平均，就會得到<br>
<br>
0:49:35.360,0:49:38.760<br>
如果你要求他們用共同的 Gaussian 的時候<br>
<br>
0:49:38.760,0:49:41.200<br>
所得到的 covariance matrix<br>
<br>
0:49:42.860,0:49:45.040<br>
那我們來看一下結果<br>
<br>
0:49:45.040,0:49:48.380<br>
假設我們仍然是用兩個 feature<br>
<br>
0:49:48.380,0:49:52.360<br>
用 Defense 跟 SP Defense 的話<br>
<br>
0:49:52.360,0:49:56.840<br>
後來我發現，我這兩個 example 選的不是很好<br>
<br>
0:49:56.840,0:49:58.920<br>
因為如果你看 Defense 跟 SP Defense<br>
<br>
0:49:58.920,0:50:03.000<br>
你是沒有辦法把水系跟一般系的神奇寶貝分開<br>
<br>
0:50:03.000,0:50:04.460<br>
後來我研究了一下，我覺得<br>
<br>
0:50:04.460,0:50:08.100<br>
好像用一般攻擊力和一般防禦力<br>
<br>
0:50:08.100,0:50:11.440<br>
合起來呢，就可以分的滿開的這樣子<br>
<br>
0:50:11.440,0:50:14.400<br>
但是，我怎麼會事先知道這件事呢？<br>
<br>
0:50:14.400,0:50:17.220<br>
我又不是大木博士，對不對？<br>
<br>
0:50:17.220,0:50:23.940<br>
那如果我們今天共用 covariance matrix 會發生甚麼事？<br>
<br>
0:50:23.940,0:50:27.760<br>
在沒有共用之前，class 1 跟 class 2 的 boundary<br>
<br>
0:50:27.760,0:50:30.700<br>
是這條，是這個曲線<br>
<br>
0:50:30.700,0:50:33.820<br>
如果我們今天共用同一個 covariance matrix 的話<br>
<br>
0:50:33.820,0:50:35.460<br>
你會發現說<br>
<br>
0:50:35.460,0:50:40.140<br>
他們的 boundary，變成是一個直線<br>
<br>
0:50:40.140,0:50:43.460<br>
假設你把這兩個不同的 class<br>
<br>
0:50:43.460,0:50:46.500<br>
強迫他們的 covariance matrix 必須共用同一個的話<br>
<br>
0:50:46.500,0:50:51.080<br>
那你今天在分類的時候，你的 boundary 就會變成是<br>
<br>
0:50:51.080,0:50:52.940<br>
一條直線<br>
<br>
0:50:52.940,0:50:55.080<br>
所以，像這樣子的 model<br>
<br>
0:50:55.080,0:50:57.540<br>
我們也稱之它為 linear 的 model<br>
<br>
0:50:57.540,0:51:00.400<br>
你可能會想說，Gaussian 甚麼的不是 linear 的阿<br>
<br>
0:51:00.400,0:51:05.020<br>
但是，它分兩個 class 的 boundary 是 linear 的<br>
<br>
0:51:05.020,0:51:08.900<br>
所以，這樣的 model，我們也稱它為 linear 的 model<br>
<br>
0:51:09.920,0:51:14.160<br>
如果今天兩個 class，<br>
你用不同的 covariance matrix 的話呢<br>
<br>
0:51:14.160,0:51:15.940<br>
它們就不是 linear 的 model<br>
<br>
0:51:16.280,0:51:19.800<br>
如果，我們考慮所有的 feature 會怎麼樣呢？<br>
<br>
0:51:19.800,0:51:21.920<br>
如果我們考慮所有的 feature 的話<br>
<br>
0:51:21.920,0:51:24.500<br>
原來我們只得到 50% 正確率<br>
<br>
0:51:24.500,0:51:27.520<br>
但是，神奇的是，當我們共用 covariance matrix 的時候<br>
<br>
0:51:27.520,0:51:29.760<br>
我們就得到 79% 的正確率了<br>
<br>
0:51:29.760,0:51:31.940<br>
顯然是有分對東西<br>
<br>
0:51:32.340,0:51:35.380<br>
那你說，為甚麼會做到這樣子呢，那這就很難分析了<br>
<br>
0:51:35.380,0:51:39.420<br>
因為，這個是在高維空間中發生的事情<br>
<br>
0:51:39.420,0:51:42.300<br>
是在 7 維空間中發生的事情<br>
<br>
0:51:42.300,0:51:46.580<br>
我們很難知道說，這個 boundary 是怎麼切的<br>
<br>
0:51:46.580,0:51:49.880<br>
但是，這個就是 machine learning fancy 的地方<br>
<br>
0:51:49.880,0:51:54.160<br>
就是，人沒有辦法知道怎麼做<br>
<br>
0:51:54.160,0:51:56.400<br>
但是，machine 可以幫我們做出來<br>
<br>
0:51:56.400,0:52:00.560<br>
如果今天 feature 很少，人一看就知道怎麼做<br>
<br>
0:52:00.560,0:52:03.060<br>
那其實可以不用用上 machine learning，對不對？<br>
<br>
0:52:03.060,0:52:06.980<br>
所以，現在可以得到 73% 的正確率<br>
<br>
0:52:07.980,0:52:13.680<br>
我們來回顧一下，我們講得這個<br>
<br>
0:52:13.680,0:52:16.000<br>
機率的模型<br>
<br>
0:52:16.000,0:52:19.900<br>
那我們講說 machine learning 就是 3 個 step<br>
<br>
0:52:19.900,0:52:23.720<br>
那這個機率模型呢，它其實也是 3 個 step<br>
<br>
0:52:23.720,0:52:28.520<br>
首先，你有一個 model，<br>
這個 model 就是你的 function set<br>
<br>
0:52:28.960,0:52:32.580<br>
這個 function set 裡面的 function 都長甚麼樣子呢？<br>
<br>
0:52:32.580,0:52:36.260<br>
這個 function set 裡面的 function 都長下面這個樣子<br>
<br>
0:52:36.260,0:52:38.500<br>
input 一個 x<br>
<br>
0:52:38.500,0:52:43.980<br>
我們有 class 1 的 required probability<br>
<br>
0:52:43.980,0:52:46.080<br>
class 2 的 required probability<br>
<br>
0:52:46.080,0:52:49.040<br>
class 1 產生 x 的 probability distribution<br>
<br>
0:52:49.040,0:52:51.400<br>
class 2 產生 x 的 probability distribution<br>
<br>
0:52:51.400,0:52:56.600<br>
這些 required probability 和 probability distribution<br>
<br>
0:52:56.600,0:52:59.840<br>
就是 model 的參數<br>
<br>
0:52:59.840,0:53:03.580<br>
你選擇不同的 probability distribution<br>
<br>
0:53:03.580,0:53:07.560<br>
你就得到不同的 function<br>
<br>
0:53:07.560,0:53:10.740<br>
那你把這些不同的 probability distribution<br>
<br>
0:53:10.740,0:53:14.160<br>
就像 Gaussian 你選不同的 mean <br>
跟不同的 covariance matrix<br>
<br>
0:53:14.160,0:53:16.780<br>
你就得到不同的 probability distribution<br>
<br>
0:53:16.780,0:53:18.980<br>
你把這些不同的 probability distribution 積分起來<br>
<br>
0:53:18.980,0:53:21.520<br>
就是一個 model，就是一個 function set<br>
<br>
0:53:22.000,0:53:24.500<br>
那怎麼決定是哪一個 class 呢？<br>
<br>
0:53:24.500,0:53:29.820<br>
如果 P(x|C1) 這個 posterior probability > 0.5 的話呢<br>
<br>
0:53:29.820,0:53:33.240<br>
就 output class 1，反之呢，就 output class 2<br>
<br>
0:53:33.240,0:53:35.360<br>
這個是 function 的樣子<br>
<br>
0:53:35.360,0:53:37.620<br>
接下來呢，我們要找<br>
<br>
0:53:37.620,0:53:43.140<br>
evaluate function set 裡面每一個 function 的好壞<br>
<br>
0:53:43.140,0:53:44.700<br>
那怎麼 evaluate 呢？<br>
<br>
0:53:44.700,0:53:47.240<br>
在這個機率模型裡面<br>
<br>
0:53:47.240,0:53:49.440<br>
假設我們今天使用 Gaussian 的話<br>
<br>
0:53:49.440,0:53:53.560<br>
那我們要 evaluate 的對象，<br>
其實就是 Gaussian 裡面的參數<br>
<br>
0:53:53.560,0:53:57.040<br>
也就是 mean 跟 covariance matrix<br>
<br>
0:53:57.040,0:53:59.260<br>
那今天呢，我們就是說<br>
<br>
0:53:59.260,0:54:02.520<br>
如果一個 mean 跟一個 covariance matrix<br>
<br>
0:54:02.520,0:54:07.440<br>
你用這些參數來定義你的 probability distribution<br>
<br>
0:54:07.440,0:54:13.180<br>
而它可以產生我們的 training data 的 likelihood<br>
<br>
0:54:13.180,0:54:16.100<br>
就是這組參數的好壞<br>
<br>
0:54:16.100,0:54:18.240<br>
所以，我們要做的事情就是<br>
<br>
0:54:18.240,0:54:21.180<br>
找一個 probability distribution<br>
<br>
0:54:21.180,0:54:26.180<br>
它可以最大化產生這些 data 的 likelihood<br>
<br>
0:54:26.980,0:54:29.940<br>
這個是定義 function 的好壞<br>
<br>
0:54:29.940,0:54:31.760<br>
定義一組參數的好壞<br>
<br>
0:54:31.760,0:54:34.160<br>
最後，怎麼找出一組最好的參數呢？<br>
<br>
0:54:34.160,0:54:37.540<br>
你就看看前面的投影片，它的結果是很 trivial 的<br>
<br>
0:54:38.740,0:54:41.160<br>
那有人就會問說<br>
<br>
0:54:41.160,0:54:45.940<br>
為甚麼要用 Gaussian，為甚麼不選別的這樣子？<br>
<br>
0:54:46.160,0:54:47.800<br>
簡單的答案就是<br>
<br>
0:54:47.800,0:54:52.520<br>
如果我選了別的機率模型，你也會問我同樣的問題<br>
<br>
0:54:52.520,0:54:57.640<br>
其實你永遠可以選一個，你自己喜歡的<br>
<br>
0:54:57.640,0:54:59.800<br>
這個 probability distribution<br>
<br>
0:54:59.800,0:55:02.520<br>
這個是你自己決定的<br>
<br>
0:55:02.520,0:55:05.440<br>
這個不是人工智慧<br>
<br>
0:55:05.440,0:55:10.440<br>
是你人的智慧，去決定說你要選哪一個人的模型<br>
<br>
0:55:10.440,0:55:12.100<br>
是比較適合的<br>
<br>
0:55:12.100,0:55:16.060<br>
那你選擇比較簡單的機率模型，參數比較少的<br>
<br>
0:55:16.060,0:55:18.520<br>
那你的 bias 就大、variance 就小<br>
<br>
0:55:18.520,0:55:21.160<br>
那你選擇複雜的，你 bias 就小、variance 就大<br>
<br>
0:55:21.160,0:55:23.480<br>
那你可能就要用 data set 決定一下<br>
<br>
0:55:23.480,0:55:25.700<br>
你要用怎麼樣的機率模型，是比較好的<br>
<br>
0:55:26.060,0:55:30.980<br>
那我們有另外一種常見的假設是這樣<br>
<br>
0:55:30.980,0:55:33.420<br>
假設我們的這個 x<br>
<br>
0:55:33.420,0:55:36.720<br>
我們知道 x 是由一組 feature 來描述它的<br>
<br>
0:55:36.720,0:55:41.120<br>
那剛才在寶可夢的例子裡面，x 可以有 7 個數值<br>
<br>
0:55:41.120,0:55:42.600<br>
7 個參數<br>
<br>
0:55:42.600,0:55:47.040<br>
那我們假設，每一個 dimension<br>
<br>
0:55:47.040,0:55:53.800<br>
它從機率模型，產生出來的機率是 independent 的<br>
<br>
0:55:53.800,0:55:57.160<br>
所以，這個 x 產生的機率<br>
<br>
0:55:57.160,0:56:00.340<br>
可以拆解成，x1 產生的機率<br>
<br>
0:56:00.340,0:56:03.540<br>
乘上 x2 產生的機率，乘上 xk 產生的機率<br>
<br>
0:56:03.540,0:56:07.240<br>
一直到乘上 xK 產生的機率<br>
<br>
0:56:07.240,0:56:10.840<br>
如果我們假設，這些機率分布是 independent 的話<br>
<br>
0:56:10.840,0:56:13.260<br>
每一個 dimension 分布是 independent 的話<br>
<br>
0:56:13.260,0:56:16.060<br>
我們可以做這樣子的假設<br>
<br>
0:56:16.700,0:56:22.080<br>
那今天你可以說，每一個機率<br>
<br>
0:56:22.080,0:56:25.640<br>
就是 x1 產生的機率、x2 產生的機率， xk 產生的機率<br>
<br>
0:56:25.640,0:56:29.960<br>
他們分別都是一維的 Gaussian<br>
<br>
0:56:29.960,0:56:31.800<br>
一維的 Gaussian，大家知道意思嗎？<br>
<br>
0:56:31.800,0:56:33.960<br>
如果你這樣假設的話，等於是說<br>
<br>
0:56:35.060,0:56:39.440<br>
我們之前討論的都是 multi-variable 的 Gaussian 嘛<br>
<br>
0:56:39.440,0:56:41.240<br>
都是多維度的 Gaussian<br>
<br>
0:56:41.240,0:56:45.640<br>
如果你假設說，每一個 dimension 分開的 model<br>
<br>
0:56:45.640,0:56:48.120<br>
他們都是一維的 Gaussian 的話，意思就是說<br>
<br>
0:56:48.120,0:56:50.120<br>
原來那個高維度的 Gaussian，它的 covariance matrix<br>
<br>
0:56:50.120,0:56:54.000<br>
它的 covariance matrix，變成是 diagonal<br>
<br>
0:56:54.000,0:56:56.860<br>
在不是對角線的地方，值都是 0<br>
<br>
0:56:56.860,0:56:58.180<br>
只有對角線的地方，有值<br>
<br>
0:56:58.180,0:57:02.640<br>
這樣你就可以更減少你的參數量<br>
<br>
0:57:02.640,0:57:04.660<br>
你就可以得到一個更簡單的模型<br>
<br>
0:57:04.660,0:57:07.380<br>
那如果試一下這個，試一下這個結果是壞的<br>
<br>
0:57:07.380,0:57:09.540<br>
所以看來這個模型太簡單了<br>
<br>
0:57:09.540,0:57:14.660<br>
model 不同的 feature 間的 covariance<br>
<br>
0:57:14.660,0:57:15.780<br>
我看也是必要的<br>
<br>
0:57:15.780,0:57:19.680<br>
我覺得，比如說，像是戰鬥跟防禦力是有正相關的<br>
<br>
0:57:19.680,0:57:23.020<br>
他們這個 model 之間的 covariance，看來還是必要的<br>
<br>
0:57:23.020,0:57:25.440<br>
那你也不一定要用 Gaussian<br>
<br>
0:57:25.440,0:57:28.500<br>
有很多時候你憑直覺就知道應該用 Gaussian<br>
<br>
0:57:28.500,0:57:33.020<br>
比如說，今天假設你有某個 feature，它是 binary 的<br>
<br>
0:57:33.020,0:57:37.520<br>
有某個 feature，它代表的是：是或不是<br>
<br>
0:57:37.520,0:57:40.480<br>
或是它的 output 就是 0 跟 1 這樣<br>
<br>
0:57:40.480,0:57:45.340<br>
比如說，有一隻寶可夢，它是神獸還是不是神獸<br>
<br>
0:57:45.340,0:57:48.820<br>
之類的，這個就是 binary 的 feature<br>
<br>
0:57:48.820,0:57:50.340<br>
如果是 binary 的 feature 的話<br>
<br>
0:57:50.340,0:57:56.180<br>
你說它是用 Gaussian distribution 產生的<br>
<br>
0:57:56.180,0:57:57.700<br>
就太自欺欺人了<br>
<br>
0:57:57.740,0:58:00.620<br>
所以，它應該不太可能是用 Gaussian 所產生的<br>
<br>
0:58:00.620,0:58:02.440<br>
這個時候，你就會假設別的 distribution<br>
<br>
0:58:02.440,0:58:04.840<br>
比如說，假設你的 feature 是 binary 的<br>
<br>
0:58:04.840,0:58:07.340<br>
它 output，要馬是 0，要馬是 1<br>
<br>
0:58:07.340,0:58:11.440<br>
這個時候，你可能就會選擇說，<br>
它是一個 Bernoulli distribution<br>
<br>
0:58:11.440,0:58:13.960<br>
而不是一個 Gaussian distribution<br>
<br>
0:58:14.520,0:58:19.480<br>
如果我們今天假設所有的 feature<br>
<br>
0:58:19.480,0:58:22.900<br>
它都是 independent 產生的<br>
<br>
0:58:22.900,0:58:26.580<br>
我們不 model feature 和 feature 間 covariance 的關係<br>
<br>
0:58:26.580,0:58:30.260<br>
那我們用這種方法做分類的話<br>
<br>
0:58:30.260,0:58:34.400<br>
我們叫做用 Naive Bayes Classifier<br>
<br>
0:58:34.400,0:58:38.960<br>
它前面有一個 Naive，因為它真的很 naive<br>
<br>
0:58:38.960,0:58:40.580<br>
它真的很簡單，這樣<br>
<br>
0:58:40.580,0:58:44.860<br>
那你可能會常聽到，有人說 Naive Bayes Classifier 很強<br>
<br>
0:58:44.860,0:58:50.360<br>
其實它強不強是 depend on 你的假設是不是精準的<br>
<br>
0:58:50.360,0:58:55.660<br>
如果你今天假設不同的 dimension 之間是 independent<br>
<br>
0:58:55.660,0:58:57.800<br>
這件事情是很切合實際的<br>
<br>
0:58:57.800,0:59:02.620<br>
那 Naive Bayes Classifier 確實可以<br>
給你提供很好的 performance<br>
<br>
0:59:02.620,0:59:05.500<br>
那如果這個假設是很不成立的話<br>
<br>
0:59:05.500,0:59:09.180<br>
那 Naive Bayes Classifier 它的 bias 就太大了<br>
<br>
0:59:09.180,0:59:11.040<br>
它就不是一個好的 Classifier<br>
<br>
0:59:11.960,0:59:14.320<br>
接下來呢，我們要做的分析是<br>
<br>
0:59:14.320,0:59:17.520<br>
我們要分析這項 Posterior Probability<br>
<br>
0:59:17.520,0:59:22.660<br>
我們在做一些整理以後，我們會發現一些有趣的現象<br>
<br>
0:59:23.460,0:59:26.560<br>
這一項呢，大家應該都沒有甚麼問題<br>
<br>
0:59:26.560,0:59:29.920<br>
把他們上下同除分子<br>
<br>
0:59:29.920,0:59:33.060<br>
上下同除分子，我們把他們上下<br>
<br>
0:59:33.060,0:59:37.040<br>
都同除 P(x|C1)*P(C1)<br>
<br>
0:59:37.040,0:59:38.940<br>
所以，分子的地方就變成 1<br>
<br>
0:59:38.940,0:59:41.080<br>
分母的地方就變成 1 加<br>
<br>
0:59:41.080,0:59:46.920<br>
[P(x|C2) * P(C2)] / [P(x|C1) * P(C1)]<br>
<br>
0:59:47.620,0:59:50.740<br>
那我們假設，這一項阿<br>
<br>
0:59:50.740,0:59:56.680<br>
這一項取 natural log 以後阿<br>
<br>
0:59:56.680,0:59:57.900<br>
它等於 z<br>
<br>
0:59:57.900,0:59:59.900<br>
我們假設這一項取 natural log 以後，它等於 z<br>
<br>
1:00:00.840,1:00:04.600<br>
那我們就可以把這個 Posterior Probability<br>
<br>
1:00:04.600,1:00:08.900<br>
1 / (1 + exp(-z))<br>
<br>
1:00:08.900,1:00:13.280<br>
這個 z 是這一項<br>
<br>
1:00:13.280,1:00:15.320<br>
那你把這一項放進去<br>
<br>
1:00:15.320,1:00:18.780<br>
乘負號，就是上下顛倒，再取 exponential<br>
<br>
1:00:18.780,1:00:23.000<br>
把 exponential 跟 natural log 抵銷，你就得到這一項<br>
<br>
1:00:23.000,1:00:25.240<br>
然後你就得到 Posterior Probability<br>
<br>
1:00:25.240,1:00:27.440<br>
相信這個，大家應該沒有甚麼問題<br>
<br>
1:00:27.440,1:00:30.760<br>
這個 function，它的 input 是 z<br>
<br>
1:00:30.760,1:00:34.760<br>
這個 function，叫做 sigmoid function<br>
<br>
1:00:34.760,1:00:38.880<br>
如果你把它 output 對 z 的關係作圖的話<br>
<br>
1:00:38.880,1:00:41.420<br>
你就會發現是這個樣子<br>
<br>
1:00:41.420,1:00:44.400<br>
也就是 z 趨近無窮大的時候<br>
<br>
1:00:44.400,1:00:46.000<br>
它的 output 就趨近於 1<br>
<br>
1:00:46.000,1:00:48.100<br>
z 趨近負無窮大的時候<br>
<br>
1:00:48.100,1:00:50.580<br>
它的 output 就趨近於 0<br>
<br>
1:00:51.140,1:00:53.740<br>
接下來，我們要做的事情是<br>
<br>
1:00:53.740,1:00:57.240<br>
我們要把這個 z 算一下<br>
<br>
1:00:57.240,1:01:01.280<br>
它到底應該長甚麼樣子<br>
<br>
1:01:01.280,1:01:03.860<br>
我們來算一下，這個 z 應該長甚麼樣子<br>
<br>
1:01:03.860,1:01:06.520<br>
接下來這邊呢，是數學比較多<br>
<br>
1:01:06.520,1:01:08.460<br>
如果你覺得這很無聊的話<br>
<br>
1:01:08.460,1:01:11.380<br>
你就睡一下，聽一下結論就好<br>
<br>
1:01:12.180,1:01:14.960<br>
那這個 z 應該長甚麼樣子呢？<br>
<br>
1:01:14.960,1:01:17.460<br>
我們已經知道這個 Posterior probability<br>
<br>
1:01:17.460,1:01:19.340<br>
它是一個 z 的 sigmoid function<br>
<br>
1:01:19.360,1:01:21.960<br>
z 長甚麼樣子？<br>
<br>
1:01:21.960,1:01:27.160<br>
我們把相乘的部分，取 ln，所以就變成相加<br>
<br>
1:01:27.160,1:01:31.240<br>
那 P(C1) / P(C2) 是甚麼呢？<br>
<br>
1:01:31.240,1:01:36.380<br>
我們都知道說，這邊 N1 代表 Class 1<br>
<br>
1:01:36.380,1:01:39.020<br>
它在 training data 裡面出現的數目<br>
<br>
1:01:39.020,1:01:43.040<br>
N2 代表 Class 2，它在 training data 裡面出現的次數<br>
<br>
1:01:43.040,1:01:46.140<br>
所以，P(C1) 就是 N1 / (N1 + N2)<br>
<br>
1:01:46.140,1:01:48.000<br>
P(C2) 就是 N2 / (N1 + N2)<br>
<br>
1:01:48.000,1:01:51.400<br>
分母的地方消掉，所以得到 N1/N2<br>
<br>
1:01:51.400,1:01:53.700<br>
這個是小學生的數學<br>
<br>
1:01:53.700,1:01:57.240<br>
那 P(x|C1) 是甚麼呢？<br>
<br>
1:01:57.240,1:02:00.120<br>
我們說它是一個 probability distribution<br>
<br>
1:02:00.660,1:02:03.140<br>
這個 Gaussian 的 distribution<br>
<br>
1:02:03.140,1:02:07.860<br>
P(x|C1) 是另一個 Gaussian 的 distribution<br>
<br>
1:02:08.540,1:02:10.980<br>
如果我們把它相除<br>
<br>
1:02:10.980,1:02:15.000<br>
我們把這兩個 Gaussian probability 相除，再取 ln<br>
<br>
1:02:15.000,1:02:16.420<br>
會得到甚麼式子呢？<br>
<br>
1:02:16.420,1:02:21.620<br>
就得到這樣子，把這個放在上面，把這個放在下面<br>
<br>
1:02:21.620,1:02:26.000<br>
那這一項跟 distribution 是沒關係的<br>
<br>
1:02:26.000,1:02:28.800<br>
就把它消掉<br>
<br>
1:02:28.800,1:02:32.560<br>
然後，這一項把它提出來<br>
<br>
1:02:32.560,1:02:36.240<br>
相乘變相加，把這一項提出來<br>
<br>
1:02:36.700,1:02:38.760<br>
然後這個 exp 的部分呢<br>
<br>
1:02:38.760,1:02:41.840<br>
相除等於 exp 裡面的相減<br>
<br>
1:02:41.840,1:02:44.660<br>
這個，也沒什麼特別的<br>
<br>
1:02:44.660,1:02:48.520<br>
把他們分開相乘，變相加<br>
<br>
1:02:48.520,1:02:52.020<br>
相乘變相加，得到這樣<br>
<br>
1:02:53.380,1:02:56.500<br>
那接下來呢？接下來你就<br>
<br>
1:02:56.500,1:03:00.440<br>
做一些運算，把它展開<br>
<br>
1:03:00.440,1:03:04.040<br>
你可能想要知道說 (x - μ1)^T<br>
<br>
1:03:04.040,1:03:07.680<br>
乘上 (Σ1)^(-1)，再乘上 (x - μ1)<br>
<br>
1:03:07.680,1:03:09.280<br>
它應該長甚麼樣子<br>
<br>
1:03:09.280,1:03:12.160<br>
把它展開<br>
<br>
1:03:12.160,1:03:15.780<br>
所以，這個乘這個乘這個，就得到它<br>
<br>
1:03:15.780,1:03:19.520<br>
這個乘這個乘這個，就得到它<br>
<br>
1:03:19.520,1:03:23.000<br>
這個乘這個乘這個，就得到它，這樣<br>
<br>
1:03:23.000,1:03:26.360<br>
那中間這兩項呢，是可以合併的<br>
<br>
1:03:26.360,1:03:28.640<br>
是可以合併的，他們其實是一樣的<br>
<br>
1:03:28.640,1:03:31.140<br>
那你就得到這樣子式子<br>
<br>
1:03:31.140,1:03:34.680<br>
所以，這一項展開，就變成這樣<br>
<br>
1:03:34.680,1:03:37.260<br>
那這一項展開呢？<br>
<br>
1:03:38.220,1:03:40.600<br>
這一項展開呢？因為這個跟這個<br>
<br>
1:03:40.600,1:03:43.700<br>
只差了一個，把 1 換成 2 嘛<br>
<br>
1:03:43.700,1:03:47.540<br>
所以你就把，下面這個式子的 1 都換成 2<br>
<br>
1:03:47.540,1:03:49.720<br>
就行了<br>
<br>
1:03:50.100,1:03:54.920<br>
所以，z 這一項呢，它寫成這樣<br>
<br>
1:03:54.920,1:04:00.500<br>
前面是跟 Σ1 和 Σ2 有關<br>
<br>
1:04:00.500,1:04:05.420<br>
前面是 Σ1 和 Σ2 的 determinant 相除<br>
<br>
1:04:06.140,1:04:10.880<br>
後面呢，把 -1/2 乘進去<br>
<br>
1:04:10.880,1:04:15.340<br>
把 -1/2 乘這項，你得到這一項<br>
<br>
1:04:16.080,1:04:21.120<br>
把 -1/2 乘進去，把 -1/2 乘這項，你得到這一項<br>
<br>
1:04:21.120,1:04:25.360<br>
最後呢，再加上這個機率<br>
<br>
1:04:25.360,1:04:27.800<br>
你就知道 z 是多少了<br>
<br>
1:04:27.800,1:04:31.180<br>
那如果你剛才沒有聽得很懂，沒有關係<br>
<br>
1:04:31.180,1:04:32.960<br>
那其實沒有特別重要<br>
<br>
1:04:32.960,1:04:35.700<br>
反正就是經過一番運算以後，我們知道 z<br>
<br>
1:04:35.700,1:04:38.460<br>
哇！長得很複雜，長這個樣子<br>
<br>
1:04:39.040,1:04:40.800<br>
但是，我們剛才有說過呢<br>
<br>
1:04:40.800,1:04:44.360<br>
一般我們會假設，covariance matrix 是共用的<br>
<br>
1:04:44.360,1:04:49.320<br>
所以，Σ1 = Σ2 = Σ<br>
<br>
1:04:49.820,1:04:53.220<br>
在這個情況下，我們就可以簡化上面這個式子<br>
<br>
1:04:53.220,1:04:55.580<br>
我們就可以簡化成<br>
<br>
1:04:55.580,1:04:58.680<br>
Σ2 的 determinant 除以 Σ1的 determinant<br>
<br>
1:04:58.680,1:05:01.980<br>
如果 Σ1 = Σ2 的話，它就可以被消掉<br>
<br>
1:05:01.980,1:05:07.140<br>
這邊有一項， -1/2 * x^T * (Σ1)^-(1) * x<br>
<br>
1:05:07.140,1:05:11.520<br>
這邊有一項， 1/2 * x^T * (Σ2)^-(1) * x<br>
<br>
1:05:11.520,1:05:15.580<br>
如果 Σ1 跟 Σ2 是一樣的話呢，它們也可以被消掉<br>
<br>
1:05:15.580,1:05:18.580<br>
所以，我們最後得到的結果呢，就只剩下<br>
<br>
1:05:18.580,1:05:23.360<br>
1, 2, 3, 4, 5，5 項<br>
<br>
1:05:23.500,1:05:26.580<br>
然後，你會發現說<br>
<br>
1:05:26.580,1:05:31.280<br>
只有這一項跟這一項，是跟 x 有關的<br>
<br>
1:05:31.280,1:05:32.820<br>
是跟 x 有關的<br>
<br>
1:05:32.820,1:05:35.820<br>
這三項，是跟 x 無關的<br>
<br>
1:05:35.820,1:05:37.900<br>
這兩項，最後都有乘上 x<br>
<br>
1:05:37.900,1:05:42.620<br>
所以，先把這兩項集合起來<br>
<br>
1:05:40.520,1:05:45.440<br>
這兩項集合起來把它的 x 提出來<br>
<br>
1:05:45.440,1:05:51.280<br>
所以這兩項集合起來，就變成呢<br>
<br>
1:05:51.280,1:05:55.240<br>
因為  Σ1 = Σ2 ，都有乘上 x<br>
<br>
1:05:55.240,1:05:59.060<br>
所以 Σ 跟 x 可以提出來，變成  Σ^(-1) * x<br>
<br>
1:05:59.060,1:06:02.940<br>
這邊有 μ1 的 transpose，跟 μ2 的 transpose<br>
<br>
1:06:02.940,1:06:04.580<br>
這邊是相減，所以<br>
<br>
1:06:04.580,1:06:10.840<br>
這項跟這項合起來，<br>
就變成 (μ1 - μ2) 的 transpose * Σ^(-1) * x<br>
<br>
1:06:10.840,1:06:13.820<br>
剩下這三項，就把它原封不動地擺在後面<br>
<br>
1:06:14.460,1:06:17.000<br>
接下來呢，我們假設說<br>
<br>
1:06:17.920,1:06:24.220<br>
這個東西，(μ1 - μ2)^T *  Σ^(-1)<br>
<br>
1:06:24.220,1:06:27.120<br>
它合起來就是一個 vector<br>
<br>
1:06:27.120,1:06:30.780<br>
假設你把 μ1 算出來，把 μ2 算出來，把 Σ 算出來<br>
<br>
1:06:30.780,1:06:34.800<br>
那你再代到這個式子裡面，把 Σ 做 inverse<br>
<br>
1:06:34.800,1:06:39.140<br>
把 (μ1 - μ2) 做 transpose，那你就得到一個 vector<br>
<br>
1:06:39.140,1:06:43.080<br>
把那個 vector 叫做 W^T<br>
<br>
1:06:43.080,1:06:46.700<br>
後面這項，你可覺得看起來很可怕<br>
<br>
1:06:46.700,1:06:48.820<br>
但它其實很簡單<br>
<br>
1:06:48.820,1:06:51.400<br>
因為，我們從這邊開始看<br>
<br>
1:06:51.400,1:06:55.540<br>
這是一個 vector，這是一個 matrix<br>
<br>
1:06:55.540,1:06:59.060<br>
這是一個 vector，把他們三個乘起來以後<br>
<br>
1:06:59.060,1:07:01.100<br>
你得到的其實就是一個 scalar<br>
<br>
1:07:01.660,1:07:04.540<br>
那它其實不是甚麼複雜的東西，它是一個數字<br>
<br>
1:07:04.540,1:07:08.780<br>
你把這一項乘這一項乘這一項<br>
<br>
1:07:08.780,1:07:13.840<br>
你把 vector 的 transpose 乘上 matrix 的 inverse<br>
<br>
1:07:13.840,1:07:16.260<br>
再乘上一個 vector，它也是一個 scalar<br>
<br>
1:07:16.260,1:07:19.740<br>
那這個 ln (N1/N2)，它也是一個 scalar<br>
<br>
1:07:19.740,1:07:23.640<br>
所以你只是把這 3 個數字加起來而已，它就是個數字<br>
<br>
1:07:23.640,1:07:28.060<br>
所以我們就拿 b 來代表這個看起來很複雜的數字<br>
<br>
1:07:28.060,1:07:33.460<br>
假設你知道 μ1,  μ2 跟 Σ，那這一項其實就是個 vector<br>
<br>
1:07:33.460,1:07:37.400<br>
這一項其實就是個 scalar<br>
<br>
1:07:37.400,1:07:39.660<br>
它就是一個數字而已<br>
<br>
1:07:39.660,1:07:42.180<br>
所以呢，我們知道說<br>
<br>
1:07:42.180,1:07:44.700<br>
我們可以把 posterior probability<br>
<br>
1:07:44.700,1:07:46.800<br>
這項機率呢，寫成 σ(z)<br>
<br>
1:07:46.800,1:07:48.980<br>
z 呢，又可以寫成這樣子<br>
<br>
1:07:48.980,1:07:53.300<br>
所以，我們其實可以把這個 posterior probability<br>
<br>
1:07:53.300,1:07:55.460<br>
就簡單寫成 σ(w * x + b)<br>
<br>
1:07:55.460,1:08:04.100<br>
w 跟 x 的 inner product，再加上一個常數 b<br>
<br>
1:08:04.100,1:08:08.780<br>
我們可以把 z 寫成 w 跟 x 的 inner product<br>
<br>
1:08:08.780,1:08:10.800<br>
再加上一個常數 b<br>
<br>
1:08:10.800,1:08:13.760<br>
其實這個 posterior probability<br>
<br>
1:08:13.760,1:08:15.920<br>
它根本就沒有這麼複雜<br>
<br>
1:08:15.920,1:08:19.120<br>
它寫起來呢，就是這個樣子<br>
<br>
1:08:19.120,1:08:21.600<br>
所以，從這個式子，你就可以看出來說<br>
<br>
1:08:21.600,1:08:27.160<br>
為甚麼我們今天把 Σ1 跟 Σ2 共用的時候<br>
<br>
1:08:27.160,1:08:29.800<br>
假設 Σ1 必須等於 Σ2 的時候<br>
<br>
1:08:29.800,1:08:34.940<br>
你的 class 1 跟 class 2 的 boundary 會是 linear<br>
<br>
1:08:34.940,1:08:38.340<br>
你從這個式子呢，就可以很明顯地看出這件事<br>
<br>
1:08:39.220,1:08:42.640<br>
那再 generative model 裡面<br>
<br>
1:08:42.640,1:08:47.140<br>
我們做的事情是，我們用某些方法<br>
<br>
1:08:47.140,1:08:50.360<br>
去找出上面這個式子裡面的<br>
<br>
1:08:50.360,1:08:52.740<br>
N1, N2, μ1, μ2, Σ<br>
<br>
1:08:52.740,1:08:56.360<br>
找出這些以後，你就算出 w，你就算出 b<br>
<br>
1:08:56.360,1:08:59.860<br>
你把它代進這個式子，你就可以算機率<br>
<br>
1:09:01.100,1:09:03.860<br>
但是，如果你看到這個式子的話<br>
<br>
1:09:03.860,1:09:05.920<br>
你可能就可以有一個直覺的想法<br>
<br>
1:09:05.920,1:09:07.980<br>
為甚麼要這麼麻煩呢？<br>
<br>
1:09:07.980,1:09:11.820<br>
假設我們最終的目標，都是要找一個 vector w<br>
<br>
1:09:11.820,1:09:14.040<br>
都是要找一個 constant b<br>
<br>
1:09:14.040,1:09:17.640<br>
我們何必先去搞個機率<br>
<br>
1:09:17.640,1:09:21.480<br>
算出一些 μ, Σ 甚麼的<br>
<br>
1:09:21.480,1:09:25.380<br>
然後再把它搞起來，再得到 w 跟 b<br>
<br>
1:09:25.380,1:09:27.480<br>
這不是捨近求遠嗎？<br>
<br>
1:09:27.480,1:09:29.500<br>
做一件你根本就不需要做的事<br>
<br>
1:09:29.500,1:09:31.820<br>
最後你只需要 w 跟 b 嘛<br>
<br>
1:09:31.820,1:09:35.560<br>
所以，我們能不能夠直接把 w 跟 b 找出來<br>
<br>
1:09:35.560,1:09:37.640<br>
這個呢，就是我們下一份投影片要講的東西<br>
<br>
1:09:37.640,1:09:40.300<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
