0:00:00.000,0:00:06.020
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心

0:00:07.540,0:00:09.980
那我們上次呢，講到 LSTM

0:00:10.380,0:00:14.040
總之就是一個複雜的東西

0:00:15.080,0:00:20.140
再來的問題是，像 Recurrent Neural Network 這種架構

0:00:20.220,0:00:27.200
他要如何做 learning 呢？
我們之前有說過說，如果要做 learning 的話

0:00:27.200,0:00:29.960
你要定一個 cost function

0:00:29.960,0:00:35.480
來 evaluate 你的 model 的 parameter 是好還是不好

0:00:35.480,0:00:39.960
然後你選一個 model parameter 可以讓這個 lost 最小

0:00:41.020,0:00:45.680
那在 RNN 裡面，你會怎麼定這個 lost 呢？

0:00:46.620,0:00:50.520
以下我們就不寫算式，直接舉個例子

0:00:50.520,0:00:54.840
假設我們現在要做的事情是 Slot Filling

0:00:54.840,0:01:00.720
那你會有 training data ，
這個 training data 是說，給你一些 sentence

0:01:02.000,0:01:36.320
解決投影機問題中

0:01:38.740,0:01:43.720
這個是 sentence, ok~

0:01:43.760,0:01:51.120
你要給 sentence label，告訴 machine  說，
第一個 word 它屬於 other 這個 slot

0:01:51.120,0:01:54.980
然後台北屬於destination 這個 slot， on 屬於 other slot

0:01:55.760,0:02:02.860
November 2nd 屬於抵達時間的 slot

0:02:03.180,0:02:07.980
接下來你希望你的 cost 會怎麼定呢？

0:02:08.160,0:02:12.220
把 arrive 丟到 RNN

0:02:12.620,0:02:15.980
RNN 會得到一個 output 'y1'

0:02:16.480,0:02:25.860
接下來 'y1' 會和一個 Reference 的 vector 算它的 cross entropy

0:02:26.140,0:02:36.380
希望說如果我們現在丟進去的是 arrive 
那 'y1' 的reference 的 vector 應該是對應到 other 那個 slot

0:02:36.620,0:02:39.280
的 dimension value 是 1 其他是 0

0:02:39.500,0:02:44.000
這個 reference vector的長度，就是你 slot 的數目

0:02:44.140,0:02:49.880
比如說你定了 40 個 slot，
那這個 reference vector 的長度，dimension 就是40

0:02:50.140,0:02:55.480
假設現在 input 的這個 word，
它應該是對應到 other 這個 slot 的話

0:02:56.040,0:02:58.680
那對應到 other 那個 dimension

0:02:59.020,0:03:01.480
就是 1 其他就是 0

0:03:02.140,0:03:09.100
那現在你把 Taipei  丟進去的時候，因為 Taipei 屬於 destination 這個 slot

0:03:09.260,0:03:13.060
所以你就會希望說把 'x2' 丟進去的時候

0:03:13.120,0:03:21.360
'y2' 它要跟 reference 的 vector 距離越近越好

0:03:21.580,0:03:27.360
那 'y2' 的 reference vector 是對應到 destination 那個 slot 是 1 其他是 0

0:03:27.500,0:03:32.520
但這邊要注意的是，你在丟 'x2' 之前，一定要先丟 'x1'

0:03:33.160,0:03:36.980
你在把 Taipei 丟進去之前，一定要先把 arrive 丟進去

0:03:37.160,0:03:42.980
不然，你就不知道存在memory 裡面的值是多少

0:03:43.350,0:03:48.020
在做 training 的時候，
你也不能把你的 *****  裡面的

0:03:48.220,0:03:53.900
這些 word sequence 打散來看，
word sequence 仍然要當做一個整體來看

0:03:54.200,0:03:56.760
同樣的道理，把 on 丟進去

0:03:58.200,0:04:02.480
它的 reference vector 對應到 other 是 1

0:04:02.480,0:04:07.020
對應到 other 那個 dimension 是 1，其他是0

0:04:07.020,0:04:10.760
所以你的 cost ，就是每一個時間點的

0:04:11.720,0:04:16.680
RNN 的 output 跟 reference vector 的 cross entropy 和

0:04:16.820,0:04:20.320
就是你要去 minimize 的對象

0:04:21.800,0:04:29.160
那現在有了這個 lost function 之後，training 要怎麼做呢？

0:04:29.860,0:04:34.120
training 其實也是用 gradient decent

0:04:34.500,0:04:38.860
也就是說如果我們現在已經定出了 lost function 大 L

0:04:39.320,0:04:45.560
我要 update 這一個 network 裡面的某一個參數 w，我要怎麼做呢？

0:04:45.640,0:04:50.600
你就計算 w 對 L 的偏微分，把這個偏微分計算出來之後

0:04:50.920,0:04:55.520
就用 gradient decent 的方式去 update 每一個參數

0:04:55.620,0:04:59.900
那我們之前在講 Neural Network 的時候，
已經講過很多次

0:04:59.900,0:05:06.540
那在講之前這個 feed forward network 的時候，我們說gradient decent

0:05:06.600,0:05:11.240
用在 feed forward network 裡面，
你要用一個比較有效的演算法，叫做 Back propagation

0:05:11.420,0:05:16.420
在 RNN 裡面，gradient decent 的原理是一模一樣的

0:05:17.700,0:05:23.480
但是為了要計算方便，
也有開發一套演算法，這套演算法呢

0:05:23.980,0:05:30.040
是 Back propagation 的進階版，叫做BPTT

0:05:30.440,0:05:33.980
那它跟 Back propagation 其實是很類似的

0:05:34.040,0:05:42.540
只是因為 RNN 是在 time sequence 上運作，所以 BPTT 它需要考慮時間的information

0:05:42.900,0:05:50.640
那我們在這邊就不講 BPTT ，反正你只要知道 RNN 是用 gradient decent train 的

0:05:50.640,0:05:53.260
它是可以 train 的，這樣就行了

0:05:56.640,0:06:02.300
然而不幸的，RNN 的 training 是比較困難的

0:06:11.880,0:06:15.160
RNN 的 training 是比較困難的

0:06:15.940,0:06:23.840
一般而言，你在做 training 的時候，
你會期待你的 learning curve 是像藍色這條線

0:06:23.920,0:06:32.420
這邊的縱軸是 Total Loss，
橫軸呢，是 Epoch, training 時的 Epoch 的數目

0:06:33.040,0:06:34.740
你會希望說呢

0:06:34.940,0:06:41.440
隨著 Epoch 越來越多，隨著參數不斷的被 update，loss 應該就是慢慢，慢慢的下降

0:06:41.600,0:06:44.040
最後趨向收斂

0:06:44.180,0:06:49.960
但不幸的，當你在訓練 RNN 時，你有時候會看到

0:06:50.160,0:06:52.720
綠色這條線

0:06:52.960,0:06:58.380
這很重要，如果你是第一次 train RNN

0:06:58.500,0:07:02.660
你看到綠色這樣的 learning curve

0:07:02.940,0:07:10.980
這個 learning curve ，非常劇烈的抖動，
然後抖到某個地方，就突然 NaN，
然後你的程式就 segmentation fault

0:07:11.380,0:07:15.980
這個時候你會有什麼想法呢？
我相信你的第一個想法就是

0:07:16.340,0:07:21.320
程式有 bug 啊！

0:07:22.520,0:07:26.980
今年春天我邀那個國外學者來台灣

0:07:27.080,0:07:30.260
他們就是發明哪個 **** 的人

0:07:30.440,0:07:34.760
他跟我分享他當時開發 RNN 的心得

0:07:36.140,0:07:44.360
最早開始做 RNN 的 language model 的人
大概在 09 年就開始做了

0:07:44.700,0:07:49.700
很長一段時間，
只有他能把 RNN  language model train 起來

0:07:49.700,0:07:51.400
其他人都 train 不起來

0:07:55.680,0:08:01.520
你知道那個年代，不像現在有什麼 tensor flow 啊

0:08:01.680,0:08:06.880
那個年代做什麼，
都是要徒手刻的，所以他徒手刻了一個 RNN

0:08:07.260,0:08:10.020
然後 train 完以後就發生這樣一個現象

0:08:10.300,0:08:16.940
他第一個想法就是，程式有 bug ...
努力 debug 後，果然有很多 bugs

0:08:19.860,0:08:24.400
他後來就把 bugs 修掉

0:08:24.440,0:08:26.540
但這現象還是在

0:08:26.700,0:08:33.600
所以他就覺得很困惑
其他人就跟他說放棄啦，不 work

0:08:33.820,0:08:37.740
可是他想知道結果為何會這樣

0:08:37.740,0:08:42.720
所以他就做分析，等一下這圖是來自於他的 paper

0:08:42.960,0:08:47.360
他分析了 RNN 的性質，他發現說 RNN 的 error surface

0:08:47.360,0:08:54.500
所謂 error surface 就是 Total Loss 對參數的變化，
是非常的陡峭，非常崎嶇的

0:08:55.240,0:09:00.180
所謂崎嶇的意思是說，這個 error surface 它有些地方非常平坦

0:09:00.400,0:09:03.860
有一些地方，非常的陡峭

0:09:03.860,0:09:08.980
就像是有懸崖峭壁

0:09:08.980,0:09:13.640
投影片上是一個示意圖，縱軸是 Total Loss

0:09:17.000,0:09:21.460
x 軸跟 y 軸代表兩個參數 w1 and w2

0:09:21.920,0:09:25.300
圖上顯示的就是 w1 跟 w2 兩個參數

0:09:25.420,0:09:29.340
對 Total Loss，那發現說對很多地方
是非常平坦的

0:09:29.340,0:09:34.660
然後在某些地方，非常的陡峭

0:09:35.560,0:09:37.780
這個會造成什麼樣的問題呢？

0:09:37.960,0:09:42.180
假設你從橙色那個點，當作你初始的點

0:09:42.480,0:09:45.320
用 gradient decent 開始調整你的參數

0:09:45.520,0:09:50.560
橙色那個點，你算一下 gradient
然後update 你的參數

0:09:50.900,0:09:55.300
跳到下一個橙色的點，再算一下 gradient
再 update 你的參數

0:09:55.600,0:09:58.820
你可能正好就跳過一個懸崖

0:09:58.860,0:10:06.080
所以你的 Loss 會突然暴增，
你就會看到 Loss 上下非常劇烈震盪

0:10:06.980,0:10:11.680
有時候可能會遇到另外一個更慘的狀況，
就是你正好就踩在

0:10:11.680,0:10:16.100
你一腳踩在這懸崖上

0:10:17.740,0:10:22.640
那你踩在這懸崖上，會發生什麼事情呢？
你踩在懸崖上，因為在懸崖上 gradient 很大

0:10:23.800,0:10:28.260
然後呢，之前的 gradient 都很小，所以你措手不及

0:10:28.260,0:10:33.100
因為之前 gradient 很小，
所以你可能把 learning rate 調得比較大

0:10:33.220,0:10:39.460
但 gradient 突然很大，很大的 gradient 再乘上很大的 learning rate

0:10:39.460,0:10:43.720
結果參數就 update 很多
然後整個參數就飛出去了

0:10:44.360,0:10:48.640
所以你就 NaN
程式就 segmentation fault

0:10:49.340,0:10:53.180
他們就想說怎麼辦呢？

0:10:53.180,0:10:58.800
他說他不是一個數學家，
所以他要用工程師的想法來解決這問題

0:10:59.100,0:11:05.940
他就想了一招，這一招應該蠻關鍵的

0:11:05.940,0:11:12.960
讓很長一段時間，
只有他的 code 可以把 RNN train 出來

0:11:13.100,0:11:17.640
很長一段時間，人們是不知道這一招的
因為這一招他實在覺得太沒什麼

0:11:18.000,0:11:22.860
所以沒寫在 paper 裡面
直到他在寫博士論文時

0:11:22.860,0:11:27.640
博士論文是比較長的
所以有些東西 trivial 很可能還是會寫進去

0:11:28.800,0:11:35.020
直到他在寫博士論文的時候，大家才發現這個秘密

0:11:35.580,0:11:36.900
這個秘密是什麼呢？

0:11:36.900,0:11:39.440
這一招說穿了就不值錢

0:11:39.440,0:11:41.440
這一招叫做 clipping

0:11:41.440,0:11:46.160
clipping 是說，當 gradient 大於某一個threshold 時後

0:11:46.400,0:11:52.140
就不要讓他超過那個 threshold

0:11:52.140,0:11:57.940
當 gradient 大於 15 的時候就等於15
結束...

0:11:58.660,0:12:04.520
因為 gradient 現在不會太大，假如你做 clipping 的時候

0:12:04.600,0:12:08.780
就算是踩在這個懸崖上，也沒有關係，
你的參數就不會飛出去

0:12:08.860,0:12:14.840
他會飛到一個比較近的地方，
這樣你仍然可以繼續做 RNN 的 training

0:12:18.700,0:12:23.480
那接下來的問題就是

0:12:23.480,0:12:26.840
為什麼 RNN 會有這種奇特的特性呢？

0:12:32.880,0:12:37.420
為什麼 RNN 會有這種奇特的特性呢？

0:12:37.460,0:12:42.560
有人可能會說是不是因為來自於 Sigmoid function

0:12:42.780,0:12:47.300
我們之前有講過，在講 ReLU activation function 的時候呢

0:12:47.300,0:12:50.620
我們有講過一個問題叫做 gradient vanishing 的問題

0:12:50.900,0:12:55.000
那我們說這個問題是從 Sigmoid function 來的

0:12:55.660,0:12:59.320
因為 Sigmoid 的關係，所以有 gradient vanishing 這個問題

0:12:59.320,0:13:08.100
RNN 會有這種很小的，很平滑的 error surface

0:13:08.460,0:13:13.100
是因為來自於 gradient vanish，gradient vanish 是因為來自於 Sigmoid function

0:13:13.100,0:13:15.720
這件事情，我覺得不是真的

0:13:15.780,0:13:20.820
想想看如果這個問題是來自於 Sigmoid function，換成 ReLU 就解決這個問題啦

0:13:20.820,0:13:22.160
所以不是這個問題

0:13:22.260,0:13:26.820
跟大家講一個秘密，如果你用 ReLU 你會發現說呢

0:13:27.180,0:13:31.660
一般在 train Neural Network 的時候呢，
很少用 ReLU 當作 activation function

0:13:31.660,0:13:39.860
為什麼呢？因為如果你把 Sigmoid 換成 ReLU，其實在 RNN 上面 performance 通常是比較差的

0:13:39.980,0:13:47.300
所以 activation function 並不是這個地方的關鍵點

0:13:50.560,0:13:54.460
如果說我們今天有講 
Back propagation through time 的話

0:13:54.460,0:13:58.820
從式子裡面，你會比較容易看出為何會有這個問題

0:13:58.820,0:14:00.800
今天沒有講 Back propagation through time

0:14:00.800,0:14:01.780
沒有關係

0:14:01.780,0:14:07.040
我們有一個更直觀的方法，
可以來知道一個 gradient 的大小

0:14:07.640,0:14:14.580
這個更直觀的方法，你把某一個參數做小小的變化，
看他對 network output 的變化有多大

0:14:14.740,0:14:18.780
就可以測出這個參數的 gradient 大小

0:14:18.780,0:14:23.260
我們這邊呢舉一個很簡單的 RNN 當作我們的例子

0:14:24.260,0:14:29.440
今天有一個全世界最簡單的 RNN，他只有一個 Neuron
這個 Neuron 是 linear

0:14:31.480,0:14:35.100
他只有一個 input 沒有 bias
input 的 weight 是 1

0:14:35.820,0:14:38.780
output weight 也是 1

0:14:38.820,0:14:43.600
transition 部分的 weight 是 w

0:14:43.600,0:14:50.740
也就是從 memory 接到 Neuron 的 input weight 是 w

0:14:51.540,0:14:57.040
現在假設我給這個network 的輸入是 [1 0 0 0 0 0]

0:14:57.960,0:15:01.180
只有第一個時間點輸入 1  ，接下來都輸入 0

0:15:01.180,0:15:03.460
那這個 network 的 output 會長什麼樣子呢？

0:15:04.100,0:15:10.440
比如說這個 network 在最後一個時間點，
第 1000 個時間點的 output 值會是多少

0:15:10.440,0:15:12.640
我相信大家都可以馬上回答我

0:15:12.640,0:15:17.380
他的值是 w 的 999 次方，對吧？

0:15:17.380,0:15:23.300
你把 1 輸入進去，再乘上 w，再乘上 w...
乘了 999  次 w

0:15:23.680,0:15:28.260
輸出就是 w 的 999 次方
後面的輸入都是 0 當然不影響

0:15:28.260,0:15:33.500
只有一開始的 1 有影響
但他會通過 999 次的 w

0:15:34.340,0:15:38.440
那我們現在來看，假設 w，是我們要認的參數

0:15:38.760,0:15:41.060
我們想要知道他的 gradient

0:15:41.060,0:15:44.820
所以我們想要知道，當我們改變 w 的值的時候

0:15:44.820,0:15:47.360
對 network 的 output 有多大的影響

0:15:47.580,0:15:51.080
現在我們假設 w = 1

0:15:51.700,0:15:56.280
y 1000，
network 在最後時間點的 output，也是 1

0:15:57.060,0:16:02.980
假設 w = 1.01，那 y 1000 是多少呢？

0:16:03.220,0:16:08.260
y 1000 是 1.01 的 999 次方，
1.01 的 999 次方是多少呢？

0:16:11.560,0:16:14.240
是 20000，是一個很大的值

0:16:14.240,0:16:19.920
這就跟蝴蝶效應一樣，這個 w 有一點小小的變化

0:16:20.500,0:16:24.400
對他的 output 影響是非常大的

0:16:24.940,0:16:30.940
所以 w 有很大的 gradient

0:16:31.340,0:16:33.280
想說這很大的 gradient 也沒有什麼

0:16:33.380,0:16:37.120
我們只要把他的 learning rate 設小一點就好

0:16:37.700,0:16:42.620
但是事實上，如果把 w 設成 0.99

0:16:42.840,0:16:45.720
那 y 1000 就等於 0

0:16:45.840,0:16:51.060
如果把 w 設 0.01，那 y 1000 還是等於 0

0:16:51.540,0:17:01.080
也就是說在 1 這個地方有很大的 gradient 
但在 0.99 的地方 gradient 就突然變得非常非常的小

0:17:01.740,0:17:04.840
這個時候你又需要一個很大的 learning rate

0:17:04.840,0:17:09.300
就會造成你設 learning rate 很麻煩，
你的 error surface 很崎嶇

0:17:09.300,0:17:13.120
因為這 gradient 是時大時小的

0:17:13.120,0:17:18.160
而且在非常短的區域內，gradient 就會有很大的變化

0:17:21.540,0:17:25.560
所以從這個例子，你可以看出來說，為什麼 RNN

0:17:26.080,0:17:29.540
會有問題，RNN training 的問題

0:17:29.900,0:17:33.540
其實是來自於，他把同樣的東西

0:17:33.680,0:17:38.200
在 transition 的時候呢，
在時間和時間轉換的時候，反覆使用

0:17:38.700,0:17:41.920
從 memory 接到 Neuron 的那一組 weight

0:17:41.920,0:17:45.200
在不同的時間點，都是反覆被使用

0:17:45.200,0:17:49.120
所以這個 w 只要一有變化

0:17:49.720,0:17:53.440
他有可能完全沒有造成任何影響，
像這邊的例子

0:17:53.440,0:17:57.940
一但他可以造成影響，那個影響都會是天崩地裂的影響

0:17:58.200,0:18:01.420
所以他有時候 gradient 很大，有時很小

0:18:07.060,0:18:11.160
RNN 會不好訓練的原因，
並不是來自於 activation function

0:18:11.380,0:18:17.520
而是來自於他有 time sequence，
同樣的 weight，在不同的時間點

0:18:17.520,0:18:20.920
被反覆的，不斷的被使用

0:18:21.520,0:18:27.120
有什麼樣的技巧可以幫助我們解決這問題呢？

0:18:31.060,0:18:36.400
其實現在廣泛被使用的技巧呢，就是 LSTM

0:18:37.320,0:18:42.000
LSTM 可以讓你的 error surface 不要那麼崎嶇

0:18:44.160,0:18:49.040
他會把那些比較平坦的地方拿掉，
他可以解決 gradient vanishing 的問題

0:18:49.240,0:18:51.300
但他不會解決 gradient explode 的問題

0:18:51.300,0:18:55.300
可能你有些地方，仍然是會非常崎嶇的

0:18:55.300,0:19:00.580
你有些地方，仍然變化會是非常劇烈的

0:19:00.580,0:19:02.900
但是不會有特別平坦的地方

0:19:02.980,0:19:04.980
因為如果你在做 LSTM 的時候

0:19:04.980,0:19:07.620
大部分的地方都變化很劇烈

0:19:07.620,0:19:13.320
所以當你在做 LSTM 的時候

0:19:13.380,0:19:17.560
你可以放心的把你的 learning rate 設的小一點

0:19:17.920,0:19:23.480
而他要在 learning rate 特別小的情況下進行訓練

0:19:24.100,0:19:32.360
那為什麼 LSTM 可以做到 
handle gradient vanish 的問題呢？

0:19:32.400,0:19:37.940
為什麼他可以避免讓 gradient 特別小呢？

0:19:38.260,0:19:48.060
我聽說有人在面試某家國際大廠的時候，
就被問這個問題

0:19:48.380,0:19:53.840
但這問題怎麼樣答比較好呢？

0:19:53.860,0:19:58.000
他那個問題是這樣，為什麼我們把 RNN 換成 LSTM?

0:19:58.300,0:20:02.760
如果你的答案是因為 LSTM 比較潮，
因為 LSTM 比較複雜

0:20:02.760,0:20:04.300
這個都太弱了

0:20:04.300,0:20:10.220
真正的理由是 LSTM 可以 handle gradient vanishing 的問題

0:20:11.020,0:20:17.080
但接下來人家就會問說，為什麼 LSTM 可以 handle gradient vanishing 的問題呢？

0:20:17.220,0:20:19.386
我在這邊來試著回答看看

0:20:19.386,0:20:26.700
之後假如有人口試再被問到，
你可以想想你有沒有辦法回答

0:20:26.920,0:20:33.160
如果你想看看 RNN 跟 LSTM，
它們在面對 memory 的時候

0:20:33.260,0:20:37.740
它們處理的 operation
其實是不一樣的

0:20:38.440,0:20:43.180
你想想看，在 RNN 裡面，在每一個時間點

0:20:43.180,0:20:47.100
其實 memory 裡面的資訊，都會被洗掉

0:20:47.560,0:20:52.060
你們看每一個時間點，
Neuron 的 output 都會被放到 memory 裡面去

0:20:52.100,0:20:56.400
所以在每一個時間點，
memory 裡面的資訊都會被覆蓋掉

0:20:56.400,0:20:58.800
都會被完全洗掉

0:21:00.180,0:21:02.600
但在 LSTM 裡面不一樣

0:21:02.600,0:21:06.860
他是把原來 memory 裡面的值，乘上一個值

0:21:06.860,0:21:09.740
再把 input 的值，加起來

0:21:09.900,0:21:13.880
放到 cell 裡面去

0:21:14.320,0:21:19.560
所以他的 memory 和 input 是相加的

0:21:21.540,0:21:25.860
今天他和 RNN 不同的地方是

0:21:25.860,0:21:31.680
如果你的 weight 可以影響到 memory 的值的話

0:21:32.100,0:21:36.800
一但發生影響，這個影響會永遠都存在

0:21:36.880,0:21:40.740
不像 RNN 在每一個時間點，值都會被 format 掉

0:21:41.620,0:21:45.180
只要影響一被 format 掉，他就消失了

0:21:45.300,0:21:49.740
但在 LSTM 裡面，一但能對 memory 造成影響

0:21:51.420,0:21:55.360
那個影響會永遠留著，除非 forget gate 被開

0:21:55.360,0:22:00.720
除非 forget gate 被使用，
除非 forget get 決定要把 memory 值洗掉

0:22:00.720,0:22:04.520
不然一但 memory 有改變的時候，
每次都只會有新的東西加進來

0:22:04.560,0:22:08.320
而不會把原來存在 memory 裡面的值洗掉

0:22:08.860,0:22:12.760
所以他不會有 gradient vanishing 的問題

0:22:14.100,0:22:16.140
那你可能會想說，現在有 forget gate 啊

0:22:16.140,0:22:19.700
forget gate 就是會把過去存的值洗掉啊

0:22:19.700,0:22:24.040
事實上 LSTM 在 97 年就被 proposed 了

0:22:24.580,0:22:30.340
LSTM 第一個版本就是
為了解決 gradient vanishing 的問題

0:22:30.820,0:22:35.600
所以他是沒有 forget gate 的，
forget gate 是後來才加上去的

0:22:35.940,0:22:39.600
那甚至現在有一個傳言是

0:22:39.600,0:22:45.060
你在訓練 LSTM 時，不要給 forget gate 特別大的 bias

0:22:45.240,0:22:49.540
你要確保 forget gate 在多數的情況下是開啟的

0:22:49.540,0:22:52.780
只有少數情況會被 format 掉

0:22:53.780,0:23:04.960
現在有另一個版本，用 gate 操控 memory 的 cell

0:23:05.100,0:23:07.540
叫做 Gated Recurrent Unit

0:23:07.560,0:23:10.660
LSTM 有 3 個 gate

0:23:10.660,0:23:14.680
這個 GRU，他只有兩個 gate

0:23:16.800,0:23:24.420
所以 GRU 相較於 LSTM，他的 gate 只有 2 個

0:23:25.120,0:23:29.960
所以他需要的參數量是比較少的

0:23:29.960,0:23:34.640
因為他需要的參數量是比較少的

0:23:34.640,0:23:40.020
所以他在 training 是比較 robust 的

0:23:40.700,0:23:45.580
所以你今天在 train LSTM 的時候，
你覺得 over fitting 情況很嚴重

0:23:45.660,0:23:49.240
你可以試一下用 GRU

0:23:49.720,0:23:53.100
GRU 的精神就是，他怎麼拿掉一個 gate

0:23:53.100,0:23:55.620
我們今天就不講 GRU 的詳細原理

0:23:55.660,0:23:59.060
他的精神就是舊的不去，新的不來

0:23:59.200,0:24:05.980
他會把 input gate 跟 forget gate 連動起來

0:24:06.320,0:24:14.320
當 input gate 被打開的時候，
forget gate 就會被自動的關閉

0:24:14.320,0:24:17.916
當 input gate 被打開的時候，
forget gate 就會被洗掉

0:24:17.920,0:24:21.280
就會 format 掉，存在 memory 裡面的值

0:24:21.600,0:24:26.720
當 forget gate 沒有要 format 值，
input gate 就會被關起來

0:24:26.840,0:24:31.600
也就是你要把存在 memory 裡面的值清掉，
才可以把新的值放進來

0:24:35.480,0:24:40.540
其實還有很多其他 techniques，
是來 handle gradient vanishing 這個問題

0:24:41.240,0:24:48.640
比如說是 Clockwise RNN 或是 SCRN，等等

0:24:48.640,0:24:51.800
我們把 reference 留在這邊，讓大家參考

0:24:51.940,0:24:58.660
最後，有一個蠻有趣的 paper

0:24:58.780,0:25:06.240
是 Hinton proposed，

0:25:06.660,0:25:09.720
他用一般的 RNN，不是用 LSTM

0:25:10.080,0:25:17.440
一般 RNN，他用 identity matrix ，
來initialize transition 的weight

0:25:17.660,0:25:20.600
然後在使用 ReLU 的 activation function 的時候

0:25:20.760,0:25:23.560
他可以得到很好的 performance

0:25:23.820,0:25:27.440
有人說那 ReLU 的 performance 不是比較差嗎？

0:25:27.440,0:25:32.540
如果你是一般 training 的方法，
你 initialization 的 weight 是 random 的話

0:25:33.000,0:25:42.200
那 ReLU 跟 Sigmoid function 來比的話，
Sigmoid 的 performance 會比較好

0:25:42.200,0:25:46.820
但是如果你今天用了 identity matrix 的話

0:25:46.820,0:25:50.880
如果你今天用了 identity matrix 來當作 initialization 的話

0:25:50.880,0:25:53.880
這時候用 ReLU 的 performance 就會比較好

0:25:54.040,0:25:59.560
這件事情真的非常的神奇

0:25:59.880,0:26:03.740
當你用了這一招以後，用一般的 RNN

0:26:03.860,0:26:08.540
不用 LSTM，他的 performance 就可以屌打 LSTM

0:26:08.980,0:26:13.980
你就覺得 LSTM 用的這麼複雜，都是白忙一場

0:26:13.980,0:26:17.260
這個是非常神奇的一篇文章

0:26:19.220,0:26:21.840
那其實 RNN 有很多的 applications

0:26:21.840,0:26:24.820
在我們前面舉的 slot filling 例子裡面

0:26:24.820,0:26:29.320
我們是假設 input 跟 output 的 element 數目是一樣多的

0:26:30.040,0:26:37.080
也就是說 input 有幾個 word 我們就給每一個 word，
一個 slot 的 label

0:26:37.840,0:26:42.040
但事實上 RNN 他可以做到呢

0:26:42.220,0:26:44.320
更複雜的事情

0:26:44.320,0:26:45.720
可以做到更複雜的事情

0:26:45.960,0:26:51.320
比如說，他可以 input 是 一個 sequence
 output 只是一個 vector

0:26:51.620,0:26:56.180
這有什麼應用呢，比如說你可以做 Sentiment Analysis

0:26:56.180,0:27:02.060
Sentiment Analysis 現在有很多的 applications
比喻來說

0:27:07.940,0:27:13.000
某家公司想要知道說
他們的產品在網路上評價呢

0:27:13.260,0:27:15.440
是 positive 還是 negative

0:27:15.440,0:27:17.840
他們可能就會寫一個爬蟲

0:27:17.840,0:27:24.100
把跟他們網路評價有關，
或跟它們產品有關係的網路文章，都爬下來

0:27:24.180,0:27:29.600
但是一篇篇看太累了，
所以你可以用一個 machine learning 的方法

0:27:29.980,0:27:36.300
自動 learn 一個 classify，
去分類說那些 documents 是正向，那些是負向

0:27:39.800,0:27:41.700
或者是在電影版上呢

0:27:41.700,0:27:45.540
Sentiment Analysis 做的事情，
就是給 machine 看很多文章

0:27:45.680,0:27:52.240
然後 machine 要自動知道說，
那些文章是正雷，那些是負雷

0:27:53.000,0:27:54.740
怎麼讓 machine 做到這件事情呢？

0:27:54.740,0:27:57.700
你就是認一個 RNN

0:27:57.700,0:28:02.220
這個 input 呢是一個 character sequence

0:28:02.220,0:28:04.740
這個 input 呢是一個 character sequence

0:28:05.020,0:28:08.800
然後 RNN 呢，
把這個 character sequence 讀過一遍

0:28:13.140,0:28:17.340
然後在最後一個時間點，把 hidden layer 拿出來

0:28:20.900,0:28:24.460
把 hidden layer 拿出來，可能再通過幾個 transform

0:28:24.460,0:28:29.060
然後呢，
你就可以得到最後的 sentiment analysis 的 prediction

0:28:29.280,0:28:30.740
比如說 input 這個 document

0:28:30.740,0:28:34.440
他是 超好／好／普／負／超負 雷

0:28:34.440,0:28:36.160
他是一個分類的問題

0:28:36.180,0:28:41.000
但 input 是一個 sequence，
所以你需要用 RNN 來處理這個 input

0:28:42.200,0:28:47.120
或是我們實驗室做過，用 RNN 來做 key term extraction

0:28:47.120,0:28:49.660
所謂 key term extraction 的意思是說

0:28:51.680,0:28:59.680
給 machine 看一篇文章，
然後 machine 要 predict 這篇文章有那些關鍵詞彙

0:28:59.840,0:29:05.020
跟我們在 final project 裡面的第三個 task 
做的其實是非常類似的事

0:29:10.560,0:29:13.580
如果你今天能收集到一堆 training data

0:29:13.580,0:29:15.360
你能夠收集到一堆 document

0:29:15.360,0:29:17.040
然後這些 document 都有 label 說

0:29:17.040,0:29:19.280
哪些詞彙是對應它對應的 key word 的話

0:29:19.380,0:29:21.920
那你就可以直接 train 一個 RNN

0:29:21.920,0:29:26.400
這個 RNN 呢，把 document word sequences 當作 input

0:29:28.440,0:29:32.260
然後通過 embedding layer

0:29:32.720,0:29:36.800
然後用 RNN 把這個 document 讀過一次

0:29:39.400,0:29:43.000
然後呢，把出現在最後一個

0:29:44.060,0:29:48.520
把這出現在最後一個時間點的 output 拿過來做 attention

0:29:48.800,0:29:53.620
我發現我們沒有講過 attention 是什麼，
這部分你就聽聽就好

0:29:53.620,0:29:57.320
用 attention 以後呢，
你可以把重要的 information 抽出來

0:29:57.400,0:30:00.440
再丟到 feed forward network 裡面去

0:30:00.580,0:30:03.080
得到最後的 output

0:30:05.280,0:30:08.360
那它也可以是多對多的

0:30:08.880,0:30:13.380
比如說你的 input/output 都是 sequences

0:30:13.480,0:30:17.540
但 output sequence 比 input sequence 短的時候

0:30:17.860,0:30:20.220
RNN 可以處理這個問題

0:30:20.660,0:30:25.500
什麼樣的任務是 input sequence 長 
output sequence 短呢？

0:30:26.180,0:30:29.320
比如說語音辨識就是這樣一個任務

0:30:29.840,0:30:35.120
在語音辨識這個任務，
input 是一串 acoustic feature sequence

0:30:36.220,0:30:38.380
語音是一段聲訊號

0:30:38.380,0:30:42.000
要做語音辨識的時候，你就說一句話

0:30:42.320,0:30:44.480
我們一般處理聲音訊號的方式

0:30:44.480,0:30:51.360
就是在聲音訊號裡面，每隔一小段時間，
就把它用一個 vector 來表示

0:30:51.540,0:30:57.380
那一個一小段時間，通常很短，比如說是 0.01 秒之類的

0:30:59.160,0:31:03.980
那它的 output 是 character 的 sequence

0:31:05.440,0:31:11.020
那如果你是用原來的 RNN，
用我們在做 Slot Filling 那個 RNN

0:31:11.020,0:31:14.040
你把這一串 input 丟進去

0:31:14.040,0:31:17.400
它充其量，只能做到說，告訴你

0:31:17.400,0:31:21.920
每一個 vector，它對應到哪一個 character

0:31:21.920,0:31:24.720
假設說中文的語音辨識

0:31:24.720,0:31:32.560
那你 output 的 target，
理論上就是這世界上所有可能的中文詞彙

0:31:32.660,0:31:38.200
所有中文的可能的 characters，常用的可能就有 8000 個

0:31:39.820,0:31:43.620
RNN output 的 class 數目，會有 8000

0:31:43.620,0:31:45.900
雖然很大，是有辦法做的

0:31:46.380,0:31:49.520
但充其量，你只能做到說

0:31:49.520,0:31:52.880
每一個 vector 屬於一個 character

0:31:52.880,0:31:54.120
但是

0:31:55.040,0:31:57.560
input 每一個 vector 對應到的時間是很短的

0:31:57.560,0:31:59.460
通常才 0.01 秒

0:31:59.580,0:32:03.960
所以通常是好多個 vector 才對應到同一個 character

0:32:03.960,0:32:07.860
所以你辨識的結果，就變成，好好好棒棒棒棒棒

0:32:08.480,0:32:11.960
可是這不是語音辨識的結果啊，怎麼辦？

0:32:12.240,0:32:14.000
有一招叫 trimming

0:32:14.000,0:32:18.040
trimming 就是把重複的東西拿掉，就變好棒

0:32:18.220,0:32:22.340
但這樣會有一個很嚴重問題，它就沒有辦法辨識 好棒棒

0:32:22.620,0:32:26.660
不知道的說一下，好棒跟好棒棒正好是相反的

0:32:53.260,0:32:57.420
所以不把好棒跟好棒棒分開來是不行的

0:32:57.540,0:33:00.500
所以需要把好棒跟好棒棒分開來

0:33:00.700,0:33:05.980
怎麼辦，我們要用一招，叫做 CTC

0:33:07.840,0:33:10.460
這一招也是那種說穿了不值錢的方法

0:33:10.460,0:33:11.800
但這一招很神妙

0:33:11.800,0:33:18.660
它說，我們在 output 的時候，
不只是 output 所有中文的 character

0:33:18.960,0:33:23.520
我們還多 output 一個符號，叫做 Null

0:33:23.840,0:33:26.880
叫做沒有任何東西

0:33:26.880,0:33:30.020
所以今天如果我 input 一串 acoustic feature sequence

0:33:30.020,0:33:35.160
它的 output 是 好 null null 棒 null null null null

0:33:35.160,0:33:39.840
然後我就把 null 的部分拿掉，它就變好棒

0:33:40.340,0:33:46.520
如果我們輸入另外一個 sequence 
它的 output 是 好 null null 棒 null 棒 null null

0:33:46.520,0:33:49.720
它的 output 就是好 棒 棒

0:33:49.960,0:33:54.180
所以就可以解決疊字的問題了

0:33:54.180,0:34:00.140
那 CTC 怎麼做訓練呢？

0:34:00.800,0:34:07.420
CTC 在做訓練的時候，你手上的 training data

0:34:07.420,0:34:10.780
就會告訴你說，這一串 acoustic feature

0:34:10.960,0:34:13.680
對應到這一串 character sequence

0:34:13.680,0:34:15.760
這個 sequence 對應到這個 sequence

0:34:16.260,0:34:20.345
但他不會告訴你說，
好 是對應第幾個 frame 到第幾個 frame

0:34:20.345,0:34:23.380
棒 
是對應第幾個 frame 到第幾個 frame

0:34:23.520,0:34:24.660
那怎麼辦呢？

0:34:24.660,0:34:28.080
窮舉所有可能的 alignment

0:34:28.360,0:34:34.020
簡單來說，我們不知道 好 對應到哪幾個 frame
棒 對應到哪幾個 frame

0:34:34.120,0:34:35.940
我們就假設所有的狀況都是可能的

0:34:35.940,0:34:37.920
可能第一個是 好，後面接 null，棒 後面接 3 個 null

0:34:37.920,0:34:41.860
可能 好，後面接 2 個 null，棒 後面接 2 個 null

0:34:41.860,0:34:45.420
可能 好，後面接 3 個 null，棒 後面接 1 個 null

0:34:45.600,0:34:48.520
我們不知道哪個是對的，就假設全部都是對的

0:34:48.520,0:34:51.280
在 training 的時候，全部都當作正確的一起去 train

0:34:51.820,0:34:56.520
可能會想說，窮舉所有的可能，那可能性感覺太多了

0:34:56.520,0:35:00.040
這個有巧妙的演算法，可以解決這個問題

0:35:00.080,0:35:03.500
那我們今天就不細講這個部分

0:35:04.780,0:35:08.760
以下是在文獻上 CTC 得到的一個結果

0:35:08.760,0:35:10.020
這是英文的

0:35:10.080,0:35:14.120
在做英文辨識的時候，你的 RNN 的 output target

0:35:14.300,0:35:16.300
就是 character

0:35:16.300,0:35:18.180
就英文的字母

0:35:18.180,0:35:20.180
加上空白，空白就是說

0:35:20.220,0:35:23.360
你也不需要給你的 RNN 10 點啊，什麼之類的

0:35:24.180,0:35:26.180
它就直接 output 字母

0:35:26.440,0:35:31.440
如果當那的字與字之間有 boundary，
它就自動用空白區隔

0:35:31.920,0:35:35.380
以下是一個例子，第一個 frame 就 output H

0:35:35.500,0:35:38.300
第二個frame output null，第三個 frame output null

0:35:38.340,0:35:41.800
第四個frame output I，第五個 frame output S

0:35:41.980,0:35:45.580
接下來 output 底線，代表空白

0:35:46.420,0:35:49.920
然後一串 null 然後 F null null R I

0:35:49.920,0:35:54.940
null null... E N D null..  ' S _

0:35:54.980,0:35:58.040
如果你看到 output 是這樣子的話

0:35:58.040,0:36:03.060
你把 null 拿掉，這句話辨識結果就是 HIS FRIEND'S

0:36:03.180,0:36:08.640
你不需要告訴 machine 說 HIS 是一個詞彙，
FRIEND'S 是一個詞彙

0:36:09.020,0:36:12.940
machine 透過 training data，它自己會學到這件事情

0:36:13.680,0:36:21.240
那傳說呢，google 的語音辨識系統，
已經全面換成 CTC 了

0:36:22.200,0:36:24.880
如果你用 CTC 來做語音辨識的話

0:36:24.880,0:36:29.100
就算是有某一個詞彙，比如說英文的人名，地名

0:36:29.100,0:36:31.860
從來在 training data 沒有出現過

0:36:31.860,0:36:33.860
machine 從來不知道這詞彙

0:36:33.960,0:36:37.420
它其實有也機會把它正確的辨識出來

0:36:40.160,0:36:42.540
另外一個神奇的 RNN 應用呢

0:36:42.640,0:36:45.460
叫做 sequence to sequence learning

0:36:45.760,0:36:51.380
在 sequence to sequence learning 裡面，
RNN 的 input and output 都是 sequence

0:36:51.420,0:36:54.680
這兩段 sequence 的長度是不一樣的

0:36:54.800,0:36:58.920
剛剛在講 CTC 的時候，input 比較長，output 比較短

0:36:58.920,0:37:05.560
在這邊我們要考慮的case是，
不確定 input output 誰比較長，誰比較短

0:37:08.720,0:37:11.720
比如說我們現在要做的是 machine translation

0:37:11.720,0:37:19.120
input 英文的 word sequence 
要把它翻成中文的 character sequence

0:37:24.620,0:37:31.620
我們並不知道英文或中文，誰比較長，誰比較短

0:37:31.760,0:37:35.140
都有可能是 output 比較長，或 output 比較短

0:37:35.260,0:37:40.100
所以怎麼辦呢？
現在假如 input 的是 machine learning

0:37:40.100,0:37:45.140
machine learning 用 RNN 讀過去

0:37:45.340,0:38:04.000
然後在最後一個時間點呢，memory 就
存了所有 input 的整個 sequence 的 information

0:38:05.060,0:38:09.260
然後接下來，你就讓 machine 吐一個 character

0:38:09.300,0:38:12.740
比如說它吐的第一個 character 就是 機

0:38:12.740,0:38:15.320
你把 machine learning 讓 machine 讀過一遍

0:38:15.320,0:38:18.700
然後在讓它 output character，它可能就會 output 機

0:38:18.840,0:38:21.940
接下來再叫它 output 下一個 character

0:38:21.940,0:38:25.340
你把之前 output 出來的 character 當作 input

0:38:25.340,0:38:28.080
再把 memory 裡面存的值讀進來

0:38:28.080,0:38:29.720
它就會 output 器

0:38:30.800,0:38:40.180
這個 機 要如何接到這裡，這地方有很多枝枝節節的技巧

0:38:42.560,0:38:45.500
這個太多了，我們以後再講

0:38:46.000,0:38:49.260
這個以後或許下學期，在 MLTS 再講

0:38:49.260,0:38:55.540
這個其實有很多枝枝節節的地方，
還有很多各種不同的變形

0:38:56.640,0:39:01.560
那它在下一個時間點，器 以後它就 output 學

0:39:01.560,0:39:03.720
然後學就 output 習

0:39:03.720,0:39:06.520
它就會一直 output 下去

0:39:06.620,0:39:09.140
習 後面接 慣，慣 後面接 性

0:39:09.160,0:39:11.660
永遠都不停止這樣

0:39:11.660,0:39:16.840
第一次看到這 model 根本不知道什麼時候該停止

0:39:17.660,0:39:23.600
那怎麼辦呢，這就讓我想到推文接龍

0:39:50.060,0:39:52.060
那你要怎麼讓他停下來呢

0:39:53.520,0:39:56.600
你要有一個冒險去推一個 ==斷==
然後它就會停下來了

0:40:03.400,0:40:06.120
所以今天讓 machine 做的事情，也是一樣

0:40:06.120,0:40:09.100
要如何阻止它不斷的繼續產生詞彙呢？

0:40:09.100,0:40:11.880
你要多加一個 symbol 叫做 斷

0:40:11.880,0:40:14.720
所以 machine 不只 output 所有可能的 character

0:40:14.720,0:40:17.860
它還有一個可能的 output，就做斷

0:40:17.920,0:40:21.760
所以如果今天 習 後面呢，
它的 output 是 斷 的話

0:40:22.140,0:40:24.600
就停下來

0:40:24.600,0:40:26.860
可能覺得說，這東西 train 得起來嗎？

0:40:26.860,0:40:29.040
恩，train 得起來

0:40:29.040,0:40:32.560
神奇的就是這一招，是有用的！

0:40:32.560,0:40:34.580
它也有被用在語音辨識上

0:40:34.580,0:40:37.060
你就直接 input acoustic feature sequence

0:40:37.180,0:40:39.540
直接就 output character sequence

0:40:39.540,0:40:42.080
只是這方法，還沒有 CTC 強

0:40:42.080,0:40:44.520
所以這方法，還不是 state of the art 的結果

0:40:44.520,0:40:50.900
但讓人真正 surprise 的地方，
這麼做是行的通，然後它的結果是沒有爛掉

0:40:52.760,0:40:58.420
在翻譯上，據說用這個方法，
已經可以達到 state of the art 的結果了

0:41:00.380,0:41:06.160
那最近呢，這應該是 google 在 12 月初發的 paper

0:41:06.160,0:41:08.380
所以是幾周前，放在 arxiv 上的 paper

0:41:08.860,0:41:13.400
他們做了一件事情，我相信這件事情很多人都想到，
只是沒人去做而已

0:41:13.400,0:41:15.220
他的想法是這樣

0:41:15.220,0:41:19.280
sequence to sequence learning 假設是做翻譯的話

0:41:19.280,0:41:21.700
也就是 input 某種語言的文字

0:41:21.720,0:41:24.440
翻成另外一種語言的文字

0:41:28.260,0:41:30.260
我們有沒有可能，直接 input 某種語言的聲音訊號

0:41:30.500,0:41:32.500
output 另為一種語言的文字呢？

0:41:32.640,0:41:36.080
我們完全不做語音辨識

0:41:38.100,0:41:42.000
比如說你要把英文翻成中文

0:41:42.260,0:41:46.440
你就收集一大堆英文句子，和他對應的中文翻譯

0:41:46.500,0:41:48.040
你完全不要做語音辨識

0:41:48.040,0:41:51.400
直接把英文的聲音訊號，丟到這個 model 裡面去

0:41:51.440,0:41:53.740
看他能不能 output 正確的中文

0:41:53.900,0:41:57.940
結果這一招居然看起來是行得通的

0:41:57.940,0:42:01.180
我相信很多人想過，大概覺得做不起來，
所以沒有人去試

0:42:01.260,0:42:03.900
這一招看起來是行得通的

0:42:03.900,0:42:08.140
你可以直接 input 一串法文的聲音訊號

0:42:08.280,0:42:13.340
然後 model 就得到辨識的結果

0:42:17.500,0:42:24.120
如果這個東西能夠成功的話，他可以帶給我們的好處是

0:42:28.240,0:42:33.760
如果我們在 collect translation 的 training data 的時候

0:42:33.760,0:42:35.360
會比較容易

0:42:35.360,0:42:37.560
假設你今天要把某種方言

0:42:37.560,0:42:40.040
比如說台語，轉成英文

0:42:40.080,0:42:44.060
但是台語的語音辨識系統比較不好做

0:42:44.060,0:42:49.860
因為台語根本就沒有一個 standard 的文字的系統

0:42:50.160,0:42:54.240
所以你要找人來 label 台語的文字，
可能也有點麻煩

0:42:54.240,0:42:56.760
如果這樣子技術是可以成功的話

0:42:56.760,0:42:59.680
未來你在訓練台語轉英文的語音辨識系統的時候

0:42:59.680,0:43:02.900
你只需要收集台語的聲音訊號

0:43:02.900,0:43:07.020
跟他的英文翻譯就可以了

0:43:07.020,0:43:10.840
你就不需要台語的語音辨識結果

0:43:10.840,0:43:15.180
你就不需要知道台語的文字，你也可以做這種翻譯

0:43:19.720,0:43:23.420
那現在還可以用 sequence to sequence 的技術

0:43:23.420,0:43:28.340
甚至可以做到 Beyond Sequence

0:43:34.680,0:43:40.100
比如說這個技術呢，
也被用在 Syntactic parsing tree 裡面

0:43:40.100,0:43:43.220
用在產生，Syntactic parsing tree 上面

0:43:44.080,0:43:47.840
這個 Syntactic parsing tree 是什麼呢？

0:43:47.840,0:43:50.180
意思就是，讓 machine 看一個句子

0:43:50.180,0:43:54.800
然後他得到這個句子的文法的結構樹

0:44:04.800,0:44:08.860
要怎麼讓 machine 得到這樣的樹狀的結構呢？

0:44:09.640,0:44:12.680
過去呢，
你可能要用 structure learning 的技術

0:44:12.680,0:44:16.260
才能夠解這一個問題

0:44:18.700,0:44:21.640
但現在有了 sequence to sequence 的技術以後

0:44:21.660,0:44:27.740
你只要把這個樹狀圖，描述成一個 sequence

0:44:27.740,0:44:31.100
樹狀圖當然可以描述成一個 sequence

0:44:32.500,0:44:34.440
root 的地方是 S

0:44:34.440,0:44:37.660
S 的左括號，S 的右括號

0:44:37.660,0:44:39.980
他下面有 NP 跟 VP

0:44:39.980,0:44:42.260
所以有 NP 的左括號，NP 的右括號

0:44:42.260,0:44:44.740
VP 的左括號，VP 的右括號

0:44:44.740,0:44:54.980
NP 下面有 NNP，VP 下面有 VBZ，NP
NP 下面有 DT/NN 等等

0:44:56.180,0:44:59.980
所以他有一個 sequence

0:45:01.020,0:45:04.220
所以如果今天是 sequence to sequence learning 的話

0:45:04.420,0:45:06.940
你就直接 learn 一個 sequence to sequence 的 model

0:45:06.980,0:45:15.320
他的 output 直接是這個 Syntactic 的 parsing tree

0:45:16.220,0:45:19.900
你可能覺得這樣真的 train 得起來嗎？

0:45:20.000,0:45:24.140
恩，可以 train 得起來，這很 surprise

0:45:25.520,0:45:34.160
當然你可能會想說 machine 今天長出來的 output sequence 他不符合文法結構呢？

0:45:34.160,0:45:37.160
如果他記得加左括號，卻忘了加右括號呢？

0:45:37.160,0:45:44.000
但神奇的地方是，LSTM 它有記憶力，
他不會忘記加上右括號

0:45:46.320,0:45:50.780
好，那我們之前講過 word vector

0:45:51.020,0:45:55.240
那如果我們要把一個 document 表示成一個 vector 的話

0:45:55.240,0:45:57.280
往往會用 bag-of-word 的方法

0:45:57.280,0:46:00.020
但當我們用 bag-of-word 的方法

0:46:00.060,0:46:05.200
我們就會忽略到 word order 的 information

0:46:05.500,0:46:08.700
舉例來說，有一個 word sequence

0:46:08.700,0:46:11.640
是 white blood cells destroying an infection

0:46:12.300,0:46:16.520
另外一個 word sequence 是 an infection destroying white blood cells

0:46:16.520,0:46:19.060
這兩句話的意思，完全是相反的

0:46:19.060,0:46:22.080
但是如果你用 bag-of-word 來描述他的話

0:46:22.160,0:46:26.740
它們的 bag-or-word 完全是一樣的

0:46:26.740,0:46:31.280
它們裡面有一模一樣的 6 個詞彙

0:46:31.280,0:46:34.540
但是因為這個詞彙的 order 是不一樣的

0:46:34.540,0:46:39.440
對他們的意思，一個變成 positive，一個變成 negative

0:46:39.440,0:46:42.620
意思是很不一樣的

0:46:43.280,0:46:46.960
那我們可以用 sequence to sequence auto-encoder 這種做法

0:46:46.960,0:46:52.200
在有考慮 word sequence order 的情況下

0:46:52.480,0:46:56.640
把一個 document 變成一個 vector

0:46:57.340,0:46:59.340
怎麼做呢？

0:46:59.360,0:47:02.660
我們就 input 一個 word sequence

0:47:05.140,0:47:07.580
通過一個 RNN

0:47:07.580,0:47:11.300
把它變成一個 embedded 的 vector

0:47:11.660,0:47:16.740
然後再把這個 embedded vector 當成 decoder 的輸入

0:47:17.440,0:47:22.900
然後讓這個 decoder 長回一個一模一樣的句子

0:47:22.900,0:47:27.040
如果今天 RNN 可以做到這件事情的話

0:47:27.100,0:47:29.260
那 encoding 的這個 vector

0:47:29.260,0:47:33.200
就代表這個 input sequence 裡面，重要的資訊

0:47:33.200,0:47:37.740
所以這個 decoder 呢，才能根據 encoder 的 vector

0:47:38.000,0:47:41.240
把這個訊號 decode 回來

0:47:41.280,0:47:45.940
train 這個 sequence to sequence auto-encoder
你是不需要 label data 的

0:47:45.960,0:47:50.620
你只需要收集到大量的文章

0:47:50.620,0:47:54.120
然後直接 train 下去就好了

0:47:55.100,0:47:59.840
那這個 sequence to sequence auto-encoder，
還有另外一個版本叫做 ******

0:47:59.840,0:48:04.480
當你是用 ****，如果是用 Seq2Seq auto encoder

0:48:04.480,0:48:06.560
input 跟 output 都是同一個句子

0:48:06.560,0:48:11.840
如果你用 **** 的話，output target會是下一個句子

0:48:11.840,0:48:17.160
如果是用 Seq2Seq auto encoder，通常你得到的 code 比較容易表達文法的意思

0:48:17.160,0:48:23.080
如果你要得到語意的意思，
用 **** 可能會得到比較好結果

0:48:23.160,0:48:27.200
這個結構，甚至可以是 Hierarchy 的

0:48:27.200,0:48:31.840
你可以每一個句子都先得到一個 vector

0:48:39.340,0:48:44.800
再把這些 vector 加起來，變成一個整個

0:48:44.800,0:48:46.940
document high level 的 vector

0:48:46.940,0:48:49.320
再用這個 document high level 的 vector

0:48:49.320,0:48:51.700
去產生一串 sentence 的 vector

0:48:51.700,0:48:53.360
再根據每一個 sentence vector

0:48:53.360,0:48:56.860
再去解回 word sequence

0:48:57.140,0:49:01.540
所以這是一個 4 層的 LSTM

0:49:01.540,0:49:04.780
你從 word 變成 sentence sequence

0:49:04.780,0:49:06.820
再變成 document level 的東西

0:49:06.820,0:49:10.460
再解回 sentence sequence，再解回 word sequence

0:49:10.460,0:49:12.820
這個東西也是可以 train 的

0:49:16.260,0:49:19.720
那剛才的東西，也可以被用在語音上

0:49:19.720,0:49:24.460
seq2seq auto encoder 除了被用在文字上，
也可以被用在語音上

0:49:24.460,0:49:27.940
如果用在語音上，它可以做到的事情，就是

0:49:28.040,0:49:35.920
它可以把一段 audio segment 
變成一段 fixed length 的 vector

0:49:37.580,0:49:42.120
比如說它可以把 dog 變成

0:49:42.420,0:49:46.660
比如說這邊有一堆聲音訊號，
它們長長短短的都不一樣

0:49:46.660,0:49:52.200
你把它們變成 vector 的話，
可能 dog/dogs 的 vector 比較接近

0:49:52.200,0:49:57.340
可能 never/ever 的 vector 是比較接近的

0:49:59.060,0:50:01.860
這個我稱之為 audio 的 word to vector

0:50:01.860,0:50:05.400
就像一般的 word to vector，它是把一個 word 變成一個 vector

0:50:05.400,0:50:10.520
這邊是把一段聲音訊號，變成一個 vector

0:50:11.700,0:50:18.100
這個東西有什麼用呢？
一開始在想這個我覺得應該沒有什麼用

0:50:18.400,0:50:21.780
但它其實可以拿來做很多事，比如說

0:50:21.780,0:50:24.760
我們可以拿來做語音的搜尋

0:50:24.900,0:50:28.260
什麼是語音的搜尋呢？
你有一個聲音的 database

0:50:28.260,0:50:29.800
比如說上課的錄影錄音

0:50:29.920,0:50:32.520
然後你說一句話

0:50:33.140,0:50:37.860
比如說你今天要找美國白宮有關的東西

0:50:37.880,0:50:39.720
你就用說的，說美國白宮

0:50:39.720,0:50:41.360
然後不需要做語音辨識

0:50:41.360,0:50:43.320
直接比對聲音訊號的相似度

0:50:43.400,0:50:47.700
machine 就可以從 database 裡面，
把有提到美國白宮的部分，找出來

0:50:48.300,0:50:51.860
那這個怎麼做呢？你有一個 audio 的 data base

0:50:51.860,0:50:54.240
把這個 database 做 segmentation

0:50:54.240,0:50:56.560
切成一段一段

0:50:56.820,0:51:02.520
然後每一段呢，
用剛才講的 audio segment to vector 的技術呢

0:51:02.520,0:51:06.060
把他們通通變成 vector

0:51:07.220,0:51:09.820
然後現在使用者輸入一個 Query

0:51:09.820,0:51:11.820
Query 也是語音的

0:51:11.820,0:51:14.660
透過 audio segment to vector 的技術呢

0:51:14.660,0:51:18.140
可以把這一段聲音訊號呢，也變成 vector

0:51:18.420,0:51:25.360
然後接下來呢，計算它們的相似程度

0:51:25.360,0:51:30.080
然後就得到搜尋的結果

0:51:41.760,0:51:43.400
這件事情怎麼做呢？

0:51:43.400,0:51:47.180
怎麼把一個 audio segment 變成一個 vector 呢？

0:51:48.540,0:51:51.800
作法是這樣，先把 audio segment

0:51:51.800,0:51:54.000
抽成 acoustic feature sequence

0:51:55.560,0:51:57.560
然後呢，把它丟到 RNN 裡面去

0:51:57.600,0:52:01.880
這個 RNN 它的角色，就是一個 encoder

0:52:01.880,0:52:05.680
而這個 RNN 它讀過這個 acoustic feature sequence 以後

0:52:05.760,0:52:09.240
它存在 memory 裡面的值，就代表了

0:52:09.240,0:52:12.440
它在最後時間點存在這 memory 裡面的值

0:52:12.440,0:52:17.720
就代表了它的整個 input 的聲音訊號
它的 information

0:52:18.900,0:52:20.900
它存在 memory 裡面的值，是一個 vector

0:52:21.100,0:52:26.380
這個東西，
其實就是我們要拿來表示一整段聲音訊號的 vector

0:52:26.560,0:52:29.260
但是只有這個 RNN encoder 我們沒有辦法 train

0:52:29.260,0:52:31.540
你同時還要 train 一個 RNN 的 decoder

0:52:31.760,0:52:34.920
RNN decoder 它的作用呢，它把

0:52:34.920,0:52:38.820
encoder 存在 memory 裡面的值呢，拿進來當作做 input

0:52:38.940,0:52:43.080
然後產生一個 acoustic feature sequence

0:52:43.080,0:52:47.160
那你會希望這個 y1 跟 x1 越接近越好

0:52:48.220,0:52:52.980
然後根據 y1 再產生 y2 y3 y4

0:52:53.160,0:52:54.800
而今天訓練的 target

0:52:54.800,0:53:02.340
就是希望 y1 到 y4 跟 x1 到 x4 它們是越接近越好

0:53:02.340,0:53:03.760
那在訓練的時候

0:53:03.760,0:53:09.000
這個 RNN 的 encoder 和 RNN decoder 他們是 jointly learned

0:53:09.000,0:53:14.740
它們是一起 train的

0:53:16.960,0:53:21.540
如果 RNN encoder/decoder，它們只有一個人，
是沒有辦法 train 的

0:53:21.540,0:53:23.400
但是把他們兩個人接起來

0:53:23.500,0:53:27.920
你就有一個 target 可以從這邊，
一路 back propagate 回來

0:53:27.920,0:53:31.040
你就可以同時 train RNN encoder 跟 decoder

0:53:31.040,0:53:36.380
這邊呢是我們在實驗上得到的一些有趣結果

0:53:36.380,0:53:39.320
這個圖上的每一個點，都是一段聲音訊號

0:53:39.380,0:53:43.160
你把聲音訊號用剛才講的
 sequence to sequence encoder 技術

0:53:43.160,0:53:46.360
把它變成平面上的一個 vector

0:53:46.380,0:53:49.300
會發現說 fear 的位置，在左上角

0:53:49.900,0:53:51.640
near 的位置在右下角

0:53:51.640,0:53:53.440
中間是這樣子的關係

0:53:53.440,0:53:55.660
fame 的位置在左上角，name 的位置在右下角

0:53:55.660,0:53:59.140
它們中間有一個這樣子的關係

0:53:59.200,0:54:05.160
哪你會發現說，把 fear 開頭的 f 換成 n
跟 fame 開頭的 f 換成 n

0:54:05.340,0:54:10.160
它們的 word vector 的變化，方向是一樣的

0:54:10.160,0:54:13.340
就好像我們之前看到的這個 vector 一樣

0:54:13.340,0:54:16.600
跟我們好像之前看到文字的 word vector 一樣

0:54:16.600,0:54:21.260
不過這邊的 vector 
還沒有辦法考慮 semantic 語意的 information

0:54:21.360,0:54:24.900
那我們下一步要做的事情，就是把語意加進去

0:54:24.900,0:54:27.380
但這部分現在還沒有完成

0:54:27.560,0:54:32.480
那接下來我有一個 demo，這個 demo  是
用 sequence to sequence auto encoder

0:54:32.660,0:54:35.540
來訓練一個 chat bot

0:54:35.540,0:54:38.060
chat bot 就是聊天機器人

0:54:40.740,0:54:43.120
那怎麼用 sequence to sequence，

0:54:43.120,0:54:44.860
喔，這不是 sequence to sequence auto encoder

0:54:44.860,0:54:46.840
這是 sequence to sequence learning

0:54:46.880,0:54:49.360
那怎麼用來
來 train 一個 chat bot 呢？

0:54:49.360,0:54:54.060
你就收集很多對話，比如說電影的台詞

0:54:54.240,0:54:57.440
假設電影的台詞裡面，有一個人說 how are you

0:54:57.440,0:54:59.520
另外一個人就接 I am fine

0:54:59.520,0:55:02.580
那就告訴 machine 說，
這個 sequence to sequence learning

0:55:02.580,0:55:05.900
它的 input 當它是 how are you 的時候

0:55:06.040,0:55:09.400
這個 model 的 output 就要是 I am fine

0:55:09.400,0:55:13.840
假如你可以收集到這種 data，然後就讓 machine 去 train

0:55:15.520,0:55:18.500
然後我們就收集了 40000 句的電視影集

0:55:18.500,0:55:20.760
和美國總統大選辯論的句子

0:55:20.760,0:55:27.340
然後就讓 machine 去學這個 
sequence to sequence 的 model

0:55:27.900,0:55:33.560
這個是跟中央大學 蔡宗翰 老師的團隊一起開發的

0:55:33.600,0:55:38.640
然後作的同學呢，台大這邊呢，是有

0:55:42.100,0:55:46.200
那其實現在除了 RNN 以外呢

0:55:46.200,0:55:50.200
還有另外一種有用到 memory 的 network

0:55:50.200,0:55:52.140
叫做 attention-base model

0:55:52.140,0:55:55.360
它可以想成是 RNN 的一個進階版本

0:55:55.760,0:56:04.840
那我們知道人的大腦，有非常強的記憶力

0:56:04.920,0:56:07.880
所以你可以記得非常多的東西

0:56:07.880,0:56:11.100
比如說你現在可能同時記得，早餐吃了什麼

0:56:11.100,0:56:15.440
可能同時記得 10 年前中二的夏天發生了什麼

0:56:15.620,0:56:19.980
可能同時記得在這幾門課學到的東西

0:56:20.280,0:56:23.460
那當然有人問你什麼是 deep learning 的時候

0:56:23.460,0:56:28.800
那你的腦中會去提取重要的 information

0:56:28.800,0:56:31.040
然後再把這些 information 組織起來

0:56:31.060,0:56:33.060
產生答案

0:56:33.740,0:56:39.080
但你的腦會自動忽略掉那些無關的事情

0:56:39.080,0:56:43.100
比如說 10 年前中二的夏天發生的事情，等等

0:56:43.800,0:56:47.620
那其實 machine 也可以做到類似的事情

0:56:47.820,0:56:52.100
machine 也可以有很大的記憶容量

0:56:52.180,0:56:54.340
它也可以有一個很大的 data base

0:56:54.340,0:56:58.740
在這個 data base 裡面，每一個 vector 就代表某種 information

0:56:58.740,0:57:00.740
被存在 machine 的記憶裡面

0:57:01.780,0:57:05.660
當你輸入一個 input 的時候，
這個 input 會被丟進一個中央處理器

0:57:05.720,0:57:09.600
這個中央處理器，可能是一個 DNN/RNN

0:57:10.180,0:57:14.240
那這個中央處理器，會操控一個讀寫頭

0:57:14.240,0:57:17.520
操恐一個 reading head controller

0:57:17.520,0:57:22.160
最後這個 reading head controller 
會決定這個 reading head

0:57:22.160,0:57:23.400
放的位置

0:57:23.400,0:57:28.200
然後 machine 再從這個 reading head 放的位置，
去讀取 information 出來

0:57:29.240,0:57:33.320
然後產生最後的 output

0:57:34.020,0:57:37.060
那我們就不打算細講這樣的 model，
如果你有興趣

0:57:37.060,0:57:40.480
可以參考我之前上課的錄影

0:57:40.940,0:57:45.660
這個 model 還有一個 2.0 的版本

0:57:45.920,0:57:52.680
這個 2.0 版本，它會去操控一個 
writing head controller

0:57:53.040,0:57:57.620
這個 writing head controller 
會去決定 writing head 放的位置

0:57:57.660,0:58:01.480
然後 machine 會把它的 information 
透過這個 writing head 呢

0:58:01.660,0:58:04.240
寫進它的 database 裡面

0:58:06.360,0:58:09.800
所以他不只有讀的功能，還可以把資訊

0:58:09.920,0:58:11.920
它 discover 出來的東西

0:58:12.260,0:58:14.700
寫到它的 memory 裡面去

0:58:14.820,0:58:18.540
這個東西就是大名鼎鼎的 Neural Turing Machine

0:58:18.940,0:58:21.100
這些其實都是很新的東西

0:58:21.100,0:58:33.700
Neural Turing Machine 應該是在 14 年的年底提出來的

0:58:33.700,0:58:35.180
我也忘了

0:58:35.580,0:58:40.980
不知道是 15 年初，還是 14 年底的時候，提出來的

0:58:40.980,0:58:42.940
所以都是很新的東西

0:58:44.540,0:58:49.000
現在 attention-based model，
常常被用在 reading comprehension

0:58:49.240,0:58:54.200
所謂 reading comprehension，就是，
讓 machine 去讀一堆 document

0:58:54.200,0:58:56.960
然後這些 document 裡面的內容呢？

0:58:56.960,0:59:00.340
每一句話，變成一個 vector 存起來

0:59:00.340,0:59:08.400
每一個 vector 代表某一句話的語意

0:59:08.580,0:59:11.020
接下來呢，你問 machine 一個問題

0:59:11.020,0:59:12.960
比如說玉山有多高之類的

0:59:13.500,0:59:17.440
然後這個問題被丟進一個中央處理器裡面

0:59:17.460,0:59:21.680
那這個中央處理去去控制一個 reading head controller

0:59:22.200,0:59:25.420
去決定現在在這個 database 裡面

0:59:25.420,0:59:30.280
那些句子是跟中央處理器有關的

0:59:30.880,0:59:35.040
所以假設呢 machine 發現說這個句子，
是跟現在這個問題有關的

0:59:35.040,0:59:37.340
它就把 reading head 放在這個地方

0:59:37.340,0:59:39.820
把 information 讀到中央處理器裡面

0:59:40.260,0:59:44.100
這個讀取 information 的過程，它可以是 iterative

0:59:44.100,0:59:46.020
它可以是重複數次的

0:59:46.020,0:59:50.160
也就是說 machine 並不會只從一個地方讀取 information

0:59:50.300,0:59:52.420
它先從這裡讀取 information 以後

0:59:52.420,0:59:57.160
它還可以換一個位置，從另外一個地方，
再去讀取 information

0:59:57.300,0:59:59.900
然後它把所有讀到的 information collect 起來

0:59:59.980,1:00:02.580
它可以給你一個最終的答案

1:00:03.060,1:00:05.380
以下呢，是 facebook AI research

1:00:05.380,1:00:11.660
在 baby **** 上面的一個實驗結果

1:00:13.140,1:00:16.540
baby **** 是一個 QA question answer 的一個 test

1:00:16.540,1:00:19.160
它其實是一個比較簡單的 test

1:00:19.160,1:00:22.360
有很多用 template 產生的 document

1:00:22.360,1:00:24.360
和一些簡單的問題，我們需要回答這些問題

1:00:25.340,1:00:28.160
我們現在要做的事情就是，讀過這五個句子

1:00:28.200,1:00:29.780
來問它 what color is Greg

1:00:29.780,1:00:32.180
它要得到正確的答案，yes

1:00:32.740,1:00:35.220
你可以從 machine attention 的位置

1:00:35.220,1:00:37.140
也就是它 reading head 的位置

1:00:37.360,1:00:38.940
看出 machine 的思路

1:00:38.940,1:00:43.840
這邊的藍色代表 machine reading head 放置的位置

1:00:44.080,1:00:49.860
hop1/2/3 代表的是時間

1:00:49.860,1:00:55.080
也就是他第一個時間點 machine 
先把它的 reading head 放在 Greg is a frog

1:00:55.080,1:00:56.900
所以他把這個 information 把它提取出來

1:00:56.900,1:00:58.960
它提取 Greg is a frog 這個 information

1:00:59.680,1:01:03.920
接下來它再提取 Brian is a frog 這個 information

1:01:04.000,1:01:07.300
接下來它再提取 Brian is a yellow 的 information

1:01:07.300,1:01:09.140
最後呢，它就得到結論說

1:01:09.140,1:01:12.520
它按了 Greg 的顏色是 yellow

1:01:13.180,1:01:16.420
這些事情是 machine 自動 learn 出來的

1:01:16.420,1:01:24.540
也就是 machine 要 attend 在哪一個位置，是透過 neural network 自己去學到知道怎麼做的

1:01:24.600,1:01:27.940
也就是說，並不是去寫程式，告訴 machine 說

1:01:27.940,1:01:30.640
你要先看這個句子，再看這個句子...，不是

1:01:30.640,1:01:33.880
是 machine 自動去決定，它要看哪一個句子

1:01:35.060,1:01:37.680
那也可以做 Visual 的 Question Answering

1:01:37.680,1:01:41.180
Visual Question Answering 就是讓 machine 看一張圖

1:01:41.280,1:01:42.320
然後問它一個問題

1:01:42.320,1:01:43.800
比如說問他這是什麼

1:01:43.800,1:01:48.600
如果它可正確回答是香蕉的話，它就超越部分人類了

1:01:48.600,1:01:51.500
那這個 Visual Question Answering 怎麼做呢？

1:01:51.500,1:01:54.440
就讓 machine 看一張圖

1:01:54.440,1:01:58.360
透過 CNN 呢，你可以把這個圖

1:01:58.360,1:02:03.320
的每一小塊 region ，
用一個 vector 來表示

1:02:03.840,1:02:06.780
那接下來呢？輸入一個 Query

1:02:07.000,1:02:11.220
然後這個 Query 被丟到中央處理器裡面

1:02:11.280,1:02:16.120
那這個中央處理器，去操控 reading head controller

1:02:16.320,1:02:23.280
然後這個 reading head controller 
決定了它要讀取資訊的位置

1:02:23.380,1:02:28.740
看看這圖片的什麼位置呢，
是跟現在輸入的問題是有關的

1:02:28.740,1:02:31.220
那把 information 讀到中央處理器裡面

1:02:31.380,1:02:34.380
這個讀取的 process 可能有好幾個步驟

1:02:34.380,1:02:37.400
machine 會分好幾次把 information 讀到中央處理器裡面

1:02:37.400,1:02:39.380
最後得到答案

1:02:41.300,1:02:45.380
那也可以做語音的 Question Answering

1:02:45.480,1:02:46.700
比如說

1:02:46.700,1:02:51.420
在語音處理實驗室，
我們讓 machine 做 TOFEL 的聽力測驗

1:02:51.560,1:02:53.280
所謂 TOFEL 的聽力測驗就是

1:02:53.280,1:02:54.980
讓 machine 聽一段聲音

1:02:54.980,1:02:58.100
然後問他問題

1:02:58.100,1:03:03.480
然後從四個正確選項裡面呢，machine 要選出正確選項

1:03:03.640,1:03:07.480
那 machine 做的事情，跟人類考生做的事情

1:03:07.480,1:03:08.820
是一模一樣的

1:03:08.820,1:03:13.540
我們用來訓練測試 machine 的資料
就是 TOFEL 聽力測驗資料

1:03:15.480,1:03:19.360
用的 model architecture 跟我們剛才看到的

1:03:19.400,1:03:21.760
其實就是大同小異

1:03:21.820,1:03:26.540
你讓 machine 先讀一下 question

1:03:26.840,1:03:29.940
然後把這個 question 做語意的分析

1:03:29.940,1:03:31.700
得到這個 question 的語意

1:03:31.720,1:03:35.480
那聲音的部分，先用語音辨識把它轉成文字

1:03:35.620,1:03:38.960
那再把這些文字做語意的分析

1:03:39.340,1:03:41.740
得到這段文字的語意

1:03:41.740,1:03:44.820
那 machine 了解了問題的語意

1:03:44.820,1:03:47.680
和 audio story 的語意以後

1:03:47.680,1:03:49.080
它就可以做 attention

1:03:49.080,1:03:53.780
決定在這個 audio story 裡面，
那些部分是和回答問題有關

1:03:54.460,1:03:58.160
這就好像是畫重點一樣，
machine 根據它畫的重點呢

1:03:58.160,1:04:00.040
產生答案

1:04:00.040,1:04:05.240
那它甚至也可以回頭過去修正它產生出來的答案

1:04:05.240,1:04:08.440
經過幾個 process 以後呢，最後 machine 得到它的答案

1:04:08.440,1:04:12.520
那它把它答案呢，跟其他選項，計算相似度

1:04:12.520,1:04:15.220
然後看哪一個選項的相似度最高

1:04:15.220,1:04:20.340
它就選哪一個選項

1:04:20.520,1:04:25.680
那這整個 task 其實就是一個大的 neural network

1:04:25.680,1:04:27.240
除了語音辨識以外

1:04:27.520,1:04:33.440
Question semantic 的部分，
還有 audio story semantic 的部分呢

1:04:33.600,1:04:34.780
都是 neural network

1:04:34.780,1:04:37.360
所以他們都是 jointly trained

1:04:37.360,1:04:40.160
你就只要給 machine TOFEL 聽一次考古題

1:04:40.160,1:04:42.620
machine 就自己會去學了

1:04:43.020,1:04:48.580
那底下是一些實驗結果啦

1:04:48.800,1:04:50.500
這個實驗結果是這樣子

1:04:50.500,1:04:54.160
random 猜阿，正確率是 25%

1:04:55.020,1:04:57.560
你會發現說有兩個方法

1:04:57.560,1:04:59.560
是遠比 25 % 強的

1:04:59.780,1:05:01.580
這是很重要的 information

1:05:01.580,1:05:04.880
這邊這五個方法，都是 naive 的方法

1:05:04.940,1:05:09.960
也就是完全不管文章的內容

1:05:09.960,1:05:12.960
就直接看問題跟選項，就猜答案

1:05:13.120,1:05:16.560
然後我們發現說，如果你選最短的那個選項

1:05:16.560,1:05:18.800
可以得到 35 % 的正確率

1:05:21.440,1:05:23.440
這是計中計，你可能會覺得應該要選最長的

1:05:23.680,1:05:25.400
其實要選最短的

1:05:25.480,1:05:30.460
另外一個是這樣，如果你分析四個選項的 semantic

1:05:30.460,1:05:33.000
你做那個 sequence to sequence auto encoder

1:05:33.000,1:05:36.200
去把每個選項的 semantic 找出來

1:05:36.580,1:05:40.440
然後你再去看說，某一個選項，跟另外三個選項

1:05:40.440,1:05:42.240
的語意上的相似度

1:05:42.240,1:05:47.400
你會發現說，如果某一個選項，
和另外三個選項的語意相似度

1:05:47.400,1:05:49.060
比較高的話

1:05:49.180,1:05:51.940
然後你就把它選出來，那你有 35% 的正確率

1:05:52.100,1:05:53.940
這跟你的直覺是相反的

1:05:53.940,1:05:57.040
我們的直覺通常會覺得說，應該選一個選項

1:05:57.040,1:05:59.920
它的語意，與另外三個選項是不像的

1:05:59.960,1:06:02.560
但人家早就計算到你會這麼做了

1:06:02.800,1:06:04.200
所以這是一個計中計

1:06:04.200,1:06:10.460
如果你要選某一個選項的語意，
與另外三個選項最像的話

1:06:10.500,1:06:12.980
你反而可以得到超過 random 的答案

1:06:12.980,1:06:14.980
如果你今天是選

1:06:15.280,1:06:17.280
最不像的，語意最不像的那個選項

1:06:17.640,1:06:19.480
你得到的答案就會接近 random

1:06:19.480,1:06:20.840
它都是設計好的

1:06:22.320,1:06:25.140
那這些都是一些 trivial 的方法

1:06:25.200,1:06:26.820
你可以用一些 machine learning 的方法

1:06:26.880,1:06:30.360
比如說用 memory network

1:06:30.360,1:06:32.060
可以得到 39% 的正確率

1:06:32.060,1:06:34.860
是比隨機弄一下的還好一些

1:06:34.860,1:06:38.800
如果用我們剛才講的那個 model 的話呢

1:06:38.880,1:06:42.160
我們現在在有語音辨識錯誤的情況之下

1:06:42.160,1:06:45.680
最好可以做到將近 50% 的正確率啦

1:06:45.680,1:06:48.020
所以其實 50% 正確率是沒有很高

1:06:48.020,1:06:52.180
我覺得這樣應該是去不了什麼美國學校的啦

1:06:52.500,1:06:54.380
但是就是兩題可以答對一題

1:06:54.520,1:06:56.380
所以如果你沒有辦法兩題答對一題，

1:06:56.380,1:06:58.020
你其實就是沒有 machine 強

1:07:00.800,1:07:03.480
以下是一些 reference 給大家參考

1:07:03.480,1:07:04.520
那最後

1:07:05.060,1:07:08.640
我這邊其實有一個問題

1:07:08.980,1:07:13.620
我們講了 Deep learning
也講了 structured learning

1:07:13.860,1:07:17.920
它們中間有什麼樣的關係呢？你想想看

1:07:20.760,1:07:26.220
我們上周講了 HMM，
講了 CRF/Structured Perceptron/SVM

1:07:26.540,1:07:29.840
它們可以做的事情，比如說做 pos taking

1:07:29.840,1:07:32.440
input 一個 sequence，output 一個 sequence

1:07:32.980,1:07:36.900
RNN/LSTM，也可以做到一樣的事情

1:07:37.860,1:07:43.960
當我們使用 deep learning 跟 structured learning 的技術

1:07:44.080,1:07:46.080
有什麼不同呢？

1:08:06.040,1:08:08.040
首先

1:08:09.100,1:08:14.920
假如我們現在用的是 uni-directional 的 RNN 或 LSTM

1:08:15.080,1:08:20.180
當你在 make decision 的時候，
你只看了 sentence 的一半

1:08:20.300,1:08:24.600
而如果你是用 structured learning 的話

1:08:24.600,1:08:26.520
透過 Viterbi 的 algorithm

1:08:26.520,1:08:29.060
你考慮的是整個句子

1:08:29.520,1:08:32.420
如果你是用 Viterbi 的 algorithm 的話，
machine 會讀過整個句子以後

1:08:33.040,1:08:34.420
才下決定

1:08:34.740,1:08:38.320
所以從這個角度來看，也許

1:08:38.480,1:08:43.120
HMM, CRF... 等等，還是有佔到一些優勢

1:08:43.680,1:08:46.740
但這個優勢並沒有很明顯，因為

1:08:47.420,1:08:51.020
RNN/LSTM 等等，它們可以做 Bi-directional

1:08:51.020,1:08:55.860
所以他們也有辦法考慮，一整個句子的 information

1:09:03.100,1:09:06.680
在 HMM/CRM 裡面啊

1:09:06.680,1:09:14.400
你可以很 explicitly 去考慮 label 和 label 之間的關係

1:09:14.980,1:09:16.100
什麼意思呢？

1:09:16.480,1:09:17.420
舉例來說

1:09:17.440,1:09:20.080
你今天在做 inference 的時候

1:09:20.460,1:09:23.460
你在用 Viterbi algorithm 求解的時候

1:09:23.700,1:09:27.220
假設你可以直接把你要的 constrain 下到

1:09:27.220,1:09:30.160
那個 Viterbi algorithm 裡面去

1:09:30.160,1:09:31.320
你了解我意思嗎？

1:09:31.320,1:09:35.980
你可以直接說，我希望每一個 label 出現的時候，
都要連續出現五次

1:09:36.080,1:09:39.120
這件事情你可以輕易地用 Viterbi algorithm 做到

1:09:39.120,1:09:42.660
因為你可以修改 Viterbi algorithm，讓 machine 在選擇

1:09:42.700,1:09:44.700
分數最高的句子的時候

1:09:44.700,1:09:48.940
排除掉不符合你要的 constrain 的那些結果

1:09:48.940,1:09:52.500
但如果是 RNN 或 LSTM 的話

1:09:52.660,1:09:56.760
你要直接下一個 constrain 進去，是比較難的

1:09:56.760,1:10:03.300
你沒有辦法要求 RNN 一定要連續吐出
某一個 level 5 次才是正確的

1:10:04.420,1:10:06.880
你可以在 training data 裡面，
給他看這種 training data

1:10:07.240,1:10:08.480
但是

1:10:08.480,1:10:11.440
但是你叫他去學，然後再這樣，是比較麻煩的

1:10:11.440,1:10:15.640
Viterbi 可以直接告訴你的 machine 要它做什麼事

1:10:15.740,1:10:21.820
所以在這點上，
structured learning 似乎是有一些優勢的

1:10:22.780,1:10:26.360
如果是 RNN 和 LSTM 你的 cost function

1:10:26.360,1:10:31.600
跟你實際上最後要考慮的 error 往往是沒有關係的

1:10:31.640,1:10:34.760
你想想看，當你在做 RNN/LSTM 的時候

1:10:34.860,1:10:38.020
你在考慮的 cost 是，比如說

1:10:38.020,1:10:41.040
每一個時間點的 cross entropy

1:10:41.040,1:10:45.300
每一個時間點，你的 RNN output 
跟 reference 的 cross entropy

1:10:45.340,1:10:49.320
它跟你的 error 往往不見得是直接相關的

1:10:49.320,1:10:56.280
因為你的 error 可能是比如說，
兩個 sequence 之間的 ***

1:10:56.800,1:11:00.720
但如果你是用 structured learning 的話，它的 cost

1:11:00.720,1:11:02.980
會是你 error 的 upper bound

1:11:02.980,1:11:08.260
所以從這個角度來看，
structured learning 也是有一些優勢的

1:11:08.560,1:11:11.620
但是最後最困難最重要的

1:11:11.620,1:11:14.720
RNN/LSTM 可以是 deep

1:11:15.040,1:11:19.520
而 HMM, CRF, ... 他們其實也可以是 deep

1:11:19.520,1:11:23.220
但是他們拿來做 deep 的 learning 其實是比較困難的

1:11:23.400,1:11:26.900
在我們下一堂課講的內容裡面

1:11:26.900,1:11:30.440
他們都是 linear

1:11:30.480,1:11:34.520
為什麼他們是 linear，
因為我們定的 evaluation function 是 linear

1:11:35.440,1:11:39.360
如果它不是 linear，你在 training 的時候會有很多麻煩

1:11:39.440,1:11:43.240
所以他們是 linear，我們才能套用上一堂課教的那些方法

1:11:43.260,1:11:46.560
來做 inference 跟 training

1:11:46.940,1:11:52.200
那在這個比較上，deep learning 會佔到很大的優勢

1:11:52.740,1:11:57.380
最後整體說起來呢，
其實如果你要得到一些 state of the art 的結果

1:11:57.620,1:12:00.720
在這種 sequence labeling task 上，
你要得到 state of the art 的結果

1:12:00.860,1:12:03.620
RNN/LSTM 是不可或缺的

1:12:03.620,1:12:09.240
所以整體說起來 RNN/LSTM 
在這種 sequence labeling task 上面表現

1:12:09.240,1:12:11.020
其實會是比較好的

1:12:11.180,1:12:13.880
deep 這件事是比較強的

1:12:15.160,1:12:17.160
它非常的重要

1:12:17.260,1:12:20.980
如果你今天用的只是 linear model

1:12:20.980,1:12:24.080
如果你的 model 是 linear，
你的 function space 就這麼大

1:12:24.100,1:12:28.540
就算你可以直接 minimize 一個 error 的 upper bound

1:12:28.540,1:12:29.560
那又怎麼樣？

1:12:29.560,1:12:32.480
因為你所有的 function 都是壞的啊

1:12:32.700,1:12:37.460
所以相比之下 deep learning 可以佔到很大的優勢

1:12:38.240,1:12:43.820
但是其實 deep learning 和 structured learning，
它們是可以被結合起來的

1:12:43.960,1:12:50.860
而且有非常多成功結合的先例

1:12:51.140,1:12:58.780
你可以說我底部呢，就是我 input 的 feature

1:12:58.940,1:13:02.780
先通過 RNN 跟 LSTM

1:13:03.420,1:13:04.820
然後先通過 RNN 跟 LSTM

1:13:04.820,1:13:11.800
RNN/LSTM 的 output 再做為 HMM, CRF... 的 input

1:13:11.800,1:13:20.280
你用 RNN/LSTM 的 output 來定義 HMM, CRF... 的 evaluation function

1:13:20.560,1:13:28.240
如此，你就可以同時又享有 deep 的好處，
同時又享有 structured learning 的好處

1:13:29.360,1:13:32.840
那這個再過去已經有很多先例，比如說呢

1:13:33.780,1:13:37.260
到最後你現在這邊有 deep，這邊有 structured

1:13:37.260,1:13:39.820
這兩個是可以 jointly 一起 learned

1:13:40.060,1:13:43.760
你可以想想看，HMM/CRF 可以用 Gradient decent train

1:13:43.760,1:13:46.100
其實 structured/SVM，我們好像沒有講

1:13:46.100,1:13:48.140
但它也可以用 Gradient decent train

1:13:48.700,1:13:53.560
所以你可以把 deep learning 部分跟 structured learning 部分 jointly 合起來

1:13:53.820,1:13:57.280
一起用 Gradient decent 來做 training

1:14:01.200,1:14:02.480
那在語音上呢

1:14:02.480,1:14:05.800
我們常常會把 
deep learning 跟 structured learning 合起來

1:14:05.840,1:14:07.840
你可以常常見到的組合是

1:14:08.080,1:14:13.520
deep learning 的 model: CNN/LSTM/DNN 
加上 HMM 的組合

1:14:13.620,1:14:19.640
所以做語音的人常常說，
我們把過去所做的東西丟掉了，其實不是

1:14:19.660,1:14:23.340
HMM 往往都還在

1:14:23.440,1:14:26.820
如果你要得到最 state of the art 的結果

1:14:26.820,1:14:31.920
現在還是用這樣 hybrid 的 system 得到的結果往往是最好

1:14:33.020,1:14:35.160
那這 hybrid system 怎麼 work 呢？

1:14:35.280,1:14:37.280
我們說在 HMM 裡面

1:14:37.320,1:14:42.420
我們必須要去計算 x 跟 y 的 joint probability

1:14:42.420,1:14:47.400
或是在 structured learning 裡面，我們要計算 x 跟 y 的 evaluation function

1:14:47.580,1:14:52.700
在語音辨識裡面，
x 是聲音訊號，y 是語音辨識的結果

1:14:53.080,1:14:57.720
在 HMM 裡面，我們有 transition 的部分

1:14:57.880,1:15:00.620
我們有 emission 的部分

1:15:02.480,1:15:07.900
DNN 做的事情，其實就是去取代 Emission 的部分

1:15:07.980,1:15:09.380
原來在 HMM 裡面

1:15:09.380,1:15:12.360
這個 emission 就是簡單的統計

1:15:12.360,1:15:14.440
你就是統計一個 Gaussian mixture model

1:15:14.440,1:15:19.140
但是把它換成 DNN 以後，
你會得到很好的 performance

1:15:19.240,1:15:21.160
怎麼換呢？

1:15:21.160,1:15:24.720
一般 RNN 它可以給我們的 output 是

1:15:24.720,1:15:26.160
input 一個 acoustic feature

1:15:26.160,1:15:28.120
它告訴你說這個 acoustic feature

1:15:28.120,1:15:30.740
屬於每一個 state 的機率

1:15:30.740,1:15:32.880
但你可能想說這跟我們要的東西不一樣啊

1:15:32.880,1:15:36.020
我們要的是 p of x given y

1:15:36.020,1:15:39.000
這邊給我們的是 p of y given x

1:15:39.000,1:15:41.500
怎麼辦呢？做一下轉換

1:15:41.800,1:15:44.160
RNN 可以給我們 p of x given y

1:15:44.160,1:15:47.920
然後你可以把它分解成 p of x, y 除以 p of y

1:15:47.940,1:15:54.120
再把它分解成 p of y given x 乘以 p of x 除以 p of y

1:15:54.220,1:15:57.660
那前面這個 p of y given x，它可以從 RNN 來

1:15:57.660,1:15:59.420
那 p of y 呢？

1:16:00.180,1:16:02.180
可以從，你就直接 count

1:16:02.220,1:16:04.700
你就可以直接從你的 **** 統計

1:16:04.700,1:16:06.480
p of y 出現的機率

1:16:06.480,1:16:09.260
這個 p of x 呢，你可以直接無視它

1:16:09.380,1:16:11.340
為什麼 p of x 可以直接無視它呢？

1:16:11.340,1:16:14.420
你想想看，最後你得到這個機率的時候

1:16:14.420,1:16:18.840
在 inference 的時候，x 是 input 是聲音訊號，是已知

1:16:19.000,1:16:20.100
你是窮舉所有的 y

1:16:20.100,1:16:23.040
看哪一個 y 可以讓 p of x,y 最大

1:16:23.040,1:16:26.180
所以跟 x 有關的項，最後不會影響

1:16:26.980,1:16:28.840
第一個 inference 的結果

1:16:28.840,1:16:32.080
所以我們不需要把 x 考慮進來

1:16:33.160,1:16:36.540
那其實加上 HMM，在語音辨識裡面

1:16:36.540,1:16:38.540
是蠻有幫助的

1:16:38.820,1:16:42.500
就算是你用 RNN，你在做辨識的時候啊

1:16:42.500,1:16:44.620
常常會遇到一個問題

1:16:44.620,1:16:46.620
假設我們是一個 frame

1:16:46.940,1:16:48.100
每一個一個 frame 丟到 RNN

1:16:48.100,1:16:49.920
然後問他說這一個 frame，這一個一個 frame

1:16:49.920,1:16:51.580
屬於哪一個 form

1:16:51.660,1:16:54.440
它往往會產生一些怪怪的結果

1:16:54.520,1:16:58.460
比如說因為一個 form 往往是 ****

1:16:59.020,1:17:01.400
所以本來理論上你應該會看到說

1:17:01.600,1:17:06.120
比如說第一個 frame 是 a，第二，第三，第四，第五個 frame 也是 a

1:17:06.120,1:17:08.300
然後接下來換成 b, b, b

1:17:08.300,1:17:11.560
但是如果你用 RNN 在做的時候

1:17:11.860,1:17:14.520
你知道 RNN 它每一個產生的

1:17:14.940,1:17:17.780
label 都是 independent 的

1:17:17.980,1:17:20.760
所以他可能會突然發狂

1:17:20.840,1:17:24.000
在這個地方突然若無其事地改成 b

1:17:24.000,1:17:26.180
然後又改回來這樣子

1:17:27.260,1:17:30.320
你會發現它很容易出現這個現象

1:17:30.360,1:17:32.900
然後如果今天這是一個比賽的話

1:17:32.900,1:17:35.780
你就會有人發現，嗯，RNN 有點弱

1:17:35.780,1:17:36.800
它就會發生這種現象

1:17:36.800,1:17:41.380
如果手動，只要比如說，某一個 output 跟前後不一樣

1:17:41.380,1:17:46.600
我就手動把它改掉，然後你就可以得到大概 2% 的進步

1:17:46.600,1:17:48.640
你就可以屌打其他同學

1:17:48.640,1:17:51.680
那如果你加上 HMM 的話

1:17:51.680,1:17:53.120
就不會有這種情形

1:17:53.120,1:17:59.600
HMM 會幫你把這種狀況自動修掉

1:17:59.600,1:18:04.560
所以加上 HMM 其實是還蠻有幫助的

1:18:04.780,1:18:07.620
對 RNN 來說，因為它在 training 的時候

1:18:07.620,1:18:10.340
它是一個一個 frame，分開考慮的

1:18:10.620,1:18:14.780
所以其實今天假如不同的錯誤

1:18:14.900,1:18:18.760
對語音辨識結果影響很大，但 RNN 不知道

1:18:18.820,1:18:24.020
如果我們今天把 b 改成錯在這個地方

1:18:24.540,1:18:27.400
對最後語音辨識的錯的影響就很小

1:18:27.480,1:18:29.480
但是 RNN 不知道這件事情

1:18:29.760,1:18:33.760
所以對它來說，
在這邊放一個錯誤跟這邊放一個錯誤是一樣的

1:18:33.760,1:18:36.300
但是 RNN 認不出這一件事情來

1:18:36.300,1:18:39.620
你要讓 RNN 可以認出這件事情來

1:18:39.620,1:18:42.660
你需要加上一些 structural learning 的概念

1:18:42.660,1:18:44.960
才能夠做到

1:18:45.840,1:18:48.780
那在做 slot filling 的時候呢？

1:18:48.780,1:18:53.280
現在也很流行用 Bi-directional LSTM

1:18:53.280,1:18:58.100
再加上 CRF 或是 structured SVM

1:18:58.140,1:19:01.980
也就是說先用 Bi-directional LSTM 抽出 feature

1:19:01.980,1:19:05.680
再拿這些 feature 來定義

1:19:05.680,1:19:09.880
CRF 或者是 structured SVM 裡面，
我們需要用到的 feature

1:19:10.020,1:19:12.340
CRF 跟 structured SVM 都是 linear 的 model

1:19:12.340,1:19:14.900
你都要先抽 feature phi of x,y

1:19:14.900,1:19:17.380
然後 learn 一個 weight w

1:19:17.380,1:19:20.920
這個 phi of x,y 的 feature，
你不要直接從 raw 的 feature 來

1:19:21.060,1:19:23.920
你直接從 bi-directional RNN 的 output

1:19:23.920,1:19:25.800
可以得到比較好的結果

1:19:29.900,1:19:36.220
有人問說 structural learning 到底是否 practical

1:19:36.220,1:19:37.420
我們知道 structural learning

1:19:37.420,1:19:39.500
你需要解三個問題

1:19:39.560,1:19:43.660
那其中 inference 那個問題，往往是很困難的

1:19:43.960,1:19:46.820
你想想看 inference 那個問題，你需要 arg

1:19:46.920,1:19:50.360
你要窮舉所有的 y 看哪一個 y 可以讓你的值最大

1:19:50.360,1:19:53.040
你要解一個 optimization 的 problem

1:19:53.040,1:19:54.400
那這個 optimization 的 problem

1:19:54.400,1:19:55.260
很多時候

1:19:55.260,1:19:58.000
並不是所有的狀況都有好的解

1:19:58.000,1:20:00.560
應該說大部分的狀況都沒有好的 solution

1:20:00.560,1:20:03.820
sequence labeling 是少數有好的 solution 的狀況

1:20:04.100,1:20:07.160
但其他狀況，都沒有什麼好的 solution

1:20:07.180,1:20:13.480
所以好像會讓人覺得 structural learning，
它的用途沒那麼廣泛

1:20:13.480,1:20:17.020
但未來還未必是這樣子

1:20:23.840,1:20:27.200
事實上你想想看，我們之前講過的 GAN

1:20:27.200,1:20:30.680
我認為 GAN 就是一種 structural learning

1:20:31.060,1:20:37.420
如果你把 Discriminator 看作是 evaluation function

1:20:37.420,1:20:41.200
就是我們之前講的，在 structural learning 裡面

1:20:41.200,1:20:44.540
你有一個 problem 1，你要找出一個 evaluation function

1:20:44.640,1:20:50.000
這個 Discriminator，
我們就可以把它看作是 evaluation function

1:20:51.240,1:20:54.460
所以我們就知道 problem 1 要怎麼做

1:20:54.460,1:20:57.960
那最困難的 problem 2，
要解一個 inference 的問題

1:20:57.960,1:21:00.460
我們要窮舉所有我們未知的東西

1:21:00.460,1:21:04.600
看看誰可以讓我們的 evaluation function 最大

1:21:05.200,1:21:08.640
這一步往往很困難，因為 x 的可能性太多了

1:21:08.640,1:21:10.920
未知的東西可能性太多

1:21:11.400,1:21:15.960
但事實上這個東西它可以就是 Generator

1:21:16.220,1:21:20.200
我們可以想成 generator 它不是就是

1:21:20.200,1:21:23.900
給一個 noise，給一個從 Gaussian sample 出來的 noise

1:21:23.900,1:21:25.680
它就 output 一個 x 嗎？

1:21:25.680,1:21:28.240
output 一個 object 出來嗎？

1:21:28.240,1:21:30.120
它 output 的這個 object

1:21:30.120,1:21:33.980
不是就是可以讓 Discriminator 
分辨不出來那個 object 嗎？

1:21:34.480,1:21:38.060
如果 Discriminator 就是 evaluation function 的話

1:21:38.120,1:21:39.420
它 output 的那個 object

1:21:39.420,1:21:43.240
就是可以讓 evaluation function 的值很大的那個 object

1:21:43.240,1:21:47.460
所以這個 Generator 它其實就是在解這個問題

1:21:47.920,1:21:53.700
這個 generator 的 output，
其實就是這個 arg max 的 output

1:21:53.980,1:21:57.860
所以你可以把 Generator 
當作是在解 inference 的這個問題

1:21:58.000,1:21:59.720
那 problem 3 你已經知道了

1:21:59.760,1:22:02.800
我們怎麼 train GAN 就是 problem 3 的 solution

1:22:03.420,1:22:04.880
事實上 GAN 的 training

1:22:04.880,1:22:10.420
它跟 structured SVM 那些方法
的 training 你不覺得其實也有異曲同工之妙嗎？

1:22:10.640,1:22:13.840
大家還記得 structured SVM 是怎麼 train 的嗎？

1:22:13.840,1:22:20.820
在 structured SVM 的 training 裡面，
我們每次找出最 competitive 的那些 example

1:22:21.320,1:22:27.960
然後我們希望正確的 example，
它的 evaluation function 的分數

1:22:27.960,1:22:30.260
大過 competitive 的 example

1:22:30.260,1:22:34.480
然後 update 我們的 model，
然後再重新選 competitive 的 example

1:22:34.480,1:22:36.480
然後在讓正確的，大過 competitive

1:22:36.540,1:22:38.760
就這樣 iterative 去做

1:22:39.080,1:22:42.060
你不覺得 GAN 也是在做一樣的事情嗎？

1:22:42.440,1:22:48.260
GAN 的 training 是我們有正確的 example

1:22:48.260,1:22:49.320
就是這邊的 x

1:22:49.380,1:22:53.040
它應該要讓 evaluation function 的值，
比 Discriminator 的值大

1:22:53.400,1:22:58.960
然後我們每次用這個 Generator，Generate 出，
最competitive 的那個 x

1:22:58.960,1:23:02.260
也就是可以讓 Discriminator 的值最大的那個 x

1:23:02.260,1:23:04.040
然後再去 train Discriminator

1:23:04.040,1:23:08.280
Discriminator 要分辨正確的，real 的跟 Generated 的

1:23:08.280,1:23:11.580
也就是 Discriminator 要給 real 的 example 比較大的值

1:23:11.580,1:23:15.240
給那些 most competitive 的 x 比較小的值

1:23:15.380,1:23:19.800
然後這個 process 就不斷的 iterative 的進行下去

1:23:19.900,1:23:24.140
你會 update 你的 Discriminator 
然後 update 你的 Generator

1:23:24.140,1:23:25.760
然後再 update 你的 Discriminator

1:23:25.800,1:23:34.660
其實這個跟 Structured SVM 的
 training 是有異曲同工之妙的

1:23:35.160,1:23:37.380
那你可能會想說在 GAN 裡面

1:23:37.380,1:23:39.300
我們之前在講 structured SVM 的時候

1:23:39.300,1:23:42.960
都是有一個 input/output，有一個 x 有一個 y

1:23:42.960,1:23:46.380
那我們之前講的 GAN 只有 x

1:23:46.380,1:23:49.280
聽起來好像不太像

1:23:49.280,1:23:52.420
那我們就另外講一個像的，給你聽看看

1:23:52.700,1:23:57.100
其實 GAN 也可以是 conditional 的 GAN

1:23:57.200,1:23:59.100
什麼是 conditional 的 GAN 呢？

1:23:59.100,1:24:03.000
我今天的 example 都是 x,y 的 pair

1:24:03.200,1:24:07.980
我要解的任務是，是 given x 找出最有可能的 y

1:24:08.760,1:24:10.420
你就想成是做語音辨識

1:24:10.420,1:24:14.800
x 是聲音訊號，y 是辨識出來的文字

1:24:15.560,1:24:19.460
如果是用 conditional GAN 的概念，怎麼做呢？

1:24:19.620,1:24:25.280
你的 Generator input 一個 x，它就會 output 一個 y

1:24:25.700,1:24:30.020
Discriminator 它是去 check 一個 x,y 的 pair，
是不是對的

1:24:30.040,1:24:34.780
如果我們給它一個真正的 x,y pair，
它會給它一個比較高的分數

1:24:34.900,1:24:41.680
你給它一個 Generator output 出來的 y，
配上它的 input x，所產生一個假的 x,y pair

1:24:41.680,1:24:43.440
它會給它比較低的分數

1:24:43.520,1:24:47.440
training 的 process 就跟原來的 GAN 是一樣的

1:24:47.600,1:24:52.680
這個東西已經被成功應用在，
用文字產生 image 的 task 上

1:24:53.040,1:24:55.040
在用文字產生 image 的 task

1:24:55.600,1:25:01.580
比如說你跟 machine 說一句話說，
有一隻藍色的鳥，它就畫一張藍色的鳥的圖

1:25:02.180,1:25:10.560
這個 task 你的 input x 就是一句話，
output y 就是一張 image

1:25:10.560,1:25:14.260
那 Generator 做的事情，就是給它一句話，在圖上

1:25:22.260,1:25:26.120
給它一句話，它就產生一張 image

1:25:26.880,1:25:31.040
Discriminator 做的事情就是，
Discriminator 給它看一張 image

1:25:31.140,1:25:36.320
扔一句話，那它判斷說這個 x,y 的 pair
這個 image/sentence pair

1:25:36.320,1:25:39.640
他們是真的，還是不是真的

1:25:40.280,1:25:44.900
那如果你把 Discriminator 換成就是 evaluation function

1:25:45.020,1:25:48.920
把 Generator 換成就是解 inference 的那些 problems

1:25:48.980,1:25:53.120
其實 conditional GAN 跟 structured learning，
它們是可以類比的

1:25:53.280,1:25:59.780
或者你可以說 GAN 就是
 train structured learning 的 model 的一種方法

1:26:00.880,1:26:01.920
你可能覺得

1:26:02.020,1:26:05.260
這聽起來，或許你沒有聽得很懂，就算了

1:26:05.260,1:26:07.880
你可能覺得這只是我隨便講講的

1:26:08.020,1:26:11.540
但是我就想說，其他人也一定就想到了

1:26:11.540,1:26:15.160
所以，我就 google 一下其他人的 publication

1:26:15.160,1:26:19.720
果然，很多人都有類似的想法

1:26:19.720,1:26:23.440
GAN 可以跟 energy based model 做 connection

1:26:23.440,1:26:26.580
GAN 可以視為 train energy based model 的一種方法

1:26:26.700,1:26:29.220
所謂 energy based model，其實我們之前有講

1:26:29.220,1:26:33.540
它就是 structured learning 的另外一種稱呼

1:26:34.820,1:26:38.040
這邊有一系列的 paper 在講這件事

1:26:38.680,1:26:43.260
那你可能覺得說把 Generator 
視做是在做 inference 這件事情

1:26:43.260,1:26:47.120
是在解 arg max 這個問題，聽起來感覺很荒謬

1:26:47.120,1:26:50.100
其實也有人就是這麼想的

1:26:50.100,1:26:53.600
也有人想說，這邊也列一些 reference 給大家參考

1:26:53.640,1:27:01.400
也有人覺得說，一個 neural network ，
它有可能就是在解 arg max 這個 problem

1:27:01.700,1:27:09.820
所以也許 deep and structured 
就是未來一個研究的重點的方向

1:27:11.920,1:27:13.900
以下為其它課程資訊

1:27:14.220,1:30:05.000
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
