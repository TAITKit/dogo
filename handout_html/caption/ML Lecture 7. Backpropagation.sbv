0:00:00.000,0:00:06.120
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:06.120,0:00:08.640
各位同學，大家好

0:00:08.640,0:00:10.800
我們開始上課吧

0:00:10.800,0:00:13.980
我們今天要講的是 Backpropagation

0:00:13.980,0:00:17.800
也就是實際上，如果你要用 Gradient Descent 的方法

0:00:17.800,0:00:21.000
來 train 一個 neural network 的時候呢

0:00:21.000,0:00:24.140
你應該要怎麼做？

0:00:24.140,0:00:28.260
那我們上次其實已經講過了 neural network 的基本架構

0:00:28.260,0:00:29.920
那我就發現，在作業二裡面

0:00:29.920,0:00:36.060
好多人都已經 implement neural network 的 approach

0:00:36.060,0:00:39.260
那或許你已經對這個方法非常地清楚了

0:00:39.260,0:00:42.700
但是我不知道說，你是不是清楚說

0:00:42.700,0:00:46.180
到底實際上，在 train neural network 的時候

0:00:46.180,0:00:50.140
Backpropagation 這個 algorithm 是怎麼運作的

0:00:50.140,0:00:53.080
我們就來講一下 Backpropagation 這個 algorithm

0:00:53.080,0:00:56.800
是怎麼讓 neural network 的 training 變得比較有效率

0:00:57.260,0:00:59.700
那在 Gradient Descent 裡面

0:00:59.700,0:01:01.860
我們知道說 Gradient Descent 的方法就是

0:01:01.860,0:01:04.600
假設你的 network 有一大堆的參數

0:01:04.600,0:01:06.480
一堆 w，一堆 b

0:01:06.820,0:01:10.120
那你先選一個初始的參數

0:01:10.120,0:01:13.940
你先選一個初始的參數在哪裡

0:01:13.940,0:01:18.200
然後計算這個 θ^0 對你的 loss function 的 Gradient

0:01:18.200,0:01:21.360
也就是計算每一個

0:01:21.360,0:01:25.360
network 裡面的參數，w1, w2, b1, b2 ......等等

0:01:25.360,0:01:28.680
對你的 L(θ) 的偏微分

0:01:28.680,0:01:32.880
計算出這個東西以後，這個 gradient 其實是一個 vector

0:01:32.880,0:01:36.940
計算出這個 vector 以後，你就可以去更新你的參數

0:01:36.940,0:01:40.440
你就把 θ^0 減掉 learning rate

0:01:40.440,0:01:44.480
乘上 gradient，然後得到 θ^1

0:01:44.480,0:01:47.040
那這個 process 就持續繼續下去

0:01:47.040,0:01:49.620
再算一遍，在 θ^1 的 gradient

0:01:49.620,0:01:52.620
然後，再把 θ^1 減掉 gradient

0:01:52.620,0:01:56.280
update 成 θ^2，這個 process 就一直持續下去

0:01:56.280,0:01:58.220
所以，在 neural network 裡面呢

0:01:58.220,0:02:03.080
當你用 Gradient Descent 方法的時候，
跟我們在做 Logistic Regression

0:02:03.080,0:02:07.160
還有 Linear Regression 等等，是沒有太大的差別的

0:02:07.160,0:02:08.940
但是，最大的差別就是

0:02:08.940,0:02:11.200
最大的問題是，在 neural network 裡面

0:02:11.200,0:02:13.920
我們有非常非常多的參數

0:02:13.920,0:02:16.900
那現在，如果你要做語音辨識系統的話呢

0:02:16.900,0:02:20.140
你的 neural network 通常會有，7, 8 層

0:02:20.140,0:02:23.720
每層有 1000 個 neuron，它有上百萬個參數

0:02:23.720,0:02:25.680
所以，這個 vector 呢

0:02:25.680,0:02:27.380
它是非常非常長的

0:02:27.380,0:02:29.780
這是一個上百萬維的 vector

0:02:29.780,0:02:31.980
所以，現在最大的困難就是

0:02:31.980,0:02:35.760
你要如何有效地去把這一個百萬維的 vector

0:02:35.760,0:02:38.260
有效地把它計算出來

0:02:38.260,0:02:41.540
那這個就是 Backpropagation 在做的事情

0:02:41.540,0:02:44.280
所以，Backpropagation 並不是一個

0:02:44.280,0:02:47.180
和 Gradient Descent 不同的

0:02:47.180,0:02:50.020
training 的方法，它就是 Gradient Descent

0:02:50.020,0:02:53.400
它只是一個比較有效率的演算法

0:02:53.400,0:02:56.440
讓你在計算這個 gradient，這個 vector 的時候

0:02:56.440,0:02:59.700
是可以比較有效率地把這個 vector 計算出來

0:03:01.020,0:03:03.940
其實，Backpropagation 呢，裡面沒有

0:03:03.940,0:03:06.080
特別高深的數學

0:03:06.080,0:03:09.480
你唯一需要記得的就只有 Chain Rule

0:03:09.480,0:03:14.440
那我們用一張投影片，迅速地幫大家複習一下，
什麼是 Chain Rule

0:03:14.440,0:03:17.880
假設你現在有兩個 function，h 跟 g

0:03:17.880,0:03:21.340
g input x 就得到 y

0:03:21.340,0:03:23.440
h input y 就得到 z

0:03:23.440,0:03:26.800
所以，如果你今天給 x 一個小小的變化的話

0:03:26.800,0:03:30.800
給 x 一個變化的話，它會影響到它的 output y

0:03:30.800,0:03:32.860
所以，y 會跟這有變化

0:03:32.860,0:03:37.100
y 有變化以後，又會影響到 z，所以 z 會跟著有變化

0:03:37.100,0:03:41.620
如果我們今天要計算 dz/dx 的話

0:03:41.620,0:03:44.260
我們要計算 x 對 z 的微分的話

0:03:44.260,0:03:47.100
要怎麼算呢？你可以把它拆成兩項

0:03:47.100,0:03:49.660
x 對 z 的微分

0:03:49.660,0:03:54.540
它就等於 y 對 z 的微分

0:03:54.540,0:03:58.100
乘上 x 對 y 的微分

0:03:58.100,0:04:01.060
那這個怎麼來的，你就問一下微積分老師

0:04:01.060,0:04:03.700
你可以就把這個 y 消掉

0:04:03.700,0:04:07.300
所以左邊就等於右邊，
但不要讓你的微積分老師知道這件事情

0:04:09.380,0:04:11.500
那第二個 case

0:04:11.500,0:04:13.760
我們來看 Chain Rule 第二個 case

0:04:13.760,0:04:17.760
假設現在有 3 個 function，g, h, k

0:04:18.180,0:04:21.460
g input s 就得到 x

0:04:22.080,0:04:24.400
h input s 就得到 y

0:04:24.400,0:04:27.640
k input x, y 就得到 z

0:04:27.640,0:04:31.560
所以，今天假如你改變了 s

0:04:31.560,0:04:33.480
改變了 x，改變了 s

0:04:33.480,0:04:37.860
你會透過 g 和 h 這兩個 function 改變 x 跟 y

0:04:37.900,0:04:42.020
改變了 s，就改變了 x 跟 y

0:04:42.260,0:04:45.620
那改變了 x 跟 y 以後，透過 k 這個 function

0:04:45.620,0:04:48.340
k 這個 function，input x 跟 y，output 是 z

0:04:48.340,0:04:52.680
改變了 x 跟 y 以後，你就改變了 z

0:04:52.840,0:04:59.360
所以，今天如果你要計算 s 對 z 的微分的話

0:04:59.720,0:05:04.460
那這個 s，是透過兩條路徑去影響 z

0:05:04.460,0:05:08.640
它可以透過 x 去影響 z，也可以透過 y 去影響 z

0:05:08.640,0:05:11.320
所以，s 對 z 的微分

0:05:11.320,0:05:14.080
就可以寫成，就拆成兩項

0:05:14.080,0:05:17.180
拆成兩項，根據這兩條 path 拆成兩項

0:05:17.180,0:05:19.520
這個 s 對 z 的微分

0:05:19.520,0:05:23.560
就可以寫成 x 對 z 的微分

0:05:23.560,0:05:26.820
乘上 s 對 x 的微分，就是上面這條路徑

0:05:26.820,0:05:31.260
加上 y 對 z 的微分乘上 s 對 y 的微分，也就是

0:05:31.260,0:05:32.820
下面這條路徑

0:05:32.820,0:05:35.580
所以，你大一微積分有好好學的話呢

0:05:35.580,0:05:38.320
這個就是我們都學過的 Chain Rule

0:05:38.320,0:05:40.760
那我們等一下呢，會需要用到這個東西

0:05:41.880,0:05:45.220
那再回到這個 neural network 的 training

0:05:45.580,0:05:49.680
我們知道說，我們會定一個 loss function

0:05:50.020,0:05:51.980
那這個 loss function 是甚麼呢？

0:05:51.980,0:05:55.120
這個 loss function 是 summation over

0:05:55.120,0:05:57.840
所有 training data 的

0:05:58.120,0:06:02.120
某一個 loss 值，C^n

0:06:02.120,0:06:06.460
我們說，假設給定我們一組 neural network 的參數 θ

0:06:06.460,0:06:09.120
我們把一個 training data x^n

0:06:09.120,0:06:12.560
代到這個 neural network 裡面，它會 output 一個 y^n

0:06:13.040,0:06:15.540
那同時呢，我們會有一個

0:06:15.540,0:06:18.720
我們希望這個 neural network 的 output：y^n\head

0:06:18.720,0:06:21.480
它希望它如果 output y^n\head 的話

0:06:21.480,0:06:23.800
就是最正確的，那我們會

0:06:23.800,0:06:28.760
定義一個 y^n 跟 y^n\head 之間距離的 function

0:06:28.760,0:06:30.300
這邊寫作 C^n

0:06:30.300,0:06:32.680
C^n 代表 y^n 跟 y^n\head 之間的距離

0:06:32.680,0:06:36.400
如果 C^n 大的話，代表 y^n 跟 y^n\head 之間的距離很遠

0:06:36.400,0:06:39.100
所以，這個 network 的 parameter 的 loss

0:06:39.100,0:06:40.800
是比較大的，是比較不好的

0:06:40.800,0:06:43.580
那如果這個 C^n 很小的話，代表

0:06:43.580,0:06:45.540
這個 parameter 是好的

0:06:45.540,0:06:49.260
那我們 summation over 所有 training data 的 C^n

0:06:49.260,0:06:51.660
summation over 所有 training data

0:06:51.660,0:06:53.840
根據這個參數 θ

0:06:53.840,0:06:55.640
它的 output 跟它的目標

0:06:55.640,0:06:58.880
它的 output y^n 跟它的目標 y^n\head 之間的距離

0:06:58.880,0:07:02.080
就是得到我們的 total loss，L

0:07:02.080,0:07:03.980
那你把這個式子

0:07:03.980,0:07:08.300
左右兩邊都對某一個參數 w 做偏微分的話呢

0:07:08.300,0:07:10.220
你就得到右邊這個式子

0:07:10.220,0:07:12.820
你就得到 ∂L/∂w 等於

0:07:12.820,0:07:14.940
summation over 所有的 training data

0:07:14.940,0:07:16.060
n = 1 到 N

0:07:16.060,0:07:18.840
∂C^n (θ)/∂w

0:07:18.840,0:07:21.380
這個應該是沒有甚麼問題

0:07:21.380,0:07:23.740
之所以寫這個式子，只是要講說

0:07:23.740,0:07:25.260
接下來，我們就不用

0:07:25.260,0:07:28.340
計算 ∂L/∂w

0:07:28.340,0:07:30.900
我們就只考慮，我們如何去計算

0:07:30.900,0:07:34.620
對某一筆 data 的 ∂C^n (θ)/∂w

0:07:34.620,0:07:38.740
你只要能夠把一筆 data 的

0:07:38.740,0:07:41.840
∂C^n (θ)/∂w 算出來

0:07:41.840,0:07:44.420
再 summation over 所有的 training data

0:07:44.420,0:07:48.820
你就可以把 total loss 對某一個參數的

0:07:48.820,0:07:50.400
偏微分算出來了

0:07:50.400,0:07:53.020
所以，我們等一下就只 focus 在怎麼計算

0:07:53.020,0:07:57.700
對某一筆 data，它的 cost C^n 對 w 的偏微分

0:07:57.700,0:08:00.240
我們就只 focus 在怎麼計算這一項上面

0:08:01.580,0:08:03.160
怎麼做呢？

0:08:03.600,0:08:07.000
我們先考慮某一個 neuron

0:08:07.000,0:08:09.820
我們先從底下這個 neural network 裡面

0:08:09.820,0:08:12.880
拿一個 neuron 出來，考慮它

0:08:14.440,0:08:19.300
那這一個 neuron，它是在第一個 layer 的 neuron

0:08:19.300,0:08:21.020
所以，它前面的 input

0:08:21.020,0:08:23.460
就是外界給它的 input，x1, x2

0:08:23.460,0:08:29.120
假設它只有兩個 input，x1 跟 x2

0:08:29.120,0:08:32.980
分別乘上 weight w1, w2，再加上 b

0:08:32.980,0:08:34.580
會得到 z

0:08:34.580,0:08:37.940
這個我想大家應該都非常熟悉，這個 z 呢

0:08:37.940,0:08:41.140
就是 x1w1 + x2w2 + b

0:08:41.540,0:08:43.880
那得到這個 z 以後，通過了 activation function

0:08:43.880,0:08:46.120
再經過了非常非常多的事情以後

0:08:46.120,0:08:48.640
你會得到最終的 output，y1, y2

0:08:48.640,0:08:51.280
那現在的問題是這樣

0:08:51.280,0:08:53.460
假設我們從這邊拿一個 w 出來

0:08:53.460,0:08:55.820
等一下，我們就拿 w 當作 z

0:08:55.820,0:08:58.960
但是 b 也是一樣，就拿 weight 當作 z

0:08:58.960,0:09:02.320
來看怎麼計算，某一個 weight 對 cost

0:09:02.320,0:09:05.100
對 example 的某一個 cost 的偏微分

0:09:05.100,0:09:09.240
那 b 的話，想必你可以以此類推，就把它算出來

0:09:09.240,0:09:14.260
那 ∂C/∂w 怎麼算？

0:09:14.500,0:09:16.840
這個 ∂C/∂w 阿

0:09:16.840,0:09:19.700
按照 Chain Rule，你就可以把它拆成兩項

0:09:19.700,0:09:23.200
∂z/∂w * ∂C/∂z

0:09:23.200,0:09:25.720
這個 z 可以把它消掉，沒有問題

0:09:25.720,0:09:30.960
所以 ∂C/∂w 可以根據 Chain Rule 拆成兩項

0:09:31.680,0:09:34.680
那這兩項，我們就分別去把它計算出來

0:09:34.680,0:09:36.440
前面這項是很簡單的

0:09:36.440,0:09:40.760
後面這項，是比較複雜的

0:09:40.760,0:09:43.520
那計算前面這一項

0:09:43.520,0:09:46.960
計算 ∂z/∂w 的這個 process 呢

0:09:46.960,0:09:48.840
我們稱之為 Forward pass

0:09:48.840,0:09:51.160
那你等下會知道說為什麼叫 Forward pass

0:09:51.160,0:09:55.580
那計算前面這一項 ∂C/∂z 的 process

0:09:55.580,0:09:58.640
我們稱之為 Backward pass

0:09:58.660,0:10:02.420
那我們等一下會講說，為什麼叫做 Backward pass

0:10:02.780,0:10:07.400
那我們就先看一下，怎麼來計算這個  ∂z/∂w

0:10:07.400,0:10:09.960
怎麼來計算  ∂z/∂w

0:10:09.960,0:10:12.760
好，那我們先看這個 w1

0:10:12.760,0:10:15.560
那你怎麼計算 ∂z/∂w1 呢？

0:10:15.560,0:10:17.420
就是秒算

0:10:17.420,0:10:19.700
就是秒算，因為 z 就長這個樣子嘛

0:10:19.700,0:10:21.960
然後，w1 在這邊

0:10:21.960,0:10:24.780
所以，一眼就可以知道說，它是 x1

0:10:24.780,0:10:27.360
那 ∂z/∂w2 呢？

0:10:27.360,0:10:29.360
所以，你一眼就可以看出說

0:10:29.360,0:10:33.540
它就是 x2，這個都是秒算這樣子

0:10:33.540,0:10:36.620
那它的規律是這樣，它的規律就是

0:10:36.620,0:10:42.520
∂z/∂w，就是看這個 w 前面接的東西是什麼

0:10:42.520,0:10:44.820
那微分以後就是什麼

0:10:44.820,0:10:48.920
這個 w1 前面，它的 input 是接 x1

0:10:48.920,0:10:51.740
它的 input 是 x1，所以微分以後就是 x1

0:10:51.740,0:10:55.160
w2 呢，它前面的 input 是 x2

0:10:55.160,0:10:57.820
所以微分以後就是 x2

0:10:57.820,0:10:59.540
那它的規律就是這個樣子

0:10:59.540,0:11:03.780
所以，今天假如給你一個 neural network

0:11:03.780,0:11:07.180
那它裡面有一大推的參數

0:11:07.180,0:11:09.480
一大堆的參數，但是你要

0:11:09.480,0:11:11.780
計算這裡面每一個參數

0:11:11.780,0:11:14.580
對 z 的偏微分

0:11:14.580,0:11:18.920
你要計算這裡面每一個參數的 ∂w 跟 ∂z

0:11:18.920,0:11:21.700
這件事情呢，非常非常的容易

0:11:21.700,0:11:23.660
因為我們剛才知道，它的規律就是

0:11:23.660,0:11:28.160
∂z/∂w 就是看你這個 w 的 input 是什麼，它就是甚麼

0:11:28.160,0:11:31.620
所以，如果有人問你說，現在 input 是 1 跟 -1

0:11:31.620,0:11:34.020
那這個 1，它對

0:11:34.020,0:11:37.160
它的 activation function 的 input z 的偏微分是什麼呢？

0:11:37.160,0:11:41.480
你就可以瞬間回答它說，就是 -1

0:11:41.480,0:11:44.000
因為這個 1，前面接 -1

0:11:44.000,0:11:48.920
所以，這個參數對 z 的偏微分就是 -1

0:11:48.920,0:11:52.700
同理，比如說這個 -1，它對 z 的偏微分就是 1

0:11:52.700,0:11:54.980
這個 -2，它對 z 的偏微分就是 1

0:11:54.980,0:11:58.120
這個 1，它對 z 的偏微分就是 1，以此類推

0:11:58.120,0:12:00.640
接下來呢

0:12:00.640,0:12:04.100
接下來假如有人問你說，這個 w

0:12:04.100,0:12:08.280
對它的 activation function 的 input z 的偏微分是什麼呢？

0:12:08.280,0:12:10.640
你其實也可以瞬間就回答它

0:12:10.640,0:12:14.220
你只要知道說，這個 w 前面接的 weight

0:12:14.220,0:12:15.940
前面接的 input 是什麼

0:12:15.940,0:12:18.820
那這個 w 前面接的 input 是

0:12:18.820,0:12:21.240
某一個 neuron 的 output，對不對？

0:12:21.240,0:12:22.900
這個 w 前面接的 input

0:12:22.900,0:12:25.440
是第一個 hidden layer 的 neuron 的 output

0:12:25.440,0:12:27.800
那這個 hidden layer 的 neuron 的 output 要怎麼算呢？

0:12:27.800,0:12:30.640
這個大家都知道，對不對？就是把

0:12:30.640,0:12:35.160
1 跟 -1 丟進去，然後根據我們熟悉的 neuron 的運算

0:12:35.160,0:12:37.620
然後看看它的 output 是什麼，就是什麼

0:12:37.620,0:12:39.600
在這個例子裡面呢

0:12:39.600,0:12:41.880
假如這個 function 是 sigmoid function

0:12:41.880,0:12:44.560
算出來是 0.98, 0.12

0:12:44.560,0:12:47.400
如果你可以算出這兩個 neuron 的 output 是

0:12:47.400,0:12:49.240
0.98 跟 0.12 的話

0:12:49.240,0:12:50.860
那這個 weight

0:12:50.860,0:12:52.900
它做完偏微分以後

0:12:52.900,0:12:54.240
這個 weight 對

0:12:54.240,0:12:57.320
它的 activation function 的 input z 做完偏微分以後

0:12:57.320,0:13:01.560
顯然就是 0.12，因為它前面接的 weight 就是 0.12

0:13:01.560,0:13:03.060
這個 -1 也是 0.12

0:13:03.060,0:13:06.160
這個 -2 是 0.98，這個 2 是 0.98

0:13:06.160,0:13:07.700
這個也很直覺

0:13:07.700,0:13:11.540
所以，同樣的 process 你就反覆的在做

0:13:11.540,0:13:15.160
你可以得到這兩個紅色 neuron 的 output 是 0.86, 0.11

0:13:15.160,0:13:18.420
那你就可以秒反應說，這個

0:13:18.420,0:13:21.720
4 對 z 的偏微分就是 0.11

0:13:21.720,0:13:25.500
所以，你要算出這個 neural network 裡面的

0:13:25.500,0:13:27.880
每一個 weight

0:13:27.880,0:13:31.420
對它的 activation function 的 input z 的偏微分

0:13:31.420,0:13:34.520
你就把你的 input 丟進去

0:13:34.520,0:13:38.420
然後，計算每一個 neuron 的 output

0:13:38.420,0:13:40.180
就結束了

0:13:40.180,0:13:43.120
所以，這個步驟叫做 Forward pass

0:13:43.120,0:13:45.000
它是非常容易理解的

0:13:45.540,0:13:48.340
再來，我們要講的是 Backward pass

0:13:48.340,0:13:52.980
也就是怎麼算 ∂C/∂z

0:13:52.980,0:13:55.820
這個你就不會覺得很困難了

0:13:55.820,0:13:58.440
因為，這個 z 阿

0:13:58.440,0:14:01.080
它通過 activation function 以後得到 output

0:14:01.080,0:14:03.840
後面還有非常非常複雜的 process

0:14:03.840,0:14:07.220
它才得到這個 C

0:14:07.220,0:14:10.120
要經過非常複雜的 process 才能得到 C

0:14:10.120,0:14:14.100
這個 ∂C/∂z 顯然是很複雜的

0:14:14.100,0:14:19.220
不過我們可以用 Chain rule 再把這一項做一下拆解

0:14:19.220,0:14:23.040
假設這個 activation function 是 sigmoid function

0:14:23.040,0:14:24.900
我這邊就寫一個 σ(z)

0:14:24.900,0:14:27.840
z 通過 sigmoid function 得到 a

0:14:27.840,0:14:29.640
這個 neuron 的 output 是 a

0:14:30.360,0:14:32.080
那接下來會發生甚麼事呢？

0:14:32.080,0:14:36.120
這個 a 會通過某一個 weight，乘上某一個 weight

0:14:36.120,0:14:39.000
再加其他一大堆的 value，得到 z'

0:14:39.000,0:14:42.940
它是下一個 neuron activation function 的 input

0:14:42.940,0:14:47.160
這個 a 會再乘上另一個 weight，這邊寫成 w4

0:14:47.160,0:14:51.280
再加上其他一大堆東西，得到 z"

0:14:51.280,0:14:54.520
後面這個 z' 跟 z"

0:14:54.520,0:14:56.640
之後可能還會發生很多很多的事情

0:14:56.640,0:15:00.140
不過我們就先只考慮下一步會發生什麼事情

0:15:00.620,0:15:02.920
所以呢

0:15:02.920,0:15:06.780
我們知道說 ∂C/∂z

0:15:06.780,0:15:11.580
你可以寫成 ∂a/∂z * ∂C/∂a

0:15:11.580,0:15:15.040
那這個就沒有什麼問題，∂a 可以消掉

0:15:15.040,0:15:18.240
那 ∂a/∂z 是什麼呢？

0:15:18.240,0:15:21.680
我們知道說，a = σ(z)

0:15:21.680,0:15:24.740
那這個 ∂a/∂z 呢

0:15:24.740,0:15:28.240
其實就是這個 sigmoid function 的微分

0:15:28.240,0:15:31.160
那 sigmoid function 長這個樣子

0:15:31.160,0:15:33.580
長這個樣子，綠色這條線

0:15:33.580,0:15:37.600
那它的微分，你就算一下，長這個樣子

0:15:37.600,0:15:40.940
長這樣子，我們之前已經看過很多次了

0:15:41.200,0:15:44.100
所以，∂a/∂z 也沒有問題

0:15:44.420,0:15:48.620
接下來的問題就是，∂C/∂a

0:15:49.400,0:15:51.320
應該長甚麼樣子呢？

0:15:51.320,0:15:54.340
它應該長甚麼樣子呢？

0:15:55.360,0:15:58.600
那我們就接下來看說，這個 a

0:15:58.600,0:16:01.640
∂a 跟 C 的關係是怎樣

0:16:01.640,0:16:04.740
你知道 a 它會影響 z'

0:16:05.260,0:16:07.600
然後，z' 會影響 z

0:16:07.760,0:16:11.860
a 會影響 z"，z" 會影響 C

0:16:12.120,0:16:16.140
a 透過 z' 跟 z" 去影響 C

0:16:16.140,0:16:19.680
所以，∂C/∂a 你可以寫成

0:16:19.680,0:16:23.940
∂z'/∂a * ∂C/∂z'

0:16:23.940,0:16:29.340
加上 ∂z"/∂a * ∂C/∂z"

0:16:29.340,0:16:34.680
我們假設 a 後面，就是這個藍色 neuron 的下一個 layer

0:16:34.680,0:16:36.840
就是紅色的 neuron 只有兩個

0:16:36.840,0:16:38.720
所以，這邊就只有兩項

0:16:38.720,0:16:41.440
這個 a 只會影響 z' 跟 z"

0:16:41.440,0:16:43.780
如果這邊有 1000 個 neuron 的話

0:16:43.780,0:16:45.100
那這邊這個 Chain rule

0:16:45.100,0:16:49.140
你的 summation 就是 summation over 1000 項

0:16:49.140,0:16:51.880
這樣大家了解我的意思嗎？

0:16:52.600,0:16:57.980
這邊呢，經過前面我們簡化上課的說明

0:16:57.980,0:17:00.780
我們假設只有兩個 neuron，這邊只有兩項

0:17:00.780,0:17:03.840
只有兩項，a 只會影響 z' 跟 z"

0:17:03.840,0:17:09.020
接下來，∂z"/∂a

0:17:09.640,0:17:15.060
會算嗎？這個就是秒算，對不對？這個就是 w3

0:17:15.440,0:17:21.960
z' 等於 a 乘上 w3，再加上一些有的沒的東西

0:17:21.960,0:17:26.040
所以，這個 z' 對 a 做偏微分

0:17:26.040,0:17:29.020
根據這個式子，顯然就是 w3

0:17:29.280,0:17:33.540
所以，同理，這個 z" 對 a 做偏微分

0:17:33.540,0:17:36.400
得到的結果顯然就是 w4

0:17:36.400,0:17:39.240
所以，這兩項算起來，也不是個問題

0:17:39.780,0:17:45.160
最後的問題就是，z" 對 C 的偏微分怎麼算呢？

0:17:45.160,0:17:49.400
這個 z" 對 C 的偏微分怎麼算呢？

0:17:49.400,0:17:51.480
因為我們不知道

0:17:51.480,0:17:53.740
z' 對 C 有什麼關係

0:17:53.740,0:17:55.340
z" 對 C 有什麼關係

0:17:55.340,0:17:58.160
這後面還有發生很多很多的事情

0:17:58.160,0:18:01.680
是很複雜的，所以我們搞不清楚後面會發生什麼事情

0:18:01.680,0:18:04.540
所以，我們一下子不知道這兩項怎麼算

0:18:04.540,0:18:07.540
不過沒關係，我們就假設我們知道

0:18:07.540,0:18:12.260
假設這兩項的值，我們已經 somehow 透過

0:18:12.260,0:18:14.240
某種方法把它算出來

0:18:14.240,0:18:18.160
我們透過一個等一下會講，但你還不知道怎麼做的方法

0:18:18.160,0:18:20.100
就已經把這兩項做出來了

0:18:20.100,0:18:23.100
那把這兩項算出來以後

0:18:23.620,0:18:25.920
把這兩項算出來以後呢

0:18:26.740,0:18:31.160
我們就可以把 ∂C/∂z

0:18:31.160,0:18:33.560
輕易的算出來

0:18:33.560,0:18:37.300
把這兩項算出來以後

0:18:37.300,0:18:40.680
我們就可以算 ∂C/∂z

0:18:40.680,0:18:44.660
你會算 ∂C/∂z' 跟 ∂C/∂z"

0:18:44.660,0:18:46.960
你就會算 ∂C/∂z

0:18:47.440,0:18:50.400
然後，再把這些值

0:18:50.400,0:18:53.220
代到我們剛才看到的 ∂C/∂z 的式子裡面

0:18:53.220,0:19:01.740
就得到這樣一個式子，σ'(z) * [w3 * ∂C/∂z' + w4 * ∂C/∂z"]

0:19:01.740,0:19:04.620
你就算出這樣一個式子，那這個式子

0:19:04.620,0:19:06.080
還滿簡單的

0:19:06.080,0:19:09.160
但是，我們會從另外一個觀點，來看待這個式子

0:19:09.740,0:19:12.680
你可以想像說

0:19:12.680,0:19:17.360
現在有另外一個 neuron 

0:19:17.360,0:19:21.020
這個 neuron 並不在我們原來的 network 裡面

0:19:21.020,0:19:23.660
有另外一個 neuron 

0:19:23.660,0:19:26.100
我把它簡化成這個三角形

0:19:26.100,0:19:27.600
把它畫成三角形

0:19:27.600,0:19:35.200
那這個 neuron 的 input，就是 ∂C/∂z' 跟 ∂C/∂z"

0:19:35.200,0:19:39.660
那第一個 input ∂C/∂z'，就乘上 w3

0:19:39.660,0:19:42.720
∂C/∂z" 它就乘上 w4

0:19:42.720,0:19:46.840
再乘上 activation function， σ'(z)

0:19:46.840,0:19:50.560
得到 output，就是 ∂C/∂z

0:19:50.560,0:19:54.660
上面這個 neuron 所做的運算

0:19:54.660,0:19:57.580
跟下面這個式子，是一模一樣的

0:19:57.580,0:19:59.760
我們只是把下面這個式子

0:19:59.760,0:20:07.740
把它畫出來，讓它看起來像是一個 neuron 一樣

0:20:08.220,0:20:11.820
那這個 σ'(z) 阿

0:20:11.820,0:20:14.840
這個 σ'(z) 其實是一個常數

0:20:14.840,0:20:20.420
對不對，它不是一個 function，它是一個constant

0:20:20.420,0:20:25.340
因為，z 其實在計算 Forward pass 的時候

0:20:25.340,0:20:27.160
就被決定好了

0:20:27.160,0:20:30.900
z 是一個已經固定的值

0:20:30.900,0:20:33.800
z 我們已經知道它是多少，所以

0:20:33.800,0:20:35.860
在給定 z 的情況下

0:20:35.860,0:20:39.720
這個 σ'(z)，它就是一個常數

0:20:39.720,0:20:42.100
所以，這個 neuron

0:20:42.100,0:20:44.780
跟我們之前看到的 sigmoid function 是不一樣的

0:20:44.780,0:20:49.260
它並不是把 input 通過一個 non-linear 的轉換

0:20:49.260,0:20:53.260
而是直接把 input 乘上一個 constant，σ'(z)

0:20:53.260,0:20:55.300
就得到一個 output 這樣

0:20:55.300,0:20:58.260
所以，我把這個 neuron 畫成三角形的

0:20:58.260,0:21:01.200
代表它跟我們之前看到的圓形 neuron

0:21:01.200,0:21:04.000
的運作方式是不一樣的，它是直接乘上一個 constant

0:21:04.000,0:21:06.280
那你可能會問說，為甚麼是三角形呢

0:21:06.280,0:21:09.800
因為我是電機系的，我覺得這是一個 op-amp 這樣子

0:21:09.800,0:21:16.080
op-amp 就會乘上一個 constant，它是一個放大器這樣

0:21:16.400,0:21:18.740
聽不懂就算了，這不太重要

0:21:19.360,0:21:25.560
然後，這樣問題都解決了

0:21:25.560,0:21:29.200
都解決了，對不對，我們現在最後的問題就只有

0:21:29.200,0:21:31.960
怎麼算這兩項而已

0:21:31.960,0:21:34.880
假設能夠算這兩項，問題也就都解決了

0:21:35.560,0:21:37.740
那現在怎麼算這兩項呢？

0:21:37.740,0:21:39.040
怎麼算這兩項呢？

0:21:39.040,0:21:42.520
我們現在假設兩個不同的 case

0:21:42.520,0:21:47.640
第一個 case 是，我們假設現在紅色的這兩個 neuron

0:21:47.640,0:21:50.240
就已經是 output layer

0:21:50.240,0:21:53.020
這兩個紅色 neuron 是在 output layer 裡面

0:21:53.020,0:21:56.240
它們的 output 就已經是整個 network 的 output 了

0:21:56.240,0:21:58.560
這邊寫成 y1, y2

0:21:58.560,0:22:01.060
它的 output 就已經是整個 network 的 output 了

0:22:01.540,0:22:05.400
所以，今天你要算 ∂C/∂z'

0:22:05.400,0:22:11.060
就很簡單，根據 Chain rule 算 ∂(y1)/∂z' * ∂C/∂(y1)

0:22:11.060,0:22:14.420
∂(y1)/∂z' 沒什麼問題

0:22:14.420,0:22:17.520
你只要知道這個 activation function 長甚麼樣子

0:22:17.520,0:22:19.660
這項就輕而易舉地算出來了

0:22:19.660,0:22:26.060
∂C/∂(y1)，y1 對 C 有什麼影響，depend on 你的

0:22:26.060,0:22:28.520
你的 cost function 是怎麼定義的

0:22:28.520,0:22:31.820
你的 output 跟 target 間是怎麼 evaluate 的

0:22:31.820,0:22:34.440
你可以用 cross entropy，你可以用 mean square error

0:22:34.440,0:22:37.560
你用不同的定義，這邊這項就不一樣，但總之

0:22:37.560,0:22:41.620
它是一個比較簡單的東西，你可以把它算出來

0:22:42.740,0:22:49.340
那 ∂C/∂z"，這你也可以算，沒有甚麼問題

0:22:49.340,0:22:52.720
就是 ∂(y2)/∂z" * ∂C/∂(y2)

0:22:52.720,0:22:55.860
這兩項一樣都是可以秒算

0:22:55.860,0:23:03.180
所以，今天假設這個藍色的 neuron 後面

0:23:03.180,0:23:07.240
它的下一個 layer 就已經是 output layer 了

0:23:07.240,0:23:14.460
這個藍色的 neuron，它在最後一個 hidden layer 裡面

0:23:14.460,0:23:16.420
它後面就已經是 output layer 了

0:23:16.420,0:23:19.560
那根據我們剛才所學的東西

0:23:19.560,0:23:26.020
你就結束了，你就可以把 w1 跟 w2 對 C 的偏微分算出來

0:23:26.020,0:23:28.020
所以，這個沒有甚麼問題

0:23:28.020,0:23:30.940
那我們真正煩惱的問題是 case 2

0:23:30.940,0:23:34.780
假設現在紅色的 neuron 它並不是整個 network 的 output

0:23:34.780,0:23:40.300
它後面還有其他東西的話，怎麼辦呢？

0:23:40.300,0:23:44.320
那它後面的其他東西，可能長甚麼樣子呢

0:23:44.320,0:23:49.600
它可能長這樣，就是 z' 再通過 activation function 得到 a'

0:23:49.600,0:23:52.000
再乘上另外一個 weight，w5

0:23:52.000,0:23:55.900
再加上一些其他的東西，得到 za

0:23:55.900,0:23:59.160
然後，你再把 a' 乘上

0:23:59.160,0:24:01.540
w6 再加上其他一大堆東西

0:24:01.540,0:24:05.720
得到 zb，然後再丟進另外兩個

0:24:05.720,0:24:07.640
activation function 裡面

0:24:09.500,0:24:11.940
那現在的問題是這樣

0:24:11.940,0:24:15.380
我們想要求 ∂C/∂z'

0:24:15.760,0:24:23.820
如果我們知道 ∂C/∂(za) 跟 ∂C/∂(zb)

0:24:23.820,0:24:29.720
我們就可以計算 ∂C/∂z'

0:24:29.720,0:24:30.560
對嗎？

0:24:31.080,0:24:35.720
我們剛才已經有講過說，假設我們知道

0:24:35.720,0:24:39.800
∂C/∂z' 跟 ∂C/∂z"

0:24:39.800,0:24:43.620
我們就可以算前面一個 layer 的 ∂C/∂z

0:24:43.620,0:24:45.600
所以，按照一模一樣的式子

0:24:45.600,0:24:49.840
如果知道 ∂C/∂(za) 跟 ∂C/∂(zb)

0:24:49.840,0:24:52.740
我們就可以算 ∂C/∂z'

0:24:52.740,0:24:54.560
按照一模一樣的式子

0:24:54.560,0:24:57.520
就是我們剛才算看到那個 op-amp 的式子

0:24:57.520,0:25:02.040
所以，你就把 ∂C/∂(za) 乘上 w5

0:25:02.040,0:25:07.120
∂C/∂(zb) 乘上 w6，加起來再通過 op-amp

0:25:07.160,0:25:10.580
乘上 σ'(z')

0:25:10.580,0:25:15.640
再乘上 op-amp 就得到這個 ∂C/∂z' 的 output

0:25:17.900,0:25:21.440
那現在這個問題，就反覆地繼續下去

0:25:21.440,0:25:25.340
我們剛才說知道 z' 跟 z" 的偏微分就可以算 z

0:25:25.340,0:25:30.920
現在知道 za 跟 zb 的偏微分就可以算 z'

0:25:30.920,0:25:35.160
但是，我們又不知道 za 跟 zb 的偏微分

0:25:35.160,0:25:36.800
怎麼算，對不對？

0:25:36.800,0:25:38.660
你不知道這兩項怎麼算

0:25:38.660,0:25:41.620
如果你會這兩項的話，你就把這一項算出來

0:25:41.620,0:25:43.620
但問題就是，你不知道

0:25:43.620,0:25:46.400
那怎麼辦呢？

0:25:46.400,0:25:49.560
我們就再往下一層去看

0:25:49.560,0:25:53.780
了解嗎？就是這個綠色的 neuron

0:25:53.780,0:25:56.620
如果它是 output layer 的話

0:25:56.620,0:25:59.280
如果這個綠色的 neuron，它是 output layer 的話

0:25:59.280,0:26:03.480
要計算這兩個東西，就是秒算，沒有問題

0:26:03.700,0:26:06.240
假設它不是 output layer 的話

0:26:06.240,0:26:09.620
你就繼續走下去

0:26:09.620,0:26:11.400
再看下一個 layer

0:26:11.400,0:26:14.340
它的 activation function input 對 C 的偏微分

0:26:14.340,0:26:16.680
那你就可以把這一項算出來

0:26:16.960,0:26:21.160
你就可以把這兩項算出來，再把這一項算出來

0:26:21.500,0:26:25.860
你如果沒辦法算這兩項，他們不是 output layer 的話

0:26:25.860,0:26:29.000
你就再去推，下一個 layer 它的偏微分會是甚 麼樣子

0:26:29.000,0:26:33.680
把這兩個東西推出來，然後再往前把這兩個東西推出來

0:26:33.860,0:26:38.540
那你可能會想說，這個方法聽起來還頗崩潰

0:26:38.540,0:26:42.040
就是你每次要算一個微分的值

0:26:42.040,0:26:46.620
你要一直往後走，走到 network 的 output

0:26:46.620,0:26:51.660
如果 network 有 10 層，那你從第一層開始往後展開

0:26:51.660,0:26:54.580
感覺應該是一個很可怕的式子

0:26:55.160,0:26:57.960
但是實際上，並不是這樣子做的

0:26:57.960,0:27:00.840
實際上，你只要換一個方向

0:27:00.840,0:27:06.620
從 output layer 的 ∂C/∂z 開始算

0:27:06.620,0:27:09.140
你就會發現說，它的運算量

0:27:09.140,0:27:12.800
跟原來的 network 的 Feedforward path

0:27:12.800,0:27:15.440
其實是一樣的

0:27:15.440,0:27:18.480
假設我們現在有 6 個 neuron

0:27:18.720,0:27:22.280
每一個 neuron，它的 activation function

0:27:22.280,0:27:25.420
input 分別是 z1, z2, z3 一直到 z6

0:27:25.420,0:27:31.340
我們現在要計算這些 z 對 C 的偏微分

0:27:31.340,0:27:36.020
那本來呢，我們應該是想要知道 z1 的偏微分

0:27:36.020,0:27:39.540
你就要算 z3, z4 的偏微分

0:27:39.540,0:27:43.740
假如想知道 z3 的偏微分，你就要算 z5 跟 z6 的偏微分

0:27:43.740,0:27:47.320
想要知道 z4 的偏微分，你就要算 z5 跟 z6 的偏微分

0:27:47.320,0:27:50.160
那你要先算出 z5, z6 的偏微分，你才能算出

0:27:50.160,0:27:52.080
z3 的偏微分，z4 的偏微分

0:27:52.080,0:27:55.080
你才能夠算出 z1 的偏微分，z2 的偏微分

0:27:55.520,0:27:58.000
那如果我們今天是從

0:27:58.000,0:28:00.140
z1, z2 的偏微分開始算

0:28:00.140,0:28:01.880
那就沒有效率

0:28:01.880,0:28:06.260
但是，如果你反過來先算 z5, z6 的偏微分的話

0:28:06.260,0:28:09.660
這個 process，就突然之間變得有效率起來了

0:28:09.660,0:28:15.000
我們就先算 z5, z6 對 C 的偏微分

0:28:15.340,0:28:17.220
然後，算出這兩項以後

0:28:17.220,0:28:21.420
你就可以算出 z3, z4 對 C 的偏微分

0:28:21.420,0:28:26.040
然後，你就可以算出 z1, z2 對 C 的偏微分

0:28:26.260,0:28:29.440
這樣大家懂嗎？講到這邊大家有問題嗎？

0:28:29.920,0:28:34.100
沒有哦，那這整件事情可以說

0:28:34.100,0:28:36.140
這兩個東西怎麼得到它的偏微分

0:28:36.140,0:28:38.200
這兩個東西怎麼得到它的偏微分呢？

0:28:38.420,0:28:41.780
我們剛才已經看過了，就是用一個 op-amp 來算

0:28:41.780,0:28:48.800
這每一個 op-amp 它放大的倍率呢

0:28:48.800,0:28:53.440
就是 σ'(z1), σ'(z2), σ'(z3), σ'(z4)

0:28:53.440,0:28:55.300
所以，你算出了

0:28:55.300,0:29:00.500
你就先很快地計算 ∂C/∂(z5), ∂C/∂(z6)

0:29:00.500,0:29:06.580
然後，再把這一個偏微分的值，跟這個偏微分的值

0:29:06.580,0:29:07.900
乘上一些 weight

0:29:07.900,0:29:09.320
乘上一些 weight

0:29:09.320,0:29:10.840
再通過 op-amp

0:29:10.840,0:29:13.520
你就得到這兩個偏微分的值

0:29:13.520,0:29:15.880
再乘上一些 weight

0:29:15.880,0:29:17.220
再乘上一些 weight

0:29:17.220,0:29:20.640
再通過一個 op-amp，就得到一些偏微分的值

0:29:20.640,0:29:24.040
就這樣，就計算完了

0:29:24.040,0:29:28.040
就計算完了，所以，你再算 Backpropagation 的時候

0:29:28.040,0:29:31.780
你在這個步驟，叫做 Backward pass

0:29:31.780,0:29:34.500
你在做這個 Backward pass 的時候

0:29:34.500,0:29:39.980
你實際上的做法就是，建另外一個 neural network

0:29:39.980,0:29:42.620
我們本來有一個正向的 neural network，裡面的

0:29:42.620,0:29:46.300
activation function 都是 sigmoid function

0:29:46.300,0:29:48.780
那現在，你在算 Backward pass 的時候

0:29:48.780,0:29:52.020
你就是建一個反向的 neural network

0:29:52.020,0:29:53.520
反向的 neural network

0:29:53.520,0:29:56.440
從右邊到左邊，反向的 neural network

0:29:56.440,0:29:59.260
這個反向的 neural network，它的 activation function 呢

0:29:59.260,0:30:01.780
你要先算完 Forward pass 以後

0:30:01.780,0:30:04.300
你才算得出來，然後，接下來呢

0:30:04.300,0:30:06.900
這個反向的 neural network，它的 input 就是這兩項

0:30:06.900,0:30:07.880
這兩項

0:30:07.880,0:30:10.780
然後，其他部分就跟一般的 neural network 運算一樣

0:30:10.780,0:30:13.340
你就做一個 Backward pass

0:30:13.340,0:30:15.560
但是，其實就是做一個 neural network 的運算

0:30:15.560,0:30:20.020
你就可以把每一個 z 對 C 的偏微分

0:30:20.020,0:30:23.100
就都算出來了，這個就是 Backward pass

0:30:23.480,0:30:25.660
所以，我們就 summarize 一下

0:30:25.660,0:30:27.540
Backpropagation 是怎麼做的

0:30:27.540,0:30:30.880
首先，你做一個 Forward pass

0:30:30.880,0:30:34.800
在做 Forward pass 的時候，你可以算出

0:30:34.800,0:30:37.940
只要你知道每一個 activation function 的 output

0:30:38.160,0:30:40.740
那 activation function 的 output 就是

0:30:40.740,0:30:43.820
它所連接的 weight 的 ∂z/∂w

0:30:45.020,0:30:47.880
那在 Backward pass 裡面，你要把

0:30:47.880,0:30:50.860
原來的 neural network 的方向呢

0:30:50.860,0:30:55.420
倒過來，你把原來的 neural network 的方向倒過來

0:30:55.420,0:30:58.900
那在這個倒過來的 neural network

0:30:58.900,0:31:01.580
它的每一個三角形的 output 呢

0:31:01.580,0:31:04.260
就是 ∂C/∂z

0:31:05.020,0:31:07.340
然後，把它們乘起來

0:31:07.340,0:31:09.580
你就知道某一個 weight

0:31:09.580,0:31:12.000
對 w 的偏微分是什麼了

0:31:12.000,0:31:14.280
就結束，這樣

0:31:14.780,0:31:17.120
講到這邊，大家有沒有甚麼問題呢？

0:31:17.560,0:31:19.720
就算你沒有聽懂這個東西

0:31:19.720,0:31:22.040
其實，也沒有什麼關係啦

0:31:22.040,0:31:25.640
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
