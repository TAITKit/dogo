<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:06.120<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:06.120,0:00:08.640<br>
各位同學，大家好<br>
<br>
0:00:08.640,0:00:10.800<br>
我們開始上課吧<br>
<br>
0:00:10.800,0:00:13.980<br>
我們今天要講的是 Backpropagation<br>
<br>
0:00:13.980,0:00:17.800<br>
也就是實際上，如果你要用 Gradient Descent 的方法<br>
<br>
0:00:17.800,0:00:21.000<br>
來 train 一個 neural network 的時候呢<br>
<br>
0:00:21.000,0:00:24.140<br>
你應該要怎麼做？<br>
<br>
0:00:24.140,0:00:28.260<br>
那我們上次其實已經講過了 neural network 的基本架構<br>
<br>
0:00:28.260,0:00:29.920<br>
那我就發現，在作業二裡面<br>
<br>
0:00:29.920,0:00:36.060<br>
好多人都已經 implement neural network 的 approach<br>
<br>
0:00:36.060,0:00:39.260<br>
那或許你已經對這個方法非常地清楚了<br>
<br>
0:00:39.260,0:00:42.700<br>
但是我不知道說，你是不是清楚說<br>
<br>
0:00:42.700,0:00:46.180<br>
到底實際上，在 train neural network 的時候<br>
<br>
0:00:46.180,0:00:50.140<br>
Backpropagation 這個 algorithm 是怎麼運作的<br>
<br>
0:00:50.140,0:00:53.080<br>
我們就來講一下 Backpropagation 這個 algorithm<br>
<br>
0:00:53.080,0:00:56.800<br>
是怎麼讓 neural network 的 training 變得比較有效率<br>
<br>
0:00:57.260,0:00:59.700<br>
那在 Gradient Descent 裡面<br>
<br>
0:00:59.700,0:01:01.860<br>
我們知道說 Gradient Descent 的方法就是<br>
<br>
0:01:01.860,0:01:04.600<br>
假設你的 network 有一大堆的參數<br>
<br>
0:01:04.600,0:01:06.480<br>
一堆 w，一堆 b<br>
<br>
0:01:06.820,0:01:10.120<br>
那你先選一個初始的參數<br>
<br>
0:01:10.120,0:01:13.940<br>
你先選一個初始的參數在哪裡<br>
<br>
0:01:13.940,0:01:18.200<br>
然後計算這個 θ^0 對你的 loss function 的 Gradient<br>
<br>
0:01:18.200,0:01:21.360<br>
也就是計算每一個<br>
<br>
0:01:21.360,0:01:25.360<br>
network 裡面的參數，w1, w2, b1, b2 ......等等<br>
<br>
0:01:25.360,0:01:28.680<br>
對你的 L(θ) 的偏微分<br>
<br>
0:01:28.680,0:01:32.880<br>
計算出這個東西以後，這個 gradient 其實是一個 vector<br>
<br>
0:01:32.880,0:01:36.940<br>
計算出這個 vector 以後，你就可以去更新你的參數<br>
<br>
0:01:36.940,0:01:40.440<br>
你就把 θ^0 減掉 learning rate<br>
<br>
0:01:40.440,0:01:44.480<br>
乘上 gradient，然後得到 θ^1<br>
<br>
0:01:44.480,0:01:47.040<br>
那這個 process 就持續繼續下去<br>
<br>
0:01:47.040,0:01:49.620<br>
再算一遍，在 θ^1 的 gradient<br>
<br>
0:01:49.620,0:01:52.620<br>
然後，再把 θ^1 減掉 gradient<br>
<br>
0:01:52.620,0:01:56.280<br>
update 成 θ^2，這個 process 就一直持續下去<br>
<br>
0:01:56.280,0:01:58.220<br>
所以，在 neural network 裡面呢<br>
<br>
0:01:58.220,0:02:03.080<br>
當你用 Gradient Descent 方法的時候，<br>
跟我們在做 Logistic Regression<br>
<br>
0:02:03.080,0:02:07.160<br>
還有 Linear Regression 等等，是沒有太大的差別的<br>
<br>
0:02:07.160,0:02:08.940<br>
但是，最大的差別就是<br>
<br>
0:02:08.940,0:02:11.200<br>
最大的問題是，在 neural network 裡面<br>
<br>
0:02:11.200,0:02:13.920<br>
我們有非常非常多的參數<br>
<br>
0:02:13.920,0:02:16.900<br>
那現在，如果你要做語音辨識系統的話呢<br>
<br>
0:02:16.900,0:02:20.140<br>
你的 neural network 通常會有，7, 8 層<br>
<br>
0:02:20.140,0:02:23.720<br>
每層有 1000 個 neuron，它有上百萬個參數<br>
<br>
0:02:23.720,0:02:25.680<br>
所以，這個 vector 呢<br>
<br>
0:02:25.680,0:02:27.380<br>
它是非常非常長的<br>
<br>
0:02:27.380,0:02:29.780<br>
這是一個上百萬維的 vector<br>
<br>
0:02:29.780,0:02:31.980<br>
所以，現在最大的困難就是<br>
<br>
0:02:31.980,0:02:35.760<br>
你要如何有效地去把這一個百萬維的 vector<br>
<br>
0:02:35.760,0:02:38.260<br>
有效地把它計算出來<br>
<br>
0:02:38.260,0:02:41.540<br>
那這個就是 Backpropagation 在做的事情<br>
<br>
0:02:41.540,0:02:44.280<br>
所以，Backpropagation 並不是一個<br>
<br>
0:02:44.280,0:02:47.180<br>
和 Gradient Descent 不同的<br>
<br>
0:02:47.180,0:02:50.020<br>
training 的方法，它就是 Gradient Descent<br>
<br>
0:02:50.020,0:02:53.400<br>
它只是一個比較有效率的演算法<br>
<br>
0:02:53.400,0:02:56.440<br>
讓你在計算這個 gradient，這個 vector 的時候<br>
<br>
0:02:56.440,0:02:59.700<br>
是可以比較有效率地把這個 vector 計算出來<br>
<br>
0:03:01.020,0:03:03.940<br>
其實，Backpropagation 呢，裡面沒有<br>
<br>
0:03:03.940,0:03:06.080<br>
特別高深的數學<br>
<br>
0:03:06.080,0:03:09.480<br>
你唯一需要記得的就只有 Chain Rule<br>
<br>
0:03:09.480,0:03:14.440<br>
那我們用一張投影片，迅速地幫大家複習一下，<br>
什麼是 Chain Rule<br>
<br>
0:03:14.440,0:03:17.880<br>
假設你現在有兩個 function，h 跟 g<br>
<br>
0:03:17.880,0:03:21.340<br>
g input x 就得到 y<br>
<br>
0:03:21.340,0:03:23.440<br>
h input y 就得到 z<br>
<br>
0:03:23.440,0:03:26.800<br>
所以，如果你今天給 x 一個小小的變化的話<br>
<br>
0:03:26.800,0:03:30.800<br>
給 x 一個變化的話，它會影響到它的 output y<br>
<br>
0:03:30.800,0:03:32.860<br>
所以，y 會跟這有變化<br>
<br>
0:03:32.860,0:03:37.100<br>
y 有變化以後，又會影響到 z，所以 z 會跟著有變化<br>
<br>
0:03:37.100,0:03:41.620<br>
如果我們今天要計算 dz/dx 的話<br>
<br>
0:03:41.620,0:03:44.260<br>
我們要計算 x 對 z 的微分的話<br>
<br>
0:03:44.260,0:03:47.100<br>
要怎麼算呢？你可以把它拆成兩項<br>
<br>
0:03:47.100,0:03:49.660<br>
x 對 z 的微分<br>
<br>
0:03:49.660,0:03:54.540<br>
它就等於 y 對 z 的微分<br>
<br>
0:03:54.540,0:03:58.100<br>
乘上 x 對 y 的微分<br>
<br>
0:03:58.100,0:04:01.060<br>
那這個怎麼來的，你就問一下微積分老師<br>
<br>
0:04:01.060,0:04:03.700<br>
你可以就把這個 y 消掉<br>
<br>
0:04:03.700,0:04:07.300<br>
所以左邊就等於右邊，<br>
但不要讓你的微積分老師知道這件事情<br>
<br>
0:04:09.380,0:04:11.500<br>
那第二個 case<br>
<br>
0:04:11.500,0:04:13.760<br>
我們來看 Chain Rule 第二個 case<br>
<br>
0:04:13.760,0:04:17.760<br>
假設現在有 3 個 function，g, h, k<br>
<br>
0:04:18.180,0:04:21.460<br>
g input s 就得到 x<br>
<br>
0:04:22.080,0:04:24.400<br>
h input s 就得到 y<br>
<br>
0:04:24.400,0:04:27.640<br>
k input x, y 就得到 z<br>
<br>
0:04:27.640,0:04:31.560<br>
所以，今天假如你改變了 s<br>
<br>
0:04:31.560,0:04:33.480<br>
改變了 x，改變了 s<br>
<br>
0:04:33.480,0:04:37.860<br>
你會透過 g 和 h 這兩個 function 改變 x 跟 y<br>
<br>
0:04:37.900,0:04:42.020<br>
改變了 s，就改變了 x 跟 y<br>
<br>
0:04:42.260,0:04:45.620<br>
那改變了 x 跟 y 以後，透過 k 這個 function<br>
<br>
0:04:45.620,0:04:48.340<br>
k 這個 function，input x 跟 y，output 是 z<br>
<br>
0:04:48.340,0:04:52.680<br>
改變了 x 跟 y 以後，你就改變了 z<br>
<br>
0:04:52.840,0:04:59.360<br>
所以，今天如果你要計算 s 對 z 的微分的話<br>
<br>
0:04:59.720,0:05:04.460<br>
那這個 s，是透過兩條路徑去影響 z<br>
<br>
0:05:04.460,0:05:08.640<br>
它可以透過 x 去影響 z，也可以透過 y 去影響 z<br>
<br>
0:05:08.640,0:05:11.320<br>
所以，s 對 z 的微分<br>
<br>
0:05:11.320,0:05:14.080<br>
就可以寫成，就拆成兩項<br>
<br>
0:05:14.080,0:05:17.180<br>
拆成兩項，根據這兩條 path 拆成兩項<br>
<br>
0:05:17.180,0:05:19.520<br>
這個 s 對 z 的微分<br>
<br>
0:05:19.520,0:05:23.560<br>
就可以寫成 x 對 z 的微分<br>
<br>
0:05:23.560,0:05:26.820<br>
乘上 s 對 x 的微分，就是上面這條路徑<br>
<br>
0:05:26.820,0:05:31.260<br>
加上 y 對 z 的微分乘上 s 對 y 的微分，也就是<br>
<br>
0:05:31.260,0:05:32.820<br>
下面這條路徑<br>
<br>
0:05:32.820,0:05:35.580<br>
所以，你大一微積分有好好學的話呢<br>
<br>
0:05:35.580,0:05:38.320<br>
這個就是我們都學過的 Chain Rule<br>
<br>
0:05:38.320,0:05:40.760<br>
那我們等一下呢，會需要用到這個東西<br>
<br>
0:05:41.880,0:05:45.220<br>
那再回到這個 neural network 的 training<br>
<br>
0:05:45.580,0:05:49.680<br>
我們知道說，我們會定一個 loss function<br>
<br>
0:05:50.020,0:05:51.980<br>
那這個 loss function 是甚麼呢？<br>
<br>
0:05:51.980,0:05:55.120<br>
這個 loss function 是 summation over<br>
<br>
0:05:55.120,0:05:57.840<br>
所有 training data 的<br>
<br>
0:05:58.120,0:06:02.120<br>
某一個 loss 值，C^n<br>
<br>
0:06:02.120,0:06:06.460<br>
我們說，假設給定我們一組 neural network 的參數 θ<br>
<br>
0:06:06.460,0:06:09.120<br>
我們把一個 training data x^n<br>
<br>
0:06:09.120,0:06:12.560<br>
代到這個 neural network 裡面，它會 output 一個 y^n<br>
<br>
0:06:13.040,0:06:15.540<br>
那同時呢，我們會有一個<br>
<br>
0:06:15.540,0:06:18.720<br>
我們希望這個 neural network 的 output：y^n\head<br>
<br>
0:06:18.720,0:06:21.480<br>
它希望它如果 output y^n\head 的話<br>
<br>
0:06:21.480,0:06:23.800<br>
就是最正確的，那我們會<br>
<br>
0:06:23.800,0:06:28.760<br>
定義一個 y^n 跟 y^n\head 之間距離的 function<br>
<br>
0:06:28.760,0:06:30.300<br>
這邊寫作 C^n<br>
<br>
0:06:30.300,0:06:32.680<br>
C^n 代表 y^n 跟 y^n\head 之間的距離<br>
<br>
0:06:32.680,0:06:36.400<br>
如果 C^n 大的話，代表 y^n 跟 y^n\head 之間的距離很遠<br>
<br>
0:06:36.400,0:06:39.100<br>
所以，這個 network 的 parameter 的 loss<br>
<br>
0:06:39.100,0:06:40.800<br>
是比較大的，是比較不好的<br>
<br>
0:06:40.800,0:06:43.580<br>
那如果這個 C^n 很小的話，代表<br>
<br>
0:06:43.580,0:06:45.540<br>
這個 parameter 是好的<br>
<br>
0:06:45.540,0:06:49.260<br>
那我們 summation over 所有 training data 的 C^n<br>
<br>
0:06:49.260,0:06:51.660<br>
summation over 所有 training data<br>
<br>
0:06:51.660,0:06:53.840<br>
根據這個參數 θ<br>
<br>
0:06:53.840,0:06:55.640<br>
它的 output 跟它的目標<br>
<br>
0:06:55.640,0:06:58.880<br>
它的 output y^n 跟它的目標 y^n\head 之間的距離<br>
<br>
0:06:58.880,0:07:02.080<br>
就是得到我們的 total loss，L<br>
<br>
0:07:02.080,0:07:03.980<br>
那你把這個式子<br>
<br>
0:07:03.980,0:07:08.300<br>
左右兩邊都對某一個參數 w 做偏微分的話呢<br>
<br>
0:07:08.300,0:07:10.220<br>
你就得到右邊這個式子<br>
<br>
0:07:10.220,0:07:12.820<br>
你就得到 ∂L/∂w 等於<br>
<br>
0:07:12.820,0:07:14.940<br>
summation over 所有的 training data<br>
<br>
0:07:14.940,0:07:16.060<br>
n = 1 到 N<br>
<br>
0:07:16.060,0:07:18.840<br>
∂C^n (θ)/∂w<br>
<br>
0:07:18.840,0:07:21.380<br>
這個應該是沒有甚麼問題<br>
<br>
0:07:21.380,0:07:23.740<br>
之所以寫這個式子，只是要講說<br>
<br>
0:07:23.740,0:07:25.260<br>
接下來，我們就不用<br>
<br>
0:07:25.260,0:07:28.340<br>
計算 ∂L/∂w<br>
<br>
0:07:28.340,0:07:30.900<br>
我們就只考慮，我們如何去計算<br>
<br>
0:07:30.900,0:07:34.620<br>
對某一筆 data 的 ∂C^n (θ)/∂w<br>
<br>
0:07:34.620,0:07:38.740<br>
你只要能夠把一筆 data 的<br>
<br>
0:07:38.740,0:07:41.840<br>
∂C^n (θ)/∂w 算出來<br>
<br>
0:07:41.840,0:07:44.420<br>
再 summation over 所有的 training data<br>
<br>
0:07:44.420,0:07:48.820<br>
你就可以把 total loss 對某一個參數的<br>
<br>
0:07:48.820,0:07:50.400<br>
偏微分算出來了<br>
<br>
0:07:50.400,0:07:53.020<br>
所以，我們等一下就只 focus 在怎麼計算<br>
<br>
0:07:53.020,0:07:57.700<br>
對某一筆 data，它的 cost C^n 對 w 的偏微分<br>
<br>
0:07:57.700,0:08:00.240<br>
我們就只 focus 在怎麼計算這一項上面<br>
<br>
0:08:01.580,0:08:03.160<br>
怎麼做呢？<br>
<br>
0:08:03.600,0:08:07.000<br>
我們先考慮某一個 neuron<br>
<br>
0:08:07.000,0:08:09.820<br>
我們先從底下這個 neural network 裡面<br>
<br>
0:08:09.820,0:08:12.880<br>
拿一個 neuron 出來，考慮它<br>
<br>
0:08:14.440,0:08:19.300<br>
那這一個 neuron，它是在第一個 layer 的 neuron<br>
<br>
0:08:19.300,0:08:21.020<br>
所以，它前面的 input<br>
<br>
0:08:21.020,0:08:23.460<br>
就是外界給它的 input，x1, x2<br>
<br>
0:08:23.460,0:08:29.120<br>
假設它只有兩個 input，x1 跟 x2<br>
<br>
0:08:29.120,0:08:32.980<br>
分別乘上 weight w1, w2，再加上 b<br>
<br>
0:08:32.980,0:08:34.580<br>
會得到 z<br>
<br>
0:08:34.580,0:08:37.940<br>
這個我想大家應該都非常熟悉，這個 z 呢<br>
<br>
0:08:37.940,0:08:41.140<br>
就是 x1w1 + x2w2 + b<br>
<br>
0:08:41.540,0:08:43.880<br>
那得到這個 z 以後，通過了 activation function<br>
<br>
0:08:43.880,0:08:46.120<br>
再經過了非常非常多的事情以後<br>
<br>
0:08:46.120,0:08:48.640<br>
你會得到最終的 output，y1, y2<br>
<br>
0:08:48.640,0:08:51.280<br>
那現在的問題是這樣<br>
<br>
0:08:51.280,0:08:53.460<br>
假設我們從這邊拿一個 w 出來<br>
<br>
0:08:53.460,0:08:55.820<br>
等一下，我們就拿 w 當作 z<br>
<br>
0:08:55.820,0:08:58.960<br>
但是 b 也是一樣，就拿 weight 當作 z<br>
<br>
0:08:58.960,0:09:02.320<br>
來看怎麼計算，某一個 weight 對 cost<br>
<br>
0:09:02.320,0:09:05.100<br>
對 example 的某一個 cost 的偏微分<br>
<br>
0:09:05.100,0:09:09.240<br>
那 b 的話，想必你可以以此類推，就把它算出來<br>
<br>
0:09:09.240,0:09:14.260<br>
那 ∂C/∂w 怎麼算？<br>
<br>
0:09:14.500,0:09:16.840<br>
這個 ∂C/∂w 阿<br>
<br>
0:09:16.840,0:09:19.700<br>
按照 Chain Rule，你就可以把它拆成兩項<br>
<br>
0:09:19.700,0:09:23.200<br>
∂z/∂w * ∂C/∂z<br>
<br>
0:09:23.200,0:09:25.720<br>
這個 z 可以把它消掉，沒有問題<br>
<br>
0:09:25.720,0:09:30.960<br>
所以 ∂C/∂w 可以根據 Chain Rule 拆成兩項<br>
<br>
0:09:31.680,0:09:34.680<br>
那這兩項，我們就分別去把它計算出來<br>
<br>
0:09:34.680,0:09:36.440<br>
前面這項是很簡單的<br>
<br>
0:09:36.440,0:09:40.760<br>
後面這項，是比較複雜的<br>
<br>
0:09:40.760,0:09:43.520<br>
那計算前面這一項<br>
<br>
0:09:43.520,0:09:46.960<br>
計算 ∂z/∂w 的這個 process 呢<br>
<br>
0:09:46.960,0:09:48.840<br>
我們稱之為 Forward pass<br>
<br>
0:09:48.840,0:09:51.160<br>
那你等下會知道說為什麼叫 Forward pass<br>
<br>
0:09:51.160,0:09:55.580<br>
那計算前面這一項 ∂C/∂z 的 process<br>
<br>
0:09:55.580,0:09:58.640<br>
我們稱之為 Backward pass<br>
<br>
0:09:58.660,0:10:02.420<br>
那我們等一下會講說，為什麼叫做 Backward pass<br>
<br>
0:10:02.780,0:10:07.400<br>
那我們就先看一下，怎麼來計算這個  ∂z/∂w<br>
<br>
0:10:07.400,0:10:09.960<br>
怎麼來計算  ∂z/∂w<br>
<br>
0:10:09.960,0:10:12.760<br>
好，那我們先看這個 w1<br>
<br>
0:10:12.760,0:10:15.560<br>
那你怎麼計算 ∂z/∂w1 呢？<br>
<br>
0:10:15.560,0:10:17.420<br>
就是秒算<br>
<br>
0:10:17.420,0:10:19.700<br>
就是秒算，因為 z 就長這個樣子嘛<br>
<br>
0:10:19.700,0:10:21.960<br>
然後，w1 在這邊<br>
<br>
0:10:21.960,0:10:24.780<br>
所以，一眼就可以知道說，它是 x1<br>
<br>
0:10:24.780,0:10:27.360<br>
那 ∂z/∂w2 呢？<br>
<br>
0:10:27.360,0:10:29.360<br>
所以，你一眼就可以看出說<br>
<br>
0:10:29.360,0:10:33.540<br>
它就是 x2，這個都是秒算這樣子<br>
<br>
0:10:33.540,0:10:36.620<br>
那它的規律是這樣，它的規律就是<br>
<br>
0:10:36.620,0:10:42.520<br>
∂z/∂w，就是看這個 w 前面接的東西是什麼<br>
<br>
0:10:42.520,0:10:44.820<br>
那微分以後就是什麼<br>
<br>
0:10:44.820,0:10:48.920<br>
這個 w1 前面，它的 input 是接 x1<br>
<br>
0:10:48.920,0:10:51.740<br>
它的 input 是 x1，所以微分以後就是 x1<br>
<br>
0:10:51.740,0:10:55.160<br>
w2 呢，它前面的 input 是 x2<br>
<br>
0:10:55.160,0:10:57.820<br>
所以微分以後就是 x2<br>
<br>
0:10:57.820,0:10:59.540<br>
那它的規律就是這個樣子<br>
<br>
0:10:59.540,0:11:03.780<br>
所以，今天假如給你一個 neural network<br>
<br>
0:11:03.780,0:11:07.180<br>
那它裡面有一大推的參數<br>
<br>
0:11:07.180,0:11:09.480<br>
一大堆的參數，但是你要<br>
<br>
0:11:09.480,0:11:11.780<br>
計算這裡面每一個參數<br>
<br>
0:11:11.780,0:11:14.580<br>
對 z 的偏微分<br>
<br>
0:11:14.580,0:11:18.920<br>
你要計算這裡面每一個參數的 ∂w 跟 ∂z<br>
<br>
0:11:18.920,0:11:21.700<br>
這件事情呢，非常非常的容易<br>
<br>
0:11:21.700,0:11:23.660<br>
因為我們剛才知道，它的規律就是<br>
<br>
0:11:23.660,0:11:28.160<br>
∂z/∂w 就是看你這個 w 的 input 是什麼，它就是甚麼<br>
<br>
0:11:28.160,0:11:31.620<br>
所以，如果有人問你說，現在 input 是 1 跟 -1<br>
<br>
0:11:31.620,0:11:34.020<br>
那這個 1，它對<br>
<br>
0:11:34.020,0:11:37.160<br>
它的 activation function 的 input z 的偏微分是什麼呢？<br>
<br>
0:11:37.160,0:11:41.480<br>
你就可以瞬間回答它說，就是 -1<br>
<br>
0:11:41.480,0:11:44.000<br>
因為這個 1，前面接 -1<br>
<br>
0:11:44.000,0:11:48.920<br>
所以，這個參數對 z 的偏微分就是 -1<br>
<br>
0:11:48.920,0:11:52.700<br>
同理，比如說這個 -1，它對 z 的偏微分就是 1<br>
<br>
0:11:52.700,0:11:54.980<br>
這個 -2，它對 z 的偏微分就是 1<br>
<br>
0:11:54.980,0:11:58.120<br>
這個 1，它對 z 的偏微分就是 1，以此類推<br>
<br>
0:11:58.120,0:12:00.640<br>
接下來呢<br>
<br>
0:12:00.640,0:12:04.100<br>
接下來假如有人問你說，這個 w<br>
<br>
0:12:04.100,0:12:08.280<br>
對它的 activation function 的 input z 的偏微分是什麼呢？<br>
<br>
0:12:08.280,0:12:10.640<br>
你其實也可以瞬間就回答它<br>
<br>
0:12:10.640,0:12:14.220<br>
你只要知道說，這個 w 前面接的 weight<br>
<br>
0:12:14.220,0:12:15.940<br>
前面接的 input 是什麼<br>
<br>
0:12:15.940,0:12:18.820<br>
那這個 w 前面接的 input 是<br>
<br>
0:12:18.820,0:12:21.240<br>
某一個 neuron 的 output，對不對？<br>
<br>
0:12:21.240,0:12:22.900<br>
這個 w 前面接的 input<br>
<br>
0:12:22.900,0:12:25.440<br>
是第一個 hidden layer 的 neuron 的 output<br>
<br>
0:12:25.440,0:12:27.800<br>
那這個 hidden layer 的 neuron 的 output 要怎麼算呢？<br>
<br>
0:12:27.800,0:12:30.640<br>
這個大家都知道，對不對？就是把<br>
<br>
0:12:30.640,0:12:35.160<br>
1 跟 -1 丟進去，然後根據我們熟悉的 neuron 的運算<br>
<br>
0:12:35.160,0:12:37.620<br>
然後看看它的 output 是什麼，就是什麼<br>
<br>
0:12:37.620,0:12:39.600<br>
在這個例子裡面呢<br>
<br>
0:12:39.600,0:12:41.880<br>
假如這個 function 是 sigmoid function<br>
<br>
0:12:41.880,0:12:44.560<br>
算出來是 0.98, 0.12<br>
<br>
0:12:44.560,0:12:47.400<br>
如果你可以算出這兩個 neuron 的 output 是<br>
<br>
0:12:47.400,0:12:49.240<br>
0.98 跟 0.12 的話<br>
<br>
0:12:49.240,0:12:50.860<br>
那這個 weight<br>
<br>
0:12:50.860,0:12:52.900<br>
它做完偏微分以後<br>
<br>
0:12:52.900,0:12:54.240<br>
這個 weight 對<br>
<br>
0:12:54.240,0:12:57.320<br>
它的 activation function 的 input z 做完偏微分以後<br>
<br>
0:12:57.320,0:13:01.560<br>
顯然就是 0.12，因為它前面接的 weight 就是 0.12<br>
<br>
0:13:01.560,0:13:03.060<br>
這個 -1 也是 0.12<br>
<br>
0:13:03.060,0:13:06.160<br>
這個 -2 是 0.98，這個 2 是 0.98<br>
<br>
0:13:06.160,0:13:07.700<br>
這個也很直覺<br>
<br>
0:13:07.700,0:13:11.540<br>
所以，同樣的 process 你就反覆的在做<br>
<br>
0:13:11.540,0:13:15.160<br>
你可以得到這兩個紅色 neuron 的 output 是 0.86, 0.11<br>
<br>
0:13:15.160,0:13:18.420<br>
那你就可以秒反應說，這個<br>
<br>
0:13:18.420,0:13:21.720<br>
4 對 z 的偏微分就是 0.11<br>
<br>
0:13:21.720,0:13:25.500<br>
所以，你要算出這個 neural network 裡面的<br>
<br>
0:13:25.500,0:13:27.880<br>
每一個 weight<br>
<br>
0:13:27.880,0:13:31.420<br>
對它的 activation function 的 input z 的偏微分<br>
<br>
0:13:31.420,0:13:34.520<br>
你就把你的 input 丟進去<br>
<br>
0:13:34.520,0:13:38.420<br>
然後，計算每一個 neuron 的 output<br>
<br>
0:13:38.420,0:13:40.180<br>
就結束了<br>
<br>
0:13:40.180,0:13:43.120<br>
所以，這個步驟叫做 Forward pass<br>
<br>
0:13:43.120,0:13:45.000<br>
它是非常容易理解的<br>
<br>
0:13:45.540,0:13:48.340<br>
再來，我們要講的是 Backward pass<br>
<br>
0:13:48.340,0:13:52.980<br>
也就是怎麼算 ∂C/∂z<br>
<br>
0:13:52.980,0:13:55.820<br>
這個你就不會覺得很困難了<br>
<br>
0:13:55.820,0:13:58.440<br>
因為，這個 z 阿<br>
<br>
0:13:58.440,0:14:01.080<br>
它通過 activation function 以後得到 output<br>
<br>
0:14:01.080,0:14:03.840<br>
後面還有非常非常複雜的 process<br>
<br>
0:14:03.840,0:14:07.220<br>
它才得到這個 C<br>
<br>
0:14:07.220,0:14:10.120<br>
要經過非常複雜的 process 才能得到 C<br>
<br>
0:14:10.120,0:14:14.100<br>
這個 ∂C/∂z 顯然是很複雜的<br>
<br>
0:14:14.100,0:14:19.220<br>
不過我們可以用 Chain rule 再把這一項做一下拆解<br>
<br>
0:14:19.220,0:14:23.040<br>
假設這個 activation function 是 sigmoid function<br>
<br>
0:14:23.040,0:14:24.900<br>
我這邊就寫一個 σ(z)<br>
<br>
0:14:24.900,0:14:27.840<br>
z 通過 sigmoid function 得到 a<br>
<br>
0:14:27.840,0:14:29.640<br>
這個 neuron 的 output 是 a<br>
<br>
0:14:30.360,0:14:32.080<br>
那接下來會發生甚麼事呢？<br>
<br>
0:14:32.080,0:14:36.120<br>
這個 a 會通過某一個 weight，乘上某一個 weight<br>
<br>
0:14:36.120,0:14:39.000<br>
再加其他一大堆的 value，得到 z'<br>
<br>
0:14:39.000,0:14:42.940<br>
它是下一個 neuron activation function 的 input<br>
<br>
0:14:42.940,0:14:47.160<br>
這個 a 會再乘上另一個 weight，這邊寫成 w4<br>
<br>
0:14:47.160,0:14:51.280<br>
再加上其他一大堆東西，得到 z"<br>
<br>
0:14:51.280,0:14:54.520<br>
後面這個 z' 跟 z"<br>
<br>
0:14:54.520,0:14:56.640<br>
之後可能還會發生很多很多的事情<br>
<br>
0:14:56.640,0:15:00.140<br>
不過我們就先只考慮下一步會發生什麼事情<br>
<br>
0:15:00.620,0:15:02.920<br>
所以呢<br>
<br>
0:15:02.920,0:15:06.780<br>
我們知道說 ∂C/∂z<br>
<br>
0:15:06.780,0:15:11.580<br>
你可以寫成 ∂a/∂z * ∂C/∂a<br>
<br>
0:15:11.580,0:15:15.040<br>
那這個就沒有什麼問題，∂a 可以消掉<br>
<br>
0:15:15.040,0:15:18.240<br>
那 ∂a/∂z 是什麼呢？<br>
<br>
0:15:18.240,0:15:21.680<br>
我們知道說，a = σ(z)<br>
<br>
0:15:21.680,0:15:24.740<br>
那這個 ∂a/∂z 呢<br>
<br>
0:15:24.740,0:15:28.240<br>
其實就是這個 sigmoid function 的微分<br>
<br>
0:15:28.240,0:15:31.160<br>
那 sigmoid function 長這個樣子<br>
<br>
0:15:31.160,0:15:33.580<br>
長這個樣子，綠色這條線<br>
<br>
0:15:33.580,0:15:37.600<br>
那它的微分，你就算一下，長這個樣子<br>
<br>
0:15:37.600,0:15:40.940<br>
長這樣子，我們之前已經看過很多次了<br>
<br>
0:15:41.200,0:15:44.100<br>
所以，∂a/∂z 也沒有問題<br>
<br>
0:15:44.420,0:15:48.620<br>
接下來的問題就是，∂C/∂a<br>
<br>
0:15:49.400,0:15:51.320<br>
應該長甚麼樣子呢？<br>
<br>
0:15:51.320,0:15:54.340<br>
它應該長甚麼樣子呢？<br>
<br>
0:15:55.360,0:15:58.600<br>
那我們就接下來看說，這個 a<br>
<br>
0:15:58.600,0:16:01.640<br>
∂a 跟 C 的關係是怎樣<br>
<br>
0:16:01.640,0:16:04.740<br>
你知道 a 它會影響 z'<br>
<br>
0:16:05.260,0:16:07.600<br>
然後，z' 會影響 z<br>
<br>
0:16:07.760,0:16:11.860<br>
a 會影響 z"，z" 會影響 C<br>
<br>
0:16:12.120,0:16:16.140<br>
a 透過 z' 跟 z" 去影響 C<br>
<br>
0:16:16.140,0:16:19.680<br>
所以，∂C/∂a 你可以寫成<br>
<br>
0:16:19.680,0:16:23.940<br>
∂z'/∂a * ∂C/∂z'<br>
<br>
0:16:23.940,0:16:29.340<br>
加上 ∂z"/∂a * ∂C/∂z"<br>
<br>
0:16:29.340,0:16:34.680<br>
我們假設 a 後面，就是這個藍色 neuron 的下一個 layer<br>
<br>
0:16:34.680,0:16:36.840<br>
就是紅色的 neuron 只有兩個<br>
<br>
0:16:36.840,0:16:38.720<br>
所以，這邊就只有兩項<br>
<br>
0:16:38.720,0:16:41.440<br>
這個 a 只會影響 z' 跟 z"<br>
<br>
0:16:41.440,0:16:43.780<br>
如果這邊有 1000 個 neuron 的話<br>
<br>
0:16:43.780,0:16:45.100<br>
那這邊這個 Chain rule<br>
<br>
0:16:45.100,0:16:49.140<br>
你的 summation 就是 summation over 1000 項<br>
<br>
0:16:49.140,0:16:51.880<br>
這樣大家了解我的意思嗎？<br>
<br>
0:16:52.600,0:16:57.980<br>
這邊呢，經過前面我們簡化上課的說明<br>
<br>
0:16:57.980,0:17:00.780<br>
我們假設只有兩個 neuron，這邊只有兩項<br>
<br>
0:17:00.780,0:17:03.840<br>
只有兩項，a 只會影響 z' 跟 z"<br>
<br>
0:17:03.840,0:17:09.020<br>
接下來，∂z"/∂a<br>
<br>
0:17:09.640,0:17:15.060<br>
會算嗎？這個就是秒算，對不對？這個就是 w3<br>
<br>
0:17:15.440,0:17:21.960<br>
z' 等於 a 乘上 w3，再加上一些有的沒的東西<br>
<br>
0:17:21.960,0:17:26.040<br>
所以，這個 z' 對 a 做偏微分<br>
<br>
0:17:26.040,0:17:29.020<br>
根據這個式子，顯然就是 w3<br>
<br>
0:17:29.280,0:17:33.540<br>
所以，同理，這個 z" 對 a 做偏微分<br>
<br>
0:17:33.540,0:17:36.400<br>
得到的結果顯然就是 w4<br>
<br>
0:17:36.400,0:17:39.240<br>
所以，這兩項算起來，也不是個問題<br>
<br>
0:17:39.780,0:17:45.160<br>
最後的問題就是，z" 對 C 的偏微分怎麼算呢？<br>
<br>
0:17:45.160,0:17:49.400<br>
這個 z" 對 C 的偏微分怎麼算呢？<br>
<br>
0:17:49.400,0:17:51.480<br>
因為我們不知道<br>
<br>
0:17:51.480,0:17:53.740<br>
z' 對 C 有什麼關係<br>
<br>
0:17:53.740,0:17:55.340<br>
z" 對 C 有什麼關係<br>
<br>
0:17:55.340,0:17:58.160<br>
這後面還有發生很多很多的事情<br>
<br>
0:17:58.160,0:18:01.680<br>
是很複雜的，所以我們搞不清楚後面會發生什麼事情<br>
<br>
0:18:01.680,0:18:04.540<br>
所以，我們一下子不知道這兩項怎麼算<br>
<br>
0:18:04.540,0:18:07.540<br>
不過沒關係，我們就假設我們知道<br>
<br>
0:18:07.540,0:18:12.260<br>
假設這兩項的值，我們已經 somehow 透過<br>
<br>
0:18:12.260,0:18:14.240<br>
某種方法把它算出來<br>
<br>
0:18:14.240,0:18:18.160<br>
我們透過一個等一下會講，但你還不知道怎麼做的方法<br>
<br>
0:18:18.160,0:18:20.100<br>
就已經把這兩項做出來了<br>
<br>
0:18:20.100,0:18:23.100<br>
那把這兩項算出來以後<br>
<br>
0:18:23.620,0:18:25.920<br>
把這兩項算出來以後呢<br>
<br>
0:18:26.740,0:18:31.160<br>
我們就可以把 ∂C/∂z<br>
<br>
0:18:31.160,0:18:33.560<br>
輕易的算出來<br>
<br>
0:18:33.560,0:18:37.300<br>
把這兩項算出來以後<br>
<br>
0:18:37.300,0:18:40.680<br>
我們就可以算 ∂C/∂z<br>
<br>
0:18:40.680,0:18:44.660<br>
你會算 ∂C/∂z' 跟 ∂C/∂z"<br>
<br>
0:18:44.660,0:18:46.960<br>
你就會算 ∂C/∂z<br>
<br>
0:18:47.440,0:18:50.400<br>
然後，再把這些值<br>
<br>
0:18:50.400,0:18:53.220<br>
代到我們剛才看到的 ∂C/∂z 的式子裡面<br>
<br>
0:18:53.220,0:19:01.740<br>
就得到這樣一個式子，σ'(z) * [w3 * ∂C/∂z' + w4 * ∂C/∂z"]<br>
<br>
0:19:01.740,0:19:04.620<br>
你就算出這樣一個式子，那這個式子<br>
<br>
0:19:04.620,0:19:06.080<br>
還滿簡單的<br>
<br>
0:19:06.080,0:19:09.160<br>
但是，我們會從另外一個觀點，來看待這個式子<br>
<br>
0:19:09.740,0:19:12.680<br>
你可以想像說<br>
<br>
0:19:12.680,0:19:17.360<br>
現在有另外一個 neuron <br>
<br>
0:19:17.360,0:19:21.020<br>
這個 neuron 並不在我們原來的 network 裡面<br>
<br>
0:19:21.020,0:19:23.660<br>
有另外一個 neuron <br>
<br>
0:19:23.660,0:19:26.100<br>
我把它簡化成這個三角形<br>
<br>
0:19:26.100,0:19:27.600<br>
把它畫成三角形<br>
<br>
0:19:27.600,0:19:35.200<br>
那這個 neuron 的 input，就是 ∂C/∂z' 跟 ∂C/∂z"<br>
<br>
0:19:35.200,0:19:39.660<br>
那第一個 input ∂C/∂z'，就乘上 w3<br>
<br>
0:19:39.660,0:19:42.720<br>
∂C/∂z" 它就乘上 w4<br>
<br>
0:19:42.720,0:19:46.840<br>
再乘上 activation function， σ'(z)<br>
<br>
0:19:46.840,0:19:50.560<br>
得到 output，就是 ∂C/∂z<br>
<br>
0:19:50.560,0:19:54.660<br>
上面這個 neuron 所做的運算<br>
<br>
0:19:54.660,0:19:57.580<br>
跟下面這個式子，是一模一樣的<br>
<br>
0:19:57.580,0:19:59.760<br>
我們只是把下面這個式子<br>
<br>
0:19:59.760,0:20:07.740<br>
把它畫出來，讓它看起來像是一個 neuron 一樣<br>
<br>
0:20:08.220,0:20:11.820<br>
那這個 σ'(z) 阿<br>
<br>
0:20:11.820,0:20:14.840<br>
這個 σ'(z) 其實是一個常數<br>
<br>
0:20:14.840,0:20:20.420<br>
對不對，它不是一個 function，它是一個constant<br>
<br>
0:20:20.420,0:20:25.340<br>
因為，z 其實在計算 Forward pass 的時候<br>
<br>
0:20:25.340,0:20:27.160<br>
就被決定好了<br>
<br>
0:20:27.160,0:20:30.900<br>
z 是一個已經固定的值<br>
<br>
0:20:30.900,0:20:33.800<br>
z 我們已經知道它是多少，所以<br>
<br>
0:20:33.800,0:20:35.860<br>
在給定 z 的情況下<br>
<br>
0:20:35.860,0:20:39.720<br>
這個 σ'(z)，它就是一個常數<br>
<br>
0:20:39.720,0:20:42.100<br>
所以，這個 neuron<br>
<br>
0:20:42.100,0:20:44.780<br>
跟我們之前看到的 sigmoid function 是不一樣的<br>
<br>
0:20:44.780,0:20:49.260<br>
它並不是把 input 通過一個 non-linear 的轉換<br>
<br>
0:20:49.260,0:20:53.260<br>
而是直接把 input 乘上一個 constant，σ'(z)<br>
<br>
0:20:53.260,0:20:55.300<br>
就得到一個 output 這樣<br>
<br>
0:20:55.300,0:20:58.260<br>
所以，我把這個 neuron 畫成三角形的<br>
<br>
0:20:58.260,0:21:01.200<br>
代表它跟我們之前看到的圓形 neuron<br>
<br>
0:21:01.200,0:21:04.000<br>
的運作方式是不一樣的，它是直接乘上一個 constant<br>
<br>
0:21:04.000,0:21:06.280<br>
那你可能會問說，為甚麼是三角形呢<br>
<br>
0:21:06.280,0:21:09.800<br>
因為我是電機系的，我覺得這是一個 op-amp 這樣子<br>
<br>
0:21:09.800,0:21:16.080<br>
op-amp 就會乘上一個 constant，它是一個放大器這樣<br>
<br>
0:21:16.400,0:21:18.740<br>
聽不懂就算了，這不太重要<br>
<br>
0:21:19.360,0:21:25.560<br>
然後，這樣問題都解決了<br>
<br>
0:21:25.560,0:21:29.200<br>
都解決了，對不對，我們現在最後的問題就只有<br>
<br>
0:21:29.200,0:21:31.960<br>
怎麼算這兩項而已<br>
<br>
0:21:31.960,0:21:34.880<br>
假設能夠算這兩項，問題也就都解決了<br>
<br>
0:21:35.560,0:21:37.740<br>
那現在怎麼算這兩項呢？<br>
<br>
0:21:37.740,0:21:39.040<br>
怎麼算這兩項呢？<br>
<br>
0:21:39.040,0:21:42.520<br>
我們現在假設兩個不同的 case<br>
<br>
0:21:42.520,0:21:47.640<br>
第一個 case 是，我們假設現在紅色的這兩個 neuron<br>
<br>
0:21:47.640,0:21:50.240<br>
就已經是 output layer<br>
<br>
0:21:50.240,0:21:53.020<br>
這兩個紅色 neuron 是在 output layer 裡面<br>
<br>
0:21:53.020,0:21:56.240<br>
它們的 output 就已經是整個 network 的 output 了<br>
<br>
0:21:56.240,0:21:58.560<br>
這邊寫成 y1, y2<br>
<br>
0:21:58.560,0:22:01.060<br>
它的 output 就已經是整個 network 的 output 了<br>
<br>
0:22:01.540,0:22:05.400<br>
所以，今天你要算 ∂C/∂z'<br>
<br>
0:22:05.400,0:22:11.060<br>
就很簡單，根據 Chain rule 算 ∂(y1)/∂z' * ∂C/∂(y1)<br>
<br>
0:22:11.060,0:22:14.420<br>
∂(y1)/∂z' 沒什麼問題<br>
<br>
0:22:14.420,0:22:17.520<br>
你只要知道這個 activation function 長甚麼樣子<br>
<br>
0:22:17.520,0:22:19.660<br>
這項就輕而易舉地算出來了<br>
<br>
0:22:19.660,0:22:26.060<br>
∂C/∂(y1)，y1 對 C 有什麼影響，depend on 你的<br>
<br>
0:22:26.060,0:22:28.520<br>
你的 cost function 是怎麼定義的<br>
<br>
0:22:28.520,0:22:31.820<br>
你的 output 跟 target 間是怎麼 evaluate 的<br>
<br>
0:22:31.820,0:22:34.440<br>
你可以用 cross entropy，你可以用 mean square error<br>
<br>
0:22:34.440,0:22:37.560<br>
你用不同的定義，這邊這項就不一樣，但總之<br>
<br>
0:22:37.560,0:22:41.620<br>
它是一個比較簡單的東西，你可以把它算出來<br>
<br>
0:22:42.740,0:22:49.340<br>
那 ∂C/∂z"，這你也可以算，沒有甚麼問題<br>
<br>
0:22:49.340,0:22:52.720<br>
就是 ∂(y2)/∂z" * ∂C/∂(y2)<br>
<br>
0:22:52.720,0:22:55.860<br>
這兩項一樣都是可以秒算<br>
<br>
0:22:55.860,0:23:03.180<br>
所以，今天假設這個藍色的 neuron 後面<br>
<br>
0:23:03.180,0:23:07.240<br>
它的下一個 layer 就已經是 output layer 了<br>
<br>
0:23:07.240,0:23:14.460<br>
這個藍色的 neuron，它在最後一個 hidden layer 裡面<br>
<br>
0:23:14.460,0:23:16.420<br>
它後面就已經是 output layer 了<br>
<br>
0:23:16.420,0:23:19.560<br>
那根據我們剛才所學的東西<br>
<br>
0:23:19.560,0:23:26.020<br>
你就結束了，你就可以把 w1 跟 w2 對 C 的偏微分算出來<br>
<br>
0:23:26.020,0:23:28.020<br>
所以，這個沒有甚麼問題<br>
<br>
0:23:28.020,0:23:30.940<br>
那我們真正煩惱的問題是 case 2<br>
<br>
0:23:30.940,0:23:34.780<br>
假設現在紅色的 neuron 它並不是整個 network 的 output<br>
<br>
0:23:34.780,0:23:40.300<br>
它後面還有其他東西的話，怎麼辦呢？<br>
<br>
0:23:40.300,0:23:44.320<br>
那它後面的其他東西，可能長甚麼樣子呢<br>
<br>
0:23:44.320,0:23:49.600<br>
它可能長這樣，就是 z' 再通過 activation function 得到 a'<br>
<br>
0:23:49.600,0:23:52.000<br>
再乘上另外一個 weight，w5<br>
<br>
0:23:52.000,0:23:55.900<br>
再加上一些其他的東西，得到 za<br>
<br>
0:23:55.900,0:23:59.160<br>
然後，你再把 a' 乘上<br>
<br>
0:23:59.160,0:24:01.540<br>
w6 再加上其他一大堆東西<br>
<br>
0:24:01.540,0:24:05.720<br>
得到 zb，然後再丟進另外兩個<br>
<br>
0:24:05.720,0:24:07.640<br>
activation function 裡面<br>
<br>
0:24:09.500,0:24:11.940<br>
那現在的問題是這樣<br>
<br>
0:24:11.940,0:24:15.380<br>
我們想要求 ∂C/∂z'<br>
<br>
0:24:15.760,0:24:23.820<br>
如果我們知道 ∂C/∂(za) 跟 ∂C/∂(zb)<br>
<br>
0:24:23.820,0:24:29.720<br>
我們就可以計算 ∂C/∂z'<br>
<br>
0:24:29.720,0:24:30.560<br>
對嗎？<br>
<br>
0:24:31.080,0:24:35.720<br>
我們剛才已經有講過說，假設我們知道<br>
<br>
0:24:35.720,0:24:39.800<br>
∂C/∂z' 跟 ∂C/∂z"<br>
<br>
0:24:39.800,0:24:43.620<br>
我們就可以算前面一個 layer 的 ∂C/∂z<br>
<br>
0:24:43.620,0:24:45.600<br>
所以，按照一模一樣的式子<br>
<br>
0:24:45.600,0:24:49.840<br>
如果知道 ∂C/∂(za) 跟 ∂C/∂(zb)<br>
<br>
0:24:49.840,0:24:52.740<br>
我們就可以算 ∂C/∂z'<br>
<br>
0:24:52.740,0:24:54.560<br>
按照一模一樣的式子<br>
<br>
0:24:54.560,0:24:57.520<br>
就是我們剛才算看到那個 op-amp 的式子<br>
<br>
0:24:57.520,0:25:02.040<br>
所以，你就把 ∂C/∂(za) 乘上 w5<br>
<br>
0:25:02.040,0:25:07.120<br>
∂C/∂(zb) 乘上 w6，加起來再通過 op-amp<br>
<br>
0:25:07.160,0:25:10.580<br>
乘上 σ'(z')<br>
<br>
0:25:10.580,0:25:15.640<br>
再乘上 op-amp 就得到這個 ∂C/∂z' 的 output<br>
<br>
0:25:17.900,0:25:21.440<br>
那現在這個問題，就反覆地繼續下去<br>
<br>
0:25:21.440,0:25:25.340<br>
我們剛才說知道 z' 跟 z" 的偏微分就可以算 z<br>
<br>
0:25:25.340,0:25:30.920<br>
現在知道 za 跟 zb 的偏微分就可以算 z'<br>
<br>
0:25:30.920,0:25:35.160<br>
但是，我們又不知道 za 跟 zb 的偏微分<br>
<br>
0:25:35.160,0:25:36.800<br>
怎麼算，對不對？<br>
<br>
0:25:36.800,0:25:38.660<br>
你不知道這兩項怎麼算<br>
<br>
0:25:38.660,0:25:41.620<br>
如果你會這兩項的話，你就把這一項算出來<br>
<br>
0:25:41.620,0:25:43.620<br>
但問題就是，你不知道<br>
<br>
0:25:43.620,0:25:46.400<br>
那怎麼辦呢？<br>
<br>
0:25:46.400,0:25:49.560<br>
我們就再往下一層去看<br>
<br>
0:25:49.560,0:25:53.780<br>
了解嗎？就是這個綠色的 neuron<br>
<br>
0:25:53.780,0:25:56.620<br>
如果它是 output layer 的話<br>
<br>
0:25:56.620,0:25:59.280<br>
如果這個綠色的 neuron，它是 output layer 的話<br>
<br>
0:25:59.280,0:26:03.480<br>
要計算這兩個東西，就是秒算，沒有問題<br>
<br>
0:26:03.700,0:26:06.240<br>
假設它不是 output layer 的話<br>
<br>
0:26:06.240,0:26:09.620<br>
你就繼續走下去<br>
<br>
0:26:09.620,0:26:11.400<br>
再看下一個 layer<br>
<br>
0:26:11.400,0:26:14.340<br>
它的 activation function input 對 C 的偏微分<br>
<br>
0:26:14.340,0:26:16.680<br>
那你就可以把這一項算出來<br>
<br>
0:26:16.960,0:26:21.160<br>
你就可以把這兩項算出來，再把這一項算出來<br>
<br>
0:26:21.500,0:26:25.860<br>
你如果沒辦法算這兩項，他們不是 output layer 的話<br>
<br>
0:26:25.860,0:26:29.000<br>
你就再去推，下一個 layer 它的偏微分會是甚 麼樣子<br>
<br>
0:26:29.000,0:26:33.680<br>
把這兩個東西推出來，然後再往前把這兩個東西推出來<br>
<br>
0:26:33.860,0:26:38.540<br>
那你可能會想說，這個方法聽起來還頗崩潰<br>
<br>
0:26:38.540,0:26:42.040<br>
就是你每次要算一個微分的值<br>
<br>
0:26:42.040,0:26:46.620<br>
你要一直往後走，走到 network 的 output<br>
<br>
0:26:46.620,0:26:51.660<br>
如果 network 有 10 層，那你從第一層開始往後展開<br>
<br>
0:26:51.660,0:26:54.580<br>
感覺應該是一個很可怕的式子<br>
<br>
0:26:55.160,0:26:57.960<br>
但是實際上，並不是這樣子做的<br>
<br>
0:26:57.960,0:27:00.840<br>
實際上，你只要換一個方向<br>
<br>
0:27:00.840,0:27:06.620<br>
從 output layer 的 ∂C/∂z 開始算<br>
<br>
0:27:06.620,0:27:09.140<br>
你就會發現說，它的運算量<br>
<br>
0:27:09.140,0:27:12.800<br>
跟原來的 network 的 Feedforward path<br>
<br>
0:27:12.800,0:27:15.440<br>
其實是一樣的<br>
<br>
0:27:15.440,0:27:18.480<br>
假設我們現在有 6 個 neuron<br>
<br>
0:27:18.720,0:27:22.280<br>
每一個 neuron，它的 activation function<br>
<br>
0:27:22.280,0:27:25.420<br>
input 分別是 z1, z2, z3 一直到 z6<br>
<br>
0:27:25.420,0:27:31.340<br>
我們現在要計算這些 z 對 C 的偏微分<br>
<br>
0:27:31.340,0:27:36.020<br>
那本來呢，我們應該是想要知道 z1 的偏微分<br>
<br>
0:27:36.020,0:27:39.540<br>
你就要算 z3, z4 的偏微分<br>
<br>
0:27:39.540,0:27:43.740<br>
假如想知道 z3 的偏微分，你就要算 z5 跟 z6 的偏微分<br>
<br>
0:27:43.740,0:27:47.320<br>
想要知道 z4 的偏微分，你就要算 z5 跟 z6 的偏微分<br>
<br>
0:27:47.320,0:27:50.160<br>
那你要先算出 z5, z6 的偏微分，你才能算出<br>
<br>
0:27:50.160,0:27:52.080<br>
z3 的偏微分，z4 的偏微分<br>
<br>
0:27:52.080,0:27:55.080<br>
你才能夠算出 z1 的偏微分，z2 的偏微分<br>
<br>
0:27:55.520,0:27:58.000<br>
那如果我們今天是從<br>
<br>
0:27:58.000,0:28:00.140<br>
z1, z2 的偏微分開始算<br>
<br>
0:28:00.140,0:28:01.880<br>
那就沒有效率<br>
<br>
0:28:01.880,0:28:06.260<br>
但是，如果你反過來先算 z5, z6 的偏微分的話<br>
<br>
0:28:06.260,0:28:09.660<br>
這個 process，就突然之間變得有效率起來了<br>
<br>
0:28:09.660,0:28:15.000<br>
我們就先算 z5, z6 對 C 的偏微分<br>
<br>
0:28:15.340,0:28:17.220<br>
然後，算出這兩項以後<br>
<br>
0:28:17.220,0:28:21.420<br>
你就可以算出 z3, z4 對 C 的偏微分<br>
<br>
0:28:21.420,0:28:26.040<br>
然後，你就可以算出 z1, z2 對 C 的偏微分<br>
<br>
0:28:26.260,0:28:29.440<br>
這樣大家懂嗎？講到這邊大家有問題嗎？<br>
<br>
0:28:29.920,0:28:34.100<br>
沒有哦，那這整件事情可以說<br>
<br>
0:28:34.100,0:28:36.140<br>
這兩個東西怎麼得到它的偏微分<br>
<br>
0:28:36.140,0:28:38.200<br>
這兩個東西怎麼得到它的偏微分呢？<br>
<br>
0:28:38.420,0:28:41.780<br>
我們剛才已經看過了，就是用一個 op-amp 來算<br>
<br>
0:28:41.780,0:28:48.800<br>
這每一個 op-amp 它放大的倍率呢<br>
<br>
0:28:48.800,0:28:53.440<br>
就是 σ'(z1), σ'(z2), σ'(z3), σ'(z4)<br>
<br>
0:28:53.440,0:28:55.300<br>
所以，你算出了<br>
<br>
0:28:55.300,0:29:00.500<br>
你就先很快地計算 ∂C/∂(z5), ∂C/∂(z6)<br>
<br>
0:29:00.500,0:29:06.580<br>
然後，再把這一個偏微分的值，跟這個偏微分的值<br>
<br>
0:29:06.580,0:29:07.900<br>
乘上一些 weight<br>
<br>
0:29:07.900,0:29:09.320<br>
乘上一些 weight<br>
<br>
0:29:09.320,0:29:10.840<br>
再通過 op-amp<br>
<br>
0:29:10.840,0:29:13.520<br>
你就得到這兩個偏微分的值<br>
<br>
0:29:13.520,0:29:15.880<br>
再乘上一些 weight<br>
<br>
0:29:15.880,0:29:17.220<br>
再乘上一些 weight<br>
<br>
0:29:17.220,0:29:20.640<br>
再通過一個 op-amp，就得到一些偏微分的值<br>
<br>
0:29:20.640,0:29:24.040<br>
就這樣，就計算完了<br>
<br>
0:29:24.040,0:29:28.040<br>
就計算完了，所以，你再算 Backpropagation 的時候<br>
<br>
0:29:28.040,0:29:31.780<br>
你在這個步驟，叫做 Backward pass<br>
<br>
0:29:31.780,0:29:34.500<br>
你在做這個 Backward pass 的時候<br>
<br>
0:29:34.500,0:29:39.980<br>
你實際上的做法就是，建另外一個 neural network<br>
<br>
0:29:39.980,0:29:42.620<br>
我們本來有一個正向的 neural network，裡面的<br>
<br>
0:29:42.620,0:29:46.300<br>
activation function 都是 sigmoid function<br>
<br>
0:29:46.300,0:29:48.780<br>
那現在，你在算 Backward pass 的時候<br>
<br>
0:29:48.780,0:29:52.020<br>
你就是建一個反向的 neural network<br>
<br>
0:29:52.020,0:29:53.520<br>
反向的 neural network<br>
<br>
0:29:53.520,0:29:56.440<br>
從右邊到左邊，反向的 neural network<br>
<br>
0:29:56.440,0:29:59.260<br>
這個反向的 neural network，它的 activation function 呢<br>
<br>
0:29:59.260,0:30:01.780<br>
你要先算完 Forward pass 以後<br>
<br>
0:30:01.780,0:30:04.300<br>
你才算得出來，然後，接下來呢<br>
<br>
0:30:04.300,0:30:06.900<br>
這個反向的 neural network，它的 input 就是這兩項<br>
<br>
0:30:06.900,0:30:07.880<br>
這兩項<br>
<br>
0:30:07.880,0:30:10.780<br>
然後，其他部分就跟一般的 neural network 運算一樣<br>
<br>
0:30:10.780,0:30:13.340<br>
你就做一個 Backward pass<br>
<br>
0:30:13.340,0:30:15.560<br>
但是，其實就是做一個 neural network 的運算<br>
<br>
0:30:15.560,0:30:20.020<br>
你就可以把每一個 z 對 C 的偏微分<br>
<br>
0:30:20.020,0:30:23.100<br>
就都算出來了，這個就是 Backward pass<br>
<br>
0:30:23.480,0:30:25.660<br>
所以，我們就 summarize 一下<br>
<br>
0:30:25.660,0:30:27.540<br>
Backpropagation 是怎麼做的<br>
<br>
0:30:27.540,0:30:30.880<br>
首先，你做一個 Forward pass<br>
<br>
0:30:30.880,0:30:34.800<br>
在做 Forward pass 的時候，你可以算出<br>
<br>
0:30:34.800,0:30:37.940<br>
只要你知道每一個 activation function 的 output<br>
<br>
0:30:38.160,0:30:40.740<br>
那 activation function 的 output 就是<br>
<br>
0:30:40.740,0:30:43.820<br>
它所連接的 weight 的 ∂z/∂w<br>
<br>
0:30:45.020,0:30:47.880<br>
那在 Backward pass 裡面，你要把<br>
<br>
0:30:47.880,0:30:50.860<br>
原來的 neural network 的方向呢<br>
<br>
0:30:50.860,0:30:55.420<br>
倒過來，你把原來的 neural network 的方向倒過來<br>
<br>
0:30:55.420,0:30:58.900<br>
那在這個倒過來的 neural network<br>
<br>
0:30:58.900,0:31:01.580<br>
它的每一個三角形的 output 呢<br>
<br>
0:31:01.580,0:31:04.260<br>
就是 ∂C/∂z<br>
<br>
0:31:05.020,0:31:07.340<br>
然後，把它們乘起來<br>
<br>
0:31:07.340,0:31:09.580<br>
你就知道某一個 weight<br>
<br>
0:31:09.580,0:31:12.000<br>
對 w 的偏微分是什麼了<br>
<br>
0:31:12.000,0:31:14.280<br>
就結束，這樣<br>
<br>
0:31:14.780,0:31:17.120<br>
講到這邊，大家有沒有甚麼問題呢？<br>
<br>
0:31:17.560,0:31:19.720<br>
就算你沒有聽懂這個東西<br>
<br>
0:31:19.720,0:31:22.040<br>
其實，也沒有什麼關係啦<br>
<br>
0:31:22.040,0:31:25.640<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
