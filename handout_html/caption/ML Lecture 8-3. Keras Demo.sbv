0:00:00.000,0:00:02.760
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:03.060,0:00:05.180
現場打一下，有同學可能會問說

0:00:05.180,0:00:08.180
怎麼不在家事先寫好這樣子

0:00:08.180,0:00:11.620
就是這個要現場寫才潮這樣子

0:00:12.780,0:00:15.360
在這裡可以讓你知道說

0:00:15.360,0:00:18.260
這個 task 有多麼的簡單

0:00:18.260,0:00:22.240
這個字會不會太大，我們把它縮小一點

0:00:26.100,0:00:29.240
有人就覺得說，這個 deep learning

0:00:29.240,0:00:30.760
聽起來這麼潮的東西

0:00:30.760,0:00:32.940
它應該非常非常的複雜

0:00:32.940,0:00:34.800
它應該非常難實作，但其實不會

0:00:34.800,0:00:37.120
因為，現在 toolkit 的 framework

0:00:37.120,0:00:38.800
都非常的方便

0:00:38.800,0:00:40.600
我覺得它就像是那個

0:00:40.600,0:00:44.120
二向箔一樣，隨手拿出來就可以給別人大為打擊

0:00:44.125,0:00:46.375
沒有人知道二向箔是甚麼就算了

0:00:48.460,0:00:50.800
這個就是隨手就可以拿出來的東西

0:00:50.800,0:00:53.515
那你要 import 一些東西

0:00:53.520,0:00:55.340
你就自己回去 import  一下

0:00:55.340,0:00:58.520
那你能這邊先寫了一個 load data 的 function

0:00:58.660,0:01:01.655
這個內部的細節，你就不用管了

0:01:01.655,0:01:04.755
那你就先 call 一下 load data 的 function

0:01:04.755,0:01:05.920
把 data 先 load 一下

0:01:05.920,0:01:08.225
我們現在要做的是手寫數字的辨識

0:01:11.315,0:01:12.715
然後，load data

0:01:15.080,0:01:18.880
等一下，我們先看一下 x_train 跟 y_train

0:01:18.880,0:01:21.200
這邊是 call TensorFlow，那你可以看到它這個

0:01:21.200,0:01:23.580
有說 using TensorFlow backend

0:01:24.440,0:01:26.940
我們來看一下 x_train 呢

0:01:26.940,0:01:30.060
長甚麼樣子，我們打 x_train.shape

0:01:32.800,0:01:36.820
那這告訴我們說，x_train 是一個二維的向量

0:01:36.820,0:01:39.360
那它的第一個維度是一萬

0:01:39.360,0:01:42.580
那第二個維度的 dimension 是 784

0:01:42.580,0:01:44.580
那這甚麼意思呢？這告訴我們說

0:01:44.580,0:01:46.940
現在 training data 總共一萬筆，每一筆呢

0:01:46.940,0:01:51.120
它是由一個 784 維的 vector 所表示的

0:01:51.120,0:01:54.000
那我們來把第一個 vector 拿出來看

0:01:54.000,0:01:55.995
它長的就是這個樣子

0:01:56.000,0:02:00.960
它是一個很長的 vector，有 784 維的 vector

0:02:01.060,0:02:03.740
那在這個 vector 裡面，多數的值都是 0

0:02:03.740,0:02:06.100
那有少部分的值，你會發現說它

0:02:06.100,0:02:08.120
是介於 0~1 之間

0:02:08.160,0:02:11.220
那這些介於 0~1 之間的值就代表說

0:02:11.220,0:02:12.760
現在這個 pixel

0:02:13.260,0:02:15.300
它有沒有被塗黑

0:02:15.300,0:02:17.520
塗得最黑就是 1

0:02:17.520,0:02:23.760
所以，這邊這個數值代表說這個 pixel 的顏色有多深

0:02:25.725,0:02:27.620
其實，你很難從這個

0:02:27.620,0:02:30.520
vector 裡面就看出說，它是哪一個數字

0:02:30.520,0:02:33.320
這樣你很難看出說它是哪一個數字

0:02:33.360,0:02:35.780
所以，我們來看一下它的 label

0:02:35.780,0:02:37.760
才會知道它是哪一個數字

0:02:37.760,0:02:39.140
來看一下 label

0:02:43.100,0:02:46.060
label 一樣第一個 dimension 是一萬維

0:02:46.060,0:02:47.680
第二個 dimension 是 10 維

0:02:47.680,0:02:49.280
那我們把第一筆 data

0:02:50.000,0:02:52.440
它的 label 拿出來看看

0:02:52.440,0:02:53.575
那你看第一筆 data

0:02:53.575,0:02:55.585
它的 label 是

0:02:56.240,0:02:59.060
它總共有 10 維

0:02:59.060,0:03:01.160
那你會發現說，多數的數字是 0

0:03:01.160,0:03:04.160
只有某一個維度的數字是 1

0:03:06.520,0:03:09.460
只有某一個維度的數字是 1

0:03:09.460,0:03:12.260
那你今天算一下，它是從 0 開始算的

0:03:12.260,0:03:14.680
這個是對第一個 dimension

0:03:14.680,0:03:18.420
這個就對應到 0，0, 1, 2, 3, 4, 5

0:03:18.420,0:03:20.240
對應到 5 的那個維度是 1

0:03:20.240,0:03:22.880
意味著說我們剛才看到的那一串數字

0:03:22.880,0:03:24.280
其實代表一個數字 5

0:03:24.280,0:03:27.320
雖然，你很難看出來它是甚麼

0:03:27.320,0:03:29.280
不過，我們用這種方式告訴 machine 說

0:03:29.280,0:03:33.460
剛才那個 vector，它對應的就是數字 5

0:03:33.555,0:03:36.615
接下來，就是實際的寫一下了

0:03:36.615,0:03:37.705
那其實呢

0:03:38.035,0:03:40.895
非常非常快，現場秒寫

0:03:50.860,0:03:53.760
其實，現在是有錄影的啦

0:03:53.760,0:03:56.120
Bandicam 免費版只能錄 10 分鐘

0:03:56.120,0:03:58.195
剛才已經過 5 分鐘，所以還有 5 分鐘

0:03:58.200,0:04:00.500
要在 5 分鐘以內把它寫完

0:04:02.665,0:04:04.625
好，網路有點卡

0:04:11.880,0:04:15.920
input dimension 是 28*28，然後

0:04:15.920,0:04:18.040
unit 等於

0:04:18.500,0:04:21.120
就胡亂設個值，就 33

0:04:23.740,0:04:26.640
你就隨便挑一個你喜歡的 activation function

0:04:26.640,0:04:28.220
比如說，sigmoid

0:04:28.225,0:04:30.905
好，行，然後再加一層

0:04:32.780,0:04:35.780
然後，第二層你就不需要再給它 input dimension

0:04:35.780,0:04:38.455
你就看 unit 要設什麼，比如說，633

0:04:38.460,0:04:41.240
如果我有拼錯的話，記得告訴我

0:04:41.240,0:04:42.820
(activation function = sigmoid)

0:04:43.560,0:04:46.940
然後，再加一層

0:04:49.115,0:04:52.335
也是 633 這樣

0:04:52.745,0:04:54.935
然後，最後 output 的 layer 呢

0:04:55.040,0:04:58.080
我們就一定要是 10 維

0:04:58.080,0:04:59.680
弄別的它不會給你過

0:05:00.125,0:05:01.985
一定只能有 10 個 unit

0:05:01.985,0:05:03.880
activation function 其實你選別的也行

0:05:03.880,0:05:05.145
不過，我們就

0:05:05.145,0:05:08.880
用傳統的方法選 softmax

0:05:10.455,0:05:13.475
這樣 network 就建完了，再來就是下 config

0:05:13.475,0:05:16.305
model.compile

0:05:16.315,0:05:18.975
然後，我們選一個 loss function

0:05:18.980,0:05:20.560
選甚麼好呢？

0:05:20.560,0:05:22.400
我們就選 mean square error

0:05:22.400,0:05:24.135
你可能會問說為甚麼選 mean square error 呢

0:05:24.135,0:05:27.275
因為它數字最短，打起來最快

0:05:28.900,0:05:32.560
optimizer 就 SGD，我們都學過 SGD 了

0:05:32.560,0:05:36.100
然後，要設一下，SGD 這邊指的就是 Gradient Descent

0:05:36.480,0:05:39.580
learning rate 設 0.1

0:05:40.060,0:05:42.240
learning rate 你是要手調一下的

0:05:42.240,0:05:44.200
然後，接下來

0:05:44.535,0:05:45.835
matrix

0:05:51.140,0:05:54.060
compile 也完了，再來就是 train 它

0:05:54.080,0:05:56.580
fit 這樣，非常快

0:05:58.820,0:06:01.325
然後，有兩個 flag 要下

0:06:01.325,0:06:04.065
這個在 video 裡面有說明

0:06:04.975,0:06:07.955
我們有說明甚麼是 batch_size

0:06:07.955,0:06:10.485
甚麼是 epochs

0:06:11.225,0:06:13.175
下 20 好了

0:06:14.060,0:06:17.220
最後，然後下完這行之後

0:06:17.220,0:06:19.420
你就可以假設你的 model 已經 train 完了

0:06:19.420,0:06:22.000
到目前為止，才過了一分半而已這樣子

0:06:22.640,0:06:24.780
再來，我們就是要把

0:06:24.780,0:06:27.940
拿 model 來做 evaluate，看它作的怎麼樣

0:06:36.895,0:06:39.965
我們就是把 testing data 拿出來

0:06:39.965,0:06:42.180
其實 fit function 裡面你也可以 call

0:06:42.180,0:06:43.800
你也可以放 validation set

0:06:43.800,0:06:46.780
它可以自動幫你做 validation

0:06:46.780,0:06:49.760
你只要再查一下怎麼用就可以了

0:06:53.000,0:06:55.300
把輸出導出來，然後

0:06:55.300,0:06:57.060
印一下輸出

0:07:12.725,0:07:15.795
這樣我就寫完了

0:07:15.795,0:07:17.475
花不到兩分鐘

0:07:17.800,0:07:20.620
接下來呢，我們就是

0:07:20.620,0:07:23.600
實際執行一下

0:07:27.145,0:07:28.680
那你可能會問說

0:07:28.680,0:07:30.760
聽說做 deep learning 要 GPU

0:07:30.760,0:07:32.080
GPU 在哪裡呢？

0:07:32.080,0:07:33.315
不要管那麼多

0:07:33.315,0:07:35.145
TensorFlow 會幫你處理這個問題的

0:07:40.120,0:07:42.020
你就 train 很快這樣子

0:07:45.640,0:07:48.700
我知道為甚麼了，這邊怎麼會打成 score 呢

0:07:49.780,0:07:53.420
太羞愧了，好，result

0:08:11.400,0:08:13.660
要不要看一下正確率是多少在 testing 上

0:08:13.660,0:08:16.820
正確率是 11% 這樣

0:08:21.995,0:08:24.945
因為是手寫數字辨識阿，所以

0:08:24.945,0:08:27.300
random 猜也要 10% 這樣

0:08:27.300,0:08:31.100
怎麼辦呢？怎麼辦，這時候就開始焦躁了

0:08:32.445,0:08:33.445
調一下參數

0:08:34.015,0:08:37.105
我知道，一定是 633 不吉利這樣

0:08:37.105,0:08:40.020
改成 689 這樣

0:08:40.020,0:08:42.460
胡亂改、亂改

0:08:46.545,0:08:47.660
好，再 train 一次

0:08:47.660,0:08:50.100
其實 train 這個，還滿快的啦

0:08:50.100,0:08:52.220
不過這是因為這是手寫數字辨識

0:08:52.220,0:08:53.580
用別的 corpus 的話，你 train 一次

0:08:53.580,0:08:55.980
一天就過去了，三天就過去了

0:08:55.980,0:08:57.520
你就焦躁到不行這樣

0:09:04.800,0:09:07.100
欸，好一點是嗎？

0:09:07.100,0:09:08.540
好一點是嗎

0:09:08.540,0:09:11.920
那接下來，我再覺得說

0:09:11.980,0:09:14.580
你知道 deep learning 就是要很 deep 這樣子

0:09:14.580,0:09:16.980
剛才怎麼才三層，不夠 deep

0:09:16.980,0:09:18.680
for 迴圈加十層

0:09:18.680,0:09:20.600
Keras 是可以 call for 迴圈的

0:09:37.880,0:09:42.240
好，十層，你說時間快到了是嗎？

0:09:47.820,0:09:50.480
啊！結果還是 10% 這樣子

0:09:50.480,0:09:53.260
然後，你就開始焦躁不安

0:09:53.260,0:09:55.145
你可能會這樣，就是

0:09:55.145,0:09:57.780
你的老師告訴你說 deep learning 很潮

0:09:57.780,0:09:59.820
回去還不趕快給我做個 application 出來

0:09:59.820,0:10:01.260
而且 Keras 又這麼簡單

0:10:01.260,0:10:03.975
你回去還不給我寫個 10 個、8 個，然後

0:10:03.980,0:10:07.000
你 train 一次三天就過去了，再 train 一次三天又過去了

0:10:07.000,0:10:09.280
然後，把它改成很 deep 三天又過去了

0:10:09.280,0:10:12.700
然後，過了一個禮拜，你就發現說你什麼都沒有做出來

0:10:14.280,0:10:17.580
然後，你參數就調來調去

0:10:17.580,0:10:21.020
你就發現說，欸，怎麼做都做不起來

0:10:21.020,0:10:24.240
然後，最後你就只好從入門到放棄

0:10:25.480,0:10:27.700
所以

0:10:27.700,0:10:29.235
很多人會擔心說

0:10:29.240,0:10:32.140
人工智慧會不會統治世界啊？

0:10:32.140,0:10:34.780
取代人類啊？我覺得在問這個問題之前

0:10:34.780,0:10:37.480
你要不要先把這個很簡單的 task 做好

0:10:37.480,0:10:38.860
而且你都做過作業一了

0:10:38.860,0:10:40.040
我相信做過作業以後

0:10:40.040,0:10:42.980
你一定不會有人工智慧統治這個世界這個問題

0:10:42.980,0:10:47.260
你會說怎麼連 Linear Regression 都做不起來呢？

0:10:50.445,0:10:53.595
就是這個樣子，所以我們今天沒有做起來

0:10:53.880,0:10:56.180
然後，到底要怎麼把它做起來呢？

0:10:56.180,0:11:00.200
就是請看介紹的錄影，會告訴你說

0:11:00.200,0:11:02.320
deep learning 有甚麼樣的 tip

0:11:02.355,0:11:04.120
當你做不起來的時候，在放棄之前

0:11:04.120,0:11:06.300
有什麼事情是你可以做的

0:11:06.300,0:11:08.340
今天就講到這邊，謝謝大家，謝謝！

0:11:08.340,0:11:12.900
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw
