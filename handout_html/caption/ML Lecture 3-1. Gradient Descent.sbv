0:00:01.240,0:00:09.980
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw

0:00:20.820,0:00:22.480
好，各位同學大家早

0:00:22.480,0:00:24.840
那我們就開始上課吧！

0:00:26.500,0:00:30.260
今天的第一堂課，我們要講的是 Gradient Descent

0:00:30.260,0:00:34.580
Gradient Descent 我們上次已經大概講過怎麼做了

0:00:34.580,0:00:38.220
但是有一些小技巧呢，你可能是不知道的

0:00:38.220,0:00:43.720
所以我們要再詳細說明一下，Gradient Descent
你要怎麼把它做得更好

0:00:44.880,0:00:47.120
好，那我們上次是這樣說的

0:00:48.020,0:00:51.180
在整個 machine learning 的第3個步驟

0:00:51.180,0:00:54.340
我們要找一個最好的 function

0:00:54.340,0:00:56.420
那找一個最好的 function 這件事呢

0:00:56.420,0:00:59.700
是要解一個 optimization 的 problem

0:00:59.700,0:01:04.180
也就是說，在第二步，
我們先定了一個 loss function： L

0:01:04.180,0:01:08.280
這個 loss function 呢，它是一個 functional unction

0:01:08.280,0:01:11.820
你把一個 function 代到這個 loss function 裡面

0:01:11.820,0:01:16.800
或者是你把一個操控 function 形狀的參數

0:01:16.800,0:01:21.780
我們現在，在這張投影片裡面，把那些參數寫成 θ

0:01:21.780,0:01:24.840
L 是 loss function，θ 是那些參數

0:01:24.840,0:01:28.640
你把一組參數代到一個 loss function 裡面

0:01:28.640,0:01:32.160
它就會告訴你說，這組參數有多不好

0:01:32.160,0:01:36.020
那你接下來，要做的事情呢，就是

0:01:36.880,0:01:41.040
從一開始，我就發現這張投影片，有一個地方寫錯了

0:01:41.780,0:01:44.580
這裡應該是 minimum 啦

0:01:44.580,0:01:46.560
因為它是 loss function 嘛！

0:01:46.560,0:01:49.320
那 loss function 我們是希望它越小越好

0:01:49.320,0:01:52.940
當然你也可以反過來定義一個 function 是

0:01:53.640,0:01:57.260
參數越好，它 output 的值越大

0:01:57.260,0:02:00.320
那這時候就不會叫它 loss function，就會叫他別的名字

0:02:00.320,0:02:02.880
比如說，叫它 ****** function

0:02:02.880,0:02:04.440
好，這邊應該是 minimum

0:02:04.440,0:02:10.480
我們要找一組參數 θ，讓這個  loss function 越小越好

0:02:10.780,0:02:12.300
那這件事情怎麼做呢？

0:02:12.300,0:02:14.260
我們可以用  Gradient Descent

0:02:14.260,0:02:19.240
假設現在這個 θ 是一個參數的 set

0:02:19.240,0:02:22.360
那裡面有兩個參數：θ1 跟 θ2

0:02:22.360,0:02:27.320
首先，你先隨機的選取一個起始的點

0:02:27.320,0:02:29.900
隨機選取一組起始的參數

0:02:29.900,0:02:34.880
這邊寫成 θ(上標0, 下標1)、θ(上標0, 下標2)

0:02:34.880,0:02:38.560
用上標 0 來代表說，它是初始的那一組參數

0:02:38.560,0:02:40.180
那用下標代表說

0:02:40.180,0:02:44.160
這個是這一組參數裡面的
第 1 個 component 跟第 2 個 component

0:02:44.160,0:02:50.140
接下來，你計算 θ(上標0, 下標1) 跟 θ(上標0, 下標2)

0:02:50.140,0:02:53.660
對這個 loss function 的偏微分

0:02:53.660,0:02:57.880
計算他們的偏微分，然後把這個

0:02:57.880,0:03:02.860
θ(上標0, 下標1)、θ(上標0, 下標2) 減掉 learning rate

0:03:02.860,0:03:07.320
乘上這個偏微分的值，得到一組新的參數

0:03:07.320,0:03:12.640
這個參數，這邊寫做
 θ(上標1, 下標1) 跟 θ(上標1, 下標2)

0:03:12.640,0:03:15.760
θ 上標1，代表說它是在第二個時間點

0:03:15.760,0:03:18.840
由 θ 上標0 update 以後得到的參數

0:03:18.840,0:03:21.700
下標代表說，它有兩個 component

0:03:21.700,0:03:24.880
那同樣的步驟，你就反覆不斷地進行

0:03:24.880,0:03:30.260
接下來你有 θ(上標1, 下標1) 跟 θ(上標1, 下標2) 以後呢

0:03:30.260,0:03:32.360
你就一樣去計算它們的偏微分

0:03:32.360,0:03:33.940
再乘上 learning rate，

0:03:33.940,0:03:40.660
再去減 θ(上標1, 下標1) 跟 θ(上標1, 下標2)

0:03:40.660,0:03:43.400
得到下一組參數，每個反覆進行這個 process

0:03:43.400,0:03:45.200
這個就是 Gradient Descent

0:03:45.200,0:03:47.900
那如果你想要也得更簡潔一點的話呢

0:03:47.900,0:03:49.040
其實你可以這樣寫

0:03:49.040,0:03:54.220
假設你現在有兩個參數：θ1 跟 θ2

0:03:54.220,0:03:59.940
這個應該是 L，不好意思，
這個 C 呢，這邊應該是寫成 L

0:03:59.940,0:04:04.840
你把這個 θ1 跟 θ2 對這個 loss function 做偏微分

0:04:04.840,0:04:09.320
你把這兩個偏微分得到的值串在一起，
變成一個 vector 以後

0:04:09.320,0:04:12.520
那這個 vector 呢，就叫做 Gradient

0:04:12.520,0:04:15.940
你把 L 前面，加一個倒三角形

0:04:15.940,0:04:18.000
這個東西呢，就叫做 Gradient

0:04:18.000,0:04:19.880
那它其實是一個 vector

0:04:19.880,0:04:22.580
所以你可以把這個式子寫成

0:04:22.580,0:04:26.620
你把這個東西，寫成一個 vector，θ 上標 1

0:04:26.620,0:04:29.540
這個東西，寫成一個 vector，θ 上標 0

0:04:29.540,0:04:32.660
這一項就是這一項

0:04:32.660,0:04:37.600
那你就寫成呢，L 在 (θ上標0) 這個地方的 Gradient

0:04:37.600,0:04:40.440
所以你可以把 update 參數呢

0:04:40.440,0:04:45.260
簡單寫成 (θ上標0) 減掉 learning rate

0:04:45.260,0:04:47.420
乘上 Gradient，就等於(θ上標1)

0:04:47.420,0:04:51.780
同理，(θ上標1) 減掉 learning rate 乘上 Gradient，
就得到 (θ上標2)

0:04:51.780,0:04:54.260
如果把它 visualize 的話呢

0:04:54.260,0:04:56.500
它看起來像是這個樣子

0:04:56.500,0:04:59.060
假設我們現在有兩個參數，θ1、θ2

0:04:59.060,0:05:02.720
那你隨機的選一個初始的位置，θ^0

0:05:03.180,0:05:07.500
然後接下來呢，你計算在 θ^0 這個點

0:05:07.500,0:05:13.780
這個參數對 loss function 的 Gradient

0:05:13.780,0:05:17.240
假設參數對  loss function 的 Gradient
是這個紅色的箭頭

0:05:17.240,0:05:20.780
Gradient 就是一個 vector，它是一個紅色的箭頭

0:05:21.140,0:05:24.600
那，如果你不知道 Gradient 是甚麼的話呢

0:05:24.600,0:05:29.180
你就想成，它是等高線的法線方向

0:05:29.180,0:05:32.420
這個箭頭，它指的方向就是

0:05:32.420,0:05:34.940
如果你把 loss 的等高線畫出來的話呢

0:05:34.940,0:05:39.020
這個箭頭指的方向，就是等高線的法線方向

0:05:39.660,0:05:42.020
那怎麼 update 參數呢？

0:05:42.020,0:05:46.880
你就把這個 Gradient 乘上 learning rate

0:05:46.880,0:05:48.720
然後再取一個負號

0:05:48.720,0:05:51.060
Gradient 乘上 learning rate，然後再取一個負號

0:05:51.060,0:05:52.580
就是這個藍色的箭頭

0:05:52.580,0:05:56.000
再把它加上 θ^0 ，就得到 θ ^1

0:05:56.000,0:05:58.540
那這個步驟就反覆地持續進行下去

0:05:58.540,0:06:02.100
再計算一遍  Gradient

0:06:02.100,0:06:04.240
你得到另外一個紅色的箭頭

0:06:04.240,0:06:06.680
紅色箭頭指向這個地方

0:06:06.680,0:06:11.840
那你現在走的方向呢，就變成是紅色的箭頭的相反

0:06:11.840,0:06:16.160
紅色箭頭乘上一個負號，再乘上 learning rate

0:06:16.160,0:06:19.720
就是現在這個藍色的箭頭 ，得到 θ^2

0:06:19.720,0:06:21.880
這個步驟就反覆一直進行下去

0:06:21.880,0:06:25.220
再算一下在 θ^2 這個地方的 Gradient

0:06:25.220,0:06:27.140
然後再決定要走的方向

0:06:27.140,0:06:29.380
再算一次 Gradient，再決定要走的方向

0:06:29.380,0:06:31.320
這個就是 Gradient Descent

0:06:31.320,0:06:33.920
這些，我們上次其實都講過了

0:06:33.920,0:06:36.900
接下來呢，要講一些 Gradient Descent 的 tip

0:06:36.960,0:06:38.440
首先，第一件事情就是

0:06:38.440,0:06:42.500
你要小心地調你的 learning rate

0:06:43.020,0:06:46.220
如果你已經開始做作業一的話呢 ，你會知道說

0:06:46.220,0:06:50.520
有時候，learning rate 是可以給你造成一些問題的

0:06:51.320,0:06:56.460
舉例來說，假設這個是我們的 loss function 的 surface，
假設長這樣子

0:06:56.460,0:06:59.300
如果你今天 learning rate 調剛剛好的話

0:06:59.300,0:07:03.380
你從左邊這邊開始，那你可能就是順著紅色的箭頭

0:07:03.380,0:07:05.720
很順利地走到了最低點

0:07:06.660,0:07:09.400
可是，如果你今天 learning rate 調的太小的話

0:07:09.400,0:07:12.620
那它走的速度呢，會變得非常慢

0:07:12.620,0:07:18.200
雖然只要給它夠多的時間，它終究會
走到這個 local minimum 的地方

0:07:18.200,0:07:19.980
但是如果它走得太慢的話呢

0:07:19.980,0:07:22.080
你會沒有辦法接受這件事情

0:07:22.700,0:07:24.400
你可能會來不及交作業

0:07:24.400,0:07:27.400
如果你今天這個 learning rate 調得稍微大一點

0:07:27.400,0:07:29.160
比如說，像綠色這個箭頭的話

0:07:29.160,0:07:32.940
那就變成說，它的步伐太大了

0:07:32.940,0:07:34.980
它變得像一個巨人一樣，步伐太大了

0:07:34.980,0:07:38.020
它永遠沒有辦法走到這個特別低的地方

0:07:38.020,0:07:41.480
它都在這個山谷的口上面震盪

0:07:41.480,0:07:43.620
它永遠走不下去，那甚至如果

0:07:43.620,0:07:45.800
你今天真的把 learning rate 調太大的話

0:07:45.800,0:07:48.640
它可能就，一瞬間就飛出去了

0:07:48.640,0:07:50.700
結果你 update 參數以後

0:07:50.700,0:07:53.060
loss 反而越 update，越大

0:07:53.440,0:07:58.660
那其實只有在你的參數是一維或二維的時候

0:07:58.660,0:08:01.980
你才能夠把這樣子的圖 visualize 出來

0:08:01.980,0:08:04.800
如果你今天是有很多維參數

0:08:04.800,0:08:07.740
這個 error 的 surface，在這個高維的空間裡面

0:08:07.740,0:08:09.540
你是沒有辦法 visualize 它的

0:08:09.540,0:08:12.520
但是，有另外一個東西，你是可以 visualize 的

0:08:12.520,0:08:13.840
什麼東西呢？

0:08:13.840,0:08:21.800
你可以 visualize 參數的變化對這個 loss 的變化

0:08:21.800,0:08:25.060
你可以 visualize  每次參數 update 的時候

0:08:25.060,0:08:27.540
loss 的改變的情形

0:08:27.540,0:08:31.540
所以，今天如果你 learning rate 設得太小的話

0:08:31.540,0:08:36.220
你就會發現說，這個 loss 它下降地非常非常慢

0:08:36.220,0:08:40.000
今天如果你 learning rate 調得太大的話

0:08:40.000,0:08:43.660
你在左邊這個圖會看到說 loss 先快速地下降

0:08:43.660,0:08:45.920
接下來呢，它就卡住了

0:08:45.920,0:08:47.860
所以，如果你 learning rate 調得太大的話

0:08:47.860,0:08:53.580
你把參數的 update 對 loss 的變化，
做出來會看到的是綠色這條線

0:08:53.580,0:08:58.140
你的 loss 很快地下降，但很快地卡住了，
很快地不再下降

0:08:58.140,0:09:01.780
那如果你今天 learning rate 是調得太大，你就會發現

0:09:01.780,0:09:07.180
你的 loss 就飛出去了，你需要調整它，讓它調到剛剛好

0:09:07.180,0:09:09.460
那你才能夠得到一個好的結果

0:09:09.460,0:09:11.380
所以你在做 Gradient Descent 的時候

0:09:12.120,0:09:15.960
你應該要把這個圖畫出來

0:09:15.960,0:09:18.580
沒有把這個圖畫出來，你就會非常非常地卡

0:09:18.580,0:09:23.880
有人就說，它就把 Gradient 的程式寫好

0:09:23.880,0:09:26.820
那寫好放下去之後開始跑，他去打一場 LOL

0:09:26.820,0:09:30.460
然後，打完回來就發現說：結果爛掉啦

0:09:30.460,0:09:32.520
然後，他也不知道爛在哪裡這樣子

0:09:32.520,0:09:34.260
所以如果你在做 Gradient Descent 的時候

0:09:34.260,0:09:35.680
你應該把這個圖畫出來

0:09:35.680,0:09:39.960
然後你要先看一下，它前幾次 update 參數的時候

0:09:39.960,0:09:42.620
它 update 的走法是怎麼樣

0:09:42.620,0:09:47.060
搞不好它，你 learning rate 調太大，它一下子就爆炸了

0:09:47.060,0:09:49.420
所以這個時候，你就知道你要趕快調 learning rate

0:09:49.420,0:09:51.920
你要確定它呢，是穩定地下降

0:09:51.920,0:09:53.660
才能去打 LOL 這樣子

0:09:54.580,0:09:58.680
好，但是要調 learning rate 很麻煩

0:09:58.680,0:10:01.560
有沒有辦法自動地調 learning rate 呢？

0:10:01.560,0:10:04.640
有一些自動的方法可以幫我們調 learning rate

0:10:04.820,0:10:07.940
最基本而簡單的大原則是

0:10:07.940,0:10:15.640
通常 learning rate 是隨著參數的 update 會越來越小的

0:10:15.640,0:10:18.140
為甚麼會這樣呢？

0:10:18.140,0:10:21.180
因為當你在剛開始的起始點的時候

0:10:21.180,0:10:25.780
它通常離最低點，是比較遠的

0:10:25.780,0:10:28.480
所以你步伐呢，要踏大一點

0:10:28.480,0:10:31.400
就是走得快一點，才能夠趕快走到最低點

0:10:31.640,0:10:36.500
但是，經過好幾次的參數 update 以後呢？

0:10:36.500,0:10:39.480
你已經比較靠近你的目標了

0:10:39.480,0:10:42.860
所以這個時候呢，你就應該減少你的 learning rate

0:10:42.860,0:10:47.020
這樣它能夠收斂在你最低點的地方

0:10:47.020,0:10:49.480
舉例來說，你 learning rate 的設法可能是這樣

0:10:49.480,0:10:52.020
好，你可以設成說

0:10:53.020,0:10:58.400
這個 learning rate  是一個 t dependent 的函數

0:10:58.400,0:11:02.720
它是 depend on 你現在參數 update 的次數

0:11:02.720,0:11:04.980
在第 t 次 update 參數的時候

0:11:04.980,0:11:10.340
你就把你的 learning rate 
設成一個 constant η，除以 sqrt (t+1)

0:11:10.340,0:11:13.020
這樣當你參數 update 的次數越多的時候呢

0:11:13.020,0:11:16.060
這個 learning rate 就會越來越小

0:11:16.380,0:11:18.760
但是光這樣呢，是不夠的

0:11:19.240,0:11:22.280
你到這邊，我們需要因材施教

0:11:22.280,0:11:25.360
所以這每一個不同的參數

0:11:25.360,0:11:28.380
最好的狀況應該是，每一個不同的參數

0:11:28.380,0:11:31.840
都給它不同的 learning rate

0:11:31.840,0:11:35.960
這件事情呢，是有很多的小技巧的

0:11:35.960,0:11:40.540
其中，我覺得最簡單，最容易實作的，叫做 Adagrad

0:11:41.160,0:11:43.540
那 Adagrad 是這樣子的

0:11:43.760,0:11:47.300
他說，每一個參數的 learning rate 呢

0:11:47.300,0:11:55.440
都把它除上，之前算出來的微分值的 root mean square

0:11:55.440,0:11:58.620
什麼意思呢，我們原來的 Gradient Descent 是這樣

0:11:58.620,0:12:01.660
假設 w 是某一個參數

0:12:01.660,0:12:05.480
這個時候 w 不是一組參數，我們現在只考慮一個參數

0:12:05.480,0:12:08.180
因為我們現在，在做 Adagrad 這個做法的時候

0:12:08.180,0:12:09.980
它是 adaptive 的 learning rate

0:12:09.980,0:12:17.160
所以，每一個參數，它都有不同的 learning rate

0:12:17.160,0:12:22.260
所以呢，我們現在要把每一個參數都分開來考慮

0:12:22.260,0:12:24.080
每一個參數都分開來考慮

0:12:24.080,0:12:27.260
那 w 呢，是某一個參數

0:12:27.260,0:12:32.580
那 w 的 learning rate，在一般的 Gradient Descent 呢

0:12:33.940,0:12:37.340
depend on 時間的值，比如說 η^t

0:12:37.340,0:12:40.840
但是你可以把這件事情呢，做的更好

0:12:40.840,0:12:46.620
在 Adagrad 裡面呢， 你把這個 η^t / σ^t

0:12:47.340,0:12:52.900
這個 σ^t 是甚麼呢？ 這個 σ^t 是過去

0:12:53.900,0:12:58.940
這邊這個 g 呢，是這個偏微分的值，g 是偏微分的值

0:12:58.940,0:13:04.860
這個 σ^t 呢，是過去所有微分的值的 root mean square

0:13:04.880,0:13:06.860
是過去所有微分的值的 root mean square

0:13:06.860,0:13:11.640
這個值，對每一個參數而言，都是不一樣的

0:13:11.640,0:13:14.280
所以現在就會變成說，不同的參數

0:13:14.280,0:13:16.860
它的 learning rate 都是不一樣的

0:13:17.120,0:13:22.300
那我們實際舉個例子，來看看這件事情是怎麼實作的

0:13:22.440,0:13:25.780
假設你現在初始的值，是 w^0

0:13:26.080,0:13:28.420
假設你初始的值，是 w^0

0:13:28.420,0:13:31.520
那接下來呢，你就計算在 w^0 那點的微分

0:13:31.520,0:13:33.760
這邊呢，寫作 g^0

0:13:34.420,0:13:37.160
然後，它的 learning rate 是多少呢？

0:13:37.160,0:13:42.380
它的 learning rate 是  η^0 / σ^0

0:13:42.380,0:13:46.160
η^0 是一個時間 dependent 的參數 ，那 σ^0 是甚麼呢？

0:13:46.160,0:13:48.960
σ^0 是一個參數 dependent 的參數

0:13:48.960,0:13:54.640
σ^0 它是過去算過所有微分值的 root mean square

0:13:54.640,0:13:57.980
那在這個 case 裡面，我們過去只算過
一個微分值，就是 g^0

0:13:58.280,0:14:01.440
所以這個 σ^0，就是 g^0 的平方再開根號

0:14:02.360,0:14:04.760
那接下來呢，你再 update 參數

0:14:04.760,0:14:09.560
你把這個 w^0 更新變成 w^1

0:14:10.020,0:14:12.620
在 w^1 這個地方，你再算一次 Gradient，就是 g^1

0:14:13.040,0:14:15.400
那 g^1 的 learning rate 應該乘上多少呢？

0:14:15.400,0:14:19.640
它要乘 η^1 / σ^1

0:14:19.640,0:14:24.940
那 σ^1，它是過去所有微分值的 root mean square

0:14:24.940,0:14:27.180
過去我們已經算過兩次微分值

0:14:27.180,0:14:29.740
一次是 g^0，一次是 g^1

0:14:29.740,0:14:34.580
所以 σ^1，就變成 g^0 跟 g^1 的 root mean square

0:14:34.580,0:14:40.580
也就是把 g^0 平方再加 g^1 平方，
再取平均值 ，再開根號

0:14:41.440,0:14:46.740
那 w^3 一樣，你就是 update 參數就得到 w^2

0:14:46.740,0:14:49.640
你有了 w^2 以後，你可以算 g^2，

0:14:49.640,0:14:51.960
在 w^2 地方的微分值就是 g^2

0:14:51.960,0:14:55.300
它的 learning rate 就是 η^2 / σ^2

0:14:55.300,0:14:58.980
那這個 σ^2呢 ，就是過去
算出來所有微分值的 root mean square

0:14:58.980,0:15:02.800
過去算出 g^0, g^1, g^2，你就把 g^0, g^1, g^2 都平方

0:15:02.800,0:15:05.320
再平均，然後再開根號

0:15:05.320,0:15:09.100
得到σ^2，然後把它放在這邊，搭配參數得到 w^3

0:15:09.100,0:15:11.620
這個步驟呢，就反覆地一直繼續下去

0:15:12.300,0:15:15.960
到第 t 次 update 參數的時候

0:15:15.960,0:15:18.460
你有一個微分直 g^t

0:15:18.880,0:15:23.960
那這個 g^t 的 learning rate 就是 η^t / σ^t

0:15:23.960,0:15:27.280
這個 σ^t，是過去所有微分的值的 root mean square

0:15:27.280,0:15:31.520
過去已經算出 g^0, g^1, g^2 .... 一直到 g^t

0:15:31.520,0:15:34.280
你就把 g^0, g^1, g^2 .... 一直到 g^t

0:15:34.280,0:15:37.620
都取平方，再加起來，再平均，再開根號

0:15:37.620,0:15:40.720
得到 σ^t，就把它放在這邊

0:15:41.580,0:15:46.660
所以，現在如果我們用 Adagrad 的時候呢

0:15:46.660,0:15:50.580
它 update 參數的式子，寫成這樣子

0:15:50.580,0:15:54.540
這個 σ^t，我們就寫成這樣子

0:15:54.540,0:15:57.800
這個 root mean square，我們在前頁投影片已經看到的

0:15:58.440,0:16:02.040
那這個 time dependent 的 learning rate 呢？

0:16:02.040,0:16:06.900
這個 time dependent 的 learning rate，
你可以寫成 η / sqrt(t+1)

0:16:07.500,0:16:11.240
那你可以發現說，當你把這一項除以這一項的時候

0:16:11.240,0:16:13.580
因為他們都有：根號 (t+1)

0:16:13.580,0:16:17.120
所以根號 (t+1) 是可以刪掉的

0:16:17.120,0:16:21.260
所以整個 Adagrad 的式子，你就可以寫成

0:16:21.260,0:16:25.100
它的 learning rate，你就可以寫成，一個 constant η

0:16:25.660,0:16:33.080
除掉 根號(過去所有 gradient 的平方和)

0:16:33.080,0:16:35.280
但不用算平均這樣

0:16:35.420,0:16:42.600
因為平均這件事情，會跟上面 
time dependent 的 learning rate 抵銷掉

0:16:42.600,0:16:46.300
所以你在寫 Adagrad 的式子的時候，你可以簡化成

0:16:46.300,0:16:50.660
不需要把 time dependent 的這件事情 explicitly 的寫出來

0:16:50.660,0:16:52.960
你就直接把你的 learning rate 寫成

0:16:52.960,0:16:58.380
η 除以 根號(過去算出來的 gradient 的平方)

0:16:58.380,0:17:00.700
再開根號就好

0:17:02.120,0:17:05.520
這個方法，你可以接受嗎？

0:17:05.920,0:17:09.160
講到這邊，大家有問題嗎？來，請說

0:17:09.160,0:17:14.240
在做 Adagrad 的時候，它在後面下降的速度，
慢到令人髮指

0:17:14.240,0:17:16.700
慢到令人髮指是嗎？

0:17:16.700,0:17:20.800
就是看的過1000筆資料，才降不到0.1

0:17:20.800,0:17:23.540
這是正常的嗎？

0:17:23.540,0:17:25.540
這其實是正常的

0:17:25.540,0:17:32.120
就是說 Adagrad 它的參數 update，其實
整體而言是會是越來越慢的

0:17:32.140,0:17:34.660
因為它有加上 time depend

0:17:34.660,0:17:37.820
如果你不喜歡這個結果的話

0:17:37.820,0:17:41.020
有很多比這個更強的方法

0:17:41.020,0:17:44.480
這個 adaptive 的 learning rate 其實是一系列的方法

0:17:44.480,0:17:47.180
今天講 Adagrad 其實是裡面最簡單的

0:17:47.180,0:17:50.380
還有很多其他的，他們都是用 Ada- 開頭這樣子

0:17:50.380,0:17:53.240
比如說 Adadelta, Adam 阿，甚麼之類的

0:17:53.240,0:17:55.360
所以，如果你用別的方法

0:17:55.360,0:18:01.620
比如說，Adam 的話，它就比較不會有這個情形這樣子

0:18:03.280,0:18:07.660
其實，如果你沒有甚麼特別偏好的話

0:18:07.660,0:18:11.900
你現在可以用 Adam 啦，它應該是現在我覺得最穩定的

0:18:11.900,0:18:15.200
但是它 implement 比較複雜就是了

0:18:15.200,0:18:17.800
但其實也沒有什麼啦，好，講到這邊大家有甚麼問題嗎？

0:18:21.060,0:18:24.300
好，那我其實有一個問題啦

0:18:24.300,0:18:28.020
我其實有一個問題

0:18:28.500,0:18:30.880
我們在做一般的 Gradient Descent 的時候

0:18:30.880,0:18:34.460
我們這個參數的 update 取決於兩件事情

0:18:34.460,0:18:37.940
一件事情是 learning rate，另外一件事情是 Gradient

0:18:38.040,0:18:43.340
我們的意思就是說 Gradient 越大，你參數 update 就越快

0:18:43.340,0:18:45.600
斜率算出來越大，參數 update 就越快

0:18:45.600,0:18:48.160
我相信你可以接受這件事情

0:18:48.580,0:18:52.140
但是在 Adagrad 裡面，你不覺得相當矛盾嗎？

0:18:52.140,0:18:54.340
你不覺得有某些怪怪的地方

0:18:54.780,0:19:01.060
這一項告訴我們，微分的值越大，你參數 update 越快

0:19:01.060,0:19:04.620
但是這一項它是相反的，對不對？

0:19:04.620,0:19:16.700
當這一項跟這一項，它們卻是不一樣

0:19:16.700,0:19:23.040
對不對，你有沒有覺得說，這邊有一些奇怪的地方

0:19:24.860,0:19:29.280
也就是說，今天當你的 Gradient 越大的時候

0:19:29.280,0:19:34.500
當 Gradient 越大的時候，你底下算出來的這項就越大

0:19:34.500,0:19:40.600
你底下算出來的這項越大，
你的參數 update 的步伐就越小

0:19:40.600,0:19:45.080
這不就跟我們原來要做的事情是有所衝突的嗎？

0:19:45.080,0:19:46.800
在分母的地方告訴我們說

0:19:46.800,0:19:51.940
Gradient 越大，踏的步伐越大 ，參數就 update 的越大

0:19:51.940,0:19:54.480
但是分母的地方卻說

0:19:54.480,0:19:58.780
如果 Gradient 越大，參數 update 的越小這樣

0:19:59.320,0:20:03.080
好，怎麼解釋這件事情呢？

0:20:03.080,0:20:07.340
有一些 paper 這樣解釋的

0:20:07.340,0:20:14.800
這個 Adagrad 它想要考慮的是：
今天這個 Gradient 有多 surprise

0:20:14.800,0:20:17.840
也就是所謂的"反差"這樣

0:20:17.840,0:20:19.820
反差，大家知道嗎？

0:20:19.820,0:20:22.160
就是比如說，反差萌的意思就是說

0:20:22.160,0:20:25.440
如果本來一個很兇惡的角色，突然對你很溫柔

0:20:25.440,0:20:29.620
你就會覺得它特別溫柔這樣，所以呢，對 Gradient 來說

0:20:29.620,0:20:31.480
也是一樣的道理

0:20:31.480,0:20:35.560
假設有某一個參數 ，它在第一次 update 參數的時候

0:20:35.560,0:20:37.300
它算出來的 Gradient 是 0.001

0:20:37.300,0:20:40.500
再來又算 0.001, 0.003, 等等...等等

0:20:40.500,0:20:43.280
到某一次呢，它 Gradient 算出來是 0.1

0:20:43.280,0:20:49.240
你就會覺得特別大，因為它比之前算出來的
Gradient 都大了 100 倍，特別大

0:20:49.640,0:20:52.400
但是，如果是有另外一個參數

0:20:52.400,0:20:56.100
它一開始算出來是 10.8, 再來算 20.9, 再來算 31.7

0:20:56.100,0:20:57.880
它的 Gradient 平常都很大

0:20:57.880,0:21:02.180
但是它在某一次算出來的 Gradient 是 0.1

0:21:02.180,0:21:04.540
這時候，你就會覺得它特別小這樣子

0:21:04.540,0:21:08.580
所以為了強調這種反差的效果

0:21:08.580,0:21:13.740
所以在 Adagrad 裡面呢，我們就把它除以這項

0:21:13.740,0:21:19.200
這項就是把過去這些 Gradient 的平方

0:21:19.200,0:21:24.140
把它算出來，我們就想要知道說過去 Gradient 有多大

0:21:24.140,0:21:27.600
然後再把它們相除，看這個反差有多大這樣

0:21:27.600,0:21:30.740
這個是直觀的解釋

0:21:30.740,0:21:34.540
那更正式的解釋呢，我有這樣的解釋

0:21:34.540,0:21:40.120
我們來考慮一個二次函數，來考慮一個二次函數

0:21:40.120,0:21:43.120
這個二次函數呢，我們就寫成這樣子

0:21:43.120,0:21:45.980
他只有一個參數，就是 x

0:21:46.000,0:21:51.320
如果我們把這個二次函數，對 x 做微分的話

0:21:51.320,0:21:56.820
把 y 對 x 做微分，這個國中生就知道，這是 2ax + b

0:21:56.820,0:21:59.680
如果它絕對值算出來的話，長這樣子

0:22:00.160,0:22:03.040
好，那這個二次函數的最低點在哪裡呢？

0:22:03.040,0:22:06.760
是 -(b/2a)，我國中就被過這個式子了

0:22:07.640,0:22:10.840
如果你今天呢，在這個二次函數上，

0:22:10.840,0:22:16.660
你隨機的選一個點開始 ，你要做 Gradient Descent

0:22:16.820,0:22:21.360
那你的步伐多大，踏出去是最好的？

0:22:21.660,0:22:24.320
假設這個起始的點是 x0

0:22:24.320,0:22:26.820
最低點是 -(b/2a)

0:22:26.820,0:22:30.120
那踏出去一步，最好的步伐，

0:22:30.120,0:22:33.180
其實就是這兩個點之間的距離

0:22:33.180,0:22:37.040
因為如果你踏出去的步伐，是這兩個點之間的距離的話

0:22:37.040,0:22:39.780
你就一步到位了

0:22:39.780,0:22:41.820
這兩個點之間的距離是甚麼呢？

0:22:41.820,0:22:43.820
這兩個點之間的距離，你整理一下，

0:22:43.820,0:22:47.480
它是 |2a x0 + b| / 2a

0:22:47.480,0:22:53.520
|2a x0 + b| 這一項，就是這一項

0:22:53.520,0:22:58.880
2a x0 + b 就是 x0 這一點的微分

0:22:58.880,0:23:01.580
x0 這一點的一次微分

0:23:01.580,0:23:04.540
所以 Gradient Descent 你不覺得說聽起來很有道理

0:23:04.540,0:23:08.120
就是說，如果我今天算出來的微分越大

0:23:08.460,0:23:10.640
我就離原點越遠

0:23:11.080,0:23:17.140
如果踏出去的(我最好的)步伐，是跟微分的大小成正比

0:23:17.140,0:23:21.060
如果踏出去的步伐跟微分的大小成正比

0:23:21.060,0:23:24.420
它可能是最好的步伐

0:23:25.140,0:23:31.460
但是，這件事情只有在，只考慮一個參數的時候才成立

0:23:31.460,0:23:35.220
如果我們今天呢，同時有好幾個參數

0:23:35.240,0:23:38.100
我們要同時考慮好幾個參數的時候

0:23:38.100,0:23:41.420
這個時候呢，剛才的論述就不見得成立了

0:23:41.420,0:23:46.760
也就是說，Gradient 的值越大就跟最低點的距離越遠

0:23:46.760,0:23:50.740
這件事情，在有好多個參數的時候 ，是不一定成立的

0:23:50.740,0:23:55.540
比如說，你想看看，我們現在考慮 w1 跟 w2 兩個參數

0:23:55.540,0:24:00.320
這個圖上面的顏色，是它的 loss

0:24:01.160,0:24:05.240
那如果我們考慮 w1 的變化

0:24:05.240,0:24:09.620
我們就在藍色這條線這邊切一刀

0:24:09.620,0:24:12.800
我們把藍色這條線切一刀，

0:24:12.800,0:24:17.300
我們看到的 error surface 長得是這個樣子

0:24:17.640,0:24:21.200
如果你比較圖上的兩個點，a 點跟 b 點

0:24:21.200,0:24:26.440
那確實 a 點的微分值比較大，那它就距離最低點比較遠

0:24:26.440,0:24:29.240
但是，如果我們同時考慮幾個參數

0:24:29.240,0:24:33.040
我們同時考慮 w2 這個參數

0:24:33.040,0:24:36.840
我們在綠色的這條線上切一刀

0:24:36.840,0:24:39.840
如果我們在綠色這條線上切一刀的話

0:24:39.840,0:24:42.260
我們得到的值是這樣子

0:24:42.260,0:24:43.860
我們得到的 error surface 是這樣子

0:24:43.860,0:24:47.820
它是比較尖的，這個谷呢，是比較深的

0:24:47.820,0:24:52.340
因為你會發現說，w2 在這個方向的變化是比較猛烈的

0:24:52.340,0:24:59.380
如果我們只比較在 w2 這條線上的兩個點 , c 跟 d 的話

0:24:59.380,0:25:01.320
確實 c 的微分比較大

0:25:01.320,0:25:05.460
所以，它距離最低點是比較遠的

0:25:05.580,0:25:09.540
但是，如果我們今天的比較是跨參數的話

0:25:09.540,0:25:14.480
如果我們比較 a 這的點對 w1 的微分

0:25:14.480,0:25:17.000
c 這個點對 w2 的微分

0:25:17.000,0:25:19.360
這個結論呢，就不成立了

0:25:19.360,0:25:24.660
雖然說，c 這個點對 w2 的微分值是比較大的

0:25:24.660,0:25:26.480
這個微分值是比較小的

0:25:26.480,0:25:31.140
但 c 呢，是離最低點比較近的，而 a 是比較遠的

0:25:31.140,0:25:34.700
所以，當我們 update 參數

0:25:34.700,0:25:39.440
當我們 update 參數選擇跟微分值成正比

0:25:39.440,0:25:44.840
這樣的論述是在，沒有考慮跨參數的條件下

0:25:44.840,0:25:46.680
這件事情才成立的

0:25:46.680,0:25:49.860
當我們要同時考慮好幾個參數的時候呢

0:25:49.860,0:25:52.760
我們這樣想呢，就不足夠了

0:25:52.760,0:25:56.560
所以，如果我們今天要同時考慮好幾個參數的話

0:25:56.560,0:25:58.440
我們應該要怎麼想呢？

0:25:58.440,0:26:01.840
如果你看看，我們說的最好的 step 的話

0:26:01.840,0:26:03.460
我們看最好的這個 step

0:26:03.480,0:26:07.560
它其實還有分母這一項 ，它的分母這一項呢，是 2a

0:26:07.560,0:26:12.180
這個 2a 哪來的呢？這個 2a 是甚麼呢？

0:26:12.180,0:26:20.460
這個 2a 呢，如果我們今天把這個 y 做2次微分

0:26:20.460,0:26:23.260
我們做一次微分得到這個式子

0:26:24.300,0:26:28.380
那如果我們做二次微分的話，就得到 2a

0:26:28.380,0:26:30.980
那它是一個 constant

0:26:30.980,0:26:35.320
這個 2a 呢，就出現在最好的 step 的分母的地方

0:26:35.320,0:26:41.720
所以，今天最好的 step，它不只是要正比於一次微分

0:26:41.720,0:26:45.620
它同時要和二次微分的大小成反比

0:26:45.620,0:26:48.060
如果你二次微分比較大

0:26:48.060,0:26:51.080
這個時候你參數 update 量應該要小

0:26:51.080,0:26:55.600
如果二次微分小的話，你參數 update 量應該要比較大

0:26:55.600,0:26:59.460
所以，最好的 step 應該要把二次微分考慮進來

0:26:59.700,0:27:03.560
所以，如果我們今天把二次微分考慮進來的話

0:27:03.560,0:27:08.360
你會發現說，在 w1 這個方向上

0:27:09.300,0:27:11.760
你的二次微分是比較小的

0:27:11.760,0:27:15.700
因為這個是一個比較平滑的弧

0:27:15.700,0:27:18.120
所以這個二次微分是比較小的

0:27:18.120,0:27:21.860
在 w2 的方向上

0:27:22.480,0:27:25.260
這個是一個比較尖的弧、比較深的弧

0:27:25.500,0:27:30.580
它是一個比較尖的弧，所以它的二次微分是比較大的

0:27:30.580,0:27:36.400
所以你光比較 a 跟 c 的微分值呢 ，是不夠的

0:27:36.400,0:27:41.400
你要比較 a 的微分值除掉它的二次

0:27:41.400,0:27:46.080
跟 c 的微分值除掉它的二次，再去比

0:27:46.080,0:27:49.760
如果你做這件事，你才能夠真正顯示

0:27:49.760,0:27:53.040
這些點跟最低點的距離這樣

0:27:53.040,0:27:55.540
雖然 a 這個點，它的微分是比較小的

0:27:55.540,0:27:59.100
但它的二次也同時是比較小的

0:27:59.100,0:28:01.900
c 比較大、二次是比較大的

0:28:01.900,0:28:06.040
所以，如果你把二次微分的值呢，考慮進去

0:28:06.040,0:28:10.740
做這個評檢、做調整的話

0:28:10.740,0:28:17.940
那你這個時候，才能真正反映，
你現在所在位置跟最低點的距離

0:28:18.760,0:28:21.920
好，那這件事情跟 Adagrad 的關係是甚麼呢？

0:28:22.380,0:28:25.760
如果你把 Adagrad 的式子列出來的話

0:28:25.760,0:28:27.500
你把 Adagrad 的式子列出來的話

0:28:27.500,0:28:31.380
它參數的 update 量是這個樣子的

0:28:31.380,0:28:33.680
η 是一個 constant，所以我們就不理它

0:28:33.680,0:28:38.560
這個 g^t 阿，它就是一次微分，對不對

0:28:38.560,0:28:44.040
下面這個，過去所有微分值的平方和開根號

0:28:44.040,0:28:48.780
神奇的是，它想要代表的是二次微分

0:28:48.780,0:28:52.640
那你可能會問說，怎麼不直接算二次微分呢？

0:28:52.640,0:28:55.460
你可以直接算二次微分

0:28:55.460,0:29:00.440
確實可以這麼做，也有這樣的方法，
而且你確實可以這麼做

0:29:00.440,0:29:03.480
但是，有時候你會遇到的狀況是

0:29:03.840,0:29:07.340
你在作業一裡面是比較簡單的 case

0:29:07.340,0:29:11.220
相信你都秒算，秒給你結果

0:29:11.220,0:29:16.620
但是，有時候你參數量大、data 多的時候

0:29:16.620,0:29:19.620
你可能算一次微分就花一天這樣子

0:29:19.620,0:29:22.980
然後你再算二次微分，你要再多花一天

0:29:22.980,0:29:26.020
有時候，這樣子的結果是你不能承受的

0:29:26.020,0:29:29.540
而且你多花一天 performance 還不見得會比較好

0:29:29.580,0:29:31.500
其實這個結果，是你不能承受的

0:29:31.500,0:29:34.320
所以，Adagrad 它提供的做法就是

0:29:34.320,0:29:37.840
我們在沒有增加任何額外運算的前提之下

0:29:38.260,0:29:40.920
想辦法能不能夠做一件事情

0:29:40.920,0:29:43.920
去估一下，二次的微分應該是多少

0:29:43.920,0:29:47.220
在 Adagrad 裡面，你只需要一次微分的值

0:29:47.220,0:29:49.380
那這個東西我們本來就要算它了

0:29:49.380,0:29:52.800
所以並沒有，多做任何多餘的運算

0:29:53.760,0:29:56.360
好，怎麼做呢？

0:29:56.360,0:30:01.520
如果我們考慮一個二次微分比較小的峽谷

0:30:01.520,0:30:06.060
跟一個二次微分比較大的峽谷

0:30:06.260,0:30:11.140
然後我們把它的一次微分的值，考慮進來的話

0:30:11.140,0:30:13.580
這個是長這樣

0:30:13.580,0:30:15.700
這個是長這樣

0:30:15.700,0:30:20.140
如果你只是在，這個區間和這個區間裡面

0:30:20.140,0:30:23.200
隨機 sample 一個點，算它的一次微分的話

0:30:23.200,0:30:26.920
你看不出來它的二次微分值是多少

0:30:26.920,0:30:30.100
但是如果你 sample 夠多點

0:30:30.100,0:30:33.840
你在某一個 range 之內，sample 夠多點的話

0:30:33.840,0:30:37.940
那你就會發現說，在這個比較平滑的峽谷裡面

0:30:37.940,0:30:40.280
它的一次微分通常就是比較小的

0:30:40.280,0:30:44.180
在比較尖的峽谷裡面，它的一次微分通常是比較大的

0:30:44.180,0:30:47.460
而 Adagrad 這邊，這一件事情

0:30:47.460,0:30:51.500
summation over 過去所有的微分的平方，這件事情

0:30:51.500,0:30:56.280
你就可以想成，在這個地方呢，做 sampling

0:30:56.280,0:30:58.360
就在這個地方呢，做 sampling

0:30:58.360,0:31:01.840
那你再把它的平方和呢，再開根號算出來

0:31:01.840,0:31:06.260
那這個東西，就反映了二次微分的大小

0:31:07.460,0:31:11.520
這個 Adagrad 怎麼做，其實我們上次已經有示範過了

0:31:11.520,0:31:13.860
那所以我們就不再示範

0:31:13.860,0:31:18.620
接下來我們要獎的另外一件事情呢，
是 Stochastic 的 Gradient Descent

0:31:18.620,0:31:22.140
那它可以讓你的 training 呢，更快一點

0:31:22.640,0:31:23.800
好，這個怎麼說呢？

0:31:23.800,0:31:27.880
我們之前講說，我們的 loss function

0:31:27.880,0:31:31.500
它的 loss function，它的樣子呢

0:31:31.500,0:31:35.440
如果我們今天做的是這個 Regression 的話

0:31:35.440,0:31:37.980
這個是 Regression 的式子

0:31:37.980,0:31:40.900
Regression 得到的 estimation 的結果

0:31:40.900,0:31:43.480
那你把 Regression 得到 estimation 的結果

0:31:43.480,0:31:47.780
減掉 y\head，再去平方

0:31:47.780,0:31:50.540
再 summation over 所有的 training data

0:31:50.540,0:31:52.200
這是我們的 loss function

0:31:52.200,0:31:55.100
所以，這個式子非常合理

0:31:55.100,0:31:58.720
我們的 loss 本來就應該考慮所有的 example

0:31:58.720,0:32:02.300
它本來就應該 summation over 所有的 example

0:32:02.300,0:32:05.000
有這些以後，你就可以去算 Gradient

0:32:05.000,0:32:07.420
然後你就可以做 Gradient Descent

0:32:07.420,0:32:10.580
但 Stochastic Gradient Descent，他的想法不一樣

0:32:10.580,0:32:13.040
Stochastic Gradient Descent 它做的事情是

0:32:13.040,0:32:17.280
每次就拿一個 x^n 出來

0:32:17.280,0:32:21.980
這邊你可以隨機取，也可以按照順序取

0:32:21.980,0:32:23.420
那其實隨機取的時候

0:32:23.420,0:32:26.100
如果你今天是在做 deep learning 的 case

0:32:26.100,0:32:30.000
也就是說你的 error surface 不是 convex

0:32:30.000,0:32:33.020
是非常崎嶇的，隨機取呢，是有幫助的

0:32:33.020,0:32:36.180
總之，你就取一個 example 出來

0:32:36.180,0:32:38.120
假設取出來的 example 是 x^n

0:32:38.120,0:32:41.300
這個時候呢，你要計算你的 loss

0:32:41.300,0:32:45.140
你的 loss 呢，只考慮一個 example

0:32:45.140,0:32:51.580
你只考慮你現在的參數，對這個 example 的 y 的估測值

0:32:51.580,0:32:54.480
再減掉它的正確答案，再做平方

0:32:54.480,0:32:56.680
然後就不 summation over 所有的 example

0:32:56.680,0:32:59.360
因為你現在只取一個 example 出來

0:32:59.800,0:33:03.800
你只算某一個 example 的 loss

0:33:04.400,0:33:07.060
那接下來呢，你在 update 參數的時候

0:33:07.060,0:33:11.180
你只考慮那一個 example

0:33:11.180,0:33:14.700
我們只考慮一個 example 的 loss function，我們就寫成

0:33:14.700,0:33:18.580
L^n，代表它是考慮第 n 個 example 的 loss function

0:33:18.580,0:33:20.380
那你在算 Gradient 的時候呢？

0:33:20.380,0:33:26.700
你不是算對 total 所有的 data，它的 Gradient 的和

0:33:26.700,0:33:32.280
你只算對某一個 example，它的 loss 的 Gradient

0:33:32.280,0:33:35.920
然後呢，你就很急躁的 update 參數了

0:33:35.920,0:33:40.720
所以在原來的 Gradient Descent 裡面，
你計算所有 data 的 loss

0:33:40.720,0:33:42.060
然後才 update 參數

0:33:42.060,0:33:44.500
但是在 Stochastic Gradient Descent  裡面

0:33:44.500,0:33:48.240
你看一個 example，就 update 一個參數這樣

0:33:48.240,0:33:51.280
你可能想說，這有啥好呢？

0:33:51.280,0:33:53.380
聽起來好像沒有甚麼好的

0:33:53.380,0:33:55.680
那我們實際來操作一下好了

0:33:55.680,0:34:01.920
剛才看到圖呢，它可能是這個樣子的

0:34:04.800,0:34:08.380
我們剛才看到的圖呢，它可能是這個樣子

0:34:08.380,0:34:12.100
原來的 Gradient Descent，你看完所有的 example 以後

0:34:12.100,0:34:14.820
你就 update 一次參數

0:34:14.820,0:34:17.220
那它其實是比較穩定

0:34:17.220,0:34:19.680
你會發現說，它走的方向

0:34:19.680,0:34:24.460
就是按照 Gradient 建議我們的方向呢，來走

0:34:24.460,0:34:28.120
但是如果你是用 Stochastic Gradient Descent 的話

0:34:28.120,0:34:33.840
你每看到一個 example ，你就 update 一次參數

0:34:34.400,0:34:37.140
如果你有 20 個 example 的時候

0:34:37.140,0:34:39.880
那你就 update 20 次參數

0:34:39.880,0:34:43.260
那這邊他是看完 20 個 example 才 update 一次參數

0:34:43.260,0:34:46.340
這邊是，每一個 example 都 update 一次參數

0:34:46.340,0:34:48.780
所以在它看 20 個 example 的時候

0:34:48.780,0:34:53.360
你這邊也已經看了 20 個 example，
而且 update 20 次參數了

0:34:53.360,0:34:57.000
所以 update 20 次參數的結果呢，看起來就像是這樣

0:34:57.000,0:35:01.780
從一樣的起始點開始，但它已經 update 了 20 次參數

0:35:01.780,0:35:06.060
所以，這個如果只看一個 example 的話

0:35:06.060,0:35:08.140
它的步伐是小的

0:35:08.140,0:35:11.880
而且可能是散亂的

0:35:11.880,0:35:15.800
因為你每次只考慮一個 example

0:35:15.800,0:35:19.060
所以它參數 update 的方向，跟這個 Gradient Descent

0:35:19.060,0:35:23.100
total loss 的 error surface 界定我們走的方向

0:35:23.100,0:35:30.680
不見得是一致的，但是因為我們可以看很多個 example

0:35:30.680,0:35:34.120
所以天下武功，為快不破。在它走一步的時候

0:35:34.120,0:35:37.960
你已經出 20 拳了，所以它走的反而是比較快的

0:35:40.420,0:35:44.860
然後呢，接下來我們要講的是第三個 tip

0:35:44.860,0:35:47.640
就是你可以做 Feature 的 Scaling

0:35:47.640,0:35:51.400
所謂的 Feature Scaling 意思呢是這樣子

0:35:51.400,0:35:55.600
假設我們現在要做 Regression

0:35:55.600,0:35:58.040
那我們這個 Regression 的 function 裡面

0:35:58.040,0:36:02.500
input 的 feature 有兩個，x1 跟 x2

0:36:02.500,0:36:06.360
比如說，如果是要 predict 寶可夢進化以後 CP 值的話

0:36:06.360,0:36:12.640
那 x1 是進化前的 CP值，x2 是它的生命值...等等這樣

0:36:12.640,0:36:16.000
你有兩個 input feature, x1 跟 x2

0:36:16.400,0:36:21.820
那如果你看你今天的 x1 跟 x2

0:36:21.820,0:36:25.800
它們分佈的 range 是很不一樣的話

0:36:25.800,0:36:29.400
那就建議你呢，把它們做 scaling

0:36:29.400,0:36:31.940
把它們的 range 分佈變成是一樣

0:36:31.940,0:36:37.840
比如，這邊的 x2 它的分佈是遠比 x1 大

0:36:37.840,0:36:42.220
那就建議你把 x2 這個值呢，做一下 rescaling

0:36:42.220,0:36:48.300
把它的值縮小，讓 x2 的分佈跟 x1 的分佈是比較像的

0:36:48.300,0:36:52.880
你希望不同的 feature，他們的 scale 是一樣的

0:36:53.280,0:36:55.420
為甚麼要這麼做呢？

0:36:55.820,0:36:57.440
我們舉個例子

0:36:57.440,0:37:02.220
假設這個是我們的 Regression 的 function

0:37:02.220,0:37:05.140
那我們寫成這樣，這邊這個意思跟這個是一樣的啦

0:37:05.140,0:37:08.120
y = b + w1*x1 + w2*x2

0:37:08.120,0:37:14.420
y = b + w1*x1 再加 w2*x2

0:37:14.420,0:37:20.300
那假設 x1 平常的值，都是比較小的，假設說 1, 2 之類的

0:37:20.300,0:37:22.600
假設 x2 它平常的值都很大

0:37:22.600,0:37:25.920
它 input 的值都很大，100, 200 之類的

0:37:26.800,0:37:30.640
那這個時候，如果你把 loss 的 surface 畫出來

0:37:30.640,0:37:33.040
會遇到甚麼樣的狀況呢？

0:37:33.340,0:37:39.080
你會發現說，如果你更動 w1 跟 w2 的值

0:37:39.080,0:37:46.140
假設你把 w1 跟 w2 的值都做一樣的更動

0:37:46.140,0:37:49.760
都加個 △w ，你會發現說

0:37:50.180,0:37:54.440
w1 的變化，對 y 的變化而言是比較小的

0:37:54.440,0:37:58.760
w2 的變化，對 y 的變化而言是比較大的

0:37:58.760,0:38:00.340
對不對，這件事情是很合理的

0:38:00.340,0:38:04.840
因為你要把 w2 乘上它 input 的這些值

0:38:04.840,0:38:07.200
你要把 w1 乘上它 input 的這些值

0:38:07.200,0:38:11.720
如果 w2 它乘的這些 input 的值是比較大的

0:38:11.720,0:38:16.120
那只要把 w2 小小的變化，那 y 就會有很大的變化

0:38:16.120,0:38:20.040
那同樣的變化，w1 它 input 的值是比較小的

0:38:20.040,0:38:23.360
它對 y 的影響呢，就變成是比較小的

0:38:23.360,0:38:26.360
所以如果你把他們的 error surface 畫出來的話呢

0:38:26.360,0:38:30.520
你看到的可能像是這個樣子

0:38:30.520,0:38:35.420
所以如果你把他們的 error surface 畫出來的話呢，
你看到的可能像是這樣

0:38:35.420,0:38:38.260
這個圖，是甚麼意思呢？

0:38:38.260,0:38:44.240
因為 w1 對 y 的影響比較小

0:38:44.240,0:38:47.500
所以 w1 就對 loss 的影響比較小

0:38:47.500,0:38:52.560
所以 w1 對 loss 是有比較小的微分的

0:38:52.560,0:38:55.380
所以 w1 這個方向上，它是比較平滑

0:38:55.880,0:39:00.620
w2 對 y 的影響比較大，所以它對 loss 的影響比較大

0:39:00.620,0:39:03.380
所以改變 w2 的時候，它對 loss 的影響比較大

0:39:03.380,0:39:06.900
所以，它在這個方向上，是比較 shock 的

0:39:06.900,0:39:09.760
所以這個方向上，有一個比較尖的峽谷

0:39:10.220,0:39:15.280
那如果今天，x1 跟 x2 的值，它們的 scale 是接近的

0:39:15.740,0:39:17.780
那如果你把 loss 畫出來的話呢

0:39:17.780,0:39:19.640
它就會比較接近圓形

0:39:19.640,0:39:24.240
w1 跟 w2 呢，對你的 loss 是有差不多的影響力

0:39:24.620,0:39:27.740
這個對做 Gradient Descent 會有甚麼樣的影響呢？

0:39:27.740,0:39:28.920
是會有影響的

0:39:28.920,0:39:31.860
比如說，如果你從這個地方開始

0:39:31.860,0:39:34.460
其實我們上次已經有看到了，就是這樣

0:39:34.460,0:39:37.900
這種長橢圓的 error surface 阿

0:39:37.900,0:39:42.160
如果你不出些 Adagrad 甚麼的，你是很難搞定它的

0:39:42.160,0:39:45.920
因為就在這個方向上，和這個方向上

0:39:45.920,0:39:48.080
你會需要非常不同的 learning rate

0:39:48.080,0:39:49.540
你同一組 learning rate 會搞不定它

0:39:49.540,0:39:53.060
你要 adaptive learning 才能夠搞定它

0:39:53.060,0:39:58.520
所以這樣子的狀況，沒有 scaling 的時候，
它 update 參數是比較難的

0:39:58.520,0:40:02.960
但是，如果你有 scale 的話，它就變成一個正圓形

0:40:02.960,0:40:06.540
如果是在正圓形的時候 ，update 參數就會變得比較容易

0:40:06.540,0:40:12.000
而且，你知道說 Gradient Descent
它並不是向著最低點走

0:40:12.000,0:40:14.800
在這個藍色圈圈，它的最低點是在這邊

0:40:14.800,0:40:16.620
綠色圈圈最低點是在這邊

0:40:16.620,0:40:20.380
但是你今天在 update 參數的時候，走的方向是順著

0:40:20.380,0:40:23.640
等高線的方向，是順著 Gradient 箭頭的方向

0:40:23.640,0:40:25.260
所以，雖然最低點在這邊

0:40:25.260,0:40:28.840
你從邊開始走，你還是會走這個方向，再走進去

0:40:28.840,0:40:32.400
你不會只向那個最低點去走

0:40:32.480,0:40:35.640
那如果是綠色的呢，綠色的又不一樣

0:40:35.640,0:40:37.880
因為，它如果真的是一個正圓的話

0:40:37.880,0:40:42.020
你不管在這個區域的哪一個點

0:40:42.020,0:40:45.580
它都會向著圓心走

0:40:45.580,0:40:47.740
所以，如果你有做 feature scaling 的時候

0:40:47.740,0:40:50.380
你在做參數的 update 的時候呢

0:40:50.380,0:40:52.840
它是會比較有效率的

0:40:53.140,0:40:55.520
那你可能會問說，怎麼做 scaling

0:40:55.520,0:40:58.540
這個方法有千百種啦

0:40:58.540,0:41:00.520
你就選一個你喜歡的就是了

0:41:00.520,0:41:02.400
那常見的作法是這樣

0:41:02.400,0:41:07.380
假設我有 r 個 example, x^1, x^2 到 x^R

0:41:08.120,0:41:13.320
每一筆 example，裡面都有一組 feature

0:41:13.880,0:41:16.920
x^1  它第一個 component 就是 x(1,1)

0:41:16.920,0:41:19.960
x^2  它第一個 component 就是 x(2,1)

0:41:19.960,0:41:23.240
x^1  它第二個 component 就是 x(1,2)
x^2  它第二個 component 就是 x(2,2)

0:41:23.600,0:41:25.640
那怎麼做 feature scaling？

0:41:25.640,0:41:28.460
你就對每一個 dimension i

0:41:28.460,0:41:31.940
都去算它的 mean，這邊寫成 m_i

0:41:32.680,0:41:37.200
都去算它的 deviation，這邊寫成 σ_i

0:41:38.260,0:41:44.680
然後呢，對第 r 個 example 的第 i 個 component

0:41:44.680,0:41:49.460
對第 r 個 example 的第 i 個 component

0:41:49.460,0:41:55.440
你就把它減掉，所有的 data 的
第 i 個 component 的 mean，也就是 m_i

0:41:55.440,0:41:59.020
你就把它減掉所有的 data 的
第 i 個 component 的 mean

0:41:59.020,0:42:08.780
再除掉所有的 data 的第 i 個 component 的 standard deviation

0:42:08.780,0:42:11.200
然後呢，你就會得到說

0:42:11.200,0:42:15.180
你做完這件事以後

0:42:15.180,0:42:18.340
你所有 dimension 的 mean 就會是 0

0:42:18.340,0:42:20.840
你的 variance 就會是 1

0:42:20.840,0:42:23.960
這是其中一個常見地做 localization 的方法

0:42:24.740,0:42:29.700
最後，在下課前呢，我們來講一下

0:42:29.700,0:42:32.740
為甚麼 Gradient Descent 它會 work

0:42:32.740,0:42:35.580
Gradient Descent 背後的理論基礎是什麼

0:42:35.580,0:42:39.020
那在真正深入數學部分的基礎之前呢

0:42:39.020,0:42:40.680
我們來問大家一個問題

0:42:40.680,0:42:44.480
大家都已經知道 Gradient Descent 是怎麼做的

0:42:44.480,0:42:48.000
假設，我問你一個這樣的是非題

0:42:49.340,0:42:53.600
每一次，我們在 update 參數的時候

0:42:53.600,0:42:56.440
我們都得到一個新的 θ

0:42:57.100,0:43:02.140
這個新的 θ，總是會讓我們的 loss 比較小

0:43:02.140,0:43:08.220
這個陳述，是對的嗎？

0:43:09.580,0:43:14.420
好，也就是意思就是說 θ_0 你把它代到 L 裡面

0:43:14.420,0:43:19.060
它會大於 θ_1 代到 L 裡面，它會大於 θ_2 代到 L 裡面

0:43:19.060,0:43:24.520
每次 update 參數的時候，
這個 loss 的值，它都是越來越小的

0:43:24.520,0:43:26.960
這陳述，是正確的嗎？

0:43:26.960,0:43:30.240
你覺得它是正確的同學舉手

0:43:30.240,0:43:34.520
那你覺得這個陳述，它是不對的同學舉手

0:43:34.520,0:43:36.200
好，手放下

0:43:36.200,0:43:38.220
大家的觀念都很正確，沒錯

0:43:38.220,0:43:41.840
就是 update 參數以後，loss 不見得會下降的

0:43:41.840,0:43:45.820
所以如果你今天自己 implement Gradient Descent

0:43:45.820,0:43:48.440
做出來，update 參數以後的 loss 沒有下降

0:43:48.440,0:43:50.260
那不見得是你的程式有 bug

0:43:50.260,0:43:52.940
因為，本來就有可能發生這種事情

0:43:52.940,0:43:55.900
我們剛已經看過說，如果你 learning rate 調太大的話

0:43:55.900,0:43:57.400
會發生這種事情

0:43:57.400,0:44:00.840
或許，我們可以在下課前，做一個 demo

0:44:02.540,0:44:06.180
好，那在解釋 Gradient Descent 的 Theory 之前

0:44:06.180,0:44:08.920
這邊有一個 Warning of Math ，意思就是說

0:44:08.920,0:44:11.740
這個部分，就算是你沒有聽懂，也沒有關係

0:44:11.740,0:44:14.260
太陽明天依舊會升起

0:44:15.500,0:44:20.180
好，那我們先不要管 Gradient Descent

0:44:20.180,0:44:25.100
我們先來想想看，
假如你要解一個 Optimization 的 problem

0:44:25.100,0:44:30.220
你要在這一個 figure 上面，找他的最低點

0:44:30.220,0:44:32.560
你到底應該要怎麼做？

0:44:32.560,0:44:34.520
那有一個這樣子的作法

0:44:35.340,0:44:39.800
如果今天給我一個起始的點 ，也就是 θ_0

0:44:40.360,0:44:44.280
我們有方法，在這個起始點的附近

0:44:44.280,0:44:48.740
畫一個圓圏、畫一個範圍、畫一個紅色圈圈

0:44:48.740,0:44:51.840
然後，在這個紅色圈圈裡面

0:44:51.840,0:44:53.380
找出它的最低點

0:44:53.380,0:44:57.380
比如說，紅色圈圈裡面的最低點，就是在這個邊上

0:44:58.020,0:45:02.260
這個意思就是說，如果你給我一整個 error function

0:45:02.260,0:45:05.360
我沒有辦法，馬上一秒鐘就告訴你說

0:45:05.360,0:45:08.440
我沒有辦法馬上告訴你說，它的最低點在哪裡

0:45:08.660,0:45:12.700
但是如果你給我一個 error function，加上一個初始的點

0:45:12.700,0:45:16.680
我可以告訴你說，在這個初始點附近，畫一個範圍之內

0:45:16.680,0:45:19.140
哦，有問題是嗎？

0:45:23.100,0:45:24.860
謝謝，謝謝，沒有問題

0:45:24.860,0:45:28.720
我們可以在那個附近，找出一個最小的值

0:45:40.180,0:45:42.700
那你假設找到最小的值以後

0:45:42.700,0:45:47.940
我們就更新我們中間的位置

0:45:47.940,0:45:50.080
中間的位置挪到 θ_1

0:45:50.080,0:45:52.280
接下來呢，再畫一個圓圈

0:45:52.280,0:45:54.560
我們可以在這個圓圈範圍之內

0:45:54.560,0:45:56.300
再找一個最小的點

0:45:56.300,0:45:59.180
假設呢，它是落在這個地方

0:45:59.580,0:46:01.980
然後，你就再更新中心點的參數

0:46:01.980,0:46:04.020
到 θ_2 這個地方

0:46:04.020,0:46:07.500
然後，你就可以再找小小範圍內的最小值

0:46:07.500,0:46:11.300
然後，再更新你的參數，就一直這樣下去

0:46:11.420,0:46:13.140
好，那現在的問題就是

0:46:13.140,0:46:16.360
怎麼很快的在紅色圈圈裡面

0:46:16.360,0:46:21.360
找一個可以讓 loss 最小的參數呢？

0:46:21.360,0:46:23.240
怎麼做這件事呢？

0:46:23.240,0:46:26.680
這個地方要從 Taylor series 說起

0:46:26.720,0:46:29.980
假設你是知道 Taylor series 的，那個微積分有教過

0:46:29.980,0:46:32.040
Taylor series 告訴我們什麼呢？

0:46:32.460,0:46:34.460
它告訴我們說，任何一個 function h(x)

0:46:34.460,0:46:41.060
如果它在 x = x_0 這點呢

0:46:41.060,0:46:43.820
是 infinitely differentiable

0:46:43.820,0:46:49.380
那你可以把這個 h(x) 寫成以下這個樣子

0:46:49.380,0:46:52.500
你可以把 h(x) 寫成

0:46:52.500,0:46:57.760
Σ(k=0, ∞)，這裡 k 代表微分的次數

0:46:57.760,0:47:06.200
(h 在 x_0 微分 k 次以後的值) / k!

0:47:06.200,0:47:08.740
然後 (x-x_0)^k

0:47:09.540,0:47:14.220
不過，把它展開的話，你可以把 h(x) 寫成 h(x_0)

0:47:14.220,0:47:17.600
+ h'(x_0) * (x - x_0)

0:47:17.600,0:47:23.060
+ h''(x_0) * (x - x_0)^2

0:47:23.800,0:47:27.660
那當 x 很接近 x_0 的時候

0:47:27.660,0:47:29.960
當 x 很接近 x_0 的時候

0:47:29.960,0:47:36.560
(x - x_0) 就會遠大於 (x - x_0)^2，就會遠大於
後面的 3次,、4次，到無窮多次

0:47:36.560,0:47:41.520
所以，這個時候，你可以把後面的高次項刪掉

0:47:41.520,0:47:44.600
所以，當 x 很接近 x_0 的時候

0:47:45.240,0:47:48.840
這個只有在 x 很接近 x_0 的時候才成立

0:47:48.840,0:47:50.520
h(x)  就可以寫成

0:47:50.520,0:47:54.840
h(x_0) + h'(x_0) * (x - x_0)

0:47:55.240,0:47:59.800
那這個是只有考慮一個 variable 的 case

0:47:59.800,0:48:03.400
那其實，我這邊有個例子

0:48:03.400,0:48:06.380
假設 h(x) = sin(x)

0:48:07.140,0:48:10.480
那在 x_0 約等於 (π/4) 的地方

0:48:10.480,0:48:12.620
sin(x) 你可以寫成什麼樣子呢？

0:48:12.620,0:48:17.600
你用計算機算一下，它算出來是這樣子

0:48:17.600,0:48:24.740
這個 sin(x)，可以寫成這麼多這麼多這麼多項的相加

0:48:25.620,0:48:30.620
那如果我們把這些項，畫出來的話

0:48:30.620,0:48:34.020
你得到這樣子，一個結果

0:48:34.020,0:48:37.880
如果是 1/sqrt(2)，只有考慮 0 次的話

0:48:37.880,0:48:40.060
是這條水平線

0:48:40.400,0:48:45.380
考慮 1/sqrt(2) + (x-π/4)/sqrt(2)

0:48:45.380,0:48:49.240
考慮一次的話，是這條斜線

0:48:49.520,0:48:53.280
如果你有再把 2 次考慮進去，考慮 0 次, 1 次, 2 次的話

0:48:53.280,0:48:57.120
我猜你得到的，可能是這條線

0:48:57.120,0:49:00.720
如果你再把 3 次考慮進去的話，你得到這條線

0:49:01.220,0:49:05.780
如果你再把 4 次考慮進去的話，你可能得到橙色這條線

0:49:06.200,0:49:08.540
但是，雖然說，比如說如果你看

0:49:08.680,0:49:14.620
成色這條線，應該是 sin(x)，不好意思

0:49:15.200,0:49:20.260
好，你發現說，如果你只有考慮一次的時候

0:49:20.260,0:49:26.920
它其實跟這個 sin(x)，橙色這條線差很多啊，根本不像

0:49:26.920,0:49:29.980
但是，它在 (π/4) 2的附近

0:49:29.980,0:49:32.740
在這個地方附近，它是像的

0:49:32.740,0:49:35.880
因為，如果 x 很接近 (π/4) 的話

0:49:35.880,0:49:39.700
那後面這些項，平方項、三次方項這些都很小

0:49:39.700,0:49:43.180
所以就可以忽略它們，只考慮一次的部分

0:49:44.120,0:49:47.760
那這個 Taylor series 也可以是有好幾個參數的

0:49:47.760,0:49:49.600
如果今天有好幾個參數的話

0:49:49.600,0:49:51.120
那你就可以這樣子做

0:49:51.120,0:49:55.780
這個 h(x, y)，假設這個 function 有兩個參數

0:49:55.780,0:49:59.480
它在 x_0 和 y_0 附近

0:49:59.480,0:50:03.820
你可以把它寫成呢

0:50:04.380,0:50:09.020
這個 h(x, y)，你可以用 Taylor series 把它展開成這樣

0:50:09.020,0:50:13.320
就有 0 次的，有考慮 (x - x_0) 的

0:50:13.320,0:50:15.420
有考慮 (y - y_0) 的

0:50:15.420,0:50:18.740
還有考慮 (x - x_0)^2 跟 (y - y_0)^2 的

0:50:18.740,0:50:21.920
如果今天 x, y 很接近 x_0, y_0 的話

0:50:21.920,0:50:24.320
那平方項呢，就可以被消掉

0:50:24.320,0:50:26.360
就只剩這個部份而已

0:50:26.960,0:50:29.980
所以，今天 x, y 如果很接近 x_0, y_0 的話

0:50:29.980,0:50:32.800
那 h(x, y) 就可以寫成呢

0:50:32.800,0:50:36.280
約等於 h(x_0, y_0) 加上

0:50:36.860,0:50:42.840
(x - x_0) * (x_0, y_0) 對 x 做偏微分

0:50:42.840,0:50:47.080
(y - y_0) * (x_0, y_0) 對 y 做偏微分

0:50:47.080,0:50:49.060
這個偏微分的值，你不要看他這麼複雜

0:50:49.060,0:50:53.000
微分的值，它其實就是一個 constant 而已

0:50:53.000,0:50:55.100
就是一個常數項而已

0:50:55.100,0:50:58.240
這個是一個常數項，這個也是一個常數項

0:50:58.760,0:51:02.680
好，那如果我們今天考慮 Gradient Descent 的話

0:51:02.680,0:51:04.880
如果我們今天考慮我們剛才講的問題

0:51:04.880,0:51:09.840
如果，今天給我一個中心點，這是 a 跟 b

0:51:09.840,0:51:13.480
那我畫了一個很小很小的圓圈

0:51:13.480,0:51:16.240
紅色的圓圈，假設它是很小的

0:51:16.240,0:51:18.800
再這個紅色圓圈的範圍之內

0:51:19.200,0:51:23.660
我其實可以把 loss function 用 Taylor series 做簡化

0:51:23.660,0:51:27.160
我可以把 loss function, L(θ) 寫成

0:51:27.160,0:51:36.640
L(a, b) + θ_1 對 loss function 的偏微分，
在 (a, b) 這個位置的偏微分 ，乘上 (θ_1 - a)

0:51:36.640,0:51:43.160
加上 θ_2 對 loss function 在 (a, b) 這個位置 的偏微分，再乘上 (θ_2 - b)

0:51:43.160,0:51:48.140
所以在紅色的圈圈內，loss function 可以寫成這樣子

0:51:49.020,0:51:58.780
那我們把 L(a,b)，L 用 (a, b) 代進去，
它就是一個 constant，用一個 s 來表示

0:51:59.400,0:52:02.800
那 θ_1 對 L 的偏微分

0:52:02.800,0:52:06.900
在 (a, b) 這個位置，這也是一個 constant，
所以我們用 u 來表示

0:52:06.900,0:52:09.580
這也是一個 constant，所以我們用 v 來表示

0:52:09.580,0:52:11.860
這樣這個式子呢，看起來就非常簡單了

0:52:11.860,0:52:13.360
所以在這個範圍之內

0:52:13.920,0:52:17.260
L 對 θ 跟 θ_1

0:52:28.980,0:52:31.820
所以呢，在紅色圈圈範圍內呢

0:52:31.820,0:52:34.240
這個式子是非常簡單的

0:52:34.240,0:52:37.540
就寫成左下角這個樣子

0:52:38.440,0:52:44.180
再來，如果告訴你說，紅色圈圈內的式子都長這個樣子

0:52:44.180,0:52:49.980
你能不能秒算，
哪一個 θ_1 跟 θ_2 可以讓它的 loss 最小呢？

0:52:49.980,0:52:52.300
我相信你可以秒算這個結果

0:52:52.300,0:52:54.780
不過，我們還是很快地稍微看一下

0:52:54.780,0:52:56.800
好，L 寫成這樣

0:52:56.800,0:53:01.760
s, u, v 都是常數，我們就把它放在藍色的框框那裡面

0:53:01.760,0:53:04.100
不用管它值是多少

0:53:04.100,0:53:06.300
我們現在的問題，就是找

0:53:07.060,0:53:12.000
在紅色的圈圈內呢，找 θ_1 跟 θ_2 讓 loss 最小

0:53:12.000,0:53:15.020
那所謂的在紅色圈圈內的意思就是說

0:53:15.020,0:53:18.000
紅色圈圈的中心就是 a 跟 b

0:53:18.000,0:53:23.200
所以你這個 (θ_1 - a)^2 + (θ_2 - b)^2 ≦ d^2

0:53:23.200,0:53:26.840
他們要在這個紅色圈圈的範圍內

0:53:26.840,0:53:29.700
這件事情，其實就是秒算對不對

0:53:29.700,0:53:32.880
太簡單了，你一眼就可以看出來

0:53:33.340,0:53:38.340
如果你今天把 (θ_1 - a) 都用 △θ_1 表示

0:53:38.340,0:53:42.780
(θ_2 - b) 都用 △θ_2 來表示

0:53:42.780,0:53:47.500
s 你可以不用理它，因為它跟 θ 沒關係啊

0:53:47.500,0:53:51.680
所以你要找不同 θ 讓它值最小，不用管 s

0:53:52.260,0:53:54.400
好，如果我們看一下 L

0:53:54.400,0:53:59.320
你會發現說它是  u * △θ_1 + v * △θ_2

0:53:59.320,0:54:01.460
也就是說，它就好像是

0:54:01.460,0:54:06.920
它的值就是，有一個 vector ，叫做 (△θ_1, △θ_2)

0:54:07.320,0:54:10.540
有另外一個 vector，叫做 (u, v)

0:54:10.940,0:54:14.640
你把這個 vector 跟這個 vector 做 inner product

0:54:14.640,0:54:18.000
你就把 △θ_1 * u + θ_2 * v

0:54:18.000,0:54:22.560
你就得到這個值，如果我們忽略 s 的話，
你就得到這個值

0:54:22.560,0:54:26.700
接下來的問題就是，如果我們要讓 L(θ) 最小

0:54:27.000,0:54:33.720
我們應該選擇什麼樣的 (△θ_1, △θ_2) 呢？

0:54:33.720,0:54:36.660
我們要選擇什麼樣的 (△θ_1, △θ_2)

0:54:36.660,0:54:39.460
我們才能夠讓 L(θ) 最小呢？

0:54:39.460,0:54:47.660
這個，太容易了，就是選正對面的，對不對？

0:54:47.660,0:54:56.580
如果我們今天把 (△θ_1, △θ_2) 轉成跟 (u, v) 這條反方向

0:54:57.280,0:55:02.100
然後，再把 (△θ_1, △θ_2) 的長度增長

0:55:02.100,0:55:06.520
我們把它轉到反方向，再把它伸長

0:55:06.520,0:55:10.580
長到極限，也就是長到這個紅色圈圈的邊緣

0:55:11.080,0:55:14.420
那這個 (△θ_1, △θ_2) 跟 (u, v)

0:55:14.420,0:55:18.000
它們做 inner product 的時候，它的值是最大的

0:55:18.420,0:55:20.720
所以，這告訴我們說

0:55:20.720,0:55:24.840
什麼樣的 (△θ_1, △θ_2) 可以讓 loss 的值最小呢？

0:55:24.840,0:55:28.340
就是它是 (u, v) 乘上負號

0:55:28.340,0:55:31.300
再乘上一個 scale

0:55:31.300,0:55:34.560
再乘上一個 constant，也就是說你要把 (△θ_1, △θ_2)

0:55:34.560,0:55:40.880
調整它的長度，長到正好頂到這個紅色圈圈的邊邊

0:55:40.900,0:55:43.520
這個時候呢，它算出來的 loss 是最小的

0:55:43.520,0:55:49.100
這一項應該跟長度是成正比的

0:55:50.380,0:55:52.640
所以呢，我們再整理一下式子

0:55:52.640,0:55:57.700
△θ_1 就是 (θ_1 - a)，△θ_2 就是 (θ_2 - b)

0:55:57.700,0:56:00.860
所以，如果我們今天要再紅色圈圈裡面

0:56:00.860,0:56:03.540
找一個 θ_1 跟 θ_2 讓 loss 最小的話

0:56:03.840,0:56:08.800
那怎麼做呢？那個最小的值，就是中心點 (a, b)

0:56:08.800,0:56:13.500
減掉某一個 constant 乘上 (u, v)

0:56:13.500,0:56:17.260
中心點 (a, b) 減掉某一個 constant 乘上 (u, v)

0:56:18.800,0:56:22.000
所以，我們就知道了這件事

0:56:22.000,0:56:27.820
那你接下來要做的事，就是把 (u, v) 帶進去

0:56:27.820,0:56:30.000
把它帶進去，就得到這樣子的式子

0:56:30.000,0:56:32.380
那這個式子，你就發現它其實

0:56:32.380,0:56:36.280
exactly 就是 Gradient Descent

0:56:36.280,0:56:40.180
對不對？我們做 Gradient Descent 的時候，就是找一個初始值

0:56:40.180,0:56:43.340
算每一個參數在初始值的地方的偏微分

0:56:43.340,0:56:45.080
把它排成一個 vector，就是 Gradient

0:56:45.080,0:56:48.040
然後再乘上某一個東西，叫做 learning rate，再把它減掉

0:56:48.040,0:56:52.260
所以這個式子，exactly 就是 Gradient Descent 的式子

0:56:52.940,0:56:56.160
但你要想想看，我們今天可以做這件事情

0:56:56.160,0:56:58.160
我們可以用這個方法，找一個最小值

0:56:58.160,0:57:02.560
它的前提是什麼？ 它的前提是

0:57:02.560,0:57:06.540
你的上面這個式子，要成立

0:57:06.540,0:57:10.540
Maclaurin series給你的這個 approximation 是夠精確的

0:57:10.540,0:57:13.820
什麼樣 Taylor series 給我們的 
approximation 才夠精確呢？

0:57:13.820,0:57:17.120
當你今天畫出來的紅色圈圈夠小的時候

0:57:17.120,0:57:21.980
Taylor series 給我們的 approximation 才會夠精確

0:57:21.980,0:57:24.820
好，才會夠精確

0:57:24.820,0:57:27.400
所以，這個就告訴我們什麼？

0:57:27.400,0:57:30.120
這個告訴我們說，你這個紅色圈圈的半徑是小的

0:57:30.120,0:57:31.780
那這個 η，這個 learning rate

0:57:31.780,0:57:34.760
它跟紅色圈圈的半徑是成正比的

0:57:34.760,0:57:36.500
所以這個 learning rate 不能太大

0:57:36.500,0:57:40.640
你 learning rate 要很小，
你這個 learning rate 無窮小的時候呢

0:57:40.640,0:57:41.920
這個式子才會成立

0:57:41.920,0:57:45.160
所以 Gradient Descent，如果你要讓你每次 update 參數的時候

0:57:45.160,0:57:47.020
你的 loss 都越來越小的話

0:57:47.020,0:57:52.120
其實，理論上你的 learning rate 要無窮小，
你才能夠保證這件事情

0:57:52.120,0:57:56.200
雖然實作上，只要夠小就行了

0:57:56.200,0:58:00.040
所以，你會發現說，如果你的 learning rate 沒有設好

0:58:00.040,0:58:02.940
是有可能說，你每次 update 參數的時候

0:58:02.940,0:58:04.360
這個式子是不成立的

0:58:04.360,0:58:08.960
所以導致你做 Gradient Descent 的時候，
你沒有辦法讓 loss 越來越小

0:58:09.440,0:58:10.820
那你會發現說

0:58:10.820,0:58:13.720
這個 L，它只考慮了

0:58:14.360,0:58:16.720
Taylor series 裡面的一次式

0:58:16.720,0:58:18.480
可不可以考慮二次式呢？

0:58:18.480,0:58:22.860
Taylor series 不是有二次、三次，還有很多嗎？

0:58:22.860,0:58:25.300
如果你把二次式考慮進來

0:58:25.300,0:58:26.460
你把二次式考慮進來

0:58:26.460,0:58:31.280
理論上，你的 learning rate 就可以設大一點

0:58:31.280,0:58:34.560
對不對，如果我們把二次式考慮進來

0:58:34.560,0:58:36.580
可不可以呢？是可以的

0:58:36.580,0:58:40.820
那有一些方法，我們今天沒有要講，
是有考慮到二次式的

0:58:40.820,0:58:42.460
比如說，牛頓法這樣子

0:58:42.460,0:58:45.840
那在實作上，尤其是假設你在做 deep learning 的時候

0:58:45.840,0:58:49.900
這樣的方法，不見得太普及，不見得太 practical

0:58:49.900,0:58:53.360
為甚麼呢？因為你現在要算二次微分

0:58:53.360,0:58:58.400
甚至它還會包含一個 Hessian 的 matrix

0:58:58.400,0:59:02.360
和 Hessian matrix 的 inverse，總之，你會多很多運算

0:59:02.360,0:59:04.980
而這些運算，在做 deep learning 的時候呢

0:59:04.980,0:59:07.420
你是無法承受的

0:59:07.420,0:59:11.340
你用這個運算，來換你 update 的時候比較有效率

0:59:11.340,0:59:13.100
會覺得是不划算的

0:59:13.100,0:59:16.320
所以，今天如果在做，比如說，deep learning 的時候

0:59:16.320,0:59:21.700
通常，還是 Gradient Descent 是比較普及、主流的作法

0:59:22.440,0:59:24.620
上面如果你沒有聽懂的話，也沒關係

0:59:24.620,0:59:29.220
在最後一頁，我們要講的是 Gradient Descent 的限制

0:59:29.480,0:59:32.220
 Gradient Descent 有什麼樣的限制呢？

0:59:32.220,0:59:37.200
有一個大家都知道的是，
它會卡在這個 local minimum 的地方

0:59:37.200,0:59:40.380
它會卡在 local minimum 的地方

0:59:41.500,0:59:44.700
所以，如果這是你的 error surface

0:59:44.880,0:59:49.320
那你從這個地方，當作你的初始值，去更新你的參數

0:59:49.320,0:59:53.460
最後走到一個微分值是 0，也就是 local minimum 的地方

0:59:53.460,0:59:56.260
你參數的更新，就停止了

0:59:56.980,1:00:00.420
但是，一般人就只知道這個問題而已

1:00:00.420,1:00:02.240
那其實還有別的問題

1:00:02.240,1:00:06.860
事實上，這個微分值是 0 的地方，
並不是只有 local minimum 阿

1:00:06.860,1:00:12.440
對不對，settle point 也是微分值是 0

1:00:12.440,1:00:14.940
所以，你今天在參數 update 的時候

1:00:14.940,1:00:20.380
你也是有可能卡在一個不是 local minimum，
但是微分值是 0 的地方

1:00:20.380,1:00:23.140
這件事情，也是有可能發生的

1:00:23.820,1:00:28.140
但是，這什麼卡在 local minimum 或微分值是 0 的地方啊

1:00:28.140,1:00:31.120
這都只是幻想啦

1:00:31.240,1:00:33.520
其實，真正的問題是這樣

1:00:33.520,1:00:35.560
你今天其實只要

1:00:35.560,1:00:38.980
你想想看，你 implement 作業一了

1:00:38.980,1:00:44.120
你幾時是真的算出來，那個微分值 exactly 等於 0 的時候

1:00:44.120,1:00:45.440
就把它停下來了

1:00:45.440,1:00:49.700
也就是，你最多就做微分值小於 10^(-6)

1:00:49.700,1:00:52.160
小於一個很小的值，你就把它停下來了，對不對？

1:00:52.260,1:00:56.960
但是，你怎麼知道，那個微分值算出來很小的時候

1:00:56.960,1:00:59.440
它就很接近 local minimum 呢？

1:00:59.440,1:01:01.240
不見得很接近 local minimum 阿

1:01:01.240,1:01:06.340
有可能，微分值算出來很小，
但它其實是在一個高原的地方

1:01:06.740,1:01:09.960
而那高原的地方，微分值算出來很小，你就覺得說

1:01:09.960,1:01:13.440
哦，那這個一定就是很接近 local minimum

1:01:13.440,1:01:15.600
在 local minimum 附近，所以你就停下來了

1:01:15.600,1:01:19.020
因為你們真的很少有機會 exactly 微分值算出來是 0 嘛

1:01:19.020,1:01:23.200
對不對，你可能覺得說，
微分算出來很小，就很接近 local minimum

1:01:23.200,1:01:26.980
你就把它停下來，那其實搞不好，它是一個高原的地方

1:01:26.980,1:01:29.460
它離那個 local minimum 還很遠啊

1:01:29.460,1:01:31.640
這也是有可能的

1:01:32.380,1:01:35.300
講到這邊，有人都會問我一個問題

1:01:35.300,1:01:37.460
這個問題，我很難回答，他說

1:01:37.460,1:01:41.100
你怎麼會不知道你是不是接近 local minimum 了呢？

1:01:41.100,1:01:44.060
我一眼就知道說 local minimum 在這邊阿

1:01:45.180,1:01:47.220
呵呵，你怎麼會不知道呢？

1:01:47.220,1:01:49.800
所以我覺得這些圖都沒有辦法
表示 Gradient Descent 的精神

1:01:49.800,1:01:51.800
臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
