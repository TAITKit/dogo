<head><meta http-equiv='Content-Type' content='text/html; charset=utf-8'></head>
﻿0:00:00.000,0:00:09.220<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
<br>
0:00:13.080,0:00:20.080<br>
那我們上次講了 generation 這件事情<br>
<br>
0:00:20.080,0:00:24.380<br>
然後，可以用 PixelRNN 來做<br>
<br>
0:00:24.380,0:00:29.980<br>
這邊 create 了一個 generative 的 task<br>
<br>
0:00:29.980,0:00:33.100<br>
上次還講了 VAE<br>
<br>
0:00:33.100,0:00:36.000<br>
我們沒有講太多它的原理<br>
<br>
0:00:36.000,0:00:38.580<br>
我們來複習一下 VAE 做的事情<br>
<br>
0:00:38.580,0:00:41.280<br>
Auto-encoder 我想大家都很熟悉了<br>
<br>
0:00:41.280,0:00:43.480<br>
那 VAE 做的事情是甚麼呢？<br>
<br>
0:00:43.480,0:00:46.100<br>
VAE 做的事情是說<br>
<br>
0:00:46.100,0:00:49.980<br>
你一樣有一個 encode，有一個 decoder<br>
<br>
0:00:49.980,0:00:55.180<br>
你現在 decoder 會 output 兩組 vector<br>
<br>
0:00:55.180,0:00:57.600<br>
這邊一組是 m1 到 m3<br>
<br>
0:00:57.600,0:01:00.200<br>
另外一組是 σ1 到 σ3<br>
<br>
0:01:00.200,0:01:04.860<br>
接下來，你會 generate 一個 normal distribution<br>
<br>
0:01:04.860,0:01:07.840<br>
你會 generate 一個 vector<br>
<br>
0:01:07.920,0:01:11.560<br>
這個 vector 是從 normal distribution sample 出來的<br>
<br>
0:01:11.560,0:01:15.620<br>
接下來，你把 σ 這個 vector 取 exponential<br>
<br>
0:01:15.620,0:01:19.220<br>
然後，再乘上 random sample 出來的這個 vector<br>
<br>
0:01:19.220,0:01:21.300<br>
再加上原來 m 這個 vector<br>
<br>
0:01:21.300,0:01:24.920<br>
得到你的 code, c，然後把那個 code 丟進<br>
<br>
0:01:24.920,0:01:28.200<br>
decoder 裡面，產生 image，那你希望說<br>
<br>
0:01:28.200,0:01:30.160<br>
input 跟 output 越接近越好<br>
<br>
0:01:30.160,0:01:33.080<br>
另外 Auto-encoder 還有另外一項 constraint<br>
<br>
0:01:33.080,0:01:35.140<br>
那我回去的時候，我發現我上次的投影片呢<br>
<br>
0:01:35.140,0:01:38.080<br>
寫的是相反的，所以<br>
<br>
0:01:38.080,0:01:39.420<br>
差了一個負號<br>
<br>
0:01:39.420,0:01:42.740<br>
所以如果按照原來的寫法，這邊應該是 maximize<br>
<br>
0:01:42.740,0:01:46.040<br>
那如果是用 minimize 的話，下面就應該再加一個負號<br>
<br>
0:01:46.980,0:01:49.460<br>
再來的問題就是<br>
<br>
0:01:49.460,0:01:52.020<br>
為甚麼要用 VAE 這個方法<br>
<br>
0:01:52.120,0:01:55.360<br>
原來的 Auto-encoder 會有甚麼樣的問題呢？<br>
<br>
0:01:55.360,0:01:57.720<br>
如果你看文獻上的話<br>
<br>
0:01:57.720,0:02:00.240<br>
VAE，如果你看它原來的 paper 的話<br>
<br>
0:02:00.240,0:02:03.320<br>
它有很多很多的式子，你就會看的一頭霧水的<br>
<br>
0:02:03.320,0:02:05.280<br>
那在講那些式子之前呢<br>
<br>
0:02:05.280,0:02:08.820<br>
我們先來看 intuitive 的理由<br>
<br>
0:02:08.820,0:02:10.280<br>
為甚麼要用 VAE？<br>
<br>
0:02:10.280,0:02:12.980<br>
如果是原來的 Auto-encoder 的話<br>
<br>
0:02:12.980,0:02:16.340<br>
原來的 Auto-encoder 它做的事情是<br>
<br>
0:02:16.340,0:02:18.800<br>
我們把每一張 image<br>
<br>
0:02:18.800,0:02:23.000<br>
變成一個 code，假設我們現在的 code 是一維<br>
<br>
0:02:23.000,0:02:25.680<br>
就是圖上的這條紅色的線<br>
<br>
0:02:25.680,0:02:27.900<br>
那你把滿月的這個圖<br>
<br>
0:02:27.900,0:02:29.680<br>
變成 code 上的一個 value<br>
<br>
0:02:29.680,0:02:32.540<br>
然後，再從這個 value 做 decode<br>
<br>
0:02:32.540,0:02:35.680<br>
把它變成原來的圖，那如果是弦月的圖<br>
<br>
0:02:35.680,0:02:38.340<br>
也是一樣變成 code 上的一個 value<br>
<br>
0:02:38.340,0:02:42.920<br>
接下來，你再把它從 code 上的一個 value 變回原來的圖<br>
<br>
0:02:43.260,0:02:47.500<br>
假設我們今天是在滿月和弦月的 code 中間<br>
<br>
0:02:47.500,0:02:49.580<br>
sample 一個點，你覺得它原來<br>
<br>
0:02:49.580,0:02:51.600<br>
然後，再把這個點做 decode<br>
<br>
0:02:51.600,0:02:54.380<br>
變回一張 image，它會變成甚麼樣子呢？<br>
<br>
0:02:54.380,0:02:56.320<br>
你心裡或許期待著說<br>
<br>
0:02:56.320,0:03:02.640<br>
它會變成滿月和弦月中間的樣子<br>
<br>
0:03:02.640,0:03:04.700<br>
但是，這只是你的想像而已<br>
<br>
0:03:04.700,0:03:08.380<br>
其實，因為我們今天用的 encoder 和 decoder<br>
<br>
0:03:08.380,0:03:11.680<br>
我們今天用的 encoder 和 decoder，它都是 non-learner<br>
<br>
0:03:11.680,0:03:13.440<br>
它都是一個 neural network<br>
<br>
0:03:13.440,0:03:15.560<br>
所以，你其實很難預測說<br>
<br>
0:03:15.560,0:03:20.180<br>
在這個滿月和弦月中間到底會發生甚麼事情<br>
<br>
0:03:20.180,0:03:22.880<br>
你可能想像是，滿月和弦月中間的月象<br>
<br>
0:03:22.880,0:03:25.760<br>
但未必，它可能根本就是另外一個東西<br>
<br>
0:03:25.760,0:03:28.480<br>
那如果用 VAE 有甚麼好處呢？<br>
<br>
0:03:28.480,0:03:30.360<br>
如果用 VAE 的好處是<br>
<br>
0:03:30.360,0:03:34.680<br>
實際上，VAE 在做的事情<br>
<br>
0:03:34.680,0:03:36.460<br>
就等於我下面說的這件事情<br>
<br>
0:03:36.460,0:03:40.740<br>
當你把這個滿月的圖變成一個 code 的時候<br>
<br>
0:03:40.740,0:03:44.920<br>
它會在這 code 上面再加上 noise<br>
<br>
0:03:44.920,0:03:48.140<br>
它會希望，在加上 noise 以後<br>
<br>
0:03:48.140,0:03:51.640<br>
這個 code reconstruct 以後，還是一張滿月<br>
<br>
0:03:51.640,0:03:55.320<br>
也就是說，原來的 Auto encoder 只有這個點<br>
<br>
0:03:55.320,0:03:58.540<br>
需要被 reconstruct 回滿月的圖<br>
<br>
0:03:58.540,0:04:01.440<br>
但是對 VAE 來說，你會將上 noise<br>
<br>
0:04:01.440,0:04:05.120<br>
在這個範圍之內的圖 reconstruct 回來以後<br>
<br>
0:04:05.120,0:04:07.260<br>
都應該仍然要是滿月的圖<br>
<br>
0:04:07.260,0:04:09.520<br>
這個弦月的圖也是一樣<br>
<br>
0:04:09.520,0:04:12.120<br>
弦月的 code 再加一個 noise<br>
<br>
0:04:12.120,0:04:15.080<br>
reconstruct 回來以後<br>
<br>
0:04:15.080,0:04:17.480<br>
這個 range 的 code 都要變成弦月的圖<br>
<br>
0:04:17.480,0:04:19.680<br>
你會發現說，在這個位置<br>
<br>
0:04:19.680,0:04:22.820<br>
在這個地方，這個 code 的點<br>
<br>
0:04:22.820,0:04:27.900<br>
它同時希望被 reconstruct 回弦月的圖<br>
<br>
0:04:27.900,0:04:31.600<br>
同時也希望被 reconstruct 回滿月的圖<br>
<br>
0:04:31.600,0:04:34.640<br>
那可是你只能夠 reconstruct 回一張圖而已<br>
<br>
0:04:34.640,0:04:36.180<br>
怎麼辦？<br>
<br>
0:04:36.180,0:04:38.060<br>
那 VAE training 的時候<br>
<br>
0:04:38.060,0:04:41.520<br>
你要 minimize  這個 mean square error<br>
<br>
0:04:41.520,0:04:44.120<br>
所以，最後產生這個位置<br>
<br>
0:04:44.120,0:04:45.380<br>
所產生的圖<br>
<br>
0:04:45.380,0:04:49.380<br>
會是一張介於滿月和弦月中間的圖<br>
<br>
0:04:49.380,0:04:51.700<br>
同時讓它最像滿月，也最像弦月<br>
<br>
0:04:51.700,0:04:53.540<br>
那你產生的圖會是甚麼樣子呢？<br>
<br>
0:04:53.540,0:04:57.860<br>
或許就是，界於滿月和弦月之間的月象圖<br>
<br>
0:04:57.860,0:05:00.360<br>
所以，如果你用 VAE 的話<br>
<br>
0:05:00.360,0:05:03.000<br>
你從你 code 的 space上面<br>
<br>
0:05:03.000,0:05:05.680<br>
去 sample 一個 code<br>
<br>
0:05:05.680,0:05:07.120<br>
在產生 image 的時候<br>
<br>
0:05:07.120,0:05:09.800<br>
你可能會得到比較好的 image<br>
<br>
0:05:09.800,0:05:11.980<br>
如果是原來的 Auto-encoder 的話<br>
<br>
0:05:11.980,0:05:13.700<br>
你 random sample 一個 point<br>
<br>
0:05:13.700,0:05:18.700<br>
你得到的，可能看起來都不像是一個真實的 image<br>
<br>
0:05:18.700,0:05:21.520<br>
所以，VAE 就是這樣<br>
<br>
0:05:21.520,0:05:27.200<br>
這個 m，這個 encoder 的 output, m 代表是原來的 code<br>
<br>
0:05:27.200,0:05:33.620<br>
那這個 c，代表是加上 noise 以後的 code<br>
<br>
0:05:33.620,0:05:37.600<br>
decoder 要根據加上 noise 以後的 code<br>
<br>
0:05:37.600,0:05:40.400<br>
把它 reconstruct 回原來的 image<br>
<br>
0:05:40.400,0:05:43.280<br>
那這個 σ 跟 e 是甚麼意思呢？<br>
<br>
0:05:43.280,0:05:47.600<br>
這個 σ，它就代表了<br>
<br>
0:05:47.600,0:05:50.160<br>
現在這個 noise的 variance<br>
<br>
0:05:50.160,0:05:53.640<br>
它代表了你的 noise 應該要有多大<br>
<br>
0:05:53.640,0:05:55.600<br>
因為 variance 是正的<br>
<br>
0:05:55.600,0:05:58.640<br>
所以，這一邊會取一個 exponential<br>
<br>
0:05:58.640,0:06:00.600<br>
因為 neural network 的 output<br>
<br>
0:06:00.600,0:06:03.320<br>
假設你沒有用 activation function 去控制它的話<br>
<br>
0:06:03.320,0:06:05.520<br>
它的 output 可正可負<br>
<br>
0:06:05.520,0:06:08.220<br>
假設你這一邊是 linear的 output 的話<br>
<br>
0:06:08.220,0:06:09.700<br>
那 output 可正可負<br>
<br>
0:06:09.700,0:06:12.780<br>
所以取一個 exponential 確保它一定是正的<br>
<br>
0:06:12.780,0:06:15.880<br>
可以被當做是 variance 來看待<br>
<br>
0:06:15.880,0:06:19.500<br>
那現在當你把這個 σ<br>
<br>
0:06:19.500,0:06:21.540<br>
乘上這個 e<br>
<br>
0:06:21.540,0:06:25.920<br>
這個 e 是從一個 normal distribution sample 出來的值<br>
<br>
0:06:25.920,0:06:30.160<br>
當你把這個 σ 乘上 e 再加到 m 的時候<br>
<br>
0:06:30.160,0:06:34.460<br>
就等於是你把這個 m 加上了 noise<br>
<br>
0:06:34.460,0:06:37.380<br>
就等於是你把原來 code 加上 noise<br>
<br>
0:06:37.380,0:06:38.700<br>
那這個 e 呢<br>
<br>
0:06:38.700,0:06:41.320<br>
是從 normal distribution sample 出來的<br>
<br>
0:06:41.320,0:06:43.380<br>
所以，它的 variance 是固定的<br>
<br>
0:06:43.380,0:06:46.400<br>
但是，乘上不同 σ 以後呢<br>
<br>
0:06:46.400,0:06:49.180<br>
它的 variance 的大小就有所改變<br>
<br>
0:06:49.180,0:06:52.500<br>
所以這一個 variance 決定了 noise的大小<br>
<br>
0:06:52.500,0:06:54.240<br>
而這一個 variance大小<br>
<br>
0:06:54.240,0:06:56.820<br>
這個 variance 是從 encoder 產生的<br>
<br>
0:06:56.820,0:06:58.820<br>
也就是說 machine 在 training 的時候<br>
<br>
0:06:58.820,0:07:03.600<br>
它會自動去 learn 說，這個 variance 應該要有多大<br>
<br>
0:07:03.600,0:07:07.460<br>
但是，如果就只是這樣子是不夠的<br>
<br>
0:07:07.460,0:07:10.500<br>
假如你現在的 training 就只考慮說<br>
<br>
0:07:10.500,0:07:12.280<br>
我現在 input 一張 image<br>
<br>
0:07:12.280,0:07:14.740<br>
然後，我中間有這個加 noise 的機制<br>
<br>
0:07:14.740,0:07:16.960<br>
noise 的 variance 是自己 learn 的<br>
<br>
0:07:16.960,0:07:18.680<br>
然後，decoder 要 reconstruct 回原來的 image<br>
<br>
0:07:18.680,0:07:22.020<br>
那你要 minimize 這個 reconstruction error<br>
<br>
0:07:22.020,0:07:24.340<br>
如果你只有做這一件事情的話，是不夠的<br>
<br>
0:07:24.340,0:07:25.760<br>
你 train 出來的結果呢<br>
<br>
0:07:25.760,0:07:28.620<br>
是不會，並不會如同你預期的樣子<br>
<br>
0:07:28.620,0:07:29.520<br>
為甚麼呢？<br>
<br>
0:07:29.520,0:07:32.400<br>
因為這個 variance 現在自己學的<br>
<br>
0:07:32.400,0:07:35.480<br>
假設你讓 machine 自己決定說 variance 是多少<br>
<br>
0:07:35.480,0:07:37.580<br>
那它一定會決定說<br>
<br>
0:07:37.580,0:07:39.940<br>
variance 是 0 就好了 ，對不對<br>
<br>
0:07:39.940,0:07:41.960<br>
就讓大家自己決定分數是多少<br>
<br>
0:07:41.960,0:07:43.440<br>
那每一個人都會是 100 分<br>
<br>
0:07:43.440,0:07:46.300<br>
所以，這邊這個 variance<br>
<br>
0:07:46.300,0:07:48.360<br>
如果你只讓 machine 自己決定的話<br>
<br>
0:07:48.360,0:07:50.740<br>
它就會覺得說，variance 是 0 就好了<br>
<br>
0:07:50.740,0:07:53.040<br>
那你就等於是原來的 Auto-encoder<br>
<br>
0:07:53.040,0:07:55.240<br>
因為 variance 是 0 的話，就不會有這個<br>
<br>
0:07:55.240,0:07:57.160<br>
不同的 image overlap的情形<br>
<br>
0:07:57.160,0:07:59.520<br>
這樣你 reconstruction error 是最小的<br>
<br>
0:07:59.520,0:08:02.240<br>
所以，你要在這個 variance 上面呢<br>
<br>
0:08:02.240,0:08:03.720<br>
去做一些限制<br>
<br>
0:08:03.720,0:08:06.400<br>
你要強迫它的 variance 不可以太小<br>
<br>
0:08:06.400,0:08:10.200<br>
怎麼做呢？所以，我們另外再加了這一項<br>
<br>
0:08:10.200,0:08:14.620<br>
其實就是對 variance 做了一些限制<br>
<br>
0:08:14.620,0:08:17.680<br>
怎麼說呢，這一項是這樣子<br>
<br>
0:08:17.680,0:08:24.380<br>
你看它這一邊有 exp(σi) - (1 + σi)<br>
<br>
0:08:24.380,0:08:28.440<br>
那 exp(σi) 畫在圖上的話<br>
<br>
0:08:28.440,0:08:31.020<br>
它是藍色的這一條線<br>
<br>
0:08:31.020,0:08:37.300<br>
(1+σi) 畫在圖上，它是紅色的這一條線<br>
<br>
0:08:37.300,0:08:41.100<br>
當你把藍色這一條線減紅色這一條線的時候<br>
<br>
0:08:41.100,0:08:43.100<br>
你得到是綠色的這一條線<br>
<br>
0:08:43.100,0:08:45.700<br>
綠色這一條線的最低點呢<br>
<br>
0:08:45.700,0:08:48.000<br>
是落在 σ = 0 的地方<br>
<br>
0:08:48.000,0:08:52.700<br>
注意一下，σ 之後會再乘 exponential，所以σ = 0<br>
<br>
0:08:52.700,0:08:55.220<br>
意味這說，它的 variance 是 1<br>
<br>
0:08:55.220,0:08:56.620<br>
exp = 0，是 1<br>
<br>
0:08:56.620,0:09:00.180<br>
所以 σ = 0 的時候，loss 最低<br>
<br>
0:09:00.180,0:09:02.180<br>
意味著說， 你的 variance 等於 1 的時候<br>
<br>
0:09:02.180,0:09:04.220<br>
loss 最低<br>
<br>
0:09:04.220,0:09:07.300<br>
所以，machine 就不會說，讓 variance 等於 0，然後<br>
<br>
0:09:07.340,0:09:09.940<br>
minimizes reconstruction error，它還要考慮說<br>
<br>
0:09:09.940,0:09:12.960<br>
variance 是不能夠太小<br>
<br>
0:09:12.960,0:09:16.420<br>
那最後這一項 ( mi )^2<br>
<br>
0:09:16.420,0:09:18.860<br>
對這個 code 做這個<br>
<br>
0:09:18.860,0:09:21.200<br>
要 minimize code 的這個 L2-norm<br>
<br>
0:09:21.200,0:09:24.100<br>
怎麼解釋呢，其實很容易解釋<br>
<br>
0:09:24.100,0:09:27.080<br>
你就想成是我們現在加了 L2 的 regularization<br>
<br>
0:09:27.080,0:09:29.680<br>
我們本來常常在 train Auto-encoder 的時候<br>
<br>
0:09:29.680,0:09:31.440<br>
你就會在你的 code 上面呢<br>
<br>
0:09:31.440,0:09:33.300<br>
加一些 regularization<br>
<br>
0:09:33.300,0:09:35.120<br>
讓它結果比較 sparse<br>
<br>
0:09:35.120,0:09:36.820<br>
比較不會 overfitting<br>
<br>
0:09:36.820,0:09:40.380<br>
比較不會 learn 出太多 trivial 的 solution<br>
<br>
0:09:40.380,0:09:44.000<br>
那這個是直觀的理由<br>
<br>
0:09:44.000,0:09:46.360<br>
如果比較正式解釋的話<br>
<br>
0:09:46.360,0:09:48.100<br>
要怎麼解釋它呢<br>
<br>
0:09:48.100,0:09:53.300<br>
以下就是 paper 上，比較常見的說法<br>
<br>
0:09:53.300,0:09:56.980<br>
假設我們回歸到我們到底要做的事情是什麼<br>
<br>
0:09:56.980,0:09:59.800<br>
假設你現在要叫 machine 做的事情<br>
<br>
0:09:59.800,0:10:01.940<br>
是 generate 寶可夢的圖<br>
<br>
0:10:01.940,0:10:04.480<br>
每一張寶可夢的圖<br>
<br>
0:10:04.480,0:10:09.340<br>
你都可以想成是高維空間中的一個點<br>
<br>
0:10:09.340,0:10:11.660<br>
一張 image，假設它是 20*20 的 image<br>
<br>
0:10:11.660,0:10:13.260<br>
它在高維的空間中<br>
<br>
0:10:13.260,0:10:16.280<br>
就是一個 20*20，也就是一個 400 維的點<br>
<br>
0:10:16.280,0:10:17.960<br>
我們這邊寫做 x<br>
<br>
0:10:17.960,0:10:20.580<br>
雖然在圖上，我們只用一維來描述它<br>
<br>
0:10:20.580,0:10:23.680<br>
但它其實是一個高維的空間<br>
<br>
0:10:23.680,0:10:26.080<br>
那我們現在要做的事情<br>
<br>
0:10:26.080,0:10:32.400<br>
其實就是 estimate 高維空間上面的機率分佈，P(x)<br>
<br>
0:10:32.400,0:10:35.280<br>
我們要做的事情就是 estimate 這個 P(x)<br>
<br>
0:10:35.280,0:10:38.920<br>
只要我們能夠 estimate 出這個 P(x) 的樣子<br>
<br>
0:10:38.920,0:10:42.380<br>
注意，這個 x 其實是一個 vector<br>
<br>
0:10:42.380,0:10:45.500<br>
假設我們可以 estimate 出 P(x) 的樣子<br>
<br>
0:10:45.500,0:10:47.340<br>
我們就可以根據這個 P(x)<br>
<br>
0:10:47.340,0:10:48.940<br>
去 sample 出一張圖<br>
<br>
0:10:48.940,0:10:50.200<br>
那找出來的圖<br>
<br>
0:10:50.200,0:10:52.100<br>
就會像是寶可夢的樣子<br>
<br>
0:10:52.100,0:10:55.740<br>
因為你取 P(x) 的時候，你會從機率高的地方<br>
<br>
0:10:55.740,0:10:57.760<br>
比較容易被 sample 出來<br>
<br>
0:10:57.760,0:11:00.820<br>
所以，這個 P(x) 理論上應該是在<br>
<br>
0:11:00.820,0:11:03.120<br>
有寶可夢的圖的地方<br>
<br>
0:11:03.120,0:11:05.500<br>
這有寶可夢的圖，如果你今天<br>
<br>
0:11:05.500,0:11:09.220<br>
這個圖長得像一隻寶可夢的話，它的機率是大的<br>
<br>
0:11:09.220,0:11:10.300<br>
它的機率是大的<br>
<br>
0:11:10.300,0:11:13.140<br>
這個是噴火龍家族，他們的機率是大的<br>
<br>
0:11:13.140,0:11:14.980<br>
這個是水箭龜家族，他們機率是大的<br>
<br>
0:11:14.980,0:11:17.020<br>
如果是一張怪怪的圖的話<br>
<br>
0:11:17.020,0:11:19.900<br>
比如說，這一個看起來像是皮卡丘，又有一點不像<br>
<br>
0:11:19.900,0:11:22.940<br>
這個看起來像是一隻綿羊，又像是一個雲<br>
<br>
0:11:22.940,0:11:24.540<br>
這一邊呢，機率是低的<br>
<br>
0:11:24.540,0:11:25.980<br>
這一邊機率是低的<br>
<br>
0:11:25.980,0:11:28.180<br>
如果我們今天能夠 estimate 出<br>
<br>
0:11:28.180,0:11:30.040<br>
這一個 probability distribution<br>
<br>
0:11:30.040,0:11:31.520<br>
那就結束了<br>
<br>
0:11:31.520,0:11:34.960<br>
那怎麼 estimate 一個 probability 的 distribution 呢？<br>
<br>
0:11:34.960,0:11:39.180<br>
我們可以用 Gaussian mixture model<br>
<br>
0:11:39.180,0:11:42.560<br>
我不知道在座有多少人知道 Gaussian mixture model<br>
<br>
0:11:42.560,0:11:43.980<br>
我好奇問一下<br>
<br>
0:11:43.980,0:11:46.500<br>
知道 Gaussian mixture model 的同學舉手一下<br>
<br>
0:11:46.500,0:11:48.340<br>
好手放下，謝謝<br>
<br>
0:11:48.340,0:11:50.620<br>
很多人都知道 Gaussian mixture model<br>
<br>
0:11:50.620,0:11:52.200<br>
那太好了<br>
<br>
0:11:52.200,0:11:56.920<br>
有學過語音課的話，就聽過 Gaussian mixture model<br>
<br>
0:11:56.920,0:11:58.100<br>
Gaussian mixture model 在做甚麼呢？<br>
<br>
0:11:58.100,0:12:00.100<br>
我們現在有一個 distribution，它長這個樣子<br>
<br>
0:12:00.100,0:12:03.260<br>
黑色的、很複雜<br>
<br>
0:12:03.260,0:12:06.620<br>
我們說這個很複雜的黑色 distribution，它其實是<br>
<br>
0:12:06.620,0:12:08.780<br>
很多的 Gaussian<br>
<br>
0:12:08.780,0:12:10.920<br>
我這一邊藍色的代表 Gaussian<br>
<br>
0:12:10.920,0:12:12.420<br>
有很多的 Gaussian<br>
<br>
0:12:12.420,0:12:16.000<br>
用不同的 weight 疊合起來的結果<br>
<br>
0:12:16.000,0:12:18.520<br>
假設你今天 Gaussian 的數目夠多<br>
<br>
0:12:18.520,0:12:22.360<br>
你就可以產生很複雜的 distribution<br>
<br>
0:12:22.360,0:12:23.780<br>
所以，雖然黑色很複雜<br>
<br>
0:12:23.780,0:12:26.780<br>
但它背後其實是有很多 Gaussian 疊合起來的結果<br>
<br>
0:12:26.980,0:12:29.440<br>
那這個式子怎麼寫它呢？<br>
<br>
0:12:29.440,0:12:34.440<br>
你會把它寫成這樣<br>
<br>
0:12:34.440,0:12:39.440<br>
首先，如果你要從 P(x) sample 一個東西的時候<br>
<br>
0:12:39.440,0:12:40.860<br>
你怎麼做？<br>
<br>
0:12:40.860,0:12:44.780<br>
你先決定你要從哪一個 Gaussian sample 東西<br>
<br>
0:12:44.780,0:12:47.600<br>
假設現在有一把 Gaussian<br>
<br>
0:12:47.600,0:12:49.040<br>
有一把 Gaussian<br>
<br>
0:12:49.040,0:12:51.960<br>
每一個 Gaussian 背後都有一個 weight<br>
<br>
0:12:51.960,0:12:53.820<br>
每一個 Gaussian 都有自己的 weight<br>
<br>
0:12:53.820,0:12:57.060<br>
這一邊有一把 Gaussian，每一個都有它自己的 weight<br>
<br>
0:12:57.060,0:12:58.260<br>
那接下來呢<br>
<br>
0:12:58.260,0:13:02.680<br>
你再根據每一個 Gaussian 的 weight<br>
<br>
0:13:02.680,0:13:06.100<br>
去決定你要從哪一個 Gaussian sample data<br>
<br>
0:13:06.100,0:13:09.600<br>
然後，再從你選擇的那個 Gaussian 裡面 sample data<br>
<br>
0:13:09.600,0:13:12.400<br>
如果你選擇 1 這個 Gaussian的話<br>
<br>
0:13:12.400,0:13:14.580<br>
那你就從這個地方sample data<br>
<br>
0:13:14.580,0:13:16.700<br>
如果選擇 2 的話，就從這個地方<br>
<br>
0:13:16.700,0:13:19.480<br>
3 就從這個地方、4 就從這個地方、5 就從這個地方<br>
<br>
0:13:19.480,0:13:20.600<br>
以此類推<br>
<br>
0:13:20.600,0:13:25.240<br>
所以，怎麼從 Gaussian mixture model <br>
sample 一個 data 呢？<br>
<br>
0:13:25.240,0:13:26.280<br>
你就這樣做<br>
<br>
0:13:26.280,0:13:30.500<br>
首先，你有一個 multinomial  的 distribution<br>
<br>
0:13:30.500,0:13:32.160<br>
你從這一個 multinomial distribution 裡面<br>
<br>
0:13:32.160,0:13:35.460<br>
決定你要去 sample 哪一個 Gaussian<br>
<br>
0:13:35.460,0:13:37.640<br>
今天 m 代表第幾個 Gaussian<br>
<br>
0:13:37.640,0:13:39.180<br>
它是一個 integer<br>
<br>
0:13:39.180,0:13:42.540<br>
決定好你要從哪一個 m sample Gaussian 以後<br>
<br>
0:13:42.540,0:13:45.220<br>
你要從哪一個 Gaussian sample data 以後<br>
<br>
0:13:45.220,0:13:46.520<br>
決定哪一個 Gaussian 以後<br>
<br>
0:13:46.520,0:13:49.480<br>
每一個 Gaussian 有自己的 mean<br>
<br>
0:13:49.480,0:13:55.100<br>
μ(上標 m)，有一個自己的 variance,  Σ(上標 m)<br>
<br>
0:13:55.100,0:13:57.300<br>
所以，你有了這個 m 以後<br>
<br>
0:13:57.300,0:13:59.560<br>
你就可以從，你就可以找到這個<br>
<br>
0:13:59.560,0:14:00.740<br>
mean 跟 variance<br>
<br>
0:14:00.740,0:14:02.160<br>
根據這個 mean 跟 variance<br>
<br>
0:14:02.160,0:14:04.720<br>
你就可以 sample 一個 x 出來<br>
<br>
0:14:04.720,0:14:07.140<br>
所以，今天整個 P(x) 寫成<br>
<br>
0:14:07.140,0:14:09.960<br>
summation over 所有的 Gaussian<br>
<br>
0:14:09.960,0:14:14.880<br>
那一個 Gaussian 的 weight, P(m) 再乘上<br>
<br>
0:14:14.880,0:14:16.860<br>
有了那一個 Gaussian以後<br>
<br>
0:14:16.860,0:14:21.720<br>
從那一個 Gaussian sample 出 x 的機率，P(x|m)<br>
<br>
0:14:21.720,0:14:25.980<br>
那在 Gaussian mixture model 裡面<br>
<br>
0:14:25.980,0:14:29.720<br>
有種種的問題，比如說你需要決定 mixture 的數目<br>
<br>
0:14:29.720,0:14:33.240<br>
但是，如果你知道 mixture的數目的話<br>
<br>
0:14:33.240,0:14:37.080<br>
接下來給你一些 data，x<br>
<br>
0:14:37.080,0:14:39.380<br>
你要 estimate 這一把 Gaussian<br>
<br>
0:14:39.380,0:14:41.160<br>
跟它的每一個 Gaussian 的 weight<br>
<br>
0:14:41.160,0:14:43.280<br>
跟它的 mean 跟 variance<br>
<br>
0:14:43.280,0:14:44.780<br>
其實是很容易的<br>
<br>
0:14:44.780,0:14:46.680<br>
你只要用 EM Algorithm 就好了<br>
<br>
0:14:46.680,0:14:48.460<br>
你不知道這個是甚麼沒有關係<br>
<br>
0:14:48.460,0:14:50.720<br>
反正這是很容易的事情<br>
<br>
0:14:54.180,0:14:56.280<br>
現在每一個 x<br>
<br>
0:14:56.280,0:14:59.760<br>
它都是從某一個 mixture 被 sample 出來的<br>
<br>
0:14:59.760,0:15:02.820<br>
這件事情其實就很像是<br>
<br>
0:15:02.820,0:15:05.360<br>
在做 classification 一樣<br>
<br>
0:15:05.360,0:15:07.240<br>
每一個我們所看到的 x<br>
<br>
0:15:07.240,0:15:10.380<br>
它都是來自於某一個分類<br>
<br>
0:15:10.380,0:15:12.780<br>
它都是來自於某一個 class<br>
<br>
0:15:12.780,0:15:14.760<br>
但是，我們之前有講過說<br>
<br>
0:15:14.760,0:15:17.140<br>
把 data 做 classification<br>
<br>
0:15:17.140,0:15:19.640<br>
做 clustering 其實是不夠的<br>
<br>
0:15:19.640,0:15:22.460<br>
更好的表示方式是用<br>
<br>
0:15:22.460,0:15:25.480<br>
Distributed 的 representation<br>
<br>
0:15:25.480,0:15:27.880<br>
也就是說，每一個 x<br>
<br>
0:15:27.880,0:15:29.740<br>
它並不是屬於某一個 class<br>
<br>
0:15:29.740,0:15:32.800<br>
某一個 cluster，而是它有一個 vector<br>
<br>
0:15:32.800,0:15:36.180<br>
來描述它的各個不同面相的 attribute<br>
<br>
0:15:36.180,0:15:38.440<br>
描述它各個不同面向的特質<br>
<br>
0:15:38.440,0:15:42.740<br>
所以 ，VAE 其實就是 Gaussian mixture model 的<br>
<br>
0:15:42.740,0:15:45.440<br>
distributive representation 的版本<br>
<br>
0:15:45.440,0:15:47.860<br>
怎麼說？首先呢<br>
<br>
0:15:47.860,0:15:51.800<br>
我們要 sample 一個 z<br>
<br>
0:15:51.800,0:15:56.320<br>
這個 z 是從一個 normal distribution sample 出來的<br>
<br>
0:15:56.320,0:15:57.420<br>
z 是一個 vector<br>
<br>
0:15:57.420,0:16:00.680<br>
它從一個 normal distribution 被 sample 出來<br>
<br>
0:16:00.680,0:16:02.660<br>
那這個 z 是一個 vector<br>
<br>
0:16:02.660,0:16:05.240<br>
這個 vector 的每一個 dimension<br>
<br>
0:16:05.240,0:16:08.020<br>
就代表了某種 attribute<br>
<br>
0:16:08.020,0:16:12.060<br>
代表你現在要 sample 的那個東西的某種特質<br>
<br>
0:16:12.060,0:16:13.700<br>
z 的每一個 dimension<br>
<br>
0:16:13.700,0:16:16.220<br>
就代表了它要 sample 的某種東西的特質<br>
<br>
0:16:16.220,0:16:18.400<br>
假設 z 它長這樣<br>
<br>
0:16:18.400,0:16:19.420<br>
假設 z 長這樣<br>
<br>
0:16:19.420,0:16:21.140<br>
它是一個 Gaussian distribution<br>
<br>
0:16:21.140,0:16:22.440<br>
現在我們在這個圖上呢<br>
<br>
0:16:22.440,0:16:24.520<br>
就假設它是一維的<br>
<br>
0:16:24.520,0:16:27.740<br>
但在實際上 z 可能是一個 10 維的、100 維的 vector<br>
<br>
0:16:27.740,0:16:29.800<br>
到底有幾維，是由你自己決定<br>
<br>
0:16:29.800,0:16:33.440<br>
假設現在 z 呢，就是一維的 Gaussian<br>
<br>
0:16:33.440,0:16:38.120<br>
接下來，你sample 出這個 z 以後<br>
<br>
0:16:38.120,0:16:42.340<br>
根據 z ，你可以決定 μ 跟 σ<br>
<br>
0:16:42.340,0:16:45.260<br>
你可以決定 Gaussian 的 mean 跟 variance<br>
<br>
0:16:45.260,0:16:48.000<br>
剛才在 Gaussian mixture model 裡面<br>
<br>
0:16:48.000,0:16:50.180<br>
你有 10 個 mixture，你就是 10 個 mean<br>
<br>
0:16:50.180,0:16:52.120<br>
跟 10 個 variance<br>
<br>
0:16:52.120,0:16:53.740<br>
但是，今天在這個地方<br>
<br>
0:16:53.740,0:16:56.740<br>
你的 z 有無窮多個可能<br>
<br>
0:16:56.740,0:16:58.800<br>
它是 continuous，不是 discrete<br>
<br>
0:16:58.800,0:17:01.100<br>
所以你的 z，有無窮多的可能<br>
<br>
0:17:01.100,0:17:05.060<br>
所以，你的 mean 跟 variance<br>
<br>
0:17:05.060,0:17:06.580<br>
也有無窮多的可能<br>
<br>
0:17:06.580,0:17:09.160<br>
那怎麼找到這個 mean 跟 variance 呢？<br>
<br>
0:17:09.160,0:17:11.680<br>
怎麼給一個 z，找 mean 跟 variance 呢？<br>
<br>
0:17:11.680,0:17:13.280<br>
你這一邊的做法就是<br>
<br>
0:17:13.280,0:17:19.780<br>
假設 mean 跟 variance 都來自一個 function<br>
<br>
0:17:19.780,0:17:21.020<br>
都來自一個 function<br>
<br>
0:17:21.020,0:17:23.760<br>
你把 z 代進產生 mean 的這個 function<br>
<br>
0:17:23.760,0:17:26.340<br>
它就給你 μ(z)<br>
<br>
0:17:26.340,0:17:30.180<br>
μ(z)代表說，現在，如果你的<br>
<br>
0:17:30.180,0:17:34.780<br>
你的 hidden 的東西<br>
<br>
0:17:34.780,0:17:36.820<br>
你的 attribute 是 z 的時候<br>
<br>
0:17:36.820,0:17:40.840<br>
你在這個 x 這個 space 上的 mean是多少<br>
<br>
0:17:40.840,0:17:46.620<br>
同理，σ(z) 代表說你的 variance 是多少<br>
<br>
0:17:46.620,0:17:48.900<br>
代表說，你現在如果從<br>
<br>
0:17:48.900,0:17:51.740<br>
這個 latent 的 space 裡面得到 z 的時候<br>
<br>
0:17:51.740,0:17:53.280<br>
你的 variance 是多少<br>
<br>
0:17:53.280,0:17:55.520<br>
所以，實際上<br>
<br>
0:17:57.020,0:18:00.740<br>
所以，實際上這個 P(x) 是怎麼產生的呢？<br>
<br>
0:18:00.740,0:18:05.360<br>
在 z 這個  space上面<br>
<br>
0:18:05.360,0:18:08.500<br>
每一個點都有可能被 sample到<br>
<br>
0:18:08.500,0:18:10.200<br>
只是在中間這邊呢<br>
<br>
0:18:10.200,0:18:12.200<br>
這個點被 sample 到的機率比較大<br>
<br>
0:18:12.200,0:18:15.660<br>
在 tail 的地方，點被 sample 到的機率比較小<br>
<br>
0:18:15.660,0:18:18.380<br>
當你 sample 出一個點以後<br>
<br>
0:18:18.380,0:18:21.020<br>
你在 z 的 space 上面 sample 出一個 point 以後<br>
<br>
0:18:21.020,0:18:24.880<br>
那一個 point 會對應到 一個 Gaussian<br>
<br>
0:18:24.880,0:18:28.160<br>
這一個點對應到這一個 Gaussian<br>
<br>
0:18:28.160,0:18:31.780<br>
這個點對應到這一個 Gaussian<br>
<br>
0:18:31.780,0:18:33.640<br>
這一個 點對應到這一個Gaussian，等等<br>
<br>
0:18:33.640,0:18:35.320<br>
每一個點都對應到一個 Gaussian<br>
<br>
0:18:35.320,0:18:38.520<br>
至於某一個點對應到什麼樣的 Gaussian<br>
<br>
0:18:38.520,0:18:40.140<br>
它的 mean 跟 variance是多少<br>
<br>
0:18:40.140,0:18:43.440<br>
是由某一個 function 所決定的<br>
<br>
0:18:43.440,0:18:45.780<br>
所以，當你用這一個概念<br>
<br>
0:18:45.780,0:18:48.840<br>
當你今天你的這個 Gaussian<br>
<br>
0:18:48.840,0:18:52.380<br>
是從一個 normal distribution 所產生的時候<br>
<br>
0:18:52.380,0:18:55.540<br>
現在你等於就是有無窮多的 Gaussian<br>
<br>
0:18:55.540,0:18:56.980<br>
原來 Gaussian mixture model 裡面<br>
<br>
0:18:56.980,0:18:59.140<br>
最多甚麼 512 的，那個太少<br>
<br>
0:18:59.140,0:19:01.040<br>
我們現在有無窮多個 Gaussian<br>
<br>
0:19:01.040,0:19:03.000<br>
那另外一個問題就是<br>
<br>
0:19:03.000,0:19:05.700<br>
我們怎麼知道每一個 z<br>
<br>
0:19:05.700,0:19:07.840<br>
應該對應到怎樣的 mean 跟 variance<br>
<br>
0:19:07.840,0:19:09.580<br>
這個 function 怎麼找呢？<br>
<br>
0:19:09.580,0:19:13.260<br>
我們知道說 neural network 就是一個 function<br>
<br>
0:19:13.260,0:19:15.520<br>
所以，你可以說我就是 traing 一個 neural network<br>
<br>
0:19:15.520,0:19:17.620<br>
這個 neural network input z<br>
<br>
0:19:17.620,0:19:21.000<br>
然後，它的 output  就是兩個 vector<br>
<br>
0:19:21.000,0:19:24.660<br>
第一個 vector 代表了 input 是 z 的時候<br>
<br>
0:19:24.660,0:19:26.080<br>
你 Gaussian 的 mean<br>
<br>
0:19:26.080,0:19:28.380<br>
這個 σ 代表了 variance<br>
<br>
0:19:28.380,0:19:31.780<br>
那 variance 時常說，它是一個 matrix<br>
<br>
0:19:31.780,0:19:34.680<br>
你可以說，你可以把 matrix 拉直當作它的 output<br>
<br>
0:19:34.680,0:19:37.540<br>
或者是你可以只 output diagonal 的地方<br>
<br>
0:19:37.540,0:19:40.680<br>
然後，假設 non-diagonal 的地方都是 0<br>
<br>
0:19:40.680,0:19:42.880<br>
這樣都是可以的<br>
<br>
0:19:42.880,0:19:45.880<br>
反正，我們有一個 neural network<br>
<br>
0:19:45.880,0:19:47.540<br>
它可以告訴我們說<br>
<br>
0:19:47.540,0:19:50.980<br>
在 z 這一個 space 上每一個點<br>
<br>
0:19:50.980,0:19:52.960<br>
它對應到 x space 的時候呢<br>
<br>
0:19:52.960,0:19:55.980<br>
你的這一個 distribution，mean 跟 variance 分別是多少<br>
<br>
0:19:55.980,0:20:00.680<br>
那現在這個 P(x) 的 distribution 會長什麼樣子呢？<br>
<br>
0:20:00.680,0:20:02.460<br>
這個 P(x) 的 distribution 呢<br>
<br>
0:20:02.460,0:20:05.440<br>
就會變成是 P(z)的機率<br>
<br>
0:20:05.440,0:20:11.420<br>
跟我們知道 z 的時候，x 的機率<br>
<br>
0:20:11.420,0:20:15.560<br>
再對所有可能的 z 做積分<br>
<br>
0:20:15.560,0:20:17.900<br>
這邊不能夠是相加<br>
<br>
0:20:17.900,0:20:20.340<br>
不能夠是 summation，必須要是積分<br>
<br>
0:20:20.340,0:20:22.360<br>
因為這個 z 是 continuous 的<br>
<br>
0:20:22.360,0:20:25.300<br>
那有人可能會有一個困惑就是<br>
<br>
0:20:25.300,0:20:29.660<br>
為甚麼這邊一定是這個 Gaussian 呢<br>
<br>
0:20:29.660,0:20:31.600<br>
為什麼這邊一定是 Gaussian 呢？<br>
<br>
0:20:31.600,0:20:33.740<br>
你可以不是 Gaussian 這個樣子<br>
<br>
0:20:33.740,0:20:35.440<br>
它可以是一種花的樣子<br>
<br>
0:20:35.440,0:20:39.160<br>
在文獻上確實有人會把它弄成一朵花的樣子<br>
<br>
0:20:39.160,0:20:42.900<br>
它可以是任何的東西<br>
<br>
0:20:42.900,0:20:45.060<br>
這個是你自己決定的<br>
<br>
0:20:45.060,0:20:48.380<br>
當然這個 Gaussian，說起來是合理的<br>
<br>
0:20:48.380,0:20:52.860<br>
你就假設說，每一個 attribute 它的分佈就是 Gaussian<br>
<br>
0:20:52.860,0:20:54.680<br>
比較極端的 case 總是比較少的嘛<br>
<br>
0:20:54.680,0:20:57.020<br>
比較沒有特色的東西總是比較多的嘛<br>
<br>
0:20:57.020,0:21:01.060<br>
然後，attribute 跟 attribute之間是 independent 的<br>
<br>
0:21:01.060,0:21:03.480<br>
這樣子的假設其實也是合理的<br>
<br>
0:21:03.480,0:21:05.940<br>
不過，這個形狀是你自己假設的<br>
<br>
0:21:05.940,0:21:07.620<br>
你可以假設是任何的形狀<br>
<br>
0:21:07.620,0:21:08.880<br>
你可以假設任何形狀<br>
<br>
0:21:08.880,0:21:10.920<br>
那現在這個<br>
<br>
0:21:10.920,0:21:12.760<br>
但是，你不用擔心說<br>
<br>
0:21:12.760,0:21:17.500<br>
你如果假設 Gaussian 會不會對 P(x) 帶來很大的限制<br>
<br>
0:21:17.500,0:21:21.260<br>
會不會說，如果假設 z 是 Gaussian distribution 的話<br>
<br>
0:21:21.260,0:21:23.720<br>
有一些 P(x)，你就沒有辦法描述<br>
<br>
0:21:23.720,0:21:25.540<br>
其實，你不用太擔心這個問題<br>
<br>
0:21:25.540,0:21:27.620<br>
你不要忘了這個 NN 是非常 powerful 的<br>
<br>
0:21:27.620,0:21:30.460<br>
NN 可以 represent 任何 function<br>
<br>
0:21:30.460,0:21:33.020<br>
只要 neuron 夠多，NN 可以 represent 任何 function<br>
<br>
0:21:33.020,0:21:36.620<br>
所以，今天從 z 到 x 中間的 mapping 可以是很複雜<br>
<br>
0:21:36.620,0:21:39.640<br>
所以，就算你的 z 是一個 normal distribution<br>
<br>
0:21:39.640,0:21:41.080<br>
最後這個 P(x) 呢<br>
<br>
0:21:41.080,0:21:44.280<br>
它也可以是一個非常複雜的 distribution<br>
<br>
0:21:44.280,0:21:48.280<br>
再來呢，所以我們現在的式子是這樣子的<br>
<br>
0:21:48.280,0:21:53.740<br>
我們知道 P(x) 可以寫成對 z 的積分<br>
<br>
0:21:53.740,0:21:56.860<br>
然後乘上P(z)，還有乘上 P(x|z)<br>
<br>
0:21:56.860,0:21:59.200<br>
P(z) 是一個 normal distribution<br>
<br>
0:21:59.200,0:22:01.840<br>
這個 x given z 呢<br>
<br>
0:22:01.840,0:22:05.620<br>
我們先知道 z 是什麼，然後我們就可以決定 x<br>
<br>
0:22:05.620,0:22:09.800<br>
它是從什麼樣子的 mean 跟 variance 的 Gaussian裡面<br>
<br>
0:22:09.800,0:22:10.980<br>
被 sample 出來的<br>
<br>
0:22:10.980,0:22:15.960<br>
但是這一個 function 有 z<br>
<br>
0:22:15.960,0:22:18.280<br>
它有什麼樣的 mean 跟 variance<br>
<br>
0:22:18.280,0:22:20.340<br>
它們中間的關係是不知道的<br>
<br>
0:22:20.340,0:22:23.420<br>
是等著要被找出來的<br>
<br>
0:22:23.420,0:22:25.440<br>
但是，問題是要怎麼找呢？<br>
<br>
0:22:25.440,0:22:29.980<br>
它的 criterion 就是要 maximize 我們的  likelihood<br>
<br>
0:22:29.980,0:22:33.640<br>
我們現在手上已經有一筆 data x<br>
<br>
0:22:33.640,0:22:35.900<br>
那你希望找到一組<br>
<br>
0:22:35.900,0:22:38.620<br>
找到一個 μ 的 function<br>
<br>
0:22:38.620,0:22:40.420<br>
找到一個 σ 的 function<br>
<br>
0:22:40.420,0:22:42.820<br>
它可以讓這個<br>
<br>
0:22:42.820,0:22:45.280<br>
你現在已經觀察到的 data<br>
<br>
0:22:45.280,0:22:47.400<br>
你現在手上已經有的 image<br>
<br>
0:22:47.400,0:22:50.340<br>
它的每一個 x 代表了一個 image<br>
<br>
0:22:50.340,0:22:51.960<br>
你現在手上已經有的 image<br>
<br>
0:22:51.960,0:22:54.360<br>
它的 P(x) 取 log 以後<br>
<br>
0:22:54.360,0:22:58.060<br>
它的值相加以後是被 maximize 的<br>
<br>
0:22:58.060,0:23:03.100<br>
這個就是 maximize 我們已經看到 image 的  likelihood<br>
<br>
0:23:03.100,0:23:07.840<br>
這邊只是複習一下這個 z 怎麼產生這個 μ 跟 σ 呢<br>
<br>
0:23:07.840,0:23:09.140<br>
它是透過了一個 NN<br>
<br>
0:23:09.140,0:23:12.800<br>
所以，我們要做的事情就是，調這個 NN 裡面的參數<br>
<br>
0:23:12.800,0:23:15.500<br>
調這個 NN 裡面每一個 neuron 的 weight 跟 bias<br>
<br>
0:23:15.500,0:23:19.840<br>
使得這個 likelihood 可以被 maximize<br>
<br>
0:23:19.840,0:23:25.440<br>
但是在這邊，等一下會引入另外一個 distribution<br>
<br>
0:23:25.440,0:23:28.580<br>
它叫做 q(z|x)<br>
<br>
0:23:28.580,0:23:30.900<br>
它跟這個 NN 是相反的<br>
<br>
0:23:30.900,0:23:35.020<br>
它是 given z，決定這個 x 的 mean 跟 variance<br>
<br>
0:23:35.020,0:23:39.280<br>
這邊是 given x，決定在 z 這個 space 上面的<br>
<br>
0:23:39.280,0:23:41.380<br>
mean  跟 variance<br>
<br>
0:23:41.380,0:23:43.460<br>
也就是說，我們有另外一個 NN<br>
<br>
0:23:43.460,0:23:46.620<br>
這邊寫成 NN'，你 input x 以後<br>
<br>
0:23:46.620,0:23:49.780<br>
它會告訴你說，對應的 z 的 mean<br>
<br>
0:23:49.780,0:23:52.880<br>
跟對應的 z  的 variance<br>
<br>
0:23:52.880,0:23:54.460<br>
你給它 x 以後<br>
<br>
0:23:54.460,0:23:57.960<br>
它會決定這個 z 要從什樣的 mean 跟甚麼樣的 variance<br>
<br>
0:23:57.960,0:23:59.600<br>
被 sample 出來<br>
<br>
0:23:59.600,0:24:04.040<br>
上面這個 NN，其實就是 VAE 裡面的 decoder<br>
<br>
0:24:04.040,0:24:08.800<br>
下面這個 NN，其實就是 VAE 裡面的 encoder<br>
<br>
0:24:08.800,0:24:13.100<br>
那我們現在先不要管 NN 這一件事情<br>
<br>
0:24:13.100,0:24:16.080<br>
我們現在就先只看 這個式子就好<br>
<br>
0:24:16.080,0:24:20.360<br>
P(x|z) 我們就先不要在意它是不是從 NN 產生的<br>
<br>
0:24:20.360,0:24:24.360<br>
反正這個就是一個機率，我們要去把它找出來<br>
<br>
0:24:24.360,0:24:26.160<br>
怎麼找呢？<br>
<br>
0:24:26.160,0:24:28.060<br>
這個 log P(x)<br>
<br>
0:24:28.060,0:24:31.300<br>
它可以寫成積分<br>
<br>
0:24:31.300,0:24:40.020<br>
over z 的積分，然後 q(z|x) * logP(x) dz這樣<br>
<br>
0:24:40.020,0:24:42.520<br>
我們想說為什麼是這樣呢？<br>
<br>
0:24:42.520,0:24:47.380<br>
因為 q(z|x)，它是一個 distribution<br>
<br>
0:24:47.380,0:24:51.120<br>
這個式子對任何 distribution 都成立<br>
<br>
0:24:51.120,0:24:55.200<br>
我們假設 q(z|x) 是一個從路邊撿來的 distribution<br>
<br>
0:24:55.200,0:24:57.160<br>
它可以是任何一個 distribution<br>
<br>
0:24:57.160,0:25:01.360<br>
那任何一個 distribution，你都可以寫成<br>
<br>
0:25:01.360,0:25:03.860<br>
寫成這個樣子<br>
<br>
0:25:03.860,0:25:08.080<br>
對不對？因為這個積分跟 P(x) 是無關的<br>
<br>
0:25:08.080,0:25:10.000<br>
所以，可以把 P(x)這一項提出 來<br>
<br>
0:25:10.000,0:25:12.960<br>
然後，積分的部分就會變成 1<br>
<br>
0:25:12.960,0:25:15.140<br>
所以，左式就等於右式<br>
<br>
0:25:15.140,0:25:17.240<br>
這個沒有什麼好講的，這式子什麼都沒有做<br>
<br>
0:25:17.240,0:25:20.800<br>
再來呢，也是一個其實什麼都沒有做的式子<br>
<br>
0:25:20.800,0:25:26.640<br>
這個 P(x) 可以寫成 P(z,x) / P(z|x)<br>
<br>
0:25:26.640,0:25:30.320<br>
那你把 P(z|x) 展開 一下就會發現說<br>
<br>
0:25:30.320,0:25:33.180<br>
這一項等於這一項，這也沒什麼好講的<br>
<br>
0:25:33.180,0:25:37.080<br>
那接下來又是一個甚麼都沒有做的式子<br>
<br>
0:25:37.080,0:25:45.000<br>
就是，本來我們把 P(z, x) / q(z|x)<br>
<br>
0:25:45.000,0:25:50.940<br>
然後，再把 q(z|x) / P(z|x)<br>
<br>
0:25:51.100,0:25:52.920<br>
左式也等於右式<br>
<br>
0:25:52.940,0:25:55.760<br>
因為這個 q 其實是可以消掉的<br>
<br>
0:25:55.760,0:25:59.000<br>
這個小學生應該就知道<br>
<br>
0:25:59.000,0:26:01.600<br>
這個式子也等於是什麼事都沒有做這樣子<br>
<br>
0:26:01.600,0:26:06.040<br>
接下來，這個東西被放在 log 裡面<br>
<br>
0:26:06.040,0:26:11.380<br>
我們知道 log 相乘等於拆開後相加<br>
<br>
0:26:11.380,0:26:15.540<br>
所以，log 這一項乘這一項<br>
<br>
0:26:15.540,0:26:19.200<br>
等於 log 這一項加 log 這一項<br>
<br>
0:26:19.200,0:26:24.480<br>
那接下來呢，觀察一下這兩項到底代表什麼事情<br>
<br>
0:26:24.480,0:26:29.260<br>
右邊這一項，它代表了一個 KL divergence<br>
<br>
0:26:29.260,0:26:36.100<br>
這個 P(z|x) 是一個 distribution<br>
<br>
0:26:36.100,0:26:41.320<br>
q(z|x) 是另外一個 distribution<br>
<br>
0:26:41.320,0:26:45.380<br>
現在 x 是給定的，所以你有兩個 distribution<br>
<br>
0:26:45.380,0:26:48.140<br>
當有兩個 distribution 的時候<br>
<br>
0:26:48.140,0:26:50.900<br>
你可以算一個東西，叫做 KL divergence<br>
<br>
0:26:50.900,0:26:56.720<br>
KL divergence 代表的是這兩個 distribution 相近的程度<br>
<br>
0:26:56.720,0:26:58.520<br>
它們兩個相近的程度<br>
<br>
0:26:58.520,0:27:00.960<br>
如果 KL divergence 它越大<br>
<br>
0:27:00.960,0:27:03.140<br>
代表這兩個 distribution 越不像<br>
<br>
0:27:03.140,0:27:07.000<br>
這兩個 distribution 一模一樣的時候，<br>
KL divergence 會是 0<br>
<br>
0:27:07.000,0:27:10.140<br>
所以，KL divergence 它是一個距離的概念<br>
<br>
0:27:10.140,0:27:13.000<br>
它衡量了兩個 distribution 之間的距離<br>
<br>
0:27:13.000,0:27:17.540<br>
這一項就是 KL divergence 的式子<br>
<br>
0:27:17.540,0:27:19.220<br>
這一項是一個距離<br>
<br>
0:27:19.220,0:27:21.200<br>
所以，它一定是大於等於 0 的<br>
<br>
0:27:21.200,0:27:23.420<br>
最小也是 0 而已<br>
<br>
0:27:23.420,0:27:26.280<br>
至於這個為什麼 KL divergence的這個<br>
<br>
0:27:26.280,0:27:28.060<br>
反正你就記起來就是了<br>
<br>
0:27:28.060,0:27:33.260<br>
那因為這一項一定是大於等於 0 的<br>
<br>
0:27:33.260,0:27:37.680<br>
所以，這一項會是<br>
<br>
0:27:37.680,0:27:40.180<br>
L 的 lower bound<br>
<br>
0:27:40.180,0:27:42.780<br>
L 一定會大於等於這一項<br>
<br>
0:27:42.780,0:27:47.960<br>
這一項你可以再拆一下，P(z, x) = P(x|z) * P(z)<br>
<br>
0:27:47.960,0:27:51.020<br>
所以，L 一定會大於這一項<br>
<br>
0:27:51.020,0:27:53.700<br>
那這一項就是一個 lower bound<br>
<br>
0:27:53.700,0:27:55.540<br>
我們叫它 L(下標 b)<br>
<br>
0:27:55.540,0:27:58.880<br>
那現在我們知道事情是這樣<br>
<br>
0:27:58.880,0:28:05.000<br>
這個 log Probability，就是我們要 maximize  的對象<br>
<br>
0:28:05.000,0:28:08.620<br>
它是由這兩項加起來的結果<br>
<br>
0:28:08.620,0:28:11.900<br>
Lb 它長成這個樣子<br>
<br>
0:28:11.900,0:28:13.500<br>
它長成這個樣子<br>
<br>
0:28:13.500,0:28:14.940<br>
在這個式子裡面<br>
<br>
0:28:14.940,0:28:18.660<br>
P(z)是 normal distribution，是已知的<br>
<br>
0:28:18.660,0:28:24.380<br>
我們不知道的是 P(x|z)  跟 q(z|x)<br>
<br>
0:28:24.380,0:28:26.420<br>
那我們本來要做的事情<br>
<br>
0:28:26.420,0:28:34.360<br>
我們本來要做的事情是要找 P(x|z)<br>
<br>
0:28:34.360,0:28:40.460<br>
讓這個 P，讓這個 likelihood 越大越好<br>
<br>
0:28:40.460,0:28:42.420<br>
現在我們要做的事情<br>
<br>
0:28:42.420,0:28:48.560<br>
變成找 P(x|z) 和 q(z|x)<br>
<br>
0:28:48.560,0:28:51.820<br>
讓 Lb 越大越好<br>
<br>
0:28:51.820,0:28:54.780<br>
我們本來只要找這一項，本來只要找這一項<br>
<br>
0:28:54.780,0:28:56.680<br>
現在順便也要找這一項<br>
<br>
0:28:56.680,0:28:58.660<br>
把這兩項合起來<br>
<br>
0:28:58.660,0:29:00.480<br>
我們希望同時找這兩項<br>
<br>
0:29:00.480,0:29:04.460<br>
然後去 maximize 這個 Lb<br>
<br>
0:29:04.460,0:29:08.960<br>
突然多找一項是要做什麼呢？<br>
<br>
0:29:08.960,0:29:11.080<br>
如果我們現在只找這一項的話<br>
<br>
0:29:11.080,0:29:14.200<br>
如果假設我們現在只找這一項<br>
<br>
0:29:14.200,0:29:16.100<br>
然後去 maximize Lb 的話<br>
<br>
0:29:16.100,0:29:19.660<br>
你如果 maximize 這一項<br>
<br>
0:29:19.660,0:29:21.480<br>
你如果調整這一項<br>
<br>
0:29:21.480,0:29:23.540<br>
你如果找這一項 P(x|z)<br>
<br>
0:29:23.540,0:29:26.440<br>
讓 Lb 被 maximize 的話<br>
<br>
0:29:26.440,0:29:28.800<br>
那因為你要找的這一個 log<br>
<br>
0:29:28.800,0:29:31.460<br>
你要找的這個 likelihood，它是 Lb 的 upper bound<br>
<br>
0:29:31.460,0:29:34.940<br>
所以，你增加 Lb 的時候<br>
<br>
0:29:34.940,0:29:38.480<br>
你有可能會增加你的 likelihood<br>
<br>
0:29:38.480,0:29:41.100<br>
但是，你不知道你的這個 likelihood<br>
<br>
0:29:41.100,0:29:43.760<br>
跟你的 lower bound 之間到底有什麼樣的距離<br>
<br>
0:29:43.760,0:29:46.200<br>
你想像你希望能做到的事情是<br>
<br>
0:29:46.200,0:29:49.540<br>
當你的 lower bound 上升的時候<br>
<br>
0:29:49.540,0:29:52.340<br>
當你的 lower bound 上升的時候<br>
<br>
0:29:52.340,0:29:56.240<br>
你的 likelihood 是會比 lower  bound 高，<br>
然後你的 likelihood 也跟著上升<br>
<br>
0:29:56.240,0:29:58.740<br>
但是，你有可能會遇到一個比較糟糕的狀況是<br>
<br>
0:29:58.740,0:30:01.080<br>
你的 lower bound 上升的時候<br>
<br>
0:30:01.080,0:30:02.520<br>
likelihood 反而下降<br>
<br>
0:30:02.520,0:30:04.920<br>
雖然，它還是 lower bound，它還是比 lower bound 大<br>
<br>
0:30:04.920,0:30:07.620<br>
但是，它有可能下降<br>
因為根本不知道它們之間的差距是多少<br>
<br>
0:30:07.620,0:30:10.400<br>
所以，引入 q 這一項呢<br>
<br>
0:30:10.400,0:30:13.800<br>
其實可以解決剛才說的那一個問題<br>
<br>
0:30:13.800,0:30:17.620<br>
為什麼呢？因為你看這個是 likelihood<br>
<br>
0:30:17.620,0:30:21.420<br>
likelihood = Lb + KL divergence<br>
<br>
0:30:21.420,0:30:29.240<br>
如果你今天去這個調這個 q(z|x)，調 q 這一項<br>
<br>
0:30:29.240,0:30:32.540<br>
去 maximize Lb 的話，會發生什麼事呢？<br>
<br>
0:30:32.540,0:30:35.040<br>
你會發現說，首先 q 這一項<br>
<br>
0:30:35.040,0:30:39.160<br>
跟 log P(x) 是一點關係都沒有的<br>
<br>
0:30:39.160,0:30:43.580<br>
對不對？log P(x) 只跟 P(x|z) 有關<br>
<br>
0:30:43.580,0:30:47.640<br>
這個 q 代什麼東西，這個值都是不變的<br>
<br>
0:30:47.640,0:30:50.200<br>
所以，這個值都是不變的<br>
<br>
0:30:50.200,0:30:52.700<br>
藍色這一條長度都是一樣的<br>
<br>
0:30:52.700,0:30:59.200<br>
但是，我們現在卻去 maximize Lb<br>
<br>
0:30:59.200,0:31:04.400<br>
maximize Lb 代表說你 minimize 了這個 KL divergence<br>
<br>
0:31:04.400,0:31:06.380<br>
也就是說你會讓<br>
<br>
0:31:06.380,0:31:10.120<br>
你的 lower bound 跟你的這個 likelihood<br>
<br>
0:31:10.120,0:31:11.900<br>
越來越接近<br>
<br>
0:31:11.900,0:31:16.840<br>
如果你 maximize q 這一項的話<br>
<br>
0:31:16.840,0:31:21.800<br>
所以，今天假如你固定住這個<br>
<br>
0:31:21.800,0:31:28.360<br>
假如你固定住這個 P，假如你固定住 P(x|z) 這一項<br>
<br>
0:31:28.360,0:31:31.740<br>
然後一直去調 q(z|x) 這一項的話<br>
<br>
0:31:31.740,0:31:34.400<br>
你會讓這個 Lb 一直上升，一直上升，一直上升<br>
<br>
0:31:34.400,0:31:37.140<br>
最後這一個 KL divergence 會完全不見<br>
<br>
0:31:37.140,0:31:39.700<br>
假如你最後可以找到一個 q<br>
<br>
0:31:39.700,0:31:44.140<br>
它跟這個 p(z|x) 正好完全 distribution 一模一樣的話<br>
<br>
0:31:44.140,0:31:46.600<br>
你就會發現說你的 likelihood 就會跟<br>
<br>
0:31:46.600,0:31:49.980<br>
lower bound 完全停在一起<br>
<br>
0:31:49.980,0:31:53.060<br>
它們就完全是一樣大<br>
<br>
0:31:53.060,0:31:56.380<br>
這個時候呢，如果你再把 lower bound 上升的話<br>
<br>
0:31:56.380,0:32:00.140<br>
因為你的 likelihood 一定要比 lower bond 大<br>
<br>
0:32:00.140,0:32:01.920<br>
所以這個時候你的 likelihood 呢<br>
<br>
0:32:01.920,0:32:03.580<br>
你就可以確定它一定會上升<br>
<br>
0:32:03.580,0:32:07.700<br>
所以，這個就是引入 q 這一項它有趣的地方<br>
<br>
0:32:07.700,0:32:09.620<br>
今天我會得到一個副產物<br>
<br>
0:32:09.620,0:32:12.760<br>
當你在 maximize q 這一項的時候<br>
<br>
0:32:12.760,0:32:15.420<br>
你會讓這個 KL divergence 越來越小<br>
<br>
0:32:15.420,0:32:20.720<br>
意謂這說，你就是讓這個 q 跟 P(z|x)<br>
<br>
0:32:20.720,0:32:23.120<br>
注意一下，這兩項是不一樣的<br>
<br>
0:32:23.120,0:32:24.500<br>
這個方向是不一樣的<br>
<br>
0:32:24.500,0:32:31.620<br>
你會讓這個 q(z|x) 跟 P(z|x) 越來越接近<br>
<br>
0:32:31.620,0:32:33.860<br>
所以我們接下要做的事情呢<br>
<br>
0:32:33.860,0:32:37.620<br>
就是找這一個跟這一個<br>
<br>
0:32:37.620,0:32:39.960<br>
然後可以讓 Lb 越大越好<br>
<br>
0:32:39.960,0:32:41.380<br>
讓 Lb 越大越好<br>
<br>
0:32:41.380,0:32:44.660<br>
就等同於我們可以讓 likelihood 越來越大<br>
<br>
0:32:44.660,0:32:47.060<br>
而且你順便會找到<br>
<br>
0:32:47.060,0:32:51.640<br>
這個 q 可以去 approximation of p(z|x)<br>
<br>
0:32:51.640,0:32:54.200<br>
那這一項 Lb 它長什麼樣子呢<br>
<br>
0:32:54.200,0:32:58.240<br>
這一項 Lb 我們剛才講過它就是長這個樣子<br>
<br>
0:32:58.240,0:33:02.000<br>
然後 log 裡面相乘，可以把它拆開<br>
<br>
0:33:02.000,0:33:04.360<br>
可以把它拆開<br>
<br>
0:33:04.360,0:33:09.180<br>
我們把 P (z) 跟 q(z|x) 放在一邊<br>
<br>
0:33:09.180,0:33:12.860<br>
把這一項放在另外一邊<br>
<br>
0:33:12.860,0:33:14.680<br>
那如果你觀察一下的話<br>
<br>
0:33:14.680,0:33:16.700<br>
會發現 P(z) 是一個 distribution<br>
<br>
0:33:16.700,0:33:18.820<br>
q(z|x) 也是一個 distribution<br>
<br>
0:33:18.820,0:33:22.880<br>
所以，這一項是一個 KL divergence<br>
<br>
0:33:22.880,0:33:29.820<br>
這一項是 P(z) 跟 q(z|x) 的 KL divergence<br>
<br>
0:33:29.820,0:33:33.700<br>
那如果複習一下，這個 q 是什麼呢<br>
<br>
0:33:33.700,0:33:34.980<br>
q 是一個 neural network<br>
<br>
0:33:34.980,0:33:38.600<br>
q是一個 neural network<br>
<br>
0:33:38.600,0:33:41.560<br>
當你給 x 的時候，它會告訴你說<br>
<br>
0:33:41.560,0:33:44.480<br>
q(z|x) 它是從什麼樣的<br>
<br>
0:33:44.480,0:33:47.960<br>
mean 跟 variance 的 Gaussian 裡面 sample 出來的<br>
<br>
0:33:47.960,0:33:51.960<br>
所以，我們現在如果你要<br>
<br>
0:33:51.960,0:33:58.960<br>
minimize 這個 P(z) 跟 q(z|x) 的 KL divergence 的話呢<br>
<br>
0:33:58.960,0:34:03.560<br>
你就是去調這個 output、這個 output<br>
<br>
0:34:03.560,0:34:08.440<br>
你去調你的這個 q 對應的那一個 neural network<br>
<br>
0:34:08.440,0:34:10.700<br>
你去調你的那個 q 對應的那一個 neural network<br>
<br>
0:34:10.700,0:34:14.340<br>
讓它產生的 distribution 可以跟這個<br>
<br>
0:34:14.340,0:34:16.960<br>
一個 normal distribution 越接近越好<br>
<br>
0:34:16.960,0:34:19.980<br>
這一件事情的這個推導呢<br>
<br>
0:34:19.980,0:34:24.740<br>
我們就把他放在，你就參照 VAE 原始的 paper<br>
<br>
0:34:24.740,0:34:26.920<br>
那 minimize 這一項<br>
<br>
0:34:26.920,0:34:30.700<br>
其實就是我們剛才說的這一項<br>
<br>
0:34:30.700,0:34:33.440<br>
剛才說的在 reconstruction error 外<br>
<br>
0:34:33.440,0:34:36.900<br>
另外再加的那一個，看起來像是 regularization 的式子<br>
<br>
0:34:36.900,0:34:40.220<br>
它要做的事情就是 minimize 這個 KL divergence<br>
<br>
0:34:40.220,0:34:41.980<br>
它要做的事情就是希望說<br>
<br>
0:34:41.980,0:34:44.580<br>
這一個 q(z|x) 的 output<br>
<br>
0:34:44.580,0:34:47.320<br>
跟 normal distribution 是接近的<br>
<br>
0:34:47.320,0:34:49.580<br>
那我們還有另外一項<br>
<br>
0:34:49.580,0:34:51.040<br>
另外一項是這樣子<br>
<br>
0:34:51.040,0:34:55.160<br>
另外一項是要這個積分<br>
<br>
0:34:55.160,0:35:04.240<br>
over q(z|x) * log[P(x|z)] 對 z 做積分<br>
<br>
0:35:04.240,0:35:07.860<br>
這一項的意思就是<br>
<br>
0:35:07.860,0:35:11.180<br>
你可以想像，我們有一個 log P(x|z)<br>
<br>
0:35:11.180,0:35:19.740<br>
然後，它用 q(z|x) 來做這個 weighted sum<br>
<br>
0:35:19.740,0:35:22.840<br>
所以，你可以把它寫成<br>
<br>
0:35:22.840,0:35:29.920<br>
[log P(x|z)] 根據 q(z|x) 的這個期望值<br>
<br>
0:35:29.920,0:35:31.460<br>
根據它的期望值<br>
<br>
0:35:31.460,0:35:34.260<br>
所以這一邊這個式子的意思呢<br>
<br>
0:35:34.280,0:35:36.480<br>
這一邊這個式子的意思就好像是說<br>
<br>
0:35:36.480,0:35:40.980<br>
我們從 q(z|x)  去 sample data<br>
<br>
0:35:40.980,0:35:42.620<br>
給我們一個 x 的時候<br>
<br>
0:35:42.620,0:35:47.680<br>
我們去計算，我們去根據這個 q(z|x)，這個機率分佈<br>
<br>
0:35:47.680,0:35:49.180<br>
去 sample 一個 data<br>
<br>
0:35:49.180,0:35:53.960<br>
然後，要讓 log P(x|z) 的機率越大越好<br>
<br>
0:35:53.960,0:35:57.960<br>
那這一件事情其實就 Auto-encoder 在做的事情<br>
<br>
0:35:57.960,0:35:59.320<br>
什麼意思呢？<br>
<br>
0:35:59.320,0:36:03.520<br>
怎麼從 q(z|x) 去 sample 一個 data 呢？<br>
<br>
0:36:03.520,0:36:07.220<br>
你就把 x 丟到 neural network 裡面去<br>
<br>
0:36:07.220,0:36:11.420<br>
它產生一個 mean 跟一個 variance<br>
<br>
0:36:11.420,0:36:13.860<br>
根據這個 mean 跟 variance<br>
<br>
0:36:13.860,0:36:16.740<br>
你就可以 sample 出一個 z<br>
<br>
0:36:16.740,0:36:19.460<br>
接下來，我們要做的事情<br>
<br>
0:36:19.460,0:36:20.660<br>
你已經做這一項了<br>
<br>
0:36:20.660,0:36:22.420<br>
這一邊就是這一項<br>
<br>
0:36:22.420,0:36:25.000<br>
你已經根據現在的 x sample 出 一個 z<br>
<br>
0:36:25.000,0:36:29.060<br>
接下來，你要 maximize 這一個 z<br>
<br>
0:36:29.060,0:36:32.080<br>
產生這個 x 的機率<br>
<br>
0:36:32.080,0:36:35.100<br>
那這個 z 產生這個 x 的 機率是甚麼呢<br>
<br>
0:36:35.100,0:36:36.760<br>
這個 z 產生這個 x 的機率<br>
<br>
0:36:36.760,0:36:39.560<br>
是把這個 z 丟到另外一個 neural network 裡面去<br>
<br>
0:36:39.560,0:36:43.460<br>
它產生一個 mean 跟 variance<br>
<br>
0:36:43.460,0:36:48.580<br>
要怎麼讓這個機率越大越好呢？<br>
<br>
0:36:48.580,0:36:50.940<br>
要怎麼讓這個 NN output<br>
<br>
0:36:50.940,0:36:54.480<br>
所代表 distribution 產生 x 的 機率越大越好呢？<br>
<br>
0:36:54.480,0:36:58.200<br>
假設我們無視 variance 這一件事情的話<br>
<br>
0:36:58.200,0:37:00.080<br>
後來在一般實作裡面<br>
<br>
0:37:00.080,0:37:02.660<br>
你可能不會把 variance  這一件事情考慮進去<br>
<br>
0:37:02.660,0:37:05.040<br>
你只考慮 mean 這一項的話<br>
<br>
0:37:05.040,0:37:06.600<br>
那你要做的事情就是<br>
<br>
0:37:06.600,0:37:08.100<br>
讓這個 mean 呢<br>
<br>
0:37:08.100,0:37:11.960<br>
讓你的這個 mean 跟你的 x 越接近越好<br>
<br>
0:37:11.960,0:37:13.220<br>
你現在是一個 normal<br>
<br>
0:37:13.220,0:37:15.800<br>
你現在是一個 Gaussian distribution<br>
<br>
0:37:15.800,0:37:18.940<br>
那 Gaussian distribution 在 mean 的地方機率是最高的<br>
<br>
0:37:18.940,0:37:20.980<br>
所以，如果你讓這個 NN<br>
<br>
0:37:20.980,0:37:25.020<br>
output 的這個 mean 正好等於你現在這個 data x 的話<br>
<br>
0:37:25.020,0:37:29.420<br>
這一項 log P(x|z) 它的值是最大的<br>
<br>
0:37:29.420,0:37:31.980<br>
所以，現在這整個 case 就變成說<br>
<br>
0:37:31.980,0:37:36.020<br>
input 一個 x，然後，產生兩個 vector<br>
<br>
0:37:36.020,0:37:39.100<br>
然後 sample 一下產生一個 z，再根據這個 z<br>
<br>
0:37:39.100,0:37:40.760<br>
你要產生另外一個 vector<br>
<br>
0:37:40.760,0:37:42.840<br>
這個 vector 要跟原來的 x 越接近越好<br>
<br>
0:37:42.840,0:37:44.980<br>
這件事情其實就是<br>
<br>
0:37:44.980,0:37:47.920<br>
就是 Auto-encoder 在做的事情<br>
<br>
0:37:47.920,0:37:51.020<br>
你要讓你的 input 跟 output 越接近越好<br>
<br>
0:37:51.020,0:37:53.140<br>
它就是 Auto-encoder 在做的事情<br>
<br>
0:37:53.140,0:37:55.180<br>
所以這兩項合起來<br>
<br>
0:37:55.180,0:38:00.080<br>
就是剛才我們前面看到的 VAE 的 loss function<br>
<br>
0:38:00.080,0:38:04.220<br>
如果你聽不懂的話也沒有關係<br>
<br>
0:38:04.220,0:38:08.900<br>
前面有提供了比較  intuitive 的想法<br>
<br>
0:38:08.900,0:38:14.040<br>
那其實 VAE 有另外一個是叫做 conditional 的 VAE<br>
<br>
0:38:14.060,0:38:17.120<br>
conditional VAE 這邊我們就簡單講一下概念就好<br>
<br>
0:38:17.120,0:38:19.360<br>
conditional VAE 它可以做的事情是說<br>
<br>
0:38:19.360,0:38:23.960<br>
比如說，如果你現在讓 VAE 可以產生手寫的數字<br>
<br>
0:38:23.960,0:38:26.080<br>
讓 VAE 可以產生手寫的數字<br>
<br>
0:38:26.080,0:38:29.220<br>
它就是看一個，給它一個 digit<br>
<br>
0:38:29.220,0:38:33.500<br>
然後，它把這個 digit 的特性抽出來<br>
<br>
0:38:33.500,0:38:35.380<br>
它抽出它的特性<br>
<br>
0:38:35.380,0:38:38.560<br>
比如說，它的筆劃的粗細等等<br>
<br>
0:38:38.560,0:38:40.080<br>
然後，接下來呢<br>
<br>
0:38:40.080,0:38:41.860<br>
你在丟進 encoder 的時候<br>
<br>
0:38:41.860,0:38:43.294<br>
你一方面給它<br>
<br>
0:38:43.294,0:38:46.880<br>
有關這一個數字的特性的 distribution<br>
<br>
0:38:46.880,0:38:49.400<br>
另外一方面告訴 decoder 說<br>
<br>
0:38:49.400,0:38:50.680<br>
它是什麼數字<br>
<br>
0:38:50.680,0:38:53.720<br>
那你就可以 generate 一大排<br>
<br>
0:38:53.720,0:38:55.760<br>
你就可以根據這一個 digit<br>
<br>
0:38:55.760,0:38:59.400<br>
generate 跟它 style 很相近的 digit<br>
<br>
0:38:59.400,0:39:02.320<br>
這個應該是在 MNIST 上面的結果<br>
<br>
0:39:02.320,0:39:04.160<br>
我的 reference 在下面，這是在 MNIST 上面的結果<br>
<br>
0:39:04.160,0:39:06.760<br>
這是在另外一個數字的 corpus 上面的結果<br>
<br>
0:39:06.760,0:39:08.180<br>
你會發現說<br>
<br>
0:39:08.180,0:39:11.420<br>
conditional VAE 確實可以根據某一個 digit<br>
<br>
0:39:11.420,0:39:16.000<br>
畫出其他的這個 style 相近的數字<br>
<br>
0:39:16.000,0:39:20.140<br>
這一邊是一些 reference 給大家參考<br>
<br>
0:39:20.140,0:39:23.600<br>
那 VAE 其實有一個很嚴重的問題<br>
<br>
0:39:23.600,0:39:24.820<br>
就是因為它有這問題，所以<br>
<br>
0:39:24.820,0:39:28.420<br>
之後又 propose 了 GAN<br>
<br>
0:39:28.420,0:39:30.820<br>
那 VAE 有什麼樣的問題呢？<br>
<br>
0:39:30.820,0:39:35.740<br>
VAE 其實它從來沒有去真的學怎麼產生一張<br>
<br>
0:39:35.740,0:39:39.460<br>
看起來像真的 image，對不對？<br>
<br>
0:39:39.460,0:39:41.340<br>
因為它所學到的事情是<br>
<br>
0:39:41.340,0:39:45.360<br>
它想要產生一張 image<br>
<br>
0:39:45.360,0:39:50.040<br>
跟我們在 data base 裡面某張 image 越接近越好<br>
<br>
0:39:50.040,0:39:52.280<br>
但是，它不知道的事情是<br>
<br>
0:39:52.280,0:39:55.080<br>
我們在 evaluate 它產生的 image<br>
<br>
0:39:55.080,0:39:57.560<br>
跟 data base 裡面的 image 的相似度的時候<br>
<br>
0:39:57.560,0:40:00.880<br>
我們是用，比如說，mean square error 等等<br>
<br>
0:40:00.880,0:40:03.060<br>
來 evaluate 兩張 image 中間的相似度<br>
<br>
0:40:03.060,0:40:06.400<br>
今天呢，假設我們這個<br>
<br>
0:40:06.400,0:40:09.960<br>
這個 decoder 的 output 跟真的 image 之間<br>
<br>
0:40:09.960,0:40:11.580<br>
有一個 pixel 的差距<br>
<br>
0:40:11.580,0:40:13.520<br>
它們有某一個 pixel 是不一樣的<br>
<br>
0:40:13.520,0:40:16.600<br>
但是，這個不一樣的 pixel，它落在不一樣的位置<br>
<br>
0:40:16.600,0:40:19.120<br>
其實是會得到非常不一樣的結果<br>
<br>
0:40:19.120,0:40:21.720<br>
假設這個不一樣的 pixel<br>
<br>
0:40:21.720,0:40:24.500<br>
它是落在這個地方<br>
<br>
0:40:24.500,0:40:25.660<br>
它落在這個地方<br>
<br>
0:40:25.660,0:40:27.640<br>
它只是讓 7 的筆劃比較長一點<br>
<br>
0:40:27.640,0:40:30.960<br>
跟它落在另外一個地方<br>
<br>
0:40:30.960,0:40:33.160<br>
它落在這個地方<br>
<br>
0:40:33.160,0:40:36.260<br>
對人來說 ，你一眼就可以看出說<br>
<br>
0:40:36.260,0:40:39.700<br>
這個是 machine generate，是怪怪的 digit<br>
<br>
0:40:39.700,0:40:41.480<br>
這個搞不好是真的<br>
<br>
0:40:41.480,0:40:44.200<br>
因為你根本看不出來跟原來的 7 有什麼差異<br>
<br>
0:40:44.200,0:40:46.560<br>
它只是稍微長一點，看起來還是很正常<br>
<br>
0:40:46.560,0:40:50.480<br>
但是，對 VAE 來說，都是一個 pixel 的差異<br>
<br>
0:40:50.480,0:40:56.340<br>
對它來說，這兩張 image 是一樣的好或一樣的不好<br>
<br>
0:40:56.340,0:40:58.160<br>
所以，VAE 它學的事情<br>
<br>
0:40:58.160,0:41:04.520<br>
只是怎麼產生一張 image <br>
跟 data base 裡面的 image 一模一樣<br>
<br>
0:41:04.520,0:41:06.940<br>
它從來沒有想過說要<br>
<br>
0:41:06.940,0:41:10.220<br>
真的產生一張可以假亂真的 image<br>
<br>
0:41:10.220,0:41:14.220<br>
所以，如果你用 VAE 來做 training 的時候<br>
<br>
0:41:14.220,0:41:16.840<br>
其實你產生出來的 image<br>
<br>
0:41:16.840,0:41:18.640<br>
VAE 所產生出來的 image<br>
<br>
0:41:18.640,0:41:21.040<br>
往往都是 data base 裡面的 image<br>
<br>
0:41:21.040,0:41:22.740<br>
的 linear combination 而已<br>
<br>
0:41:22.740,0:41:26.160<br>
因為它從來沒有學過要產生新的 image<br>
<br>
0:41:26.160,0:41:28.480<br>
它唯一做的事情只有模仿而已<br>
<br>
0:41:28.480,0:41:31.640<br>
它唯一做的事情只有<br>
<br>
0:41:31.640,0:41:35.560<br>
希望它產生的 image 跟 data base 的某張 image 越像越好<br>
<br>
0:41:35.560,0:41:36.700<br>
它只是模仿而已<br>
<br>
0:41:36.700,0:41:40.200<br>
或者最多就是把原來 data base 裡面的image 做 linear combination<br>
<br>
0:41:40.200,0:41:44.500<br>
它做一些 combination，它沒辦法產生一些新的 image<br>
<br>
0:41:44.500,0:41:48.560<br>
所以，這樣感覺沒有非常的 intelligent<br>
<br>
0:41:48.560,0:41:50.040<br>
所以，接下來就有人 propose<br>
<br>
0:41:50.040,0:41:55.880<br>
有另外一個方法叫做 Generative Adversarial Network<br>
<br>
0:41:55.880,0:41:58.680<br>
Adversarial 是對抗的意思<br>
<br>
0:41:58.680,0:42:00.120<br>
它縮寫是 GAN<br>
<br>
0:42:00.120,0:42:02.380<br>
你會發現它是很新的 paper<br>
<br>
0:42:02.380,0:42:06.400<br>
它最早出現的時候 是 2014 年的 1 2月<br>
<br>
0:42:06.400,0:42:08.900<br>
所以，大概是兩年前的 paper<br>
<br>
0:42:08.900,0:42:13.880<br>
以下呢，我們引用了 Yann LeCun 對 GAN 的 comment<br>
<br>
0:42:13.880,0:42:16.100<br>
就是有人在 Quora 上面<br>
<br>
0:42:16.100,0:42:17.600<br>
問了說這個<br>
<br>
0:42:17.600,0:42:22.620<br>
Unsupervised learning 的 approach <br>
哪一個是最有 potential 的<br>
<br>
0:42:22.620,0:42:26.900<br>
然後，Yann LeCun 他親自來回答，他說呢<br>
<br>
0:42:26.900,0:42:31.500<br>
Adversarial Training is the coolest thing since sliced bread.<br>
<br>
0:42:31.500,0:42:33.400<br>
since sliced bread，大家知道是什麼意思嗎？<br>
<br>
0:42:33.400,0:42:34.660<br>
我 google了一下，這是個片語<br>
<br>
0:42:34.660,0:42:37.800<br>
如果翻譯成中文的話，是有史以來的意思<br>
<br>
0:42:37.800,0:42:40.000<br>
since sliced bread 是什麼意思呢？<br>
<br>
0:42:40.000,0:42:42.420<br>
sliced bread 是切片麵包的意思<br>
<br>
0:42:42.420,0:42:45.960<br>
那這個片語的典故，好像是說<br>
<br>
0:42:45.960,0:42:49.980<br>
在過去麵包店是不幫你切麵麵包的<br>
<br>
0:42:49.980,0:42:52.280<br>
吐司麵包烤完之後，他是不幫你切的<br>
<br>
0:42:52.280,0:42:53.620<br>
所以你買回去之後，要自己切很麻煩<br>
<br>
0:42:53.620,0:42:57.100<br>
後來就人發明說，應該先切了以後再賣<br>
<br>
0:42:57.100,0:42:58.460<br>
然後，大家都很高興這個樣子<br>
<br>
0:42:58.460,0:43:00.420<br>
所以，since sliced bread<br>
<br>
0:43:00.420,0:43:03.560<br>
它的英文片語就是有史以來的意思<br>
<br>
0:43:03.560,0:43:07.560<br>
它說這是有史以來最強的、最酷的方法<br>
<br>
0:43:07.560,0:43:10.460<br>
他這邊還講了一些別的，他說<br>
<br>
0:43:10.460,0:43:14.600<br>
What's missing at the moment is a good understanding of it.<br>
<br>
0:43:14.600,0:43:18.080<br>
so we can make it work reliably.<br>
<br>
0:43:18.080,0:43:20.820<br>
It's very finicky.<br>
<br>
0:43:20.820,0:43:24.140<br>
Sort of like CovNet were in the 1990s,<br>
<br>
0:43:24.140,0:43:30.520<br>
when I had the reputation of being the only person who could make them work(which wasn't true).<br>
<br>
0:43:30.520,0:43:32.860<br>
這其實是 GAN 非常難 train<br>
<br>
0:43:32.860,0:43:36.220<br>
感覺好像只有 Ian 跟 Goodfellow<br>
<br>
0:43:36.220,0:43:37.960<br>
才 propose 他們可以做得起來<br>
<br>
0:43:37.960,0:43:41.800<br>
其他人做起來，你可以 google 一下那個 GAN 的 code<br>
<br>
0:43:41.800,0:43:43.000<br>
很多都在 MNIST 上面<br>
<br>
0:43:43.000,0:43:45.920<br>
他們產生的 digit，都不是很好看<br>
<br>
0:43:45.920,0:43:48.260<br>
我們用 VAE 隨便做都可以打爆這些東西<br>
<br>
0:43:48.260,0:43:50.480<br>
所以產生的 image 很怪<br>
<br>
0:43:50.480,0:43:52.520<br>
但是，你如果看 paper 的話<br>
<br>
0:43:52.520,0:43:54.760<br>
它的 performance 是滿好的<br>
<br>
0:43:54.760,0:43:59.160<br>
所以，它裡面還有很多不為人知的技巧<br>
<br>
0:43:59.160,0:44:02.980<br>
像過去大家相信說只有 Yann LeCun 可以 train 起來 CNN<br>
<br>
0:44:02.980,0:44:04.740<br>
不過其實不是這樣子<br>
<br>
0:44:04.740,0:44:09.480<br>
那其實我很無聊，我又收集了，找到另外一則這樣<br>
<br>
0:44:09.480,0:44:11.160<br>
就是有人問說<br>
<br>
0:44:11.160,0:44:16.240<br>
有沒有什麼最近的 breakthroughs 在 deep learning 裡面<br>
<br>
0:44:16.240,0:44:19.760<br>
然後，Yann LeCun 又來回答了，他說<br>
<br>
0:44:19.760,0:44:23.540<br>
The most important one, in my opinion, <br>
is adversarial training.<br>
<br>
0:44:23.540,0:44:26.420<br>
also called GAN.<br>
<br>
0:44:26.420,0:44:29.520<br>
This is an idea proposed by Ian Goodfellow.<br>
<br>
0:44:29.560,0:44:33.100<br>
他說這個是 the most interest idea<br>
<br>
0:44:33.100,0:44:36.160<br>
in the last ten years in ML<br>
<br>
0:44:36.160,0:44:42.480<br>
所以，你就來看這個十年來最有趣的想法到底是怎麼樣<br>
<br>
0:44:42.480,0:44:46.000<br>
這個 GAN 的概念<br>
<br>
0:44:46.000,0:44:49.200<br>
有點像似擬態的演化<br>
<br>
0:44:49.200,0:44:51.840<br>
比如說，這是一個枯葉蝶<br>
<br>
0:44:51.840,0:44:53.400<br>
這個是一個枯葉蝶<br>
<br>
0:44:53.400,0:44:55.900<br>
他長得就跟枯葉一模一樣<br>
<br>
0:44:55.900,0:45:00.240<br>
枯葉蝶是怎麼變的跟枯葉一模一樣呢？<br>
<br>
0:45:00.240,0:45:01.820<br>
怎麼變成這麼像的呢？<br>
<br>
0:45:01.820,0:45:04.600<br>
也許一開始他長的是這個樣子<br>
<br>
0:45:04.600,0:45:07.760<br>
然後呢，但是他有天敵<br>
<br>
0:45:07.760,0:45:09.580<br>
類似麻雀的天敵<br>
<br>
0:45:09.580,0:45:11.940<br>
比如像波波這樣子的天敵<br>
<br>
0:45:11.940,0:45:16.460<br>
天敵會吃這個蝴蝶<br>
<br>
0:45:16.460,0:45:21.080<br>
天敵辨識是不是蝴蝶的方式，就是他知道蝴蝶不是棕色<br>
<br>
0:45:21.080,0:45:23.100<br>
他就吃不是棕色的東西<br>
<br>
0:45:23.100,0:45:27.220<br>
所以蝴蝶就演化，他就變成是棕色的<br>
<br>
0:45:27.220,0:45:31.040<br>
但是，他的天敵也會跟著演化<br>
<br>
0:45:31.040,0:45:33.360<br>
波波就會變成比比鳥這樣<br>
<br>
0:45:33.360,0:45:37.480<br>
然後這個比比鳥知道說，蝴蝶是沒有葉脈的<br>
<br>
0:45:37.480,0:45:40.240<br>
所以，他會吃沒有葉脈的東西<br>
<br>
0:45:40.240,0:45:42.020<br>
他會 ignore 有葉脈的東西<br>
<br>
0:45:42.020,0:45:44.300<br>
所以，蝴蝶又再演化<br>
<br>
0:45:44.300,0:45:46.620<br>
就會變成枯葉蝶，他就產生葉脈<br>
<br>
0:45:46.620,0:45:49.060<br>
但是，他的天敵也會再演化<br>
<br>
0:45:49.060,0:45:50.520<br>
這個好像是神獸<br>
<br>
0:45:50.520,0:45:53.020<br>
這個好像不是波波演化來的，不過沒有關係<br>
<br>
0:45:53.020,0:45:55.860<br>
然後，他的天敵也還會再演化<br>
<br>
0:45:55.860,0:45:57.640<br>
所以，這兩個東西呢<br>
<br>
0:45:57.640,0:46:03.320<br>
天敵和枯葉蝶，他們就會共同的演化<br>
<br>
0:46:03.320,0:46:05.840<br>
所以，枯葉蝶就會長得越來越樣枯葉<br>
<br>
0:46:05.840,0:46:08.080<br>
直到最後沒有辦法分辨為止<br>
<br>
0:46:08.080,0:46:12.200<br>
所以這跟 GAN 的概念，是非常類似的<br>
<br>
0:46:12.200,0:46:13.760<br>
GAN 的概念是這個樣子<br>
<br>
0:46:13.760,0:46:16.120<br>
首先，有一個第一代的 Generator<br>
<br>
0:46:16.120,0:46:19.700<br>
第一代的 Generator 它很廢，它可能根本就是 random 的<br>
<br>
0:46:19.700,0:46:22.140<br>
它會 generate 一大堆奇怪的東西<br>
<br>
0:46:22.140,0:46:24.220<br>
看起來不像是真正地 image 的東西<br>
<br>
0:46:24.220,0:46:26.440<br>
假如我們現在叫它 Generate 4 個 digit<br>
<br>
0:46:26.440,0:46:30.820<br>
那接下來有一個的第一代 Discriminator<br>
<br>
0:46:30.820,0:46:32.200<br>
他就是那個天敵<br>
<br>
0:46:32.200,0:46:33.780<br>
Discriminator 做的事情是<br>
<br>
0:46:33.780,0:46:37.280<br>
他會根據 real 的 image<br>
<br>
0:46:37.280,0:46:40.500<br>
跟 Generator 所產生的 image<br>
<br>
0:46:40.500,0:46:43.080<br>
去調整它裡面的參數<br>
<br>
0:46:43.080,0:46:46.840<br>
去評斷說，一張 image 是真正的 image<br>
<br>
0:46:46.840,0:46:49.500<br>
還是 Generator 所產生的 image<br>
<br>
0:46:49.500,0:46:53.660<br>
接下來呢，這個 Generator 根據這個 Discriminator<br>
<br>
0:46:53.660,0:46:55.440<br>
等一下會講說 Generator<br>
<br>
0:46:55.440,0:46:57.940<br>
怎麼根據 Discriminator去演化<br>
<br>
0:46:57.940,0:47:00.100<br>
Generator 根據 Discriminator<br>
<br>
0:47:00.100,0:47:02.780<br>
他又去調整了他的參數<br>
<br>
0:47:02.780,0:47:04.500<br>
所以，第二代的 Generator<br>
<br>
0:47:04.500,0:47:09.220<br>
他產生的參數，他產生的 digit 就可能就更像真的<br>
<br>
0:47:09.220,0:47:13.420<br>
接下來，Discriminator 會再根據第二代的Generator<br>
<br>
0:47:13.420,0:47:16.200<br>
產生的 digit 跟真正的 digit<br>
<br>
0:47:16.200,0:47:20.160<br>
去 update 他的參數<br>
<br>
0:47:20.160,0:47:22.440<br>
接下來，有了第二代的 Discriminator<br>
<br>
0:47:22.440,0:47:24.940<br>
就會再產生第三代的 Generator<br>
<br>
0:47:24.940,0:47:31.200<br>
第三代 Generator 產生的數字又更像真正的這個數字<br>
<br>
0:47:31.200,0:47:33.980<br>
就是第三代 Generator 他產生的這些數字<br>
<br>
0:47:33.980,0:47:35.660<br>
可以騙過第二代的 Discriminator<br>
<br>
0:47:35.660,0:47:38.180<br>
第二代產生的這些數字，可以騙過第一代的 Generator<br>
<br>
0:47:38.180,0:47:40.220<br>
但是，這個第一代的 Discriminator<br>
<br>
0:47:40.220,0:47:42.740<br>
他產生的數字，可以騙過第二代的 Discriminator<br>
<br>
0:47:42.740,0:47:45.800<br>
但是，Discriminator 會再演化<br>
<br>
0:47:45.800,0:47:50.120<br>
可能又可以再分辨第三代 Generator 產生的數字<br>
<br>
0:47:50.120,0:47:51.840<br>
跟真正的數字之間的差距<br>
<br>
0:47:51.840,0:47:55.520<br>
你要注意一個地方就是，這個 Generator 啊<br>
<br>
0:47:55.520,0:47:58.980<br>
他從來沒有看過真正的 image長什麼樣子<br>
<br>
0:47:58.980,0:48:02.420<br>
Discriminator 有看過真正的 image 長什麼樣子<br>
<br>
0:48:02.500,0:48:05.800<br>
它會比較真正的 image 跟 Generator 的 output 的不同<br>
<br>
0:48:05.800,0:48:09.220<br>
但是，Generator 從來沒有看過真正的 image<br>
<br>
0:48:09.220,0:48:12.040<br>
他做的事情，只是想去騙過 Discriminator<br>
<br>
0:48:12.040,0:48:15.860<br>
所以，因為 generator 從來沒有看過真正 image<br>
<br>
0:48:15.860,0:48:20.040<br>
所以，Generator 他可以產生出來的那一些 image<br>
<br>
0:48:20.040,0:48:22.920<br>
是 data base 裡面從來都沒有見過的<br>
<br>
0:48:22.920,0:48:26.220<br>
所以，這比較像是，我們想要 machine 做的事情<br>
<br>
0:48:26.220,0:48:28.860<br>
我們現在看 Discriminator 是怎麼 train 的<br>
<br>
0:48:28.860,0:48:30.700<br>
這一邊是比較直覺的<br>
<br>
0:48:30.700,0:48:33.260<br>
這個 Discriminator 他就是一個 neural network<br>
<br>
0:48:33.260,0:48:35.600<br>
他的 input 就是一張 image<br>
<br>
0:48:35.600,0:48:38.740<br>
他的 output  就是一個 number<br>
<br>
0:48:38.740,0:48:41.860<br>
它的 output 就是一個 scalar<br>
<br>
0:48:41.860,0:48:46.580<br>
你可能通過 sigmoid function，讓他的值介於 0 到 1 之間<br>
<br>
0:48:46.580,0:48:50.380<br>
1 就代表說 input 這張 image 是真正的 image<br>
<br>
0:48:50.380,0:48:52.500<br>
假如你是要做手寫數字辨識的話<br>
<br>
0:48:52.500,0:48:55.160<br>
那 input image 就是真正的人手寫的數字<br>
<br>
0:48:55.160,0:48:59.300<br>
0 代表是假的，是 Generator 所產生的<br>
<br>
0:48:59.300,0:49:01.600<br>
那 Generator 是什麼呢？<br>
<br>
0:49:01.600,0:49:03.940<br>
Generator 在這一邊，其實<br>
<br>
0:49:03.940,0:49:07.820<br>
他的那個架構就跟 VAE 的 decoder 是一模一樣的<br>
<br>
0:49:07.820,0:49:09.700<br>
他也是一個 neural network<br>
<br>
0:49:09.700,0:49:12.580<br>
他的 input 就是從一個 distribution<br>
<br>
0:49:12.580,0:49:15.240<br>
他可以是某某 distribution 或是任何其他的 distribution<br>
<br>
0:49:15.240,0:49:17.700<br>
從某一個 distribution sample 出來的一個 vector<br>
<br>
0:49:17.700,0:49:20.080<br>
你把這個 sample 出來的 vector<br>
<br>
0:49:20.080,0:49:22.120<br>
丟到 Generator 裡面<br>
<br>
0:49:22.120,0:49:24.560<br>
他就會產生一個數字<br>
<br>
0:49:24.560,0:49:25.600<br>
產生一個 image<br>
<br>
0:49:25.600,0:49:26.960<br>
你給他不同的 vector<br>
<br>
0:49:26.960,0:49:29.920<br>
他就產生不同樣子的 image<br>
<br>
0:49:29.920,0:49:35.440<br>
那先用 Generator 產生一堆假的 image<br>
<br>
0:49:35.440,0:49:41.600<br>
然後，我們有真正的 image<br>
<br>
0:49:41.600,0:49:45.100<br>
Discriminator 就是把這一些 Generator<br>
<br>
0:49:45.100,0:49:47.280<br>
所產生的 image<br>
<br>
0:49:47.280,0:49:49.920<br>
都 label 為 0，也都 label 為 fake<br>
<br>
0:49:49.920,0:49:52.700<br>
然後，把這個真正的 image<br>
<br>
0:49:52.700,0:49:54.120<br>
都 label 為 1<br>
<br>
0:49:54.120,0:49:55.700<br>
也就是都 label 為 true<br>
<br>
0:49:55.700,0:49:58.700<br>
接下來，就只是一個 binary classification 的 problem<br>
<br>
0:49:58.700,0:50:00.300<br>
大家都很熟<br>
<br>
0:50:00.300,0:50:03.500<br>
你就可以 learn 一個 Discriminator<br>
<br>
0:50:03.500,0:50:06.960<br>
接下來，怎麼 learn 這個 Generator 呢？<br>
<br>
0:50:06.960,0:50:08.760<br>
Generator 的 learn 法是這個樣子<br>
<br>
0:50:08.760,0:50:12.780<br>
現在已經有了第一代的 Discriminator<br>
<br>
0:50:12.780,0:50:15.360<br>
怎麼根據第一代的 Discriminator<br>
<br>
0:50:15.360,0:50:19.920<br>
把第一代的 Generator 再 update 呢<br>
<br>
0:50:19.920,0:50:23.720<br>
首先，如果我們隨便給，輸入一個 vector<br>
<br>
0:50:23.720,0:50:27.380<br>
他會產生一張隨便的 image<br>
<br>
0:50:27.380,0:50:32.640<br>
那這一個 image 可能沒有辦法騙過這個 Discriminator<br>
<br>
0:50:32.640,0:50:36.020<br>
你把 Generator 產生的 image 丟到 Discriminator 裡面<br>
<br>
0:50:36.020,0:50:38.760<br>
他可能說，這有 87% 像這樣子<br>
<br>
0:50:38.760,0:50:42.400<br>
然後，接下來要做的事情是甚麼呢？<br>
<br>
0:50:42.400,0:50:46.080<br>
接下來，我們要做的事情是調這個 Generator 的參數<br>
<br>
0:50:46.080,0:50:47.960<br>
調這個 Generator 的參數<br>
<br>
0:50:47.960,0:50:52.380<br>
讓 Discriminator 會認為說 Generator<br>
<br>
0:50:52.380,0:50:54.980<br>
generate 出來的 image 是真的，也就是說<br>
<br>
0:50:54.980,0:50:57.500<br>
要讓 Generator generate 出來的 image<br>
<br>
0:50:57.500,0:51:01.720<br>
丟到 Discriminator 以後，Discriminator 的 output<br>
<br>
0:51:01.720,0:51:04.240<br>
必須要越接近越好<br>
<br>
0:51:04.240,0:51:08.860<br>
所以，你希望 Generator generate 是長這樣子的 image<br>
<br>
0:51:08.860,0:51:11.180<br>
他可以騙過 Discriminator<br>
<br>
0:51:11.180,0:51:12.860<br>
Discriminator output 是 1.0<br>
<br>
0:51:12.860,0:51:14.460<br>
覺得他是一個真正的 image<br>
<br>
0:51:14.460,0:51:16.320<br>
這件事情怎麼做呢<br>
<br>
0:51:16.320,0:51:21.120<br>
其實，因為你知道這個 Generator 是一個 neural network<br>
<br>
0:51:21.120,0:51:24.540<br>
那 Discriminator 也是一個 neural network<br>
<br>
0:51:24.540,0:51:26.720<br>
你把這個 Generator 的 output<br>
<br>
0:51:26.720,0:51:30.500<br>
丟到這個當作 Discriminator 的 input<br>
<br>
0:51:30.500,0:51:32.720<br>
然後，再讓他產生一個 scalar<br>
<br>
0:51:32.720,0:51:36.040<br>
這一件事情，其實就好像是<br>
<br>
0:51:36.040,0:51:41.600<br>
你有一個很大很大的 neural network<br>
<br>
0:51:41.600,0:51:43.420<br>
他這邊有很多層<br>
<br>
0:51:43.420,0:51:44.580<br>
他這一邊也有很多層<br>
<br>
0:51:44.580,0:51:46.620<br>
然後，你丟一個 random 的 vector<br>
<br>
0:51:46.620,0:51:49.080<br>
他 output 就是一個 scalar<br>
<br>
0:51:49.080,0:51:51.680<br>
所以，一個 Generator 加一個 Discriminator，他合起來<br>
<br>
0:51:51.680,0:51:53.620<br>
就是一個很大的 network<br>
<br>
0:51:53.620,0:51:55.260<br>
他既然乘起來是一個很大的 network<br>
<br>
0:51:55.260,0:51:57.520<br>
那你要讓這個 network<br>
<br>
0:51:57.520,0:52:00.500<br>
再丟進一個 random vector<br>
<br>
0:52:00.500,0:52:04.320<br>
他 output 1 是很容易的，你就做 Gradient Descent 就好<br>
<br>
0:52:04.320,0:52:05.840<br>
你就用 Gradient Descent 調參數<br>
<br>
0:52:05.840,0:52:08.200<br>
希望丟進這一個 vector 的時候<br>
<br>
0:52:08.200,0:52:11.420<br>
他的 output 是要接近 1 的<br>
<br>
0:52:11.420,0:52:13.380<br>
但是，你這邊要注意的事情是<br>
<br>
0:52:13.380,0:52:17.080<br>
你在調這個參數的時候<br>
<br>
0:52:17.080,0:52:19.460<br>
你在調這個 network 參數的時候<br>
<br>
0:52:19.460,0:52:20.640<br>
你在做 Backpropagation 的時候<br>
<br>
0:52:20.640,0:52:24.160<br>
你只能夠調整這個 Generator 的參數<br>
<br>
0:52:24.160,0:52:28.760<br>
你只能算 generator 的參數對 output 的 gradient<br>
<br>
0:52:28.760,0:52:30.840<br>
然後去 update Generator 的參數<br>
<br>
0:52:30.840,0:52:32.540<br>
你必須要 fix 住 Discriminator 的參數<br>
<br>
0:52:32.540,0:52:36.640<br>
如果你今天不 fix 住 Discriminator 的參數，<br>
會發生什麼事情呢？<br>
<br>
0:52:36.640,0:52:38.620<br>
你會發生，對 Discriminator 來說<br>
<br>
0:52:38.620,0:52:41.400<br>
要讓他 output 1 很簡單阿<br>
<br>
0:52:41.400,0:52:44.380<br>
就他最後output 的時候，bias 設 1<br>
<br>
0:52:44.380,0:52:47.780<br>
然後其他都設 2，weight 都設 0，它 output 就 1 了<br>
<br>
0:52:47.780,0:52:51.620<br>
所以，Discriminator，你要讓這整個 network<br>
<br>
0:52:51.620,0:52:54.400<br>
input 一個 random 的 vector，output 是1 的時候<br>
<br>
0:52:54.400,0:52:57.060<br>
你要把 Discriminator 這個參數鎖住<br>
<br>
0:52:57.060,0:53:00.560<br>
Discriminator 參數必須要是 fix 住的<br>
<br>
0:53:00.560,0:53:05.300<br>
然後，input 一個 Generator，只調 generator 的參數<br>
<br>
0:53:05.300,0:53:07.300<br>
這樣 generator 產生出來的 image<br>
<br>
0:53:07.300,0:53:11.980<br>
才會像是，才是一個可以騙過 Discriminator 的 image<br>
<br>
0:53:11.980,0:53:16.240<br>
這邊有一個來自 GAN 原始 paper 的 Toy example<br>
<br>
0:53:16.240,0:53:20.000<br>
我們來說明一下，這個 Toy example 是什麼意思<br>
<br>
0:53:20.000,0:53:22.020<br>
這個 Toy example 是這樣子<br>
<br>
0:53:22.020,0:53:26.920<br>
他說，現在我們的這個 z space<br>
<br>
0:53:26.920,0:53:31.640<br>
也就是這個 decoder 的 input<br>
<br>
0:53:31.640,0:53:34.080<br>
我們知道 decoder 的 input 就是一個 z<br>
<br>
0:53:34.080,0:53:36.040<br>
就是一個 hidden 的 vector<br>
<br>
0:53:36.040,0:53:38.280<br>
hidden 的這個 vector<br>
<br>
0:53:38.280,0:53:41.680<br>
這個 z 他是一個 one dimensional 的東西<br>
<br>
0:53:41.680,0:53:44.020<br>
那他丟到 Generator 裡面<br>
<br>
0:53:44.020,0:53:48.060<br>
他會產生另外一個 one dimension 的東西<br>
<br>
0:53:48.060,0:53:51.420<br>
這個 z 可以從任何的 distribution 裡面 sample 出來<br>
<br>
0:53:51.420,0:53:52.860<br>
這邊在這個例子裡面<br>
<br>
0:53:52.860,0:53:55.320<br>
他顯然是從一個 uniform 的 distribution 裡面<br>
<br>
0:53:55.320,0:53:57.080<br>
sample 出來的<br>
<br>
0:53:57.080,0:54:00.820<br>
然後，你把這一個 z 通過 neural network 以後<br>
<br>
0:54:00.820,0:54:05.220<br>
每一個不同的 z，他會給你不同的 x<br>
<br>
0:54:05.320,0:54:09.080<br>
這個 x 的分布，就是綠色的這個分布<br>
<br>
0:54:09.080,0:54:12.340<br>
綠色這個分布，現在要做的事情是<br>
<br>
0:54:12.340,0:54:17.360<br>
希望這個 Generator 的 output 可以越像 real data 越好<br>
<br>
0:54:17.360,0:54:20.420<br>
他這一邊的 real data 就是黑色的這個點<br>
<br>
0:54:20.420,0:54:23.660<br>
假設有一組 real data 就是黑色的這個點<br>
<br>
0:54:23.660,0:54:26.180<br>
你要找的這個 distribution 是黑色的這個點<br>
<br>
0:54:26.180,0:54:27.480<br>
那你希望你的 Generator 的 output<br>
<br>
0:54:27.480,0:54:29.200<br>
也就是這個綠色 distribution<br>
<br>
0:54:29.200,0:54:32.500<br>
可以跟黑色的這個點，越接近越好<br>
<br>
0:54:32.500,0:54:34.560<br>
如果按照 GAN 的概念的話<br>
<br>
0:54:34.560,0:54:37.540<br>
你就是把這個 Generator 的 output x<br>
<br>
0:54:37.540,0:54:39.440<br>
跟這個 real 的 data<br>
<br>
0:54:39.440,0:54:42.720<br>
這些黑色的點，丟到 Discriminator 裡面<br>
<br>
0:54:42.720,0:54:45.380<br>
然後，讓 Discriminator 去判斷說<br>
<br>
0:54:45.380,0:54:49.060<br>
現在這個 value，其實現在這個 x<br>
<br>
0:54:49.060,0:54:51.700<br>
real data 都只是一個 scalar 而已<br>
<br>
0:54:51.700,0:54:57.020<br>
現在這個 scalar，他是來自真正的 data 的機率<br>
<br>
0:54:57.020,0:55:00.240<br>
跟來自於 Generator 的 output 的機率<br>
<br>
0:55:00.240,0:55:02.820<br>
如果他是真正的 data 的話就是 1<br>
<br>
0:55:02.820,0:55:04.100<br>
反之就是 0<br>
<br>
0:55:04.100,0:55:08.120<br>
Discriminator 的 output，就是綠色的 curve<br>
<br>
0:55:08.120,0:55:11.860<br>
那假設現在，Generator 他還很弱<br>
<br>
0:55:11.860,0:55:13.540<br>
所以，他產生出來的 distribution<br>
<br>
0:55:13.540,0:55:15.360<br>
是這個綠色的 distribution<br>
<br>
0:55:15.360,0:55:19.900<br>
那這個 Discriminator 他根據 real data<br>
<br>
0:55:19.900,0:55:25.260<br>
跟這個 Generator distribution 他的樣子呢<br>
<br>
0:55:25.260,0:55:27.520<br>
你給他這個 x 的值，他的 output<br>
<br>
0:55:27.520,0:55:30.480<br>
可能就會像是這一條藍色的線<br>
<br>
0:55:30.480,0:55:32.680<br>
這一條藍色的線告訴我們說<br>
<br>
0:55:32.680,0:55:36.760<br>
Discriminator 認為說，如果是在這一帶的點<br>
<br>
0:55:36.760,0:55:40.300<br>
他比較有可能是假的<br>
<br>
0:55:40.300,0:55:42.280<br>
他的這個值是比較低的<br>
<br>
0:55:42.280,0:55:43.700<br>
如果是落在這一帶的點<br>
<br>
0:55:43.700,0:55:45.940<br>
他比較有可能是從 Generator 產生的<br>
<br>
0:55:45.940,0:55:47.800<br>
落在這一帶的點<br>
<br>
0:55:47.800,0:55:50.760<br>
他比較有可能是 real data<br>
<br>
0:55:50.760,0:55:53.900<br>
接下來，Generator 就根據<br>
<br>
0:55:53.900,0:55:56.900<br>
Discriminator 的結果去調整他的參數<br>
<br>
0:55:56.900,0:56:00.560<br>
Generator 要做的事情是騙過 Discriminator<br>
<br>
0:56:00.560,0:56:02.520<br>
既然 Discriminator 認為<br>
<br>
0:56:02.520,0:56:06.120<br>
這個地方比較有可能是 real data<br>
<br>
0:56:06.120,0:56:09.340<br>
Generator 就把他的 output 往左邊移<br>
<br>
0:56:09.340,0:56:11.780<br>
他就把他的 output 往左邊移<br>
<br>
0:56:11.780,0:56:14.080<br>
那你說有沒有可能會移太多<br>
<br>
0:56:14.080,0:56:17.300<br>
比如說，通通偏到左邊去，是有可能的<br>
<br>
0:56:17.300,0:56:18.800<br>
所以 GAN 很難 train 這樣<br>
<br>
0:56:18.800,0:56:22.100<br>
這個要小心的調參數<br>
<br>
0:56:22.100,0:56:24.680<br>
小心的調參數，讓他不要移太多<br>
<br>
0:56:24.680,0:56:28.460<br>
這綠色的 distribution 就可以稍微偏一點<br>
<br>
0:56:28.460,0:56:32.600<br>
比較接近真正 real 的黑色的點的 distribution<br>
<br>
0:56:32.600,0:56:34.640<br>
所以，Generator 會騙過他<br>
<br>
0:56:34.640,0:56:36.200<br>
他就產生新的 distribution<br>
<br>
0:56:36.200,0:56:38.180<br>
然後，接下來 Discriminator<br>
<br>
0:56:38.180,0:56:40.760<br>
會再 update 綠色的這一條線<br>
<br>
0:56:40.760,0:56:44.920<br>
這一個 process 就不斷反覆地去進行<br>
<br>
0:56:44.920,0:56:46.280<br>
直到最後呢<br>
<br>
0:56:46.280,0:56:49.320<br>
Generator 產生的 output 跟 real data 一模一樣<br>
<br>
0:56:49.320,0:56:52.400<br>
那 Discriminator 會沒有任何辦法<br>
<br>
0:56:52.400,0:56:54.620<br>
分辨真正的 data<br>
<br>
0:56:54.620,0:56:56.020<br>
你有問題嗎？<br>
<br>
0:57:04.620,0:57:07.660<br>
其實這個就是現在 train GAN 的時候<br>
<br>
0:57:07.660,0:57:09.060<br>
所遇到最大的問題<br>
<br>
0:57:09.060,0:57:12.440<br>
你不知道 Discriminator 是不是對的<br>
<br>
0:57:12.440,0:57:18.280<br>
因為你說 Discriminator 現在得到一個很好的結果<br>
<br>
0:57:18.280,0:57:20.760<br>
那可能是 Generator 太廢<br>
<br>
0:57:20.760,0:57:24.120<br>
有時候 Discriminator 得到一個很差的結果<br>
<br>
0:57:24.120,0:57:26.460<br>
比如說，他認為說每一個地方<br>
<br>
0:57:26.460,0:57:29.300<br>
每一個地方他都無法分辨說<br>
<br>
0:57:29.300,0:57:31.520<br>
是 real value 還是 fake value<br>
<br>
0:57:31.520,0:57:34.640<br>
這個時候並不代表說 Generator generate 的很像<br>
<br>
0:57:34.640,0:57:36.780<br>
有可能只是 Discriminator 太弱了<br>
<br>
0:57:36.780,0:57:40.500<br>
所以，這是一個現在還沒有好的 solution 的難題<br>
<br>
0:57:40.500,0:57:42.680<br>
所以，真正在 train GAN 的時候，你會怎麼做呢？<br>
<br>
0:57:42.680,0:57:47.160<br>
你會一直坐在電腦旁邊，看他產生 image 這樣，你懂嗎 ?<br>
<br>
0:57:47.160,0:57:51.720<br>
因為你從 Discriminator 跟 Generator 的 loss<br>
<br>
0:57:51.720,0:57:54.160<br>
你看不出來他 generate 的 image 有沒有比較好<br>
<br>
0:57:54.160,0:57:58.120<br>
所以，變成說你 Generator 每 update 一次參數<br>
<br>
0:57:58.120,0:57:59.940<br>
Discriminator 每 update 一次參數<br>
<br>
0:57:59.940,0:58:01.180<br>
你就去看看他<br>
<br>
0:58:01.180,0:58:03.940<br>
你就拿 generated 的 image 看看有沒有比較好<br>
<br>
0:58:03.940,0:58:05.680<br>
如果變差以後<br>
<br>
0:58:05.680,0:58:08.180<br>
方向走錯了，再重新調一下參數這樣子<br>
<br>
0:58:08.180,0:58:11.380<br>
所以，這個非常非常的困難<br>
<br>
0:58:11.380,0:58:12.480<br>
非常非常的困難<br>
<br>
0:58:12.480,0:58:15.640<br>
我們這一邊其實有人在線上放了一個 demo<br>
<br>
0:58:15.640,0:58:17.420<br>
我們來看一下這個 demo<br>
<br>
0:58:17.420,0:58:20.740<br>
非常 realistic 的 image<br>
<br>
0:58:20.740,0:58:23.680<br>
這個是 OpenAI 產生的 image<br>
<br>
0:58:23.680,0:58:28.240<br>
如果我們問你說<br>
<br>
0:58:28.240,0:58:30.780<br>
你覺得左邊是 real image<br>
<br>
0:58:30.780,0:58:31.960<br>
還是右邊是 real image<br>
<br>
0:58:31.960,0:58:35.980<br>
你覺得左邊是電腦產生的 image 的同學舉手一下<br>
<br>
0:58:35.980,0:58:39.340<br>
有人，請放下<br>
<br>
0:58:39.340,0:58:42.240<br>
覺得右邊是電腦產生的 image 的同學舉手一下<br>
<br>
0:58:42.240,0:58:43.560<br>
好，手放下<br>
<br>
0:58:43.560,0:58:45.540<br>
其實他還是沒有辦法騙過人<br>
<br>
0:58:45.540,0:58:47.540<br>
你看這邊還有很多怪怪的的東西就是了<br>
<br>
0:58:47.540,0:58:49.300<br>
很多東西很像<br>
<br>
0:58:49.300,0:58:51.560<br>
這個馬還蠻像<br>
<br>
0:58:51.560,0:58:54.620<br>
這個有飛魚，有大嘴巴的貓<br>
<br>
0:58:54.620,0:58:55.900<br>
有很多怪怪的東西<br>
<br>
0:58:55.900,0:58:57.480<br>
所以，他其實沒有辦法騙過人<br>
<br>
0:58:57.480,0:59:00.740<br>
我覺得如果放單一一張，光看這個馬<br>
<br>
0:59:00.740,0:59:02.760<br>
他可能可以騙過人<br>
<br>
0:59:02.760,0:59:05.800<br>
OpenAI 他們有做那個實驗<br>
<br>
0:59:05.800,0:59:09.840<br>
好像有 21% 的 image<br>
<br>
0:59:09.840,0:59:13.540<br>
就有 21% machine generate 的 image 會被誤判成 real<br>
<br>
0:59:13.540,0:59:15.900<br>
所以，他其實是可以騙過部分的人<br>
<br>
0:59:15.900,0:59:19.740<br>
另外，這一邊又有一個很驚人的結果<br>
<br>
0:59:19.740,0:59:21.600<br>
在文獻上非常驚人的結果<br>
<br>
0:59:21.600,0:59:24.800<br>
就是說先拿很多房間的照片<br>
<br>
0:59:24.800,0:59:26.460<br>
讓 machine 去 train GAN<br>
<br>
0:59:26.460,0:59:28.060<br>
他可以 generate 房間的照片<br>
<br>
0:59:28.060,0:59:31.140<br>
那我們說，那個 Generator 就是你 input 一個 vector 給他<br>
<br>
0:59:31.140,0:59:32.640<br>
他就會 output 一張 image 給你<br>
<br>
0:59:32.640,0:59:35.080<br>
那你現在可以在那個<br>
<br>
0:59:35.080,0:59:40.580<br>
input 的 space 上調你的 vector，去產生不同的 output<br>
<br>
0:59:40.580,0:59:43.780<br>
所以，他說他先 random 找幾個 vector<br>
<br>
0:59:43.780,0:59:45.460<br>
random 找 5 個 vector<br>
<br>
0:59:45.460,0:59:48.580<br>
產生 5 張房間的圖<br>
<br>
0:59:48.580,0:59:56.460<br>
接著，再從這一個點移動你的這個 vector 到這個點這樣<br>
<br>
0:59:56.460,0:59:59.020<br>
所以，就發現說你的 image 逐漸地變化<br>
<br>
0:59:59.020,1:00:01.020<br>
逐漸的變化，然後跑到這個點<br>
<br>
1:00:01.020,1:00:03.900<br>
然後再逐漸的變化，再跑到這個點<br>
<br>
1:00:03.900,1:00:06.280<br>
你會發現一些有趣的地方，比如說，這邊有一個窗戶<br>
<br>
1:00:06.280,1:00:09.200<br>
它慢慢的就變成一個類似電視的東西<br>
<br>
1:00:09.200,1:00:10.620<br>
或是這邊有一個電視<br>
<br>
1:00:10.620,1:00:13.700<br>
它慢慢的就變成了窗戶這樣子<br>
<br>
1:00:13.700,1:00:15.940<br>
我覺得最驚人結果<br>
<br>
1:00:15.940,1:00:19.160<br>
是有人，有日本人他用 GAN<br>
<br>
1:00:19.160,1:00:22.200<br>
很神奇的，很神奇的東西<br>
<br>
1:00:22.200,1:00:23.700<br>
就傳說中，你只要能夠<br>
<br>
1:00:23.700,1:00:25.620<br>
一旦你能夠成功使用他<br>
<br>
1:00:25.620,1:00:27.360<br>
他就可以召喚不可思議的力量<br>
<br>
1:00:27.360,1:00:31.200<br>
但是，大部分的時候，你都沒有辦法成功的召喚它<br>
<br>
1:00:31.200,1:00:33.920<br>
它有點像是神之卡的感覺這樣<br>
<br>
1:00:33.920,1:00:37.640<br>
你只要能夠操控那個神，就可以獲得不可思議的力量<br>
<br>
1:00:37.640,1:00:39.840<br>
他大部分的時候你都無法操控他<br>
<br>
1:00:39.840,1:00:41.600<br>
昨天晚上我想說，我可不可以自己<br>
<br>
1:00:41.600,1:00:43.480<br>
generate 一些寶可夢<br>
<br>
1:00:43.480,1:00:46.780<br>
弄到 5 點我搞不起來，所以後來我想我還是去睡好了<br>
<br>
1:00:46.780,1:00:51.160<br>
就很麻煩<br>
<br>
1:00:51.160,1:00:56.600<br>
因為，它最大的問題就是你沒有一個很明確的 signal<br>
<br>
1:00:56.600,1:00:59.600<br>
它可以告訴你說，現在的 Generator<br>
<br>
1:00:59.600,1:01:01.460<br>
到底做的怎麼樣<br>
<br>
1:01:01.460,1:01:05.480<br>
沒有一個很明確的 signal 可以告訴你這件事<br>
<br>
1:01:05.480,1:01:07.940<br>
在一個 stander NN 的 training 裡面<br>
<br>
1:01:07.940,1:01:10.180<br>
你就看那一個 loss，loss 越來越小<br>
<br>
1:01:10.180,1:01:12.280<br>
代表說現在 training 越來越好<br>
<br>
1:01:12.280,1:01:16.620<br>
但是，在 GAN 裡面，你其實要做的事情是<br>
<br>
1:01:16.620,1:01:21.340<br>
keep 你的 Generator 跟 Discriminator，<br>
他們是 well match 的<br>
<br>
1:01:21.340,1:01:24.220<br>
他們必需要不斷屬於一種競爭的狀態<br>
<br>
1:01:24.220,1:01:26.740<br>
他們必須要不斷處於可以<br>
<br>
1:01:26.740,1:01:28.800<br>
他們要像塔史亮跟進藤光一樣<br>
<br>
1:01:28.800,1:01:31.740<br>
不斷處於這種勢均力敵的狀態<br>
<br>
1:01:31.740,1:01:33.880<br>
他們必須要成為對手<br>
<br>
1:01:33.880,1:01:36.460<br>
那個第三堂課的時候<br>
<br>
1:01:36.460,1:01:40.720<br>
會請作業一、二、三做得特別好的同學來分享一下<br>
<br>
1:01:40.720,1:01:42.520<br>
他是怎麼做的<br>
<br>
1:01:45.720,1:01:48.520<br>
作業三就有人用 GAN<br>
<br>
1:01:48.520,1:01:50.280<br>
所以，代表是有人有做起來<br>
<br>
1:01:56.800,1:02:01.120<br>
那這很麻煩，因為在 GAN 裡面<br>
<br>
1:02:01.120,1:02:04.600<br>
你要讓 Discriminator 跟 Generator<br>
<br>
1:02:04.600,1:02:07.660<br>
他們一直維持一種勢均力敵的狀態<br>
<br>
1:02:07.660,1:02:09.900<br>
所以，你必須要用不可思議的平衡感<br>
<br>
1:02:09.900,1:02:15.220<br>
來調整這兩個 Discriminator 跟 Generator 的參數<br>
<br>
1:02:15.220,1:02:17.860<br>
讓他們一直處於勢均力敵的狀態<br>
<br>
1:02:17.860,1:02:22.940<br>
今天這個其實很像是在做 Alpha Go 一樣<br>
<br>
1:02:22.940,1:02:24.240<br>
你有兩個 agent<br>
<br>
1:02:24.240,1:02:26.760<br>
然後，你要讓他們一直是處於一樣強的狀態<br>
<br>
1:02:26.760,1:02:29.880<br>
當今天你的 Discriminator fail 的時候<br>
<br>
1:02:29.880,1:02:33.820<br>
因為我們最後 training 的終極的目標<br>
<br>
1:02:33.820,1:02:36.080<br>
是希望 Generator 產生出來的東西<br>
<br>
1:02:36.080,1:02:38.180<br>
是 Discriminator 完全無法分別的<br>
<br>
1:02:38.180,1:02:42.380<br>
就是 Discriminator 在鑑別真的或假的 image 上面<br>
<br>
1:02:42.380,1:02:44.280<br>
它的正確率是 0<br>
<br>
1:02:44.280,1:02:48.420<br>
但是，往往當你發現你的 Discriminator 整個 fail 掉的時候<br>
<br>
1:02:48.420,1:02:52.360<br>
並不代表說 Generator 真的 generate 很好的 image<br>
<br>
1:02:52.360,1:02:56.040<br>
往 你遇到的狀況是你的 Generator 太弱<br>
<br>
1:02:56.040,1:02:59.520<br>
那很多時候，我在 train 的時候還會遇到的狀況<br>
<br>
1:02:59.520,1:03:04.400<br>
就是 Generator 它不管 input 什麼樣的 vector<br>
<br>
1:03:04.400,1:03:07.480<br>
它 output 都給你一張非常像的東西<br>
<br>
1:03:07.480,1:03:09.700<br>
那一張非常像的東西<br>
<br>
1:03:09.700,1:03:11.840<br>
不知道怎麼回事就騙過了 Discriminator<br>
<br>
1:03:11.840,1:03:13.960<br>
那個就是 Discriminator 的罩門<br>
<br>
1:03:13.960,1:03:15.880<br>
它無法分辨那一張 image<br>
<br>
1:03:15.880,1:03:18.740<br>
那它整個就 fail 掉了，但並不代表說你的 machine<br>
<br>
1:03:18.740,1:03:21.400<br>
真的得到好的結果<br>
<br>
1:03:21.400,1:03:24.680<br>
我要說的大概就是這樣<br>
後面是一些 reference 給大家參考<br>
<br>
1:03:24.680,1:03:30.580<br>
臺灣大學人工智慧中心<br>
科技部人工智慧技術暨全幅健康照護聯合研究中心<br>
http://aintu.tw<br>
