# ML Lecture 16: 

##### Unsupervised Learning - Generation

> 臺灣大學人工智慧中心
> 科技部人工智慧技術暨全幅健康照護聯合研究中心
> http://aintu.tw

### Generative Models

- Component-by-component (PixelRNN)
- Variational Auto-encoder (VAE)
- Generative Adversarial Network (GAN)

上述方法都很新，皆是都是近幾年提出的。



### Component-by-component

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_5.jpg" width="70%">

將圖片攤平，用 RNN 以之前的 pixel (RGB三圍向量)去 predict 下一個 pixel，把整張圖畫出來 (unsupervised)

不只用在圖片，還可以用在語音，像是WaveNet。也可以用在影片：給定一段 video，讓他predict 接下來會發生甚麼事。

### Auto-encoder

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_15.jpg" width="70%">

- Input: image  =>  Encode: low dimension code  =>  Decode: Image
  - 讓 Input & Output兩張圖越接近越好
- Given random code => Decode: Image?
  - 效果差

### Variance Auto-encoder

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_18.jpg" width="70%">

- 比起 Auto-encoder，加了小 trick：不直接 output code，而是先 output 兩個 vector，再與random出來的 Vector 做如圖的運算，當作 code。
- 目的是：minimize reconstruction error
- 雖然結果沒有 PixelRNN 清晰，但 code 的每一個 dimension 代表特定意思
- 也可以用來寫詩，將 IO 從 Image 換成 Sentence



### Generative Adversarial Network (GAN)

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_41.jpg" width="70%">

目的：從 Random 到特定的 Distribution。 

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_42.jpg" width="70%">

Generator負責產出與 Target 相同 Distribution 的 Vector。

Discriminator負責分辨真偽 ( 真正的 Target & Generator 生的 Target )

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_43.jpg" width="70%">

將兩個 Model 一起訓練，然後去 Minimize JS Divergence。

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_44.jpg" width="70%">

以 Gradient Descent 去 Update。



### WGAN

Using Wasserstein distance instead of JS divergence 

GAN的進化版，原始的GAN在變好的過程中， JS divergence 不會逐漸著這變小，比較難train。

| GAN                                                          | WGAN                                                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| <img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_54.jpg" width="70%"> | <img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_55.jpg" width="70%"> |

當變得更好時，得到的 JS divergence 越低



### Text To Image

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_60.jpg" width="70%">

### Cycle GAN

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_65.jpg" width="70%">



### Disco GAN

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_66.jpg" width="70%">



### GAN 的各種加強版

<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Deep_Generative_Model/Unsupervised_Learning-Deep_Generative_Model_68.jpg" width="70%">

---

臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
http://aintu.tw