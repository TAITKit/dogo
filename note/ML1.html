<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>ML1</title></head>
<body><h1>ML Lecture 1: Regression - Case Study</h1>
<blockquote><p>臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 <a href='http://ai.ntu.edu.tw/'>http://ai.ntu.edu.tw</a></p>
</blockquote>
<h3>Regression 的應用</h3>
<ul>
<li><p>股票預測</p>
<blockquote><p>Input：過去各股票起伏的資料及各公司的資料 $\Longrightarrow$ Output：預測明天的道瓊工業指數</p>
</blockquote>
</li>
<li><p>自駕車</p>
<blockquote><p>Input：車子各個 sensor 感應到的資料 $\Longrightarrow$ Output：方向盤的角度</p>
</blockquote>
</li>
<li><p>推薦系統：</p>
<blockquote><p>Input：一位使用者 A 和商品 B $\Longrightarrow$ Output：使用者 A 購買 B 的可能性</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_1.jpg" width="70%"></p>
<hr />
<h3>預測寶可夢的 cp 值</h3>
<ul>
<li><p>原因：如果可以預測寶可夢進化後的 cp 值，就可以知道抓到之後，要進化他還是犧牲他，做最有效的利用</p>
</li>
<li><p>期待：Input 一隻寶可夢 $\Longrightarrow$ Output 進化後的 cp 值</p>
</li>
<li><p>定義：寶可夢的各數值</p>
<blockquote><p>$x$ 代表「寶可夢」、$x_s$ 代表屬於哪個「物種」、$x_{hp}$ 代表「生命值」</p>
<p>$x_w$ 代表「重量」、$x_h$ 代表「高度」、$x_{cp}$ 表示「CP值」、$y$ 表示「進化後的 CP 值」</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_2.jpg" width="70%"></p>
<h4>Step 1. 找合適的 model</h4>
<ul>
<li><p>假設 $y=b+w*x_{cp}$</p>
<blockquote><p>$y$ 為進化後的 cp 值、$b$ 為一常數、$w$ 為一數值、$x_{cp}$ 為進化前的 cp 值</p>
</blockquote>
<p>$w,\ b$ 可填入不同的數字，得到無窮多不同的 function，例</p>
<blockquote><p>$f_1: y=10.0+9.0*x_{cp}$
$f_2: y=9.8+9.2*x_{cp}$
$f_3: y=-0.8-1.2*x_{cp}$ ......</p>
</blockquote>
<p>再由 training data 判斷哪個是合理的 function</p>
</li>
<li><p>Linear model：$y=b+\Sigma w_ix_i$</p>
<blockquote><p>$x_i$ 為 input 寶可夢的各種不同屬性，是從 input object 裡面抽出的數值，稱之為 <strong>feature</strong></p>
<p>$w_i$ 和 $b$ 分別為 weight 和 bias</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_3.jpg" width="70%"></p>
<h4>Step 2. 判斷 function 的好壞</h4>
<ol start='' >
<li><p>收集 training data：</p>
<ul>
<li>此為 <u>supervised learning</u> 的 task：收集 function 的 <u>input 和 output</u></li>
<li>此為 <u>regression</u> 的 task：function 的 <u>output 是一個數值</u></li>

</ul>
<blockquote><p>例：</p>
<p>傑尼龜以 $x^1$ 表示，進化後卡咪龜的 cp 值 <strong>979</strong>，為 $x^1$ 的 output，以 $\hat{y^1}$ 表示（$\hat{y^1}=979$）</p>
<p>伊布以 $x^2$ 表示，進化後雷精靈的 cp 值 <strong>1420</strong>，就是 $\hat{y^2}$</p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_4.jpg" width="70%"></p>
<ul>
<li>收集 10 隻神奇寶貝：$(x^1,\hat{y^1}),(x^2,\hat{y^2}),...,(x^{10},\hat{y^{10}})$
以「進化前的 cp 值」為橫軸，「進化後的 cp 值」為縱軸繪圖，如下圖所示</li>

</ul>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_5.jpg" width="70%"></p>
<ol start='2' >
<li><p>判斷 function 好壞：</p>
<ul>
<li><p><strong>Loss function ($L$)</strong> 定義：Input 一個 function, $f$，Output 這個 function 的好壞，以數值表示</p>
<p>$L(f)=L(w,b)$：衡量 function 的好壞亦即衡量一組參數的好壞，因為一個 function 是由裡面的兩個參數 $w,b$ 所決定</p>
</li>
<li><p><strong>定義 $L$</strong>：$L(f)=\Sigma^{10}_{n=1}(\hat{y^n}-f(x^n_{cp}))$</p>
<blockquote><p>將各組 $w,b$ 實際代入 $y=b+w*x_{cp}$ 中，得預測的 $y$ 值, $f(x^n_{cp})$</p>
<p>$\hat{y^n}-f(x^n_{cp})$ 即為估測誤差，最後 sum 起來，就是 loss function</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_6.jpg" width="70%"></p>
<ul>
<li><p>Loss function 作圖：</p>
<ul>
<li>每一個點代表一組 $(w,b)$</li>
<li>顏色代表根據 loss function 判斷的這個 function 的好壞；紅色代表誤差越大，藍色代表誤差越小、function 越好</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_7.jpg" width="70%"></p>
</li>

</ol>
<h4>Step 3. 選出 Best Function</h4>
<ul>
<li><p><strong>列式</strong>：</p>
<ul>
<li>找最佳 function：$f^*=arg\ min_fL(f)$ </li>
<li>找最佳參數：$w^*, b^*=arg\ min_{w,b}L(w,b)\ =\ arg\min_{w,b}\Sigma^{10}_{n=1}(\hat{y^n}-(b+w*x^n_{cp})^2$ </li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_8.jpg" width="70%"></p>
</li>
<li><p><strong>Gradient Descent</strong>：</p>
<p><em>單一參數</em></p>
<ol start='' >
<li><p>隨機選取一個初始點 $w^0$</p>
</li>
<li><p>對 $w^0$ 作微分, 亦即計算 $\frac{dL}{dw}|_{w=w^0}$</p>
</li>
<li><p>根據計算出的值</p>
<blockquote><p>負 $\Rightarrow$ 增加 $w$ 的值</p>
<p>正 $\Rightarrow$ 減少 $w$ 的值</p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_9.jpg" width="70%"></p>
</li>
<li><p>更新 $w^0$ 的值：$w^1 \larr\ w^0-\eta \frac{dL}{dw}|_{w=w^0}$</p>
<blockquote><p>(1) 取決於 $\frac{dL}{dw}$ 的值：$\frac{dL}{dw}$ 越大，更新的距離就越大；反之，則越小</p>
<p>(2) 取決於 learning rate ($\eta$)：$\eta$ 是一個事先決定好的值，$\eta$ 越大則參數每次更新的幅度就越大；反之，則越小</p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_10.jpg" width="70%"></p>
</li>
<li><p>不斷重複步驟 4.，會走到 <strong>local minimum</strong> (微分是 0 的地方)，就卡住了</p>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_11.jpg" width="70%"></p>
<p><em>兩個參數</em>, <em>Gradient</em>, $\nabla L =\left[\begin{matrix}\frac{∂L}{∂w}\\\frac{∂L}{∂b}\end{matrix}\right]$</p>
<ol start='' >
<li><p>隨機選取兩個初始值 $w^0, b^0$</p>
</li>
<li><p>計算 $\frac{dL}{dw}|_{w=w^0, b=b^0}, \frac{dL}{db}|_{w=w^0, b=b^0}$</p>
</li>
<li><p>不斷更新 $w, b$</p>
<p>例：  $w^1 \larr\ w^0-\eta \frac{dL}{dw}|_{w=w^0,b=b^0}$,   $b^1\larr\ b^0-\eta \frac{dL}{dw}|_{w=w^0,b=b^0}$</p>
</li>
<li><p>最後，找到一組 loss 相對較小的 $w$ 跟 $b$ 值，就結束了</p>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_12.jpg" width="70%"></p>
<p><em>Visualization</em></p>
<ol start='' >
<li>最左下角的紅點代表一開始隨機選的初始值</li>
<li>計算偏微分乘上$\eta$， 偏微分的方向即為等高線的法線方向，不斷更新參數</li>
<li>圖上的顏色代表 loss function 的數值，越偏藍色代表 loss 越小，最後走到圖中間的地方</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_13.jpg" width="70%"></p>
</li>

</ul>
<p><em>實際計算</em> $\frac{∂L}{∂w}$, $\frac{∂L}{∂b}$</p>
<blockquote><p>$L(w,b)=\Sigma^{10}_{n=1}(\hat y^n-(b+w*x^n_{cp}))^2$</p>
<p>$\frac{∂L}{∂w}=\Sigma^{10}_{n=1}\ 2(\hat y^n-(b+w*x^n_{cp}))(-x^n_{cp})$</p>
<p>$\frac{∂L}{∂b}=\Sigma^{10}_{n=1}\ 2(\hat y^n-(b+w*x^n_{cp}))$</p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_17.jpg" width="70%"></p>
<h4>How&#39;s the Result ?</h4>
<ul>
<li><p>圖中紅色的線是 function, $y=b+w*x_{cp}$，每一個藍色的點是 training data</p>
<p>error 就是兩者之間的「距離」，此時，平均誤差為 31.9（如下圖）</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_19.jpg" width="70%"></p>
<ul>
<li><p>我們關心的是 <strong>testing data</strong> 上的誤差（Gereralization 的 case）</p>
<p>計算 testing data 和紅線的距離，得 35.0，比 training data 上的誤差大一點</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_20.jpg" width="70%"></p>
<ul>
<li><p>testing data 的誤差<strong>有沒有可能可以更小呢</strong>？</p>
<ul>
<li><p>觀察：發現 cp 值特別小及 cp 值特別大的地方，誤差比較大</p>
</li>
<li><p>推想：最好的 function 可能不是一直線，可能稍微複雜一點，可能需要引入<strong>二次式</strong></p>
</li>
<li><p>設計：將 model 重新設計為 $y=b+w_1*x_{cp}+w_2*(x_{cp})^2$，引入 $(x_{cp})^2$ 這一項</p>
</li>
<li><p>重複上面的 Step 2. 及 Step 3.，判斷 function 的好壞 以及找出最佳的 function</p>
<p>$b = -10.3,\ w_1=1.0,\ w_2=2.7*10^{-3}$，training data 的誤差為 $15.4$，testing data 的誤差為 $18.4$（更小了）</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_21.jpg" width="70%"></p>
</li>
<li><p>同樣的問題：有沒有可能<strong>讓誤差更小</strong>呢？</p>
<ul>
<li>重複上述步驟，引入 $(x_{cp})^3$，得到下圖的結果，誤差為 $18.1$，誤差又更小了，那更複雜的 model 呢？</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_22.jpg" width="70%"></p>
<ul>
<li>重複上述步驟，引入 $(x_{cp})^4$，得到下圖的結果，誤差為 $28.8$，反而變大了！？</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_23.jpg" width="70%"></p>
<ul>
<li>重複上述步驟，引入 $(x_{cp})^5$，得到下圖結果，誤差為 $232.1$，結果又更糟，完全爛掉了。</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_24.jpg" width="70%"></p>
</li>
<li><p>分析上述的五個 function</p>
<ul>
<li><p>將五個 function 在 <strong>training data</strong> 的 average error 畫在圖上，如下圖所示</p>
<p>則「<u><strong>越複雜 的 model，training data 上的 error 越小</strong></u>」</p>
<blockquote><p>因為越複雜的 funciton，會包含上一層較簡單的式子，所以理論上，function 越複雜，error 越低</p>
</blockquote>
</li>
<li><p>但，要在 <strong><u>gradient descent 能夠幫我們找出 best function</u></strong> 的 <strong>前提下</strong> </p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_25.jpg" width="70%"></p>
<ul>
<li><p>在 testing data 上，<strong><u>model 越複雜，error 越低</u></strong>在 <u>第三個 function</u> 之前是對的</p>
<p>但是，第四個 function 開始，error 暴增</p>
</li>
<li><p>所以，我們得到一個觀察</p>
<p><strong><u><em>越複雜的model  可以在training data上面給我們越好的結果；但並不一定能在testing data上給我們越好的結果。</em></u></strong></p>
<p>這件事就稱為 &quot;<u><strong><em>overfitting</em></strong></u>&quot;</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_26.jpg" width="70%"></p>
</li>

</ul>
<h4>Collecting more data</h4>
<ul>
<li><p>當我們收集到 60 隻寶可夢的時候，會發現前面白忙一場，中間有一些 hidden factor 在影響進化後的 cp 值</p>
<p>其實就是寶可夢的<strong>「物種」</strong>，進化後的 cp 值受物種的影響其實很大，不應只考慮進化前的 cp 值</p>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_28.jpg" width="70%"></p>
</li>

</ul>
<h4>Back to Step1. 重新設計 function set</h4>
<ul>
<li>設計 $x_S$ 代表 input 寶可夢的<strong>物種</strong>，不同的物種用不同的式子代表，如下圖所示</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_29.jpg" width="70%"></p>
<ul>
<li><p>再將式子<strong>改寫</strong>成一個 <strong>linear function</strong>，$y=b+\Sigma w_ix_i$
藍色框框內代表：當 $x_s=物種\ i$ 時，其對應到的 $b_i, w_i$ 值不為 0；其他的參數值為 0
例：$x_s$=Pidgey 時，$y=b_1+w_1*x_{cp}$</p>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_30.jpg" width="70%"></p>
</li>

</ul>
<h4>How&#39;s the result?</h4>
<ul>
<li><p>在 training data 上，不同種類的寶可夢 model 是不一樣的（如下圖，紅色是伊布，藍色是比比鳥）</p>
<p>所以，我們的 model 在 training data 上可以得到更低的 error</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_31.jpg" width="70%"></p>
<ul>
<li><p>在 testing data 上，表現的比剛才 18 點多的 error 更好了，但再觀察一下，感覺有些東西可以做得更好</p>
<p>有一些略高略低於藍色綠色線的值，這些 <strong>difference 可能來自哪裡呢</strong>？</p>
</li>
<li><p>可能來自 weight, HP, Height......</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_32.jpg" width="70%"></p>
<h4>Back to Step1. again</h4>
<ul>
<li><p>重新設計 model，將所有有可能的參數都塞到 function 裡面</p>
<blockquote><p>$y=y'+w_9*x_{hp}+w_{10}*x_{hp}^2+w_{11}*x_h+w_{12}*x_h^2+w_{13}*x_w+w_{14}*x_w^2$</p>
</blockquote>
</li>
<li><p>training set 上得到很好的結果：1.9，但是，testing set 上得到102.3（overfitting），壞掉了</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_33.jpg" width="70%"></p>
<h4>Back to Step2. Regularization, 重新定義我們的 loss function</h4>
<ul>
<li><p>我們的 model：$y=b+\Sigma w_ix_i$</p>
<p>原始的 loss function：$L=\Sigma_n(\hat y^n-(b+\Sigma w_ix_i))^2+\lambda\Sigma(w_i)^2$</p>
<p>只考慮了 <u>prediction 的 error</u> </p>
</li>
<li><p>Regularization 再加上一項，$\lambda \Sigma(w_i)^2$</p>
<p>這項代表，我們期待 $w_i$ 的值越小越好。如此一來，function 會越平滑（亦即 output 對輸入的變化較不敏感）</p>
<p>較不 sensitive 的 function，較不易受雜訊干擾，受到的影響較小，結果較好</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_34.jpg" width="70%"></p>
<ul>
<li><p>做實驗，將 $\lambda$ 的值由 0, 1, 10, 100....增加到 100000</p>
<ul>
<li>$\lambda$ 值越大，代表考慮 smooth 這項的影響力越大，得到的 function 就越平滑</li>
<li>$\lambda$ 值越大，training data 上得到的 error 越大（因為傾向考慮 w 的值而減少考慮 error）</li>
<li>$\lambda$ 值越大，testing data 上的 error 可能會變小 ($\lambda = 100$)，但是 $\lambda$ 太大時，error 又會變大 ($\lambda=1000$)</li>

</ul>
<p>所以，我們必須調整 $\lambda$ 來決定 function 的平滑程度，找到最小的 testing error</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_35.jpg" width="70%"></p>
<hr />
<h3>Conclusion</h3>
<ul>
<li><p>「寶可夢進化後的cp值」跟「進化前的cp值」以及是哪個「物種」，是非常有關係的，後兩項幾乎能決定進化後的cp值</p>
<p>但可能還有一些 hidden factor</p>
</li>
<li><p>我們有提到 Gradient Descent 及 Regularization，後面的課程會再詳細的說明</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Regression/Regression_36.jpg" width="70%"></p>
<p>&nbsp;</p>
</body>
</html>