# ML Lecture 8-1: "Hello World" of Deep Learning

> 臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 [http://ai.ntu.edu.tw](http://ai.ntu.edu.tw/)

### Toolkit Introduction

- **Tensorflow** & **Theano**：
  都非常 Flexible，需要花較多的時間去熟悉使用，故課程堂中介紹的是另一個 toolkit

- **Keras**：
  其實是 Tensorflow 跟 Theano 的 interface，操作上較簡單，多有現成的 function 可以 call

  > Documentation：http://keras.io/
  > Example：https://github.com/fchollet/keras/tree/master/examples 



### "Hello World" (First Program of Deep Learning)

---

#### Problem Definition

- 範例：Handwriting Digit Recognition
- Data：**MNIST**,  https://yann.lecun.com/exdb/mnist/
  Keras 提供自動下載 MNIST data 的 function, https://keras.io/datasets/
- Input：一張 image，Size: 28*28
  Output：這張 image 是 0~9 的哪個數字

<img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_4.jpg" width="70%">

---

#### Implementation

1. **Define a set of function**：

   ```python
   model = Sequential( )
   # 宣告一個 model
   ```

   ```python
   model.add( Dense( input dim = 28*28, output dim = 500))
   # Dense  -> Fully Connected Layer
   # Input  -> 28*28 vector
   # Output -> 500 neuron
   ```

   ```python
   model.add( Activation('sigmoid') )
   # activation function -> Sigmoid function
   # activation function 也可是 softplus, softsign, relu, tanh, linear...等
   ```

   ```python
   model.add( Dense(output dim = 500))
   model.add( Activation('sigmoid') )
   # 再加一層新的 layer，其 input 即上一層的 output，因此只需要定義 output 就好
   ```

   ```python
   model.add( Dense(output dim = 10))
   model.add( Activation('softmax') )
   # 最後做分類要 output 10 個數字，故最後 output 設 10 維
   ```

   <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_5.jpg" width="70%">

2. **Goodness of function**：

   ```python
   Model.compile( loss = "categorical_crossentropy", 
   							 optimizer = 'adam', metrics = ['accuracy'])
   # 定義要使用的 loss function，這裡用 cross entropy
   # categorical_crossentropy 即為 cross entropy
   ```

   <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_6.jpg" width="70%">

3. **Pick the best function**：

   <3.1> Configuration

   ```python
   model.compile( loss = "categorical_crossentropy", 
   							 optimizer = 'adam', metrics = ['accuracy'])
   # 決定 optimizer(要 用什麼方式，找出最好的function)，這裡使用 adam
   # 亦可用 SGD, RMSProp, Adagrad, Adamax...等
   ```

   <3.2> Find the optimal network parameters

   ```python
   model.fit(x_train, y_train, batch_size=100, nb_epoch=20)
   # 第一個參數(x_train)是給 training data(Images)
   # 第二個參數(y_train)是給 Label(0~9的哪個數字)
   # 這兩個參數的存法(numpy array)如下圖所示
   ```

   <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_8.jpg" width="70%">

   - **Batch** & **Epoch**：

     - 做 Deep Learning 時，會將 training data 隨機的選 x 個放進一個 batch，x 即為 batch_size

     - 不斷地 pick batch，直到所有的 mini batch 都被 update 一次，即為一個 epoch

       nb_epoch 就是重複 update 幾次 epoch

       > 本例中， batc_size = 100， nb_epoch = 20
       > 1 個 epoch 中會 update 100 次參數，20 個 epoch 即總共會 update 2000 次參數

     <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_10.jpg" width="70%">ㄨ

4. **Save & Load Model**

   > https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model

   <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_14.jpg" width="70%">

5. **Testing**

   <case 1> 

   ```python
   score = model.evaluate(x_test, y_test)
   print('Total loss on Testing Set:', score[0])
   print('Accuracy of Testing Set:', score[1])
   # 有一組 testing set，也知道該 testing set 的答案
   # 可使用 Keras 算出目前的正確率是多少
   ```

   <case 2>

   ```python
   result = model.predict(x_test)
   # 無 labeled data，直接 output 出結果
   ```

   

---

#### Speed Comparison（實作上的比較）

- **Stochastic Gradient Descent** (Batch size = 1) v.s. **Mini-batch** (Batch size > 1)

- 如下圖，實際在 GTX980 跑 MNIST 的 50000 個 examples 時
  >batch size = 1：一個 epoch 跑166s
  batch size = 10：一個 epoch 跑 17s

  亦即，batch size 越大，每算一個 epoch 的時間越快(短)
  故，在同樣時間內，**參數 <u>update的數目幾乎相同</u> **
  故，選擇 <u>**batch size 較大**</u>者，因為較 <u>**穩定**</u>
  <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_11.jpg" width="70%">

  

- 那為何不將 batch size 設非常大呢？

  1. 硬體限制，GPU 將無法平行運算
  2. 容易陷入 saddle point 或 local minimum 中 (它的 error surface 是坑坑洞洞的)

- 以矩陣運算解釋，如下圖

  > Stochastic Gradient Descent：黃色和綠色 $z^1$ <u>**依序**</u>和 $W^1$ 做運算
  > Mini-batch：黃色和綠色 $z^1$ <u>**同時**</u>和 $W^1$ 做運算

  可明顯看出 Mini-batch 運算速度較快
  <img src="http://ai.ntu.edu.tw/aho/JPG/Hello_world_of_Deep_Learning/Hello_world_of_Deep_Learning_13.jpg" width="70%">