ML Lecture 12: Semi-supervised Learning

> 臺灣大學人工智慧中心
> 科技部人工智慧技術暨全幅健康照護聯合研究中心
> http://ai.ntu.edu.tw

## Introduction

- Supervised learning: $\{(x^r, \hat y^r)\}^R_{r=1}$
  - E.g. $x^r$: image, $\hat y^r$: class labels
- Semi-supervised learning: $\{(x^r, \hat y^r)\}^R_{r=1}, \{x^u\}^{R+U}_{u=R}$
  - A set of unlabeled data, usually $U >> R$
  - Transductive learning: unlabeled data 來自於 testing data
  - Inductive learning: unlabeled data 並非來自於 testing data

> 有人會說 Transductive learning 不算是 Semi-supervised learning，但教授覺得這個也算是一種 Semi-supervised learning，只是跟 Inductive learning 很不一樣

#### 原因

為什麼做 Semi-supervised learning 呢？

有人常常會說沒有 data，其實我們從來都不缺 data，我們只是缺有 labeled 的 data。

所以 labeled data 很少、unlabeled data 會很多，而 Semi-supervised learning 如果你可以利用這些 unlabeled data 來做某些事的話。

#### 例子

假設現在要做一個辨別貓狗的圖片分類器，同時有一大堆有關貓跟狗的圖片，而這些圖片大部分是沒有 label 的，如下圖：

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_3.jpg" width="70%">

假設我們只考慮有 label 的 data (藍點和橘點) 的話，那分界可能會很直覺地畫出圖上鉛直的紅線，但是假如納入 unlabeled data (灰點) 的分布，可能直覺就會讓人改成畫出圖上的斜線作為分界。

使用 unlabeled 的方式往往伴隨著一些假設，所以 semi-supervised learning 有沒有用就取決於假設符不符合實際，有可能假設不夠精確，左下角的灰點實際上是一隻狗，只是因為背景同樣是綠色而靠近貓 labeled 的 data，因此使用 semi-supervised learning 需要在正確的假設下。

## Generative Model

### Supervised learning

在 Supervised learning 中，有一堆訓練資料，它們的 label 都是已知的。

以下圖為例，已知分別屬於 class 1 或 class 2，在這個情況下去估測 class 1 和 class 2 的 prior probability，如果我們用的是 Gaussian distribution 的假設，那估測的就是，這個 class 1 是從 mean = μ^1^, covariance = Σ 的 Gaussian distribution，及 class 2 從 mean = μ^2^, covariance = Σ 的 Gaussian distribution。於是有一個新的 data 時，便可以計算它的 posterior probability 來做分類。

> 共用 Gaussian 的 covariance 可能會有較好的表現在之前已經提過了

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_6.jpg" width="70%">



### Semi-supervised

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_7.jpg" width="70%">

如果資料的分布情形如上圖所示，則之前 supervised 所得到的結果並不合理，實際的 covariance 應該要更接近圓圈的形狀，實際的 μ^2^ 應該在下方一點的位置，而非 labeled data 因為 sample 偏差的結果，此外根據 unlabeled data，class 2 的 prior probability 應該是較大的。

#### 實作方式

0. 初始化 $\theta = {P(C_1), P(C_2), \mu^1, \mu^2, \Sigma}$
1. 計算 unlabeled data 的 posterior probability
   $$
   P_\theta(C_1|x^u)
   $$
2. 更新 model
   $$
   P(C_1) = \frac{N_1 + \sum_{x^u}P(C_1|x^u)}{N} \\
    \mu^1 = \frac1{N_1} \sum_{x^r \in C_1} x^r + \frac1{\sum_{x^r}P(C_1|x^u)} \sum_{x^u} P(C_1|x^u)x^u
   $$
   $N$: total number of examples
   $N_1$: number of examples belonging to $C_1$

3. 再回到 step 1

其實這個做法就是所謂的 EM algorithm，step 1 是 E-step，step 2 是 M-step，這個做法保證了最後一定會收斂。

##### 原因

這個做法背後的理論是在最大化 (log) likelihood。在全部都是有標記的資料下，likelihood 如下所示，要最大化是可以直接做到的：
$$
\log L(\theta) = \sum_{x^r} \log P_\theta(x^r, \hat y^r) \\
P_\theta(x^r, \hat y^r) = P_\theta(x^r| \hat y^r)P(\hat y^r)
$$


而有 unlabeled data 的時候則如下：
$$
\log L(\theta) = \sum_{x^r} \log P_\theta(x^r, \hat y^r) + \sum_{x^u} \log P_\theta(x^u) \\
P_\theta(x^u) = P_\theta(x^u|C_1)P(C_1) + P_\theta(x^u|C_2)P(C_2)
$$
前半部其實就是 labeled data，後半部的 $P_\theta(x^u)$ 則是計算在 $C_1$ 的機率及 $C_2$ 的機率下，$x^u$ 出現的機率，相加後就是這筆 unlabeled data 出現的機率

不幸的是，這個式子它不是 convex，所以要用 EM algorithm 解，iterative 的去計算，最後會收斂在一個 local minimum。

## Low-density Seperation Assumption

Low-density separation 的假設是，在兩個 class 之間會有一個非常明顯的鴻溝 (蠻寬的分界)，訓練資料可能因為不足而沒有切分在適合的地方，此時利用 unlabeled data 來增加 performance。

### Self-training

Self-training 是最簡單直覺的方法，可以在許多模型上使用。

#### 作法

1. 利用 labeled data 訓練一個 model 稱之為 $f^*$
2. 用 $f^*$ 為 unlabeled data 標記 label (pseudo-label)
3. 將一部分的 unlabeled data 放進 labeled data set 中，再回頭訓練 $f^*$

> 如何挑選要放進 labeled data set 的 unlabeled data 是一個開放的問題。另外也可以改用權重的方式，給予每個 unlabeled data 一個權重。

> 稍微思考一下可以發現，這個作法並不適用於 regression。

#### 特性

和 generative model 是相似的，它們唯一的差別是在做 Self-training 的時候用的是 hard label，在做 generative model 的時候用的是 soft label。(給予一筆 data 對應的 label，或是給予它在不同 label 下的機率)

**在 neural network 的情況下，soft label 是無法使用的。**

以之前有兩個 class 做分類的例子而言，假設 NN 算出來的 class 的機率是 $\left[\begin{matrix}0.7\\0.3\end{matrix}\right]$，在 hard label 下會被轉換成 $\left[\begin{matrix}1\\0\end{matrix}\right]$，所以有不一樣的 data 再做一次訓練；但在 soft label 下，所做的就是把 $\left[\begin{matrix}0.7\\0.3\end{matrix}\right]$再放回去已經能得出這個結果的模型去訓練，因而無法得到任何進步。

因此在 low-density seperation 的假設下，neural network 需要用 hard label 的方式來做 semi-supervised learning。

### Entropy-based Regularization

另外一個方法考慮的是輸出結果的 distribution，期望在 unlabeled data 上能得到集中而非發散的分布，因為現在有著 low-density seperation 的假設。

計算一個分布集中程度的方式是 entropy，算法如下：
$$
E(y^u) = - \sum_{m \in \text{unlabeled}} y_m^u \ln(y_m^u)
$$
於是便在原本的 loss function 加上以 $\lambda$ 為權重的 entropy，變成 $L = \sum_{x^r}C(y^r, \hat y^r) + \lambda \sum_{x^u} E(y^u)$，使得訓練來自於 labeled data 的答對率及 unlabeled data 的分佈夠分散，由於 entropy 是可微分的，所以仍能適用梯度下降的方法 (gradient descent)。

由於 entropy 在此的角色像是以前提過的 regularization，因此被稱作為 Entropy-based Regularization。

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_13.jpg" width="70%">

### Outlook: Semi-supervised SVM

> SVM 是現在尚未講過的課題，因此在這邊只是作為一個參考。

SVM 的運作方式就是在兩個 class 的 data 中，找一個邊界 (margin) 最大的分隔線 (boundry)，讓這兩個 class 的資料分的越開越好，與此同時，它也要有最小的分類的錯誤。

在有一些 unlabeled data 的清況下，Semi-supervised SVM 的做法是窮舉所有可能的 label，接著對每一個可能的結果，都用 SVM 來訓練一個模型，最後，再去檢查哪一個 unlabeled data 的可能性在窮舉所有的可能的 label 裡面，可以讓邊界最大、同時又最小化 error。（如下圖）

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_14.jpg" width="70%">

窮舉所有 unlabeled data 的 label 直覺上會讓計算量爆炸到不可行的程度，例如有一萬筆 unlabeled data ，就需要窮舉 2 的一萬次方，根本毫無可能，於是上圖 reference 的 paper 就提出了一個很近似的方法。

基本精神是

它的基本精神是你一開始先得到一些 label，然後每次改一筆 unlabeled data 的 label，看看能否讓 objective function 變大，如果變大就繼續用這個 label。

## Smoothness Assumption

這個假設簡單來說就是：如果 x 是像的，那它們的 label y 也就像，這個假設聽起來沒有甚麼，而且

這麼講這個假設其實是不精確的，因為一個正常的 model 如果它不是很深的話，一個接近的 input

，自然會有接近的 output。

精確的假設應該是如此：

- x 的分布是不平均的。它在某些地方是很集中，某些地方又很分散。

- $x^1$ 和 $x^2$ 假如在一個集中的區域裡很接近，$x^1$ 的輸出 $\hat y^1$ 和 $x^2$ 的輸出 $\hat y^2$ 才會是接近的。

這麼說仍有些難懂，以下圖為例：雖然 $x^2$, $x^3$ 在距離上是接近的，但由於 $x^1$, $x^2$ 在同一個密度高的區域裡，因此在 Smoothness assumption 下，$x^1$, $x^2$ 有更高的機率是同一個 label。

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_17.jpg" width="70%">

這個方法在文件分類上可能非常有用。假設你現在要分天文學跟旅遊的文章，那天文學的文章有它固定的 word distribution，像是會出現 asteroid, bright，而旅遊的文章會出現 yellow stone 等等。如果今天你的 unlabeled data 跟你的 labeled data 是有重疊的，那很容易處理這個問題，但是在真實的情況下， unlabeled data 跟 labeled data 中間可能沒有任何重疊的 word，因為一篇文章的篇幅往往有限。

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_20.jpg" width="70%">

這種情形下，如果收集到夠多的 unlabeled data，就可以說 d1 跟 d5 相近，d5 又跟 d6 像，就可以一路 propagate 過去。

### Cluster and then Label

實踐 Smoothness assumption 最簡單的做法是 Cluster and then label，這個方法太簡單以至於沒什麼可以談的。做法是先將資料做分群 (clustering)，接著看各個 cluster 中哪個 label 的數量最多，便將該 cluster 的 unlabeled data 視作該個 label，最後拿這樣的資料去做訓練。

這個方法會發揮作用的假設就是你可以把同一個 class 的東西 cluster 在一起，可是在圖片裡面要把同一個 class 的東西 cluster 在一起其實是沒那麼容易的，像是以前在講為甚麼要用 deep learning 的時候，提過不同 class 可能會長得很像、同一個 class 可能會長得很不像。

>教授在此提及了用 Deep autoencoder 再做 Cluster and the label，但尚未教到 Deep autoencoder，因此先略過不提。

### Graph-based Approach

用 Graph 來表達也就是說，要想辦法資料點之間的 edge 建出來，有了這個 graph 以後，所謂的 high density path 的意思就是兩個點在這個 graph 上面是相連的，走得到的就是同一個 class。

怎麼建一個 graph 呢？

有些時候是很自然就可以得到，假設現在要做的是網頁的分類，而你有記錄網頁有網頁之間的 hyperlink，這就很自然地表示了網頁間是如何連結的；或者現在要做的是論文的分類，而論文和論文之間有引用的關係，這個關係式也是另外一種 graph 的 edge。

當然有時候，你需要自己想辦法建 graph，而 graph 的好壞對結果影響是非常大。不過這就非常的 heuristic （憑經驗），藉由經驗和直覺去選擇比較好的做法。

通常的做法是如此的：先定義兩個 object 之間怎麼算它們的<u>相似度</u>（例如影像就適合用 autoencoder 抽出來的 feature 算相似度），算完相似度以後就可以建 graph 了。

建 Graph 的方法有很多種：

- K nearest Neighbor
  假設令 k = 3，每一個 point 都跟它最近的、相似度最像的 3 個點做相連。

- e-Neighborhood
  每一個點只有跟它相似度超過某一個 threshold $e$ 的那些點做相連

所謂的 edge 也不是只有相連和不相連這樣二元的選擇而已，edge 可以擁有 weight 來表示被連接起來的兩個點之間的相似度。建議比較好的選擇是用 Gaussian Radial Basis Function 來計算相似度（如下式），$||x^i - x^j||^2$ 是指兩點間的 Euclidean distance，而取指數在此可以讓相似度在距離變遠時下降得更快，在經驗上能有更好的表現。
$$
s(x^i, x^j) = \exp\left(-\gamma ||x^i - x^j||^2\right)
$$
所以 graph-based 方法的精神是，假設有兩筆資料點是屬於 class 1，和他相鄰的點是 class 1 的機率也會上升，而這個機率會在圖上連通的區域傳遞下去。因此使用 graph-based 的方法需要有足夠的資料量，要是資料量不夠大，便有可能發生 label 無法 propagate 下去的可能。

#### Definition of Smoothness of the Labels on the Graph

接下來要定義在 Graph-based 的方法下，label 有多符合 Smoothness assumption 這個假設，我們的定義是
$$
S = \frac 1 2 \sum_{i, j} w_{i, j}(y^i - y^j)^2,
$$
也就是加總所有的 data pair 的相似度乘上他們 label 的差異，數值越小代表越 smooth（舉例如下圖）。

<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_25.jpg" width="70%">

這個式子能整理成更簡潔的形式，先將 $y^i$ 串成 $R+U$ 長的向量，而 $L$ 是一個 Graph Laplacian 矩陣，$W$ 是資料點間的相似度，$D$ 則是將 $W$ 每一行加總在對角線上。
$$
S = y^TLy \\
y = [\cdots y^i \cdots y^j \cdots]^T\\
L = D - W
$$
<img src="http://ai.ntu.edu.tw/aho/JPG/Semi-supervised_Learning/Semi-supervised_Learning_26.jpg" width="70%">

而這個 Smoothness function $S$ 就能放到訓練的 loss function，作為訓練時的 regularization。
$$
L = \sum_{x^r} C(y^r, \hat y^r) + \lambda S
$$


在 Deep neural network，smoothness 可以放在神經網路的任何地方，可以把某一個 hidden layer 接出來再乘上一些別的 transform，讓它要是 smooth 的，也可以每一個 hidden layer 的輸出都要是 smooth 的，這些做法都是可以的。