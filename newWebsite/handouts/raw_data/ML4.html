<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>ML 4</title></head>
<body><h1>ML Lecture 4: Classification</h1>
<blockquote><p>臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 <a href='http://ai.ntu.edu.tw/'>http://ai.ntu.edu.tw</a></p>
</blockquote>
<h3>Brief Introduction of &quot;Classification&quot;</h3>
<ul>
<li>Definition：找出一個 <strong><u>function</u></strong>，Input <strong><em>x</em></strong>，Output <strong><em>x</em></strong> 屬於 <strong><em>n 個 class 的哪一個</em></strong></li>
<li>Application：金融業決定是否核准貸款、醫療上診斷、手寫文字辨識、人臉辨識等</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_1.jpg" width="70%"></p>
<hr />
<h3>Example Application</h3>
<ul>
<li><p>Problem Definition：已知寶可夢有 18 種屬性，找出一個 function
		  Input 一隻寶可夢，Output 他是屬於哪一種屬性</p>
</li>
<li><p>Example：</p>
<blockquote><p>皮卡丘--&gt;雷、傑尼龜--&gt;水、妙蛙種子--&gt;草</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_2.jpg" width="70%"></p>
<hr />
<h3>How to Express a Pokemon？</h3>
<ul>
<li><p>將寶可夢 <strong>數值化</strong> 作為 輸入 </p>
</li>
<li><p>在寶可夢的電玩中，一隻寶可夢有許多特性，我們就以一組數字（一個vector）來描述他的特性</p>
<blockquote><p>總強度、生命值、攻擊力、防禦力、特殊攻擊力、特殊攻擊的防禦力、速度</p>
</blockquote>
</li>
<li><p>Example：</p>
<blockquote><p>皮卡丘 （320, 35, 55, 40, 50, 50, 90）</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_3.jpg" width="70%"></p>
<ul>
<li><p>為什麼要預測寶可夢的屬性？</p>
<p>在決鬥的時候，可能遇到圖鑑上沒有的寶可夢，這時可預測寶可夢的屬性，以相剋的屬性應戰。</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_4.jpg" width="70%"></p>
<p>																				（寶可夢屬性相剋表）</p>
<hr />
<h3>How to do Classification？</h3>
<ul>
<li><p>收集 data：例如將編號400以下的當作 training data，編號400以上的當作 testing data（共800隻的情況）</p>
</li>
<li><p>data 表示法，以 pair 表示</p>
<blockquote><p>例如：（皮卡丘、電）,（傑尼龜、水）,（妙蛙種子、草） </p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_6.jpg" width="70%"></p>
<ul>
<li><p>可將 Classification 當作 Regression 的問題硬解嗎？</p>
<ul>
<li><p>Case 1：若圖形分佈如左圖（$y=b+w_1*x_1+w_2*x_2$）
		</p>
<blockquote><p>Regression 的 output 為綠色為等於 0 的線，恰為 Class 1 及 Class 2 的分界線
==&gt; Regression 可得出和 Classification 相似的結果</p>
</blockquote>
</li>
<li><p>Case 2：圖形的右下角有一些 output 遠大於 1 的 error 的點</p>
<blockquote><p>此時，Regression 的 output 為紫色的線，但對 Classification 而言，綠色分界線才好
Regression 得出的結果 和 Classification 的結果相去甚遠</p>
</blockquote>
</li>
<li><p>結論：<u>Regression 定義function 好壞的方式對 Classification <strong>不適用</strong></u>，將無法得出好的結果</p>
</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_7.jpg" width="70%"></p>
<ul>
<li><p>Classification 理想解法：</p>
<blockquote><p>$f$ 為我們要找的 classification function(model)
$g$ 為 $f$ 中內建的一個 function
$L(f)$ 為 loss function，即 function $f$ 在 training data 上 predict 錯誤的次數總和</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_5.jpg" width="70%"></p>
<hr />
<h3>Solution（Probabilistic Perspective）</h3>
<ul>
<li>條件機率：如下圖所示，可輕易計算</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_8.jpg" width="70%"></p>
<ul>
<li><strong>將 Box 1, Box 2，換成 Class 1, Class 2</strong>
Input 一隻寶可夢，看他從哪個 class 來的機率最大，機率最大的 class 即為 Output</li>
<li><strong>Generative Model</strong>：
從 training data 估測出 $P(C_1)、P(x|C_1)、P(C_2)、P(x|C_2)$，有這 4 個值，即可算出 <u>每一個 x 出現的機率</u></li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_9.jpg" width="70%"></p>
<ul>
<li><p><strong>Prior</strong>：$P(C_1)、P(C_2)$ 的機率可稱作 Prior</p>
<blockquote><p>例如：class 1 為水系(79隻)、 class 2 為一般系(61隻)
==&gt;    $P(C_1) = 79/(79+61) = 0.56$
==&gt;    $P(C_2) = 61/(79+61) = 0.44$</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_10.jpg" width="70%"></p>
<ul>
<li><p><strong>Probability from Class</strong></p>
<p>每隻寶可夢都可以一個 <strong>vector</strong> 表示，這個 vector 又可稱之為 <strong>feature(特徵值)</strong></p>
<blockquote><p>例如：從水系神奇寶貝 sample 出一隻海龜的機率有多大？</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_11.jpg" width="70%"></p>
<ul>
<li><p>將 79 隻水系神奇寶貝依 <u><strong>防禦力</strong></u> 及 <strong><u>特殊防禦力</u></strong> 畫在二維平面上</p>
<blockquote><p><strong>轉化問題</strong>：
給一個新的點，代表的是沒有在 training data 裡面的寶可夢
從 Gaussian distribution 裡面 sample 出這個點的機率是多少？ </p>
<p>==&gt; <strong>給這 79 個點， 要怎麼找到那個 Gaussian distribution？</strong></p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_12.jpg" width="70%"></p>
<hr />
<h3>Gaussian distribution</h3>
<ul>
<li><p><strong>定義</strong></p>
<blockquote><p><strong>Input</strong>   ：vector x（代表一隻寶可夢的數值）
<strong>Output</strong>：機率（寶可夢被 s ample 出的機率）</p>
</blockquote>
<p>機率的分佈由以下兩者決定</p>
<blockquote><p><strong>Mean</strong> ($μ$)      ：是 vector
<strong>Variance</strong> ($Σ$)：是 matrix</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_13.jpg" width="70%"></p>
<p><strong>估算 Gaussian distribution</strong></p>
<blockquote><p>知道 <strong>$μ$</strong> 以及 <strong>$x$</strong>，就能將 function 估計出來
轉化問題 ==&gt; <strong>如何找 $μ$ 以及 $Σ$ 呢？</strong></p>
</blockquote>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_15.jpg" width="70%"></p>
<h3>Maximum Likelihood, $L(μ, Σ)$</h3>
<ul>
<li>給一組 Gaussian 的 <strong>$μ$ 及 $Σ$</strong>，算這個 Gaussian sample 出這 79 個點的機率
求出哪一個 Gaussian，sample 出這 79 個點的機率最大！</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_16.jpg" width="70%"></p>
<p>==&gt; 估計出 $μ^2$ 及 $Σ^2$ 後，就可以代回貝氏定理，做分類問題了</p>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_18.jpg" width="70%"></p>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_19.jpg" width="70%"></p>
<hr />
<h3>Modifying Model</h3>
<ul>
<li>遇到問題：
在 <strong>二維空間</strong> 的結果不甚好（47% 的正確率）
就提升到 <strong>高維空間</strong>，如六維（使用6個特徵值）可得到 64% 的正確率，<u><em>但仍不夠好</em></u></li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_20.jpg" width="70%"></p>
<ul>
<li><strong>共用 Covariance matrix</strong>
原因：Input 的 <strong>feature size</strong> 跟 <strong>covariance matrix 的平方</strong>成正比
當 feature size 越大，variance 就越大，<strong>容易 overfitting</strong>
==&gt; 所以強迫他們共用 covariance matrix，有效減少參數</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_21.jpg" width="70%"></p>
<ul>
<li><strong>Boundary 的改變</strong>
共用之前，boundary 是 <strong><u>曲線</u></strong>（non-linear model）
共用之後，boundary 變成 <strong><u>直線</u></strong>（linear model)
正確率：54% ----&gt; 73%</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_23.jpg" width="70%"></p>
<hr />
<h3>Three Steps of Probabilistic Model</h3>
<ol start='' >
<li><p><strong>Function Set（Model）</strong></p>
<p>Input x，將不同的 probability distribution 積分起來，就得到一個 model</p>
<blockquote><p>If  $P(C_1|x) > 0.5$,  output <strong><u>class 1</u></strong>
Else, output <strong><u>class 2</u></strong> </p>
</blockquote>
<ol start='' >
<li><strong>Goodness of a function</strong>
找一組可以「最大化產生資料集 likelihood」的 <strong>$(μ, Σ)$</strong></li>

</ol>
</li>
<li><p><strong>Find the best function</strong></p>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_24.jpg" width="70%"></p>
<hr />
<h3>Porbability Distribution</h3>
<ul>
<li>可以選擇任何你喜歡的機率模型，不一定要用 Gaussian distribution，可用 data set 決定</li>
<li><strong>$x$</strong> 是個<strong>一維向量</strong>，假設<strong>每一個 dimension</strong> 從 model 中被 generate 出來的機率是 <strong>Independent</strong> 的
這時，稱作使用 <strong>Naive Bayes Classifier</strong>
而 <strong>$P(x|C_1) = P(x_1|C_1)*P(x_2|C_1)*......*P(x_k|C_1)$</strong></li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_25.jpg" width="70%"></p>
<hr />
<h3>Posterior Probability</h3>
<ul>
<li><p>$P(C_1|x) = \frac{1}{1+exp(-z)} = σ(z)$，$σ(z)$ 為 $sigmoid function$</p>
<blockquote><p>$z$ 趨近 “正無窮大”，$σ(z)$ 趨近 1
$z$ 趨近 &quot;負無窮大&quot;，$σ(z)$趨近 0</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_26.jpg" width="70%"></p>
<ul>
<li><p><strong>z 的推導過程</strong></p>
<h5>$P(C_1|x) = σ(z)$</h5>
<h4>$z = ln\frac{P(x|C_1)*P(C_1)}{P(x|C_2)*P(C_2)} = ln\frac{P(x|C_1)}{P(x|C_2)}+ln\frac{P(C_1)}{P(C_2)}$</h4>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_28.jpg" width="70%"><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_29.jpg" width="70%">
<img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_30.jpg" width="70%"></p>
</li>
<li><p><strong>結論</strong></p>
<h5>$$P(C_1|x) = σ(w^T * b)$$，直接求出 $$w$$ 跟 $$b$$ 即可求解</h5>
<p>不需要捨近求遠算出 $N_1, N_2, μ^1, μ^2, Σ$ 再回來求 <strong>$w$</strong> 跟 <strong>$b$</strong> </p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Probabilistic_Generative_Model/Classification-Probabilistic_Generative_Model_32.jpg" width="70%"></p>
</body>
</html>