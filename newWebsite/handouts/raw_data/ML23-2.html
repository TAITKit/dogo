<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>ML23-II</title><link href='https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext' rel='stylesheet' type='text/css' /><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color: #ffffff; --text-color: #333333; --select-text-bg-color: #B5D6FC; --select-text-font-color: auto; --monospace: "Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857143; overflow-x: hidden; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; tab-size: 4; background-position: inherit inherit; background-repeat: inherit inherit; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; word-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
.typora-export .task-list-item input { pointer-events: none; }
@media screen and (max-width: 500px) { 
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; image-orientation: from-image; }
button, input, select, textarea { color: inherit; font-family: inherit; font-size: inherit; font-style: inherit; font-variant-caps: inherit; font-weight: inherit; font-stretch: inherit; line-height: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 4; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-top-left-radius: 4px; border-top-right-radius: 4px; border-bottom-right-radius: 4px; border-bottom-left-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right-width: 0px; background-color: inherit; }
.CodeMirror-linenumber { }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background-image: inherit; background-size: inherit; background-attachment: inherit; background-origin: inherit; background-clip: inherit; background-color: inherit; position: relative !important; background-position: inherit inherit; background-repeat: inherit inherit; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; background-position: 0px 0px; background-repeat: initial initial; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print { 
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; font-variant-ligatures: no-common-ligatures; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { break-after: avoid; }
  .typora-export #write::after { height: 0px; }
  .is-mac table { break-inside: avoid; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background-color: rgb(204, 204, 204); display: block; overflow-x: hidden; background-position: initial initial; background-repeat: initial initial; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
#write.first-line-indent p > .md-image:only-child:not(.md-img-error) img { left: -2em; position: relative; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-top-left-radius: 10px; border-top-right-radius: 10px; border-bottom-right-radius: 10px; border-bottom-left-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) { 
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background-color: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-top-left-radius: 3px; border-top-right-radius: 3px; border-bottom-right-radius: 3px; border-bottom-left-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; background-position: initial initial; background-repeat: initial initial; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; }
a.md-print-anchor { white-space: pre !important; border: none !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; text-shadow: initial !important; background-position: 0px 0px !important; background-repeat: initial initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom-width: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background-color: rgb(255, 255, 0); color: rgb(0, 0, 0); background-position: initial initial; background-repeat: initial initial; }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }
@media print { 
  .typora-export h1, .typora-export h2, .typora-export h3, .typora-export h4, .typora-export h5, .typora-export h6 { break-inside: avoid; }
  .typora-export h1::after, .typora-export h2::after, .typora-export h3::after, .typora-export h4::after, .typora-export h5::after, .typora-export h6::after { content: ""; display: block; height: 100px; margin-bottom: -100px; }
}


:root {
    --side-bar-bg-color: #fafafa;
    --control-text-color: #777;
}

@include-when-export url(https://fonts.loli.net/css?family=Open+Sans:400italic,700italic,700,400&subset=latin,latin-ext);

/* open-sans-regular - latin-ext_latin */
  /* open-sans-italic - latin-ext_latin */
    /* open-sans-700 - latin-ext_latin */
    /* open-sans-700italic - latin-ext_latin */
  html {
    font-size: 16px;
}

body {
    font-family: "Open Sans","Clear Sans", "Helvetica Neue", Helvetica, Arial, sans-serif;
    color: rgb(51, 51, 51);
    line-height: 1.6;
}

#write {
    max-width: 860px;
  	margin: 0 auto;
  	padding: 30px;
    padding-bottom: 100px;
}

@media only screen and (min-width: 1400px) {
	#write {
		max-width: 1024px;
	}
}

@media only screen and (min-width: 1800px) {
	#write {
		max-width: 1200px;
	}
}

#write > ul:first-child,
#write > ol:first-child{
    margin-top: 30px;
}

a {
    color: #4183C4;
}
h1,
h2,
h3,
h4,
h5,
h6 {
    position: relative;
    margin-top: 1rem;
    margin-bottom: 1rem;
    font-weight: bold;
    line-height: 1.4;
    cursor: text;
}
h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
    text-decoration: none;
}
h1 tt,
h1 code {
    font-size: inherit;
}
h2 tt,
h2 code {
    font-size: inherit;
}
h3 tt,
h3 code {
    font-size: inherit;
}
h4 tt,
h4 code {
    font-size: inherit;
}
h5 tt,
h5 code {
    font-size: inherit;
}
h6 tt,
h6 code {
    font-size: inherit;
}
h1 {
    font-size: 2.25em;
    line-height: 1.2;
    border-bottom: 1px solid #eee;
}
h2 {
    font-size: 1.75em;
    line-height: 1.225;
    border-bottom: 1px solid #eee;
}

@media print {
    .typora-export h1,
    .typora-export h2 {
        border-bottom: none;
        padding-bottom: initial;
    }

    .typora-export h1::after,
    .typora-export h2::after {
        border-top: 1px solid #eee;
        margin-top: .3em;
    }
}

h3 {
    font-size: 1.5em;
    line-height: 1.43;
}
h4 {
    font-size: 1.25em;
}
h5 {
    font-size: 1em;
}
h6 {
   font-size: 1em;
    color: #777;
}
p,
blockquote,
ul,
ol,
dl,
table{
    margin: 0.8em 0;
}
li>ol,
li>ul {
    margin: 0 0;
}
hr {
    height: 2px;
    padding: 0;
    margin: 16px 0;
    background-color: #e7e7e7;
    border: 0 none;
    overflow: hidden;
    box-sizing: content-box;
}

li p.first {
    display: inline-block;
}
ul,
ol {
    padding-left: 30px;
}
ul:first-child,
ol:first-child {
    margin-top: 0;
}
ul:last-child,
ol:last-child {
    margin-bottom: 0;
}
blockquote {
    border-left: 4px solid #dfe2e5;
    padding: 0 15px;
    color: #777777;
}
blockquote blockquote {
    padding-right: 0;
}
table {
    padding: 0;
    word-break: initial;
}
table tr {
    border-top: 1px solid #dfe2e5;
    margin: 0;
    padding: 0;
}
table tr:nth-child(2n),
thead {
    background-color: #f8f8f8;
}
table tr th {
    font-weight: bold;
    border: 1px solid #dfe2e5;
    border-bottom: 0;
    margin: 0;
    padding: 6px 13px;
}
table tr td {
    border: 1px solid #dfe2e5;
    margin: 0;
    padding: 6px 13px;
}
table tr th:first-child,
table tr td:first-child {
    margin-top: 0;
}
table tr th:last-child,
table tr td:last-child {
    margin-bottom: 0;
}

.CodeMirror-lines {
    padding-left: 4px;
}

.code-tooltip {
    box-shadow: 0 1px 1px 0 rgba(0,28,36,.3);
    border-top: 1px solid #eef2f2;
}

.md-fences,
code,
tt {
    border: 1px solid #e7eaed;
    background-color: #f8f8f8;
    border-radius: 3px;
    padding: 0;
    padding: 2px 4px 0px 4px;
    font-size: 0.9em;
}

code {
    background-color: #f3f4f4;
    padding: 0 2px 0 2px;
}

.md-fences {
    margin-bottom: 15px;
    margin-top: 15px;
    padding-top: 8px;
    padding-bottom: 6px;
}


.md-task-list-item > input {
  margin-left: -1.3em;
}

@media print {
    html {
        font-size: 13px;
    }
    table,
    pre {
        page-break-inside: avoid;
    }
    pre {
        word-wrap: break-word;
    }
}

.md-fences {
	background-color: #f8f8f8;
}
#write pre.md-meta-block {
	padding: 1rem;
    font-size: 85%;
    line-height: 1.45;
    background-color: #f7f7f7;
    border: 0;
    border-radius: 3px;
    color: #777777;
    margin-top: 0 !important;
}

.mathjax-block>.code-tooltip {
	bottom: .375rem;
}

.md-mathjax-midline {
    background: #fafafa;
}

#write>h3.md-focus:before{
	left: -1.5625rem;
	top: .375rem;
}
#write>h4.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h5.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
#write>h6.md-focus:before{
	left: -1.5625rem;
	top: .285714286rem;
}
.md-image>.md-meta {
    /*border: 1px solid #ddd;*/
    border-radius: 3px;
    padding: 2px 0px 0px 4px;
    font-size: 0.9em;
    color: inherit;
}

.md-tag {
    color: #a7a7a7;
    opacity: 1;
}

.md-toc { 
    margin-top:20px;
    padding-bottom:20px;
}

.sidebar-tabs {
    border-bottom: none;
}

#typora-quick-open {
    border: 1px solid #ddd;
    background-color: #f8f8f8;
}

#typora-quick-open-item {
    background-color: #FAFAFA;
    border-color: #FEFEFE #e5e5e5 #e5e5e5 #eee;
    border-style: solid;
    border-width: 1px;
}

/** focus mode */
.on-focus-mode blockquote {
    border-left-color: rgba(85, 85, 85, 0.12);
}

header, .context-menu, .megamenu-content, footer{
    font-family: "Segoe UI", "Arial", sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state{
    visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
    background-color: #fafafa;
    background-color: var(--side-bar-bg-color);
}

.md-lang {
    color: #b4654d;
}

.html-for-mac .context-menu {
    --item-hover-bg-color: #E6F0FE;
}

#md-notification .btn {
    border: 0;
}

.dropdown-menu .divider {
    border-color: #e5e5e5;
}

.ty-preferences .window-content {
    background-color: #fafafa;
}

.ty-preferences .nav-group-item.active {
    color: white;
    background: #999;
}


</style>
</head>
<body class='typora-export'>
<div id='write'  class=''><h1><a name="ml-lecture-23-deep-reinforcement-learning---ii" class="md-header-anchor"></a><span>ML Lecture 23: Deep Reinforcement Learning - II</span></h1><blockquote><p><span>臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 </span><a href='http://ai.ntu.edu.tw/' target='_blank' class='url'>http://ai.ntu.edu.tw/</a></p></blockquote><h4><a name="前情提要" class="md-header-anchor"></a><span>前情提要</span></h4><ul><li><p><span>Reinforcement Learning 的方法主要分為</span></p><ul><li><p><span>Policy-based</span></p><ul><li><span>會 learn 一個負責做事的 actor</span></li></ul></li><li><p><span>Value-based</span></p><ul><li><span>會 learn 一個不做事的 critic，專門批評</span></li></ul></li></ul></li></ul><hr /><h3><a name="policy-based-approach" class="md-header-anchor"></a><span>Policy-based Approach</span></h3><ul><li><p><span>Actor (or Policy)</span></p><ul><li><p><span>Actor 就是一個 function，通常寫成 pi</span></p><ul><li><span>input 就是 machine 看到的 observation</span></li><li><span>output 就是 machine 要採取的 action</span></li><li><span>透過 reward 幫助我們找出這個 Actor (function)</span></li></ul></li></ul></li></ul><p>&nbsp;</p><h3><a name="找-actor-三步驟" class="md-header-anchor"></a><span>找 Actor 三步驟</span></h3><h5><a name="一決定-function-長甚麼樣子-例如nn" class="md-header-anchor"></a><span>一、決定 function 長甚麼樣子 (例如：NN)</span></h5><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_22.jpg" width="70%"></p></li><li><p><span>Neural Network 決定了一個 function space，所以 Actor 可以是一個 NN</span></p><ul><li><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_23.jpg" width="70%"></li><li><span>Input：用一個 vector 來描述一堆 pixel 的 observation</span></li><li><span>output：現在可以採取的 action</span></li></ul></li><li><p><span>Neural Network 的好處：</span></p><ul><li><span>Policy 是 stochastic，意思是 Policy 的 output 其實是個機率，會根據機率在同一個畫面有不同action。傳統的作法是存一個 table，看到什麼 observation 就採取什麼 action</span></li><li><span>即使是沒有看過的東西，也有可能得到一個合理的結果</span></li></ul></li></ul><hr /><h5><a name="二決定一個-function-actor-的好壞" class="md-header-anchor"></a><span>二、決定一個 function (Actor) 的好壞</span></h5><ul><li><p><u><strong><span>先回顧：Supervised learning 怎麼評估 Actor 好壞</span></strong></u></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_25.jpg" width="70%"></p></li><li><p><span>假設給一個 Neural Network 其參數已經知道是 theta</span></p><ul><li><span>有一堆 training example 在做 image classification，就把 image 丟進去看 output 跟 target 像不像，如果越像代表 function 越好</span></li></ul></li><li><p><span>會定義一個東西叫做 Loss，算每一個 example 的 Loss 合起來就是 Total Loss</span></p><ul><li><span>找一個參數去 minimize 這個 Total Loss</span></li></ul></li></ul></li><li><p><u><strong><span>再來看 Reinforcement Learning 怎麼評估 Actor 好壞</span></strong></u></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_26.jpg" width="70%"></p></li><li><p><span>假設有一個 Actor (Neural Network)，他的參數是 theta，用 pi 下標 theta</span></p><ul><li><span>Actor 是一個 function，input 就是一個 s（s 就是 Actor 看到的 observation）</span></li></ul></li><li><p><span>要知道一個 Actor 表現好還是不好，就讓 Actor 實際的去玩一下這個遊戲</span></p><ul><li><span>玩完遊戲以後得到的 Total Reward 可以寫成 Rθ，Rθ 就是 r1 + r2 一直加到 rT</span></li><li><span>把所有在每一個 step 得到的 reward合起來，就是在這一個 episode 裡面得到的 Total Reward</span></li><li><span>由於 Actor 是 stochastic，遊戲本身也有隨機性，所以 Rθ 是一個 random variable</span></li><li><span>目標：maximize Rθ 的期望值</span></li></ul></li><li><p><span>Rθ 期望值實際上要怎麼計算出來</span></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_28.jpg" width="70%"></p></li><li><p><span>假設一場遊戲就是一個 trajectory τ，τ 是一個 sequence 裡面包含了 state、observation，看到這個 observation 以後 take 的 action，還有得到的 reward，還有新的 observation、take 的 action、得到的 reward 等等的一個 sequence</span></p></li><li><p><span>R(τ) 代表這個 trajectory 在這場遊戲最後得到的 Total Reward，所有的小 r summation 起來就是 total 的 reward</span></p></li><li><p><span>每一個 τ 都會有一個出現的機率， τ 代表某一種可能的從遊戲開始到結束的過程，可以用機率來描述他，寫作 P( τ | θ )</span></p></li><li><p><span>Rθ bar 就寫成 summation over 所有可能的 τ (所有可能的遊戲進行的過程) 的期望 reward</span></p><ul><li><span>每一個 τ，都有一個機率 P( τ | θ )，和一個 reward R(τ)，乘起來就是期望 reward</span></li></ul></li><li><p><span>實際上要窮舉所有的 τ 是不可能的，所以讓 Actor 去玩這個遊戲玩 N 場，有點 sample的感覺</span></p></li></ul></li></ul></li></ul><p>&nbsp;</p><hr /><h5><a name="三選一個最好的-actor" class="md-header-anchor"></a><span>三、選一個最好的 Actor</span></h5><ul><li><p><span>用 Gradient Descent</span></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_30.jpg" width="70%"></p></li><li><p><span>目標就是要最大化這個 Rθ bar</span></p><ul><li><span>先隨機的找一個初始的 θ0，隨機找一個初始的 Actor</span></li><li><span>然後計算在使用初始的 Actor 的情況下，你的參數對 Rθ bar 的微分，再去 update 你的參數得到 θ1</span></li><li><span>依此類推⋯⋯</span></li></ul></li></ul></li><li><p><span>實際運算</span></p><ul><li><span>Rθ bar = summation over 所有的 τ ，R( τ ) * P( τ | θ )</span></li><li><span>R(τ) 跟 θ 是沒任何關係的，不需要微分，所以可以是個不可微的黑盒子</span></li></ul></li><li><p><span>一些問題排解：如果 R(τ) 永遠是正的，會發生什麼事</span></p><ul><li><span>像玩 Space Invader，得到的 reward 都是正的，殺了外星人就得到正的分數，最糟就是殺不到外星人得到分數是 0</span></li><li><span>因為實作的時候，我們做的是 sampling，有可能只 sample 到 b、c 這個 action (純移動)，而沒 sample 到 a (射擊)</span></li><li><span>可能 a 這個 action machine 從來沒試過它，不知道它的 R(τ) 到底有多大，又因為 b 跟 c 機率都會增加，a 沒 sample 到，機率就自動減少</span></li></ul></li><li><p><span>解法：reward 要減掉一個 bias</span></p><ul><li><span>這個 bias 叫做 baseline，好過 baseline 才把那個 action 的機率增加，小於 baseline 把它 action 的機率減小</span></li><li><span>這樣子就不會造成某一個 action 沒被 sample 到它的機率就會變小</span></li></ul></li></ul><p>&nbsp;</p><hr /><h3><a name="policy-gradient" class="md-header-anchor"></a><span>Policy Gradient</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_34.jpg" width="70%"></p><p>&nbsp;</p><ul><li><p><span>首先先有一個 actor，參數是 θ (random initialize)</span></p></li><li><p><span>先收集資料：</span></p><ul><li><p><span>拿這個初始的 actor, θ 去玩 N 次遊戲，收集到 N 個 trajectory (τ)</span></p><ul><li><p><span>假設收集到一個 τ^1 (trajectory 1)</span></p><ul><li><span>τ^1 裡面有 state 1，state 1 採取了 action  a1</span></li><li><span>以此類推</span></li></ul></li><li><p><span>玩完這個遊戲以後，可以算出一個  total reward，R(τ)</span></p></li></ul></li><li><p><span>用上圖式子去 update 參數 θ，有了一個新的 actor</span></p></li></ul></li><li><p><span>再收集資料：（因為 actor 是新的可能，會得到不太一樣的分布，會得到一個不太一樣的結果）</span></p><ul><li><span>再 update 參數 θ</span></li></ul></li><li><p><span>以此類推</span></p></li></ul><h5><a name="問題上圖公式是什麼意思" class="md-header-anchor"></a><span>問題：上圖公式是什麼意思：</span></h5><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_35.jpg" width="70%"></p><ul><li><p><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="2.453ex" height="2.577ex" viewBox="0 -806.1 1056 1109.7" role="img" focusable="false" style="vertical-align: -0.705ex;"><defs><path stroke-width="0" id="E1-MJSZ1-2211" d="M61 748Q64 750 489 750H913L954 640Q965 609 976 579T993 533T999 516H979L959 517Q936 579 886 621T777 682Q724 700 655 705T436 710H319Q183 710 183 709Q186 706 348 484T511 259Q517 250 513 244L490 216Q466 188 420 134T330 27L149 -187Q149 -188 362 -188Q388 -188 436 -188T506 -189Q679 -189 778 -162T936 -43Q946 -27 959 6H999L913 -249L489 -250Q65 -250 62 -248Q56 -246 56 -239Q56 -234 118 -161Q186 -81 245 -11L428 206Q428 207 242 462L57 717L56 728Q56 744 61 748Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E1-MJSZ1-2211" x="0" y="0"></use></g></svg></span><script type="math/tex">\sum</script><span>：summation over 某一個 trajectory 所有的 time step</span></p></li><li><p><span>∇ log p( a(上標 n, 下標 t) | s(上標n, 下標t) )</span></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_36.jpg" width="70%"></p></li><li><p><span>假設現在要做的是一個分類的問題</span></p><ul><li><span>有一個 network，有一個 actor，這個 actor 當作是一個 classifier</span></li><li><span>這個 classifier 做的事情是 given 一個畫面 S，它分類說，我們現在應該要採取哪一個 action（有 3 個可以採取的 action）</span></li></ul></li><li><p><span>在做分類的時候，要 train 一個 classifier，要有 labeled data， 要給 network 一個 target</span></p><ul><li><p><span>假設現在的目標是 1、0、0（left 是正確的類別）</span></p></li><li><p><span>把 network 的 output 叫做 yi，target 叫做 yi\head</span></p></li><li><p><span>分類問題是在 minimize </span><u><strong><span>cross entropy</span></strong></u></p><ul><li><span>cross entropy 就是：summation over 每一個 dimension、summation over 所有的 class</span></li><li><span>把 yi\head * log(yi)，前面取負號</span></li></ul></li></ul></li><li><p><span>實際上我們在做的事情，本來是一個負號加 minimize</span></p><ul><li><span>也是 maximize log(yi)</span></li><li><span>所謂的 yi ，其實就是 P(&quot;left&quot;|s)</span></li><li><span>我們希望我們 network 的 output，跟訂下來的 target (left) 越接近越好</span></li></ul></li><li><p><span>所以這是所要的 objective function，要去 maximize 它，對它算一個 gradient</span></p></li></ul></li><li><p><span>R(τ) ：常數項， total reward</span></p><ul><li><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_37.jpg" width="70%"></p></li><li><p><span>如果把 R(τ^n) 拿掉，當作那一項等於 1</span></p><ul><li><span>意思就是：training data裡面有一個 s1、a1(left)</span></li><li><span>把 s1 丟到 network 裡面，他會給我們 left、right 跟 fire 的機率，希望這個機率跟 1、0、0 越接近越好</span></li><li><span>就變成了一個分類的問題</span></li></ul></li><li><p><span>實際上，我們在 update 這個式子的時候，真正在做的事情是</span></p><ul><li><span>現在有一筆 training data，input 和 target 就是這個樣子，請把這個分類問題做對</span></li><li><span>有一個不一樣的地方就是，加了 reward，就是給一個權重，讓這筆example 被複製 R(τ)  次</span></li></ul></li></ul></li></ul><hr /><h3><a name="value-based-approach" class="md-header-anchor"></a><span>Value-based Approach</span></h3><ul><li><p><span>Critic 概念</span></p><ul><li><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_40.jpg" width="70%"></li><li><span>learn 一個 network 它不做事（不會決定 Action）</span></li><li><span>learn 一個 function，這個 function 可以知道現在看到的 observation 有多好</span></li><li><span>（其實也可以從 Critic 得到一個 Actor，這樣就是 Q Learning）</span></li></ul></li></ul><hr /><h3><a name="critic" class="md-header-anchor"></a><span>Critic</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_41.jpg" width="70%"></p><ul><li><p><span>Critic 並沒有辦法決定要採取哪一個 action</span></p><ul><li><span>給一個 actor pi，Critic 可以告訴你說這個 actor pi 有多好</span></li></ul></li><li><p><span>state value function，寫成 V(pi) of x</span></p><ul><li><span>在給定一個 actor pi 的前提下，假設看到一個 observation or state s，會告訴你接下來一直到遊戲結束的時候，得到 reward 的總和期望值有多大</span></li><li><span>得到的是看到這個 state 之後，所有 accumulated 的 reward 的期望值</span></li></ul></li><li><p><span>以下圍棋為例：</span></p><ul><li><span>假設你已經有一個下圍棋的 agent，叫做 pi</span></li><li><span>給它一個 observation，就是棋盤的盤勢，比如說，出手天元</span></li><li><span>V(pi) of x 就是：假設出手下在天元，接下來獲勝的機率有多大</span></li></ul></li><li><p><span>Critic 的工作，就是衡量一個 actor 好不好</span></p><ul><li><p><span>以上左圖的 observation，丟到 Critic 裡可能會 output 一個很大的正值</span></p><ul><li><span>因為還有很多 alien 可以殺，所以會得到很高的分數</span></li></ul></li><li><p><span>以上右圖的 observation，丟到 Critic 裡可能會會得到相對比較少的值</span></p><ul><li><span>因為 alien 變得比較少</span></li><li><span>而且屏障消失了，所以可能很快就會死，分數就比較少</span></li></ul></li></ul></li></ul><hr /><h3><a name="怎麼評估-critic" class="md-header-anchor"></a><span>怎麼評估 Critic</span></h3><ul><li><span>Critic 其實會隨著 actor 的不同，而得到不同的分數</span></li><li><span>兩個方法，一個是 Monte-Carlo，一個是 Temporal-Difference</span></li></ul><p>&nbsp;</p><h4><a name="monte-carlo-的方法-md" class="md-header-anchor"></a><span>Monte-Carlo 的方法 (MD)</span></h4><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_43.jpg" width="70%"></p><ul><li><p><span>較直觀</span></p><ul><li><span>直接去看那個 actor 玩遊戲，假設現在 Critic 觀察到，actor pi 在經過這個 state Sa 以後，它得到的 accumulated 的 reward 是 Ga</span></li><li><span>Critic 就要學說如果 input state Sa，那我的 output 要跟 Ga 越接近越好</span></li></ul></li><li><p><span>regression 問題</span></p></li><li><p><span>actor 要調它的參數，那它的 output 跟 Ga 越接近越好</span></p></li></ul><hr /><h4><a name="temporal-difference-的方法-td" class="md-header-anchor"></a><span>Temporal-Difference 的方法 (TD)</span></h4><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_44.jpg" width="70%"></p><ul><li><p><span>較不直觀</span></p><ul><li><span>一樣讓 Critic 去看 actor 玩遊戲，當看到 actor 在 state st 採取 action at，得到 reward rt，然後跳到 state s(t+1)</span></li><li><span>一次做一個data，不用等到遊戲結束</span></li><li><span>actor 只要在某一個 state 採取某一個行為，Critic 就可以學了</span></li></ul></li><li><p><span>為什麼Critic 這樣就可以學：</span></p><ul><li><span>based on 上圖式子，在 s(t+1) 和 s(t) 中間它們差了 reward 就是 r(t)</span></li><li><span>即使不知道 accumulated reward 是多少，但我知道 s(t) 輸出的值跟 s(t+1) 輸出的值，中間差了 r(t)，就可以 learn 下去</span></li></ul></li><li><p><span>好處：</span></p><ul><li><span>有些遊戲非常的長，這樣遊戲玩到一半的時候，就可以開始 update 你的 network，不會拖太久</span></li></ul></li></ul><hr /><h3><a name="q-function" class="md-header-anchor"></a><span>Q function</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_47.jpg" width="70%"></p><ul><li><p><span>上面提到的那種 critic 沒有辦法拿來決定 action</span></p></li><li><p><span>有另外一種 critic 可以拿來決定 action， 這種 critic 叫做 Q function</span></p><ul><li><span>input 是一個 state，</span><u><strong><span>一個 action</span></strong></u></li><li><span>在給定一個 actor pi 的前提之下，在 observation s，</span><u><strong><span>採取了 action a</span></strong></u><span>，到遊戲結束的時候，會得到多少 accumulated reward</span></li><li><span>會窮舉所有的 a，再搭配 s 代入 Q function</span></li></ul></li></ul><hr /><h3><a name="q-learning" class="md-header-anchor"></a><span>Q-Learning</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_48.jpg" width="70%"></p><ul><li><p><span>用 Q function 找出一個比較好的 actor，這一招就叫做 Q learning</span></p></li><li><p><span>整個 process：</span></p><ul><li><p><span>先有一個已經初始的 actor pi，然後這個 actor pi，去跟這個環境互動</span></p></li><li><p><span>然後 critic 去觀察 (TD or MC) 這個 actor pi 它跟環境的互動</span></p></li><li><p><span>估測說，給定這個 actor 的前提之下，在某一個 state 採取某一個 action，得到的 Q value 是多少</span></p></li><li><p><span>可以保證：我們一定能夠找到一個新的、比原來的 pi 更好的 actor pi</span></p></li><li><p><span>重點在紅色問號那步：</span></p><ul><li><span>只要量得出 Q function，就一定可以找到一個更好的 actor pi prime</span></li></ul></li></ul></li></ul><hr /><p>&nbsp;</p><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_49.jpg" width="70%"></p><ul><li><p><span>什麼叫做 pi prime 一定比 pi 好：</span></p><ul><li><p><span>pi prime 比 pi 好的定義是：</span></p><ul><li><span>給 </span><u><strong><span>所有可能的 state s</span></strong></u><span>，如果用 pi 去玩這個遊戲，得到的 reward，一定會小於用 pi prime 得到的 accumulated reward</span></li></ul></li></ul></li><li><p><span>怎麼找到一個比較好的 actor pi prime：</span></p><ul><li><span>給定一個 Q function，某一個 state 的時候，窮舉所有可能的 action，看哪一個 action 的 Q value 最大</span></li><li><span>然後這個 a，pi prime 就說這就是它的輸出了</span></li></ul></li><li><p><span>但是如果今天 action 無法窮舉，就無法使用</span></p></li><li><p><span>Q learning 的 trick =&gt; </span><em><span>rainbow</span></em><span> 的 paper</span></p><ul><li><span>有 7 種不同的 DQN 的 tip，對應到彩虹的 7 個顏色</span></li></ul></li></ul><hr /><h3><a name="actor-critic" class="md-header-anchor"></a><span>Actor-Critic</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_51.jpg" width="70%"></p><ul><li><p><span>Actor+Critic 的精神</span></p><ul><li><span>actor 不要看環境的 reward，而是看 critic </span></li><li><span>（因為環境有隨機性，reward變化太大）</span></li></ul></li><li><p><span>知名方法</span></p><ul><li><p><span>A2C</span></p><ul><li><span>Advantage Actor-Critic</span></li></ul></li><li><p><span>A3C</span></p><ul><li><span>Asynchronous Advantage Actor-Critic</span></li></ul></li></ul></li></ul><p>&nbsp;</p><hr /><h3><a name="a3c-asynchronous-advantage-actor-critic" class="md-header-anchor"></a><span>A3C (Asynchronous Advantage Actor-Critic)</span></h3><p><img src="http://ai.ntu.edu.tw/aho/JPG/Introduction_of_Reinforcement_Learning/Introduction_of_Reinforcement_Learning_53.jpg" width="70%"></p><ul><li><p><span>Asynchronous 的意思：</span></p><ul><li><span>有一個 global 的 network、global 的 actor 跟 global 的 critic</span></li><li><span>每到學習的時候呢，就去跟 global 的 actor 和 critic copy 一組參數過來，讓這個 actor 實際去跟環境互動（類似開分身的概念）</span></li><li><span>那互動完以後，Critic 就會告訴 actor 說要怎麼樣 update 參數。把這個 updated 參數，傳回去 global 的 network</span></li><li><span>每一個分身，都會傳一個 update 的方向，合起來可以一起做 update，等於就是做平行的運算</span></li></ul></li><li><p><span>實作上，要做 asynchronous 這一招，前提是要有很多很多的 machine 這樣子</span></p><ul><li><span>如果只有一台 machine，就只能用 A2C</span></li></ul></li></ul><p>&nbsp;</p><h5><a name="小結論" class="md-header-anchor"></a><span>小結論</span></h5><ul><li><p><span>Actor 跟 Critic 可以合在一起 train</span></p><ul><li><span>好處：簡單講就是比較強</span></li></ul></li></ul><p>&nbsp;</p><hr /><h3><a name="inverse-reinforcement-learning" class="md-header-anchor"></a><span>Inverse reinforcement learning </span></h3><ul><li><p><span>Imitation learning 的一種</span></p></li><li><p><span>在 inverse reinforcement learning 裡面</span></p><ul><li><span>只有 environment 跟 actor，沒有 reward function</span></li><li><span>還有這個 expert demo trajectory</span></li><li><span>意思是：有高手去把這個遊戲，玩了 N 遍給 machine 看</span></li></ul></li><li><p><span>沒有 reward function 很正常？</span></p><ul><li><span>多數生活中的 case，都是沒有 reward function 的 (不像圍棋有明確輸贏，電玩有明確得分)</span></li><li><span>比如：自駕車、chat bot (用一些自己訂出來的 reward，有時候會很奇怪)</span></li></ul></li><li><p><span>雖然不知道最好的 actor 是什麼，但是我們有專家 (expert)</span></p><ul><li><span>專家去玩了 N 場遊戲，告訴我們說厲害的人玩這個遊戲，看起來是怎麼樣的</span></li><li><span>根據專家的 demo，還有 environment，加上 inverse reinforcement learning，可以推出 reward function 應該長什麼樣子</span></li></ul></li><li><p><span>用 inverse reinforcement learning 的方法去 </span><u><strong><span>推出 reward function</span></strong></u><span>，最後再用 reinforcement learning 的方法去找出最好的 actor</span></p><ul><li><span>概念：那些 experts 永遠是對的</span></li><li><span>訂一個 reward function，一定要讓 expert 得到的分數，比 actor 得到的分數高 (先射箭，再畫靶)</span></li><li><span>根據新的 reward，actor 去 maximize 新的 reward function 以後，再去跟環境互動，他就會得到新的 trajectory</span></li><li><span>當他變得跟老師一樣厲害以後，再改一下規格，讓老師算出來的分數，還是比較高</span></li></ul></li><li><p><span>整體概念跟 GAN 很像</span></p><ul><li><span>generator 換個名字叫做 actor</span></li><li><span>discriminator 換個名字叫做 reward function</span></li></ul></li></ul></div>
</body>
</html>