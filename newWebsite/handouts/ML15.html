<!DOCTYPE html>
<html>
   <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    </script> 
      <meta charset="utf-8">
      <title>AINTU 講義</title>
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
   </head>

   <body>
        <div id="head"></div>
       <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-light bg-light static-top primary">
          <a class="navbar-brand ml-4" href="../index.html">
                <img src="../logo1.png" width=326.83rem height= 60rem alt="">
              </a>
          <div class="collapse navbar-collapse mr-4" id="navbarSupportedContent">
            <ul class="navbar-nav ml-auto mr-sm-4">
              <li class="nav-item">
                <a class="nav-link active" href="../index.html">機器學習講義 <span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="../datasource.html">機器學習資料源</a>
              </li>
            </ul>
          </div>
      </nav>
      
      <div class="container">
          <p class="mt-4">- 版權屬於講師所有 勿任意轉載、散佈、更改 引用請知會版權擁有權人 -</p>
 
                

    <div class="row">
      
         
      <div class="col-9 ml-4 mt-4" style="/*height: 40rem; overflow-y: scroll;*/">
         <!--main content start-->
      <section id="main-content">
        <section class="wrapper site-min-height">
          <div class="row mt">
            <div class="col-lg-9">
              <h1>ML Lecture 15: </h1>
<h5>Unsupervised Learning - Neighbor Embedding</h5>
<blockquote><p>臺灣大學人工智慧中心
科技部人工智慧技術暨全幅健康照護聯合研究中心
<a href='http://aintu.tw' target='_blank' class='url'>http://aintu.tw</a></p>
</blockquote>
<h3>Neighbor Embedding</h3>
<ul>
<li>t-SNE</li>
<li>T-distributed Stochastic <strong>Neighbor Embedding</strong></li>

</ul>
<p>&nbsp;</p>
<h3>Dimension Reduction</h3>
<ul>
<li><p>「非線性」的降維</p>
</li>
<li><p>在高維空間裡面的一個 Manifold</p>
</li>
<li><p>Ex: 地球</p>
<ul>
<li>表面就是一個 Manifold</li>
<li>塞到了一個三維的空間裡面</li>
<li>Euclidean distance (歐式幾何) 只有在很近的距離的情況下才會成立</li>

</ul>
</li>
<li><p>Ex: 附圖之S形空間</p>
<ul>
<li>藍色區塊的點距離近 \Rightarrow​ 他們比較像</li>
<li>距離比較遠(ex: 藍色跟紅色) \Rightarrow 無法直接以 Euclidean distance 計算相似度</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_1.jpg" width="70%"></p>
<h3>Manifold Learning</h3>
<ul>
<li><p>把 S 形的這塊東西展開</p>
</li>
<li><p>把塞在高維空間裡面的低維空間「攤平」，也就是降維</p>
</li>
<li><p>Pros</p>
<ul>
<li>可以用 Euclidean distance 來計算點和點之間的距離</li>
<li>對 clustering 有幫助</li>
<li>對 supervised learning 也會有幫助</li>

</ul>
<p>&nbsp;</p>
</li>

</ul>
<h3>Locally Linear Embedding (LLE)</h3>
<h5>Setting</h5>
<ul>
<li>本來有某一個點，叫做 x_i</li>
<li>然後選出這個 x_i 的 k 個 neighbors</li>
<li>假設其中一個叫做 x_j ，w_{ij} 代表 x_i 和 x_j 的關係</li>
<li>表示所有的 k 個 neighbors Neighbor 之線性組合要跟 x_i 越像越好</li>
<li>Minimize \sum_i \|x^i - \sum_j w_{ij} x^j \|_2 </li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_2.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<h5>Dimension Reduction</h5>
<ul>
<li>將所有的 x_i​ 跟 x_j​ 轉成 z_i​ 和 z_j​ ，而中間的關係 w_{ij}​ 是<strong>不變</strong>的</li>
<li>首先 w_{ij} 在原來的 space 上面找完以後，就 fix 住</li>
<li><strong>沒有</strong>一個明確的 function 說怎麼做 dimension reduction</li>
<li>憑空找出來降維後的 z_i 跟 z_j ，可能原本100維(x)，降到2維(z)</li>
<li>Minimize \sum_i \|z^i - \sum_j w_{ij} z^j \|_2 </li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_3.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<p>&nbsp;</p>
<h3>Something about Numbers of Neighbors (k)</h3>
<ul>
<li>neighbor 選的數目要剛剛好才會得到好的結果</li>
<li>Reference paper:  “Think Globally, Fit Locally” </li>
<li>k 太小，就不太robust，表現不太好</li>
<li>k 太大，會考慮到一些距離很遠的點，這些點被 transform 以後，relation 沒有辦法 keep 住</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_5.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<h3>Laplacian Eigenmap</h3>
<ul>
<li><p>考慮到先前提過的 Smoothness assumption</p>
</li>
<li><p>只算它的 Euclidean distance 來比較點跟點之間的距離是不足夠的</p>
</li>
<li><p>要看在這個 High density 的 region 之間的 distance</p>
<ul>
<li>有 high density 的 connection 才是真正的接近</li>
<li>可以用 graph 描述</li>

</ul>
</li>
<li><p>Graph Construction</p>
<ul>
<li>計算 data point 兩兩之間的相似度，超過一個 thereshold 就 connect 起來</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_7.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<ul>
<li><p>Review</p>
<ul>
<li>如果 x_1 跟  x_2 在 high density 的 region 上面是相近的，那它們的 label  y_1,\ y_2，很有可能是一樣的</li>

</ul>
</li>
<li><p>考慮 smoothness 的距離，可以被這個 graph 上面的 connection 來 approximate</p>
</li>
<li><p>有一項是考慮有 label 的 data 的項 (L ​)</p>
<ul>
<li>graph 的 laplacian</li>
<li>E - W​</li>

</ul>
</li>
<li><p>另外一項是跟 labeled data 沒有關係的 (S)</p>
<ul>
<li>那這一項的作用它要考慮說我們現在得到的 label 是不是 smooth 的 regularization 的 term</li>
<li>y_i 跟 y_j 的這個距離乘上 w_{i, j}</li>
<li>w_{i, j} 就是 x_i 跟 x_j 的相似成度</li>
<li>evaluate 現在得到的 label 有多麽的 smooth</li>
<li>y^TL\ y</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_8.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<ul>
<li>如果 x1 跟 x2 在 high density 的 region 他們是 close 的，那我們就會希望， z1 跟 z2 他們也是相近的</li>

</ul>
<p>&nbsp;</p>
<ul>
<li>如果今天 x_i 跟 x_j，這個 i 跟 j 的這兩個 datapoint 他們之間的 w_{i, j} 很像，那 z_i 跟 z_j 做完 dimension reduction 以後他們的距離就很近</li>
<li>反之，如果他們的 w_{i, j} 很小，那他們的距離要怎樣就都可以</li>

</ul>
<p>&nbsp;</p>
<ul>
<li><p>Problem</p>
<ul>
<li><p>minimize只要把所有的 z_i 跟 z_j 通通設一樣的值就好了?  直接通通變0?</p>
</li>
<li><p>光這個式子是不夠的</p>
</li>
<li><p>少了 supervise 的項，需要給一些 constraint</p>
<ul>
<li>Span\{z^1,z^2...,z^N \} = R^M</li>
<li>z 不是活在一個比 M 維更低維的空間裡面</li>
<li>z 就是 graph laplacian 的 eigenvector</li>
<li>Laplacian Eigenmap</li>

</ul>
</li>
<li><p>Spectral clustering</p>
<ul>
<li>先找出 z，再用 K-means 做 clustering</li>

</ul>
</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_8.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<p>&nbsp;</p>
<h3>t-SNE (T-distributed Stochastic Neighbor Embedding)</h3>
<h5>Problems above</h5>
<ul>
<li>先前只假設相近的點應該要是接近的，但沒有假設<strong>不相近的點不要接近、要分開</strong></li>
<li>確實會把同個 class 的點都聚集在一起，但也會把不同的class混雜</li>
<li>Example:  MINIST (手寫辨識)、 COIL-20 (Image Corpus)</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_9.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<p>&nbsp;</p>
<h5>t-SNE</h5>
<ul>
<li><p>做 t-SNE，一樣是要做降維</p>
<ul>
<li><p>把原來的 data point x 變成比較 low dimension 的 vector z</p>
</li>
<li><p>在原來的 x 這個 space 上面會計算所有的點的 pair，x_i 和 x_j 之間的 similarity</p>
<ul>
<li><p>寫成 S(x_i, x_j)</p>
</li>
<li><p>做一個 normalization，計算一個 P(x_j | x_i)</p>
</li>
<li><p>P(x_j | x_i)，他是從 x_i 跟 x_j 的 similarity 來的</p>
<ul>
<li>分子的地方是 x_i 跟 x_j​ 的 similarity</li>
<li>分母的地方是 summation over 除了 x_i 以外，所有其他的點和 x_i 之間所算出來的距離</li>

</ul>
</li>

</ul>
</li>

</ul>
</li>

</ul>
<h5>發現</h5>
<ul>
<li>xi 對其他所有的 data point，它所算出來的這個 P(x_j | x_i) 的 summation 應該要是 1。</li>
<li>另外假設我們今天已經找出了一個 low dimension 的 representation 就是 z_i 跟 z_j 的話，我們已經把 x 變成 z 的話，那我們也可以計算 z_i 和 z_j 之間的 similarity。</li>
<li>這邊 similarity 我們寫成 S'，那一樣你可以計算一個 Q( z_j | z_i)，他的分子的地方就是 S'( z_i, z_j)，分母的地方就是 \sum z_i，跟所有 database 裡面的 data point z_k 之間的距離。</li>

</ul>
<h5>Normalization</h5>
<p>那今天有做這個 normalization 其實感覺是必要的，因為你不知道在高維空間中算出來的距離，S(x_i, x_j) 跟 S’(z_i, z_j)，它們的 scale 是不是一樣的</p>
<p>如果，你今天有做這個 normalization，那你最後就可以把他們都變成機率，那這個時候他們值都會介於 0 到 1 之間，他們的 scale 會是一樣的</p>
<h5>找z</h5>
<p>我們現在還不知道 z_i 跟 z_j 他們的值到底是多少，那麼希望找一組 z_i 跟 z_j​，它可以讓這一個 distribution 跟這一個 distribution 越接近越好</p>
<p>我們要讓原來在這個根據 similarity 在 S 這個原來的 space 算出來 distribution，跟在這個 dimension reduction 以後的 space 算出來的 distribution <strong>越接近越好</strong>。</p>
<p>&nbsp;</p>
<h5>怎麼衡量兩個 distribution 之間的相似度呢？</h5>
<ul>
<li><p>KL divergence</p>
<ul>
<li>可以很直覺的衡量兩個 distribution 之間的相似度的方法</li>
<li>也就是找一組 z，可以讓 x_i 的這個 distribution，跟 x_i 對其他 point 的 distribution。跟 z_i 對其他 point 的 distribution，這兩個 distribution 之間的 KL divergence 越小越好</li>
<li>Summation over 所有的 data point，然後你要使得這一項，它的值越小越好</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_10.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<h5>Problem</h5>
<ul>
<li>因為要計算所有 data point 之間的 similarity，所以運算量很大</li>

</ul>
<h5>Solution</h5>
<ul>
<li><p>常見的做法是先做降維</p>
<ul>
<li>先用比較快的方法比如說 PCA，比如說，先降到 50 維</li>
<li>再用 t-SNE 從 50 維降到 2 維</li>

</ul>
</li>

</ul>
<h5>Feature</h5>
<ul>
<li><p>少用來training</p>
</li>
<li><p>多拿來做 visualization</p>
<ul>
<li>visualize 他們在二維空間的分佈上是什麼樣子</li>

</ul>
</li>

</ul>
<p>&nbsp;</p>
<h5>t-SNE 之 similarity 的選擇</h5>
<ul>
<li><p>原來: 選擇 RBF 的 function</p>
<ul>
<li>要在 graph 上算 similarity 的話，這種方法比較好</li>
<li>可以確保說只有非常相近的點才有值</li>

</ul>
</li>

</ul>
<p>&nbsp;</p>
<ul>
<li><p>SNE</p>
<ul>
<li>在 data point 原來的 space 上用這個 evaluation 的 measure</li>

</ul>
</li>
<li><p>t-SNE</p>
<ul>
<li>在 dimension reduction 以後的 space，選擇的 measure 跟原來的 space 是不一樣的</li>
<li>選的 space 是這個 t-distribution 的其中一種</li>

</ul>
</li>

</ul>
<p>&nbsp;</p>
<h5>Why different measure</h5>
<p>假設橫軸代表了在原來 space 上的 Euclidean distance，紅線是 RBF function，藍線是 t-distribution。</p>
<ul>
<li><p>如果本來距離比較近</p>
<ul>
<li>他們的影響是比較⼩的</li>
<li>做完 transform 以後他還是很近</li>

</ul>
</li>
<li><p>如果本來就已經有一段距離，那從原來的這個 distribution 變到 t-distribution 以後</p>
<ul>
<li>t-distribution 他的尾巴特別長，他會被拉得很遠</li>
<li>做完 transform 以後他就會被拉得很遠</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_11.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<p>&nbsp;</p>
<h3>T-SNE Visualization Example</h3>
<h5>發現</h5>
<p>t-SNE 畫出來的圖往往長得像這樣，他會把你的 data point 聚集成一群、一群、一群</p>
<p>因為 data point 之間本來只要有一個 gap，做完 t-SNE 以後，它就會<strong>把 gap 強化</strong>，gap 就會變得特別明顯</p>
<h5>Example</h5>
<ul>
<li><p>MNIST</p>
<ul>
<li>先對 pixel 做 PCA 降維以後，再做 t-SNE</li>
<li>直接做 t-SNE 效果不好</li>

</ul>
</li>
<li><p>COIL-20</p>
<ul>
<li>每一個圈圈就代表一個 object，他只是圖片轉了一圈以後的結果</li>
<li>有一些被扭曲的圈，是因為有⼀些東西其不同角度特徵差很多，比如說杯⼦</li>
<li>左下角這很多條線，好像是說，有 4 部都是⼩汽⾞，4 部小汽車看起來是蠻像的。所以這邊有四條線是被 align 在一起的</li>

</ul>
</li>

</ul>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_12.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
<h3>Appendix</h3>
<p><img src='http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Neighbor_Embedding/Unsupervised_Learning-Neighbor_Embedding_14.jpg' width="70%" alt='' referrerPolicy='no-referrer' /></p>
              
            </div> 

            
            
            <div class="col-lg-3">
              
            </div>
          </div>

  </section><! --/wrapper -->
    </section><!-- /MAIN CONTENT -->
    <!--main content end-->
    <div id="bottom"></div>
      </div>

      <div class="col mr-4 mt-4">
          <div class="list-group" style="position: sticky; position: -webkit-sticky; top: 2rem;">
            <a class="list-group-item list-group-item-action" href="#head"><i class="fa fa-arrow-up mr-2" style=""></i>講義開頭</a>
            <a class="list-group-item list-group-item-action" href="#bottom"><i class="fa fa-arrow-down mr-2" style=""></i>講義末尾</a>
            <a class="list-group-item list-group-item-action" href="../index.html"><i class="fa fa-home mr-2" style=""></i>選單主頁</a>
          </div>
       </div>
    </div>
      </div>
   </body>
        <div id="head"></div>
</html>