<!DOCTYPE html>
<html>
   <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    </script> 
      <meta charset="utf-8">
      <title>AINTU 講義</title>
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
   </head>

   <body>
        <div id="head"></div>
       <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-light bg-light static-top primary">
          <a class="navbar-brand ml-4" href="../index.html">
                <img src="../logo1.png" width=326.83rem height= 60rem alt="">
              </a>
          <div class="collapse navbar-collapse mr-4" id="navbarSupportedContent">
            <ul class="navbar-nav ml-auto mr-sm-4">
              <li class="nav-item">
                <a class="nav-link active" href="../index.html">機器學習講義 <span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="../datasource.html">機器學習資料源</a>
              </li>
            </ul>
          </div>
      </nav>
      
      <div class="container">
          <p class="mt-4">- 版權屬於講師所有 勿任意轉載、散佈、更改 引用請知會版權擁有權人 -</p>
 
                

    <div class="row">
      
         
      <div class="col-9 ml-4 mt-4" style="/*height: 40rem; overflow-y: scroll;*/">
         <!--main content start-->
      <section id="main-content">
        <section class="wrapper site-min-height">
          <div class="row mt">
           <div class="col-lg-9">
            <h1>Linear Method</h1>
            <h2>Dimension Reduction</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_1.jpg" width="70%"></p>
            <p>Dimension Reduction 分成兩種：</p>
            <ul>
            <li>一種做的事情，叫做化繁為簡。</li>
            <li>那另外一個是 Generation，也就是無中生有。</li>
            
            </ul>
            <h3>化繁為簡</h3>
            <p>又可以分成兩大類，</p>
            <ul>
            <li>一種是做 Clustering</li>
            <li>一種是做 Dimension Reduction</li>
            
            </ul>
            <p>所謂的化繁為簡的意思是說現在有很多種不同的 input，找一個 function它可以 input 看起來像樹的東西，output 則是抽象的樹。</p>
            <p>也就是把本來比較複雜的 input，變成比較簡單的 output。</p>
            <p>畢竟在做 Unsupervised Learning 的時候通常只會有 function 的其中一邊，所能擁有的 training data，就只有一大堆的 image，不知道說它的 output，應該要是長什麼樣子。</p>
            <h3>無中生有</h3>
            <p>那另外一個 Unsupervised Learning 會做的事情呢。</p>
            <p>要找一個 function，隨機給它一個 input。比如說輸入一個數字 1，然後，它就會 output 這棵樹; 輸入數字 2，就 output 另外一棵樹; 輸入數字 3，就 output 另外一棵。</p>
            <p>在這個 task 裡面，要找的是這個可以畫圖的 function，只有這一個 function 的 output，沒有這一個 function 的 input。</p>
            <p>只有一大堆的 image，但是不知道輸入怎麼樣的 code，才可以得到這些 image。</p>
            <p><mark>這一份講義主要著重於在 linear 的 Dimension Reduction上。</mark></p>
            <h2>Clustering</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_4.jpg" width="70%"></p>
            <h4>什麼是 clustering 呢?</h4>
            <p>clustering 就是有一大堆的 image，假設現在要做clustering，就是把它們分成一類、一類、一類的。</p>
            <p>把本來有些不同的 image，用同一個 class 來表示就可以做到化繁為簡這一件事情。</p>
            <p>到底應該要有幾個 cluster，就跟 neural network 要幾層一樣，是 empirical 地去決定。</p>
            <h3>K-means</h3>
            <p>在 clustering 的方法裡面，最常用的叫做 K-means。</p>
            <p>K-means 就是有一大堆的 data，它們都是 unlabeled的，x^1 一直到 x^N。</p>
            <h6>目標：要把它做成 K 個 cluster</h6>
            <ol start='' >
            <li>先找這些 cluster 的 center，每一個 cluster 都要先找一個 center，有 K 個 cluster，我們就需要 c^1 到 c^K 個 center。（初始的 center 可以從training data 裡面，隨機的找 K 個 object 出來）</li>
            <li>接下來，對所有在 training data 裡面的 x，重複以下的 3. 4. 步驟。</li>
            <li>決定每一個 object屬於 1 到 K 的哪一個 cluster（假設object, x^n跟第 i 個 cluster 的 center 最接近，那 x^n 就屬於 c^i）</li>
            <li>接下來，你update 你的 cluster。（假設要 update 第 i 個 cluster 的 center，就把所有屬於第 i 個 cluster 的 object通通拿出來，做平均得到第 i 個 cluster 的 新center。）</li>
            
            </ol>
            <p>然後，反覆的進行步驟2到4。</p>
            <h3>Hierarchical Agglomerative Clustering (HAC)</h3>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_5.jpg" width="70%"></p>
            <p>clustering 有另外一個方法叫做，Hierarchical Agglomerative Clustering (HAC)。</p>
            <p>這個方法，是先建一個 tree。</p>
            <p>假設你現在有 5 個 example：</p>
            <ul>
            <li>把這 5 個 example兩兩、兩兩去算他的相似度，然後挑最相似的那一個 pair 出來。（假設現在最相似的 pair，是第一個和第二個 example，那就把第一個 example 和第二個 example merge 起來。比如說，把它們平均起來，得到一個新的 vector。）</li>
            <li>接下來，只剩下 4 筆 data，把這 4 筆 data，再兩兩去算它的相似度，再把它們 merge 起來，把它們平均起來得到另外一筆 data。</li>
            <li>現在只剩三筆 data，再去兩兩算它的 singularity。（然後，黃色這一個和中間這一個最像，就再把它們平均起來）</li>
            <li>最後只剩紅色跟綠色，只好把它平均起來。</li>
            <li>就得到這個 tree 的 root，就根據這 5 筆 data，它們之間的相似度，建立出一個 tree structure。</li>
            
            </ul>
            <p>這個 tree structure 可以告訴我們哪些 example 是比較像的，比較早分支代表比較不像，因此可以根據想要分的cluster數目，對這個tree做拆解。</p>
            <h4>K-means VS HAC</h4>
            <p>HAC 跟 K-means最大的差別是如何決定cluster 的數目。</p>
            <ul>
            <li>在 K-means 裡面，要事先決定你 K 的 value 是多少。</li>
            <li>HAC不直接決定幾個 cluster，而決定你要在樹上切幾刀。</li>
            
            </ul>
            <h2>Dimension Reduction</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_6.jpg" width="70%"></p>
            <p>在做 cluster 的時候，就是以偏概全，因為每一個 object都必須要屬於某一個 cluster。</p>
            <p>如果你只是把你手上所有的 object分別 assign 到它屬於哪一個 cluster，這樣是以偏概全。</p>
            <p>應該要用一個 vector來表示 object，這個 vector裡面的每一個 dimension，代表了某一種特質、某一種 attribute。這件事情就叫做 Distributed 的 representation。</p>
            <p>如果原來的 object 是一個非常 high dimension 的東西，比如說 image。</p>
            <p><mark>它用它的attribute，把它用它的特質來描述，就會從比較高維的空間變成比較低維的空間，這一件事情就叫做 Dimension Reduction。</mark></p>
            <blockquote><p>實際舉例，其實只需要在 2D 的空間就可以描述這個 3D 的 information，根本不需要把這個問題放到 3D 來解，這樣是把問題複雜化。</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_2.jpg" width="70%"></p>
            </blockquote>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_3.jpg" width="70%"></p>
            <p>另一個比較具體的例子， MNIST</p>
            <p>在 MNIST 裡面，每一個 input 的 digit都是一個 image，都用 28*28 的 dimension來描述。</p>
            <p>但是，實際上多數 28*28 的 dimension 的 vector轉成一個 image看起來都不像是一個數字。所以在這個 28維 * 28維的空間裡面是 digit 的 vector，其實是很少的。所以，其實描述一個 digit，或許根本不需要用到 28*28 維。</p>
            <p>比如這邊有一堆 3，然後這一堆 3</p>
            <p>如果你是從 pixel 來看待它的話要用 28維 * 28維。然而，實際上這一些 3，只需要用一個維度，就可以來表示。因為這些 3 就只是說把原來的 3 放正，是中間這張 image右轉 10 度轉就變成它，右轉20度就變它，左轉10度變它，左轉20度就變它。</p>
            <p>所以，唯一需要記錄的事情只有今天這張 image，它是左轉多少度、右轉多少度，就可以知道說，它在 28 維的空間裡面應該長什麼樣子。</p>
            <h3>Dimension Reductiong實作</h3>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_7.jpg" width="70%"></p>
            <ul>
            <li>找一個 function，function 的 input 是一個 vector, x，它的 output 是另外一個 vector, z。</li>
            <li>因為是 Dimension Reduction，所以 output 的這個 vector, z，它的 dimension 要比 input 的 x 還要小。</li>
            
            </ul>
            <h4>Feature Selection</h4>
            <p>最簡單的方法是 Feature Selection，如果把 data 的分布拿出來看一下，本來在二維的平面，但是，你發現都集中在 x2 的 dimension 而已，所以，x1 這個 dimension 沒什麼用，把它拿掉，就等於是做到 Dimension Reduction 這件事。</p>
            <h4>Principle Component Analysis</h4>
            <p>另外一個常見的方法叫做Principle Component Analysis (PCA)。</p>
            <p>假設這個 function 是一個的 linear function，則 input, x 跟這個 output, z 之間的關係就是一個 linear 的 transform。也就是把這個 x 乘上一個 matrix, W，就能得到它的 output, z。</p>
            <p>根據一大堆的 x，我們要把這個 W 找出來。</p>
            <h2>PCA (Principle Component Analysis)</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_8.jpg" width="70%"></p>
            <p>PCA 要做的事情就是找這一個 W。</p>
            <ol start='' >
            <li><p>假設現在考慮一個 one dimensional 的 case，只要把 data project 到一維的空間上面，也就是 z 只是一個一維的 vector，也就是一個 scalar，那 W 其實就是一個一個 row ，用 w^1 來表示。</p>
            <ul>
            <li>把 x 跟 w 的第一個 row, w^1做 inner product，就能得到一個 scalar, z1。</li>
            <li>假設 w^1 的長度是 1，w^1 的 2-norm 是 1。</li>
            
            </ul>
            </li>
            <li><p>w 跟 x 是高維空間中的一個點，w^1 是高維空間中的一個 vector，那所謂的 z1 就是 x在 w^1 上面的投影，這個投影的值就是 w^1 跟 x 的 inner product。現在要做的事情就是把一堆 x透過 w^1 把它投影變成 z1。</p>
            </li>
            <li><p>則現在的目標是希望選一個 w^1經過 projection 以後，得到的這些 z1 的分布是越大越好（也就是說，我們不希望通過這個 projection 以後，所有的點通通擠在一起把本來 data point 和 data point 之間的奇異度拿掉了）。</p>
            </li>
            
            </ol>
            <p>所以，我們希望找一個 projection 的方向，它可以讓projection 後的 variance 越大越好。</p>
            <p>如果我們要用 equation 來表示它的話，就會說現在要去 maximize 的對象是 z1 的 variance，z1 的 variance 就是 summation over 所有的 z1 (z1 - z1\bar) 的平方，z1\bar 就是做 z1 的平均。</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_9.jpg" width="70%"></p>
            <p>再來，可能不只要投影到一維，比如說投影到一個二維的平面。</p>
            <ol start='' >
            <li>就把 x 跟另外一個 w^2，做 inner product，得到 z2。（這個 w^1 跟 w^2 的 transpose 排起來就是，W 的第一個 row 跟第二個 row）</li>
            <li>基本上，延續上一維的做法，首先令 w^2 它的 2-norm 是 1。</li>
            <li>接下來這個 z2 它的分佈也是越大越好。</li>
            
            </ol>
            <p>這裡有唯一的constraint 就是 w^1 跟 w^2 必須是 orthogonal的，否則就只是在找一樣的w。</p>
            <p>更多的維度就以此類推，但記得W的每個row之間必須是 orthogonal的。</p>
            <h2>Warning of Math</h2>
            <h6>以下是實際是的解法。</h6>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_11.jpg" width="70%"></p>
            <h6>前半部使用Lagrange multiplier，或是Gradient Descent (preferred)。</h6>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_12.jpg" width="70%"></p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_13.jpg" width="70%"></p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_14.jpg" width="70%"></p>
            <h6>最後Z的covariance會是diagonal的，也就代表整個PCA會做一個decorrelation，而Z的維度可以當作是一種新的feature。</h6>
            <h2>PCA — Another Point of View</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_16.jpg" width="70%"></p>
            <p>PCA第一次找出來的 w^1 是 covariance matrix，對應到最大的 eigenvalue 的 eigenvector; 然後，第二個找出來的 w^2 對應到第二大的 eigenvalue 的 eigenvector，以此類推....（上面就有一個很長的證明來說明這麼做的話，每一次投影的時候都可以讓 variance 最大）</p>
            <p>以下是另一個比較直觀的說明：</p>
            <p>假設現在考慮的是手寫數字。</p>
            <p>這些數字其實是由一些basic 的 component 所組成的，這些 basic 的 component 可能就代表筆劃（有斜的直線、橫的直線、比較長的直線、還有小圈、大圈等等...）這些 basic 的 component 把它加起來以後就可以得到一個數字。</p>
            <p>這些 basic 的 component（寫作 u^1, u^2, u^3 等等）其實就是一個一個的 vector，假設現在考慮的是 MNIST 的話，MNIST 的一張 image 是 28<em>28 pixel，那這些 component，其實也就是 28</em>28 維的 vector。</p>
            <p>把這些 vector 透過不同的線性組和加起來以後，所得到的總和 vector，就代表了一個 digit。</p>
            <p><mark>所以，每一張 image就是有一堆 component 的 linear combination，再加上它的平均所組成的。</mark></p>
            <p>因此可以用這些component 和 weight (c1, c2, c3 ...)來描述一張 image，如果component 的數目比 pixel 的數目少的話，那這個描述，通常會是比較有效的</p>
            <p>（舉例來說，7 是一倍的 u^1，一倍的 u^3，一倍的 u^5 所組合而成所以，7 你就可以說它是一個 vector，它第一維、第三維、第五維是 1）</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_17.jpg" width="70%"></p>
            <p>在 PCA 裡面，我們要找一個 matrix, W，使原來的 vector, x 乘上 W 以後，得到 Dimension Reduction 以後的結果，z。（可以把 W 的每一個 row 都寫出來 w1, w2 一直到 wK，都是 covariance matrix 的 eigenvector。）</p>
            <p>事實上，如果要解這個圖上的式子（透過把平均值移項得到的式子），找出 u^1, u^2 到 u^K。這個 w^1 到 w^K，就是由 PCA 找出來的這個解</p>
            <p>也就是可以讓上面這一個式子最小化，就可以找到讓這個 Reconstruction error 最小的 u^1 到 u^K。</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_18.jpg" width="70%"></p>
            <p>實際以data為例子，現在在 database 裡面有一大堆的 x，如果把所有的 data都用剪掉平均值後的線性組合來表示的話。</p>
            <p>就能得到一個 matrix，在這個 matrix 的橫軸，column 的數目就是 data 的數目</p>
            <p>現在要做的事情就是用這一個 matrix 去乘上另一個 matrix，希望越接近這左邊的matrix 越好</p>
            <p>所以要 minimize 這兩個相乘以後得到的 matrix跟左邊這個 matrix 之間的差距(Reconstruction error)。</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_19.jpg" width="70%"></p>
            <h6>那怎麼解這個問題呢？假如你有修過大一線代的話，就知道這個問題是怎麼解的，連結是李老師教線代的時候的投影片。</h6>
            <p>所以，根據 PCA，所找出來的那些 w，找出來的 Dimension Reduction 的 transform，就是在 minimize Reconstruction error。</p>
            <p>PCA 裡面你得到的那些 W，其實就是 component。</p>
            <h3>NN vs PCA</h3>
            <p>&lt;<img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_23.jpg" width="70%"></p>
            <p><mark>用NN去解linear combination來minimize reconstruction error跟PCA做出來是會不一樣的。</mark></p>
            <p>因為PCA 解出來這些 W 它們是 orthonormal 的，它們是垂直的。</p>
            <p>今天如果用 neural network去Gradient Descent 硬解，並不可能讓 Reconstruction error 比 PCA 找出來的還要小。所以，如果是在 linear 的情況下，直接用 PCA 來找這個 w，是比較快的。</p>
            <p>用 neural network 的好處就是，它可以是 deep 的，可以改成很多的 hidden layer，這涉及到之後會講的 Deep Autoencoder。</p>
            <h2>PCA的缺點</h2>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_30.jpg" width="70%"></p>
            <ul>
            <li>由於是unsupervised的，data太複雜或過多的話，PCA 可能會讓原本有區分的兩個cluster降維投影到單一平面後擠在一起。必須要用LDA(linear discriminant analysis)來引入labelled data，才能解決。</li>
            <li>PCA無法做到non-linear的transformation （比如說前面的3D捲軸例子），畢竟它是linear 的dimension reduction。</li>
            
            </ul>
            <h2>PCA — Examples</h2>
            <p>以下是各種PCA的例子，以及他們所找出的pricipal components。</p>
            <h3>寶可夢</h3>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_24.jpg" width="70%"></p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_25.jpg" width="70%"></p>
            <p>&nbsp;</p>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_26.jpg" width="70%"></p>
            <p>&nbsp;</p>
            <h3>MNIST 手寫數字</h3>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_28.jpg" width="70%"></p>
            <h3>人臉</h3>
            <p><img src="http://ai.ntu.edu.tw/aho/JPG/Unsupervised_Learning-Principle_Component_Analysis/Unsupervised_Learning-Principle_Component_Analysis_29.jpg" width="70%"></p>
            <p>&nbsp;</p>
            <p>臺灣大學人工智慧中心
            科技部人工智慧技術暨全幅健康照護聯合研究中心
            <a href='http://aintu.tw' target='_blank' class='url'>http://aintu.tw</a></p>
          </div>

            
            
            <div class="col-lg-3">
              
            </div>
          </div>

  </section><! --/wrapper -->
    </section><!-- /MAIN CONTENT -->
    <!--main content end-->
    <div id="bottom"></div>
      </div>

      <div class="col mr-4 mt-4">
          <div class="list-group" style="position: sticky; position: -webkit-sticky; top: 2rem;">
            <a class="list-group-item list-group-item-action" href="#head"><i class="fa fa-arrow-up mr-2" style=""></i>講義開頭</a>
            <a class="list-group-item list-group-item-action" href="#bottom"><i class="fa fa-arrow-down mr-2" style=""></i>講義末尾</a>
            <a class="list-group-item list-group-item-action" href="../index.html"><i class="fa fa-home mr-2" style=""></i>選單主頁</a>
          </div>
       </div>
    </div>
      </div>
   </body>
        <div id="head"></div>
</html>