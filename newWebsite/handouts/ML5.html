<!DOCTYPE html>
<html>
   <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <meta charset="utf-8">
      <title>AINTU 講義</title>
      <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.1/css/bootstrap.min.css">
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
   </head>

   <body>
        <div id="head"></div>
       <!-- Navigation -->
      <nav class="navbar navbar-expand-lg navbar-light bg-light static-top primary">
          <a class="navbar-brand ml-4" href="../index.html">
                <img src="../logo1.png" width=326.83rem height= 60rem alt="">
              </a>
          <div class="collapse navbar-collapse mr-4" id="navbarSupportedContent">
            <ul class="navbar-nav ml-auto mr-sm-4">
              <li class="nav-item">
                <a class="nav-link active" href="../index.html">機器學習講義 <span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item">
                <a class="nav-link" href="../datasource.html">機器學習資料源</a>
              </li>
            </ul>
          </div>
      </nav>
      
      <div class="container">
          <p class="mt-4">- 版權屬於講師所有 勿任意轉載、散佈、更改 引用請知會版權擁有權人 -</p>
 
                

    <div class="row">
      
         
      <div class="col-9 ml-4 mt-4" style="/*height: 40rem; overflow-y: scroll;*/">
        <div id="head"></div>
         <!--main content start-->
      <section id="main-content">
        <section class="wrapper site-min-height">
          <div class="row mt">
            <div class="col-lg-9">
              <h1>ML Lecture 5: Logistic Regression</h1>
<blockquote><p>臺灣大學人工智慧中心 科技部人工智慧技術暨全幅健康照護聯合研究中心 <a href='http://ai.ntu.edu.tw/'>http://ai.ntu.edu.tw</a></p>
</blockquote>
<h3>Step 1. Function Set </h3>
<ul>
<li><p><strong>Posterior Probability：</strong></p>
<blockquote><p>$P_{w,b}(C_1|x) = σ(z)$<strong>，由 $z$ 代入 $sigmod function$ 後得</strong>
$z=w*x+b$ ，$z$ <strong>由 $w$ 和 $b$ 所控制產生</strong></p>
</blockquote>
<p><strong>==&gt;</strong> <strong>所有 $w$ 和 $b$ 可產生的 $function$ 所成的集合</strong>，就是一個 <u><strong>function set</strong></u></p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_2.jpg" width="70%"></p>
<ul>
<li><p>以<strong>「圖像化」</strong>表示「Logistic Regression」 這件事</p>
<blockquote><p>Input $x_1$ 到 $x_I$ 分別乘上 weight $w_1$ 到 $w_I$ (內積)，再加上 bias, $b$，即為 $z$</p>
<p>通過 $sigmoid function$，output 的值是 <u><strong>posterior probability</strong></u></p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_3.jpg" width="70%"></p>
<ul>
<li><p><strong>比較</strong>（Output 的值）</p>
<ul>
<li>Logistic Regression：有通過 $sigmoid function$，output 的值介於 <u><strong><em>0~1</em></strong></u></li>
<li>Linear Regression：單純將 $feature*w+b$，output 可以是<strong><em><u>任何值</u></em></strong></li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_14.jpg" width="70%"></p>
<hr />
<h3>Step 2. Goodness of a Function</h3>
<ul>
<li><p><strong>目標</strong>：找出可以<strong>最大化產生這 N 筆 training data 機率</strong>的  $w^*$、$b^*$</p>
<blockquote><p>$w^*$、$b^*$ = $arg$ $max_{w, b} L(w, b)$</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_4.jpg" width="70%"></p>
<ul>
<li><p><strong>轉化目標</strong>：找出可以<strong>最小化 $-lnL(w,b)$</strong> 的 $w^*$、$b^*$  (原因：簡化計算)</p>
<blockquote><p>$w^*$、$b^*$ = $arg$ $max_{w, b} L(w, b)$ = $arg$ $min_{w, b}-lnL(w, b)$</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_5.jpg" width="70%"></p>
<ul>
<li><p><strong>計算</strong></p>
<ul>
<li>左式、右式同取 $-ln$ ，相乘變成相加</li>
<li>符號轉換：$\hat{y}$  的值代表說，現在 $x$ 屬於哪一個 $class$</li>
<li>就可以寫成 $Σ-\hat{y}^nlnf_{w,b}(x^n)+(1-\hat{y^n})ln(1-f_{w,b}(x^n))$ 
$Σ$ 後的式子，其實是兩個 Bernouli distribution 的 Cross Entropy </li>
<li>$H(p, q) = -Σp(x)ln(q(x))$</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_6.jpg" width="70%"></p>
<ul>
<li><p><strong>比較</strong>（Minimize 的對象）</p>
<ul>
<li>Logistic Regression：(function 的 output 與 targer) 的 <strong><u>cross entropy</u></strong></li>
<li>Linear Regression： (function 的 output 減 targer) 的 <strong><u>square error</u></strong></li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_15.jpg" width="70%"></p>
<hr />
<h3>Step 3. Find the best function</h3>
<ul>
<li>計算 $-lnL(w,b)$ 對 $w_i$ 的微分</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_8.jpg" width="70%"></p>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_9.jpg" width="70%"></p>
<ul>
<li><p>計算結果：</p>
<blockquote><p>$Σ_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$</p>
<p>$w_i^n$ ← $w_i-ηΣ_n-(\hat{y}^n-f_{w,b}(x^n))x_i^n$</p>
</blockquote>
<p>下面的式子代表 w 的 update 取決於三件事情</p>
<ol start='' >
<li>$η$（Learning rate)：自己設定的</li>
<li>$x_i$ ：來自於 data</li>
<li>$\hat{y}^n$：目標、$f_{w,b}(x^n)$ 現在 model 的 output
$\hat{y}^n-f_{w,b}(x^n)$ 代表現在 function 的 output 跟理想目標的差距有多大，越遠，update 量就越大</li>

</ol>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_10.jpg" width="70%"></p>
<ul>
<li><p><strong>比較</strong>（Logistic Regression 跟 Linear Regression 做 Gradient Descent 時參數 update 的方式）</p>
<ul>
<li><p>相同：update 的式子</p>
</li>
<li><p>不同：$\hat{y}^n$</p>
<blockquote><p>Logistic Regression 的 $\hat{y}^n$ 一定是 0 或 1</p>
<p>Linear Regression 的 $\hat{y}^n$ 可以是任何實數</p>
</blockquote>
</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_16.jpg" width="70%"></p>
<hr />
<h3>Discriminative vs. Generative</h3>
<ul>
<li><p>定義</p>
</li>
<li><p><strong>Discriminative</strong> 的方法：如 Logistic Regression
<strong>Generative</strong> 的方法：如使用 Gaussian 描述 Posterior probability</p>
</li>
<li><p><strong>相同</strong>：function set、model（皆為 $P(C_1|x)=σ(w·x+b)$）</p>
<p><strong>不同</strong>：兩者對 probability distribution 做不同的假設</p>
<ul>
<li>Discriminative：沒有做任何假設</li>
<li>Generative：會假設機率分佈是 Gaussian, Bernoulli, Naive Bayes... 等等</li>

</ul>
</li>
<li><p>例子分別應用兩者的結果</p>
<ul>
<li>防禦力&amp;特殊防禦力的例子，藍色是水系的寶可夢，紅色是一般系的寶可夢，都使用 7 個 feature</li>
<li>Generative model 可獲得 73% 的正確率
Discriminative model 可獲得 79% 的正確率<img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_18.jpg" width="70%"></li>

</ul>
</li>
<li><p>討論：<strong>何時 Generative model 的表現較比 Discriminative model 好</strong></p>
<ul>
<li><p>資料量大小：</p>
<ul>
<li>Discriminative model 因爲不做任何假設，故 performance 受資料影響很大
Generative model 會做假設（如同自行腦補），資料量很少時，較有優勢</li>
<li>資料量小：Discriminative model 誤差較大，Generative model 表現可能較好
資料量大：Discriminative model 誤差較小，表現較有可能優於 Generative model</li>

</ul>
</li>
<li><p>Noise 存在：</p>
<ul>
<li>資料有 noise 時，因為 label 本身就有些問題，故一些假設可能可以把有問題的 data 忽略掉
Generative model 的表現可能較 Discriminative 好</li>

</ul>
</li>
<li><p>分割資料來源：</p>
<ul>
<li><p>Discriminative model 直接假設一個 posterior probability
Generative model 可將 formulation 拆成 <strong>prior</strong> 跟 <strong>class-dependent</strong> 的 probability 兩項
而這兩項可以來自<strong>不同的資料來源</strong></p>
</li>
<li><p>舉例：
語音辨識使用 NN，是 discriminative 的方法；
但是整個語音辨識系統，是 generative 的 system。</p>
<blockquote><p>prior 的部分使用文字的 data 處理，class-dependent 的部分，需要聲音和文字的配合。</p>
</blockquote>
</li>

</ul>
</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_22.jpg" width="70%"></p>
<hr />
<h3>Process of Multi-class Classification</h3>
<ul>
<li><p>定義</p>
<ul>
<li>三個類別：$C_1, C_2, C_3$</li>
<li>每個類別相對應的 weight, bias​：$w^1, w^2, w^3$ (vector)，$b_1, b_2, b_3$ (scalar)</li>
<li>要分類的對象：$x$</li>

</ul>
</li>
<li><p>步驟</p>
<ol start='' >
<li><p>將 x 乘上 weight 加上 bias 得到 z</p>
<blockquote><p>$z_1=w^1*x+b_1$ 			ex. $z_1=3$</p>
<p>$z_2=w^2*x+b_2$			 ex. $z_2=1$</p>
<p>$z_3=w^3*x+b_3$		 	ex. $z_3=-3$</p>
</blockquote>
</li>
<li><p>將 z 丟入 Softmax function</p>
<blockquote><p>取 exponential 得 $e^{z1}, e^{z2}, e^{z3}$，相加得 total sum = $\Sigma_{j=1}^3e^{z_j}$</p>
</blockquote>
<blockquote><p>各項除以 total sum (做 normalization )，得 output, $y=(y_1, y_2, y_3)$</p>
<p>$y_1=s^{z_1}/\Sigma_{j=1}^3e^{z_j}$			ex. 計算得 $y_1=0.88$</p>
<p>$y_2=s^{z_2}/\Sigma_{j=1}^3e^{z_j}$			ex. 計算得 $y_2=0.12$</p>
<p>$y_3=s^{z_3}/\Sigma_{j=1}^3e^{z_j}$			ex. 計算得 $y_3=0$</p>
</blockquote>
</li>

</ol>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_23.jpg" width="70%"></p>
<ol start='3' >
<li><p>Minimize Cross Entropy</p>
<ul>
<li><p>計算 $y$ 跟目標函數 $\hat{y}$ 之間的 cross entropy：$-\Sigma_{i=1}^3\hat{y}lny_i$</p>
<blockquote><p>$\hat{y}_1=\left[\begin{matrix}1\\0\\0\end{matrix}\right]\hat{y}_2=\left[\begin{matrix}0\\1\\0\end{matrix}\right]\hat{y}_3=\left[\begin{matrix}0\\0\\1\end{matrix}\right]$</p>
</blockquote>
</li>
<li><p>列出 maximum likelihood 的 function，經過整理即可得 minimize cross entropy</p>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_24.jpg" width="70%"></p>
</li>

</ol>
</li>
<li><p>Softmax </p>
<ul>
<li><p>對最大值做強化（取 exponential 時使大值跟小值間的差距更大）</p>
</li>
<li><p>經過 Softmax function 後，output 值 (y) 介於 0~1 之間</p>
</li>
<li><p>$y_i$ 即為第 i 個 class ($z_i$)的 posterior probability</p>
<blockquote><p>$y_1=0.88$ 代表 $x$ 屬於 class1 的機率是 88%
$y_2=0.12$ 代表 $x$ 屬於 class2 的機率是 12%</p>
<p>$y_3=0$ 代表 $x$ 屬於 class3 的機率趨近於 0 </p>
</blockquote>
</li>
<li><p>Softmax 中為何使用 exponential</p>
<blockquote><p>可以參考 Bishop 教科書的推導，也可搜尋 &quot;Maximum Entropy&quot; 獲得更多資訊</p>
</blockquote>
</li>

</ul>
</li>

</ul>
<hr />
<h3>Limitation of Logistic Regression</h3>
<ul>
<li><p>Logistic Regression 有時無法直接對 data 做分類，
因為兩個 class 之間的 boundary 是一直線，無法好好地將資料分割</p>
</li>
<li><p>舉例：</p>
<ul>
<li><p>假設，如左下表格</p>
<blockquote><p>class1 有兩筆 data：(0,1)、(1,0)
class2 有兩筆 data：(0,0)、(1,1)</p>
</blockquote>
</li>
<li><p>如右下圖，我們無法以 Logistic Regression 好好地將紅色、藍色分成兩邊
因爲其 boundary 是一直線</p>
</li>

</ul>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_25.jpg" width="70%"></p>
<ul>
<li><p>解決：<strong>Feature Transformation</strong></p>
<ul>
<li><p>將原來的 $x_1, x_2$ 做一些轉化後，找到一個較好的 feature space，轉化成 $x_{1}^{'},x_{2}^{'}$
讓 Logistic Regression 可以處理</p>
</li>
<li><p>舉例：</p>
<blockquote><p>定義 $x_{1}^{'}$ 是某一點到 (0,0) 的距離，$x_{2}^{'}$ 是某一點到 (1,1) 的距離
轉換後，如右下圖，紅色的點重疊在一起，而 Logistic Regression 可找到 boundary 分開</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_26.jpg" width="70%"></p>
</li>
<li><p>如何讓機器自己產生 Feature Transformation </p>
<ul>
<li><p>將許多個 Logistic Regression <strong>串接</strong>起來，如下圖</p>
<blockquote><p>前面兩個 Logistic Regression (藍色, 綠色)就是在做 Feature Transformation
轉換後，再由紅色的 Logistic Regression 做分類</p>
</blockquote>
</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_27.jpg" width="70%"></p>
</li>
<li><p><strong>Neuron &amp; Neural Network</strong></p>
<ul>
<li>Neuron：我們將每一個 Logistic Regression 稱作 Neuron</li>
<li>Neural network：這些 Logistic Regression 串接起來就稱作 Neural network (類神經網路)</li>
<li>This is Deep Learning!</li>

</ul>
<p><img src="http://ai.ntu.edu.tw/aho/JPG/Classification-Logistic_Regression/Classification-Logistic_Regression_30.jpg" width="70%"></p>
</li>

</ul>
<p>&nbsp;</p>
              
            </div>
            <div class="col-lg-3">
              
            </div>
          </div>

  </section><! --/wrapper -->
    </section><!-- /MAIN CONTENT -->
    <!--main content end-->
    <div id="bottom"></div>
      </div>

      <div class="col mr-4 mt-4">
        <div class="list-group" style="position: sticky; position: -webkit-sticky; top: 2rem;">
          <a class="list-group-item list-group-item-action" href="#head"><i class="fa fa-arrow-up mr-2" style=""></i>講義開頭</a>
          <a class="list-group-item list-group-item-action" href="#bottom"><i class="fa fa-arrow-down mr-2" style=""></i>講義末尾</a>
          <a class="list-group-item list-group-item-action" href="../index.html"><i class="fa fa-home mr-2" style=""></i>選單主頁</a>
        </div>
     </div>
    </div>
      </div>
   </body>
        <div id="head"></div>
</html>